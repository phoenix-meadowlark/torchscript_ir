FunnelForMaskedLM(
  (funnel): FunnelModel(
    (embeddings): FunnelEmbeddings(
      (word_embeddings): Embedding(30522, 768)
      (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): FunnelEncoder(
      (attention_structure): FunnelAttentionStructure(
        (sin_dropout): Dropout(p=0.1, inplace=False)
        (cos_dropout): Dropout(p=0.1, inplace=False)
      )
      (blocks): ModuleList(
        (0): ModuleList(
          (0): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (1): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (2): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (3): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
        )
        (1): ModuleList(
          (0): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (1): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (2): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (3): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
        )
        (2): ModuleList(
          (0): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (1): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (2): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
          (3): FunnelLayer(
            (attention): FunnelRelMultiheadAttention(
              (hidden_dropout): Dropout(p=0.1, inplace=False)
              (attention_dropout): Dropout(p=0.1, inplace=False)
              (q_head): Linear(in_features=768, out_features=768, bias=False)
              (k_head): Linear(in_features=768, out_features=768, bias=True)
              (v_head): Linear(in_features=768, out_features=768, bias=True)
              (post_proj): Linear(in_features=768, out_features=768, bias=True)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
            (ffn): FunnelPositionwiseFFN(
              (linear_1): Linear(in_features=768, out_features=3072, bias=True)
              (activation_dropout): Dropout(p=0.0, inplace=False)
              (linear_2): Linear(in_features=3072, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
            )
          )
        )
      )
    )
    (decoder): FunnelDecoder(
      (attention_structure): FunnelAttentionStructure(
        (sin_dropout): Dropout(p=0.1, inplace=False)
        (cos_dropout): Dropout(p=0.1, inplace=False)
      )
      (layers): ModuleList(
        (0): FunnelLayer(
          (attention): FunnelRelMultiheadAttention(
            (hidden_dropout): Dropout(p=0.1, inplace=False)
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (q_head): Linear(in_features=768, out_features=768, bias=False)
            (k_head): Linear(in_features=768, out_features=768, bias=True)
            (v_head): Linear(in_features=768, out_features=768, bias=True)
            (post_proj): Linear(in_features=768, out_features=768, bias=True)
            (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
          )
          (ffn): FunnelPositionwiseFFN(
            (linear_1): Linear(in_features=768, out_features=3072, bias=True)
            (activation_dropout): Dropout(p=0.0, inplace=False)
            (linear_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
          )
        )
        (1): FunnelLayer(
          (attention): FunnelRelMultiheadAttention(
            (hidden_dropout): Dropout(p=0.1, inplace=False)
            (attention_dropout): Dropout(p=0.1, inplace=False)
            (q_head): Linear(in_features=768, out_features=768, bias=False)
            (k_head): Linear(in_features=768, out_features=768, bias=True)
            (v_head): Linear(in_features=768, out_features=768, bias=True)
            (post_proj): Linear(in_features=768, out_features=768, bias=True)
            (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
          )
          (ffn): FunnelPositionwiseFFN(
            (linear_1): Linear(in_features=768, out_features=3072, bias=True)
            (activation_dropout): Dropout(p=0.0, inplace=False)
            (linear_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)
          )
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=30522, bias=True)
)

FunnelForMaskedLM._actual_script_module
FunnelForMaskedLM.forward
  graph(%self.1 : __torch__.transformers.modeling_funnel.FunnelForMaskedLM,
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %7303 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lm_head"](%self.1)
    %7300 : __torch__.transformers.modeling_funnel.FunnelModel = prim::GetAttr[name="funnel"](%self.1)
    %7537 : Tensor = prim::CallMethod[name="forward"](%7300, %input_ids, %attention_mask.1)
    %7538 : Tensor = prim::CallMethod[name="forward"](%7303, %7537)
    %6544 : (Float(17:396786, 13:30522, 30522:1)) = prim::TupleConstruct(%7538)
    return (%6544)

FunnelForMaskedLM.funnel
FunnelModel._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_funnel.FunnelModel,
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %1 : __torch__.transformers.modeling_funnel.FunnelDecoder = prim::GetAttr[name="decoder"](%self.2)
    %2 : __torch__.transformers.modeling_funnel.FunnelEncoder = prim::GetAttr[name="encoder"](%self.2)
    %3 : __torch__.transformers.modeling_funnel.FunnelEmbeddings = prim::GetAttr[name="embeddings"](%self.2)
    %4 : int = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
    %5 : int = aten::size(%input_ids, %4), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
    %7 : Long() = prim::NumToTensor(%5), scope: __module.funnel
    %8 : int = aten::Int(%7), scope: __module.funnel
    %9 : int = prim::Constant[value=1](), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
    %10 : int = aten::size(%input_ids, %9), scope: __module.funnel # transformers/modeling_funnel.py:1009:0
    %11 : Long() = prim::NumToTensor(%10), scope: __module.funnel
    %12 : int = aten::Int(%11), scope: __module.funnel
    %13 : int[] = prim::ListConstruct(%8, %12), scope: __module.funnel
    %14 : int = prim::Constant[value=4](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
    %15 : int = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
    %16 : Device = prim::Constant[value="cpu"](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
    %17 : bool = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
    %token_type_ids : Long(17:13, 13:1) = aten::zeros(%13, %14, %15, %16, %17), scope: __module.funnel # transformers/modeling_funnel.py:1020:0
    %28 : Tensor = prim::CallMethod[name="forward"](%3, %input_ids)
    %29 : (Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %attention_mask.1, %28, %token_type_ids)
    %22 : Float(17:9984, 13:768, 768:1), %23 : Float(17:9984, 13:768, 768:1), %24 : Float(17:9984, 13:768, 768:1), %25 : Float(17:3072, 4:768, 768:1), %26 : Float(17:9984, 13:768, 768:1) = prim::TupleUnpack(%29)
    %30 : Tensor = prim::CallMethod[name="forward"](%1, %22, %23, %24, %25, %26, %token_type_ids, %attention_mask.1)
    return (%30)

FunnelForMaskedLM.lm_head
Linear._actual_script_module
  graph(%self : __torch__.torch.nn.modules.linear.Linear,
        %5 : Float(17:9984, 13:768, 768:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self)
    %2 : Tensor = prim::GetAttr[name="weight"](%self)
    %3 : Float(768:1, 30522:768) = aten::t(%2), scope: __module.lm_head # torch/nn/functional.py:1676:0
    %output : Float(17:396786, 13:30522, 30522:1) = aten::matmul(%5, %3), scope: __module.lm_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.lm_head # torch/nn/functional.py:1678:0
    %7 : Float(17:396786, 13:30522, 30522:1) = aten::add_(%output, %1, %6), scope: __module.lm_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelModel.decoder
FunnelDecoder._actual_script_module
  graph(%self.190 : __torch__.transformers.modeling_funnel.FunnelDecoder,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(17:9984, 13:768, 768:1),
        %3 : Float(17:9984, 13:768, 768:1),
        %4 : Float(17:3072, 4:768, 768:1),
        %5 : Float(17:9984, 13:768, 768:1),
        %token_type_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %8 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.190)
    %9 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="1"](%8)
    %10 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layers"](%self.190)
    %11 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="0"](%10)
    %12 : __torch__.transformers.modeling_funnel.FunnelAttentionStructure = prim::GetAttr[name="attention_structure"](%self.190)
    %13 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="cos_dropout"](%12)
    %14 : __torch__.transformers.modeling_funnel.FunnelAttentionStructure = prim::GetAttr[name="attention_structure"](%self.190)
    %15 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="sin_dropout"](%14)
    %19 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:705:0
    %20 : int = aten::size(%2, %19), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:705:0
    %target_len : Long() = prim::NumToTensor(%20), scope: __module.funnel/__module.funnel.decoder
    %25 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %26 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %27 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %28 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %29 : Float(17:3072, 4:768, 768:1) = aten::slice(%4, %25, %26, %27, %28), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %30 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %31 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %32 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %33 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %cls : Float(17:3072, 1:768, 768:1) = aten::slice(%29, %30, %31, %32, %33), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:672:0
    %35 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %36 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %37 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %38 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %39 : Float(17:3072, 4:768, 768:1) = aten::slice(%4, %35, %36, %37, %38), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %40 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %41 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %42 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %43 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %x.14 : Float(17:3072, 3:768, 768:1) = aten::slice(%39, %40, %41, %42, %43), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:673:0
    %45 : int = prim::Constant[value=4](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:674:0
    %46 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:674:0
    %input.116 : Float(17:9216, 12:768, 768:1) = aten::repeat_interleave(%x.14, %45, %46), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:674:0
    %48 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %49 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %50 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %51 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %52 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %53 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %54 : int[] = prim::ListConstruct(%48, %49, %50, %51, %52, %53), scope: __module.funnel/__module.funnel.decoder
    %55 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %output.61 : Float(17:11520, 15:768, 768:1) = aten::constant_pad_nd(%input.116, %54, %55), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %57 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %58 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %59 : Long() = aten::sub(%target_len, %57, %58), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %60 : int = aten::Int(%59), scope: __module.funnel/__module.funnel.decoder
    %61 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %62 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %63 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %65 : Float(17:11520, 15:768, 768:1) = aten::slice(%output.61, %61, %62, %63, %64), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %66 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %67 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %68 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %output.62 : Float(17:11520, 12:768, 768:1) = aten::slice(%65, %66, %67, %60, %68), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:678:0
    %70 : Tensor[] = prim::ListConstruct(%cls, %output.62), scope: __module.funnel/__module.funnel.decoder
    %71 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:679:0
    %upsampled_hidden : Float(17:9984, 13:768, 768:1) = aten::cat(%70, %71), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:679:0
    %73 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:710:0
    %input_embeds : Float(17:9984, 13:768, 768:1) = aten::add(%upsampled_hidden, %5, %73), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:710:0
    %75 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:195:0
    %76 : int = aten::size(%input_embeds, %75), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:195:0
    %seq_len.38 : Long() = prim::NumToTensor(%76), scope: __module.funnel/__module.funnel.decoder
    %78 : Scalar = aten::ScalarImplicit(%seq_len.38), scope: __module.funnel/__module.funnel.decoder
    %79 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
    %80 : int = prim::Constant[value=384](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
    %81 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
    %82 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
    %83 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
    %84 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
    %85 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
    %freq_seq : Float(384:1) = aten::arange(%79, %80, %81, %82, %83, %84, %85), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:247:0
    %87 : Long() = prim::Constant[value={384}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:248:0
    %88 : Float(384:1) = aten::div(%freq_seq, %87), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:248:0
    %89 : Float() = prim::Constant[value={10000}](), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
    %90 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
    %91 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
    %92 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
    %93 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
    %94 : None = prim::Constant(), scope: __module.funnel/__module.funnel.decoder
    %95 : Float() = aten::to(%89, %90, %91, %92, %93, %94), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
    %96 : Float() = aten::detach(%95), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
    %97 : Float(384:1) = aten::pow(%96, %88), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:420:0
    %98 : Float(384:1) = aten::reciprocal(%97), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
    %99 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
    %inv_freq : Float(384:1) = aten::mul(%98, %99), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:400:0
    %101 : Long() = aten::neg(%seq_len.38), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %102 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %103 : Long() = aten::mul(%101, %102), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %104 : Scalar = aten::ScalarImplicit(%103), scope: __module.funnel/__module.funnel.decoder
    %105 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %106 : Long() = aten::mul(%seq_len.38, %105), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %107 : Scalar = aten::ScalarImplicit(%106), scope: __module.funnel/__module.funnel.decoder
    %108 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %109 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %110 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %111 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %112 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %rel_pos_id : Float(52:1) = aten::arange(%104, %107, %108, %109, %110, %111, %112), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:250:0
    %114 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:251:0
    %zero_offset : Long() = aten::mul(%seq_len.38, %114), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:251:0
    %116 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %117 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %118 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %119 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %120 : Float(52:1) = aten::slice(%rel_pos_id, %116, %117, %118, %119), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %122 : Float(52:1, 1:1) = aten::unsqueeze(%120, %121), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %123 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %124 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq, %123), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %sinusoid : Float(52:384, 384:1) = aten::mul(%122, %124), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:252:0
    %input.117 : Float(52:384, 384:1) = aten::sin(%sinusoid), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:253:0
    %513 : Tensor = prim::CallMethod[name="forward"](%15, %input.117)
    %input.118 : Float(52:384, 384:1) = aten::cos(%sinusoid), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:254:0
    %514 : Tensor = prim::CallMethod[name="forward"](%13, %input.118)
    %130 : Tensor[] = prim::ListConstruct(%513, %514), scope: __module.funnel/__module.funnel.decoder
    %131 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:255:0
    %pos_embed : Float(52:768, 768:1) = aten::cat(%130, %131), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:255:0
    %133 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
    %135 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
    %136 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
    %137 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
    %138 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
    %pos : Float(13:1) = aten::arange(%133, %78, %134, %135, %136, %137, %138), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:257:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
    %141 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
    %142 : Float() = aten::select(%pos, %140, %141), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
    %143 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
    %145 : Float() = aten::select(%pos, %143, %144), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
    %146 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
    %ref_point.6 : Float() = aten::sub(%142, %145, %146), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:313:0
    %151 : Long() = prim::Constant[value={13}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:315:0
    %152 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:315:0
    %max_dist.6 : Float() = aten::add(%ref_point.6, %151, %152), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:315:0
    %154 : Scalar = aten::ScalarImplicit(%max_dist.6), scope: __module.funnel/__module.funnel.decoder
    %155 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
    %156 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
    %157 : Float() = aten::select(%pos, %155, %156), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
    %158 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
    %159 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
    %160 : Float() = aten::select(%pos, %158, %159), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
    %161 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
    %min_dist.6 : Float() = aten::sub(%157, %160, %161), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:316:0
    %163 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %164 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %165 : Float() = aten::sub(%min_dist.6, %163, %164), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %166 : Scalar = aten::ScalarImplicit(%165), scope: __module.funnel/__module.funnel.decoder
    %167 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %168 : int = prim::Constant[value=4](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %169 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %170 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %171 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %rel_pos.16 : Long(26:1) = aten::arange(%154, %166, %167, %168, %169, %170, %171), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:318:0
    %173 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %174 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %175 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %176 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %177 : Long(26:1) = aten::slice(%rel_pos.16, %173, %174, %175, %176), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %178 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %179 : Long(26:1, 1:1) = aten::unsqueeze(%177, %178), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %180 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %rel_pos.17 : Long(26:1, 1:1) = aten::add(%179, %zero_offset, %180), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:284:0
    %182 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
    %183 : int = aten::size(%rel_pos.17, %182), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
    %184 : Long() = prim::NumToTensor(%183), scope: __module.funnel/__module.funnel.decoder
    %185 : int = aten::Int(%184), scope: __module.funnel/__module.funnel.decoder
    %186 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
    %187 : int[] = prim::ListConstruct(%185, %186), scope: __module.funnel/__module.funnel.decoder
    %188 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
    %rel_pos.18 : Long(26:1, 768:0) = aten::expand(%rel_pos.17, %187, %188), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:285:0
    %190 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:286:0
    %191 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:286:0
    %192 : Float(26:768, 768:1) = aten::gather(%pos_embed, %190, %rel_pos.18, %191), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:286:0
    %447 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %448 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %449 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %450 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %451 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %447, %448, %449, %450), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %452 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %453 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %454 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %455 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %456 : Long(17:13, 13:1) = aten::slice(%451, %452, %453, %454, %455), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %457 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %458 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%456, %457), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %459 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %460 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %461 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %462 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %463 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %459, %460, %461, %462), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %464 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %465 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%463, %464), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:207:0
    %token_type_mat.19 : Bool(17:169, 13:13, 13:1) = aten::eq(%458, %465), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
    %467 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
    %cls_ids : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %467), scope: __module.funnel/__module.funnel.decoder # torch/tensor.py:22:0
    %469 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %470 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %471 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %472 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %473 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %469, %470, %471, %472), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %474 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %475 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %476 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %477 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %478 : Bool(17:13, 13:1) = aten::slice(%473, %474, %475, %476, %477), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %479 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %480 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%478, %479), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %481 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %482 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %483 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %484 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %485 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %481, %482, %483, %484), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %486 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %487 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%485, %486), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %cls_mat : Bool(17:169, 13:13, 13:1) = aten::__or__(%480, %487), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:210:0
    %token_type_mat.20 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat, %token_type_mat.19), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:211:0
    %490 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %491 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %492 : Long() = aten::sub(%seq_len.38, %490, %491), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %493 : int = aten::Int(%492), scope: __module.funnel/__module.funnel.decoder
    %494 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %495 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %496 : Long() = aten::sub(%seq_len.38, %494, %495), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %497 : int = aten::Int(%496), scope: __module.funnel/__module.funnel.decoder
    %498 : int[] = prim::ListConstruct(%493, %497), scope: __module.funnel/__module.funnel.decoder
    %499 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %500 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %501 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %502 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %input.119 : Float(12:12, 12:1) = aten::ones(%498, %499, %500, %501, %502), scope: __module.funnel/__module.funnel.decoder # transformers/modeling_funnel.py:199:0
    %504 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %505 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %506 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %507 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %508 : int[] = prim::ListConstruct(%504, %505, %506, %507), scope: __module.funnel/__module.funnel.decoder
    %509 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %cls_mask : Float(13:13, 13:1) = aten::constant_pad_nd(%input.119, %508, %509), scope: __module.funnel/__module.funnel.decoder # torch/nn/functional.py:3552:0
    %515 : Tensor = prim::CallMethod[name="forward"](%11, %input_embeds, %192, %cls_mask, %token_type_mat.20, %attention_mask.1)
    %516 : Tensor = prim::CallMethod[name="forward"](%9, %515, %192, %cls_mask, %token_type_mat.20, %attention_mask.1)
    return (%516)

FunnelModel.embeddings
FunnelEmbeddings._actual_script_module
  graph(%self.3 : __torch__.transformers.modeling_funnel.FunnelEmbeddings,
        %input_ids : Long(17:13, 13:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.3)
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.3)
    %4 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embeddings"](%self.3)
    %8 : Tensor = prim::CallMethod[name="forward"](%4, %input_ids)
    %9 : Tensor = prim::CallMethod[name="forward"](%3, %8)
    %10 : Tensor = prim::CallMethod[name="forward"](%2, %9)
    return (%10)

FunnelModel.encoder
FunnelEncoder._actual_script_module
  graph(%self.7 : __torch__.transformers.modeling_funnel.FunnelEncoder,
        %attention_mask.1 : Long(17:13, 13:1),
        %2 : Float(17:9984, 13:768, 768:1),
        %token_type_ids : Long(17:13, 13:1)):
    %4 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="2"](%4)
    %6 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="3"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %8 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="2"](%7)
    %9 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="2"](%8)
    %10 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="2"](%10)
    %12 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="1"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %14 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="2"](%13)
    %15 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="0"](%14)
    %16 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="1"](%16)
    %18 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="3"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %20 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="1"](%19)
    %21 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="2"](%20)
    %22 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="1"](%22)
    %24 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="1"](%23)
    %25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %26 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="1"](%25)
    %27 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="0"](%26)
    %28 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="0"](%28)
    %30 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="3"](%29)
    %31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %32 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="0"](%31)
    %33 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="2"](%32)
    %34 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="0"](%34)
    %36 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="1"](%35)
    %37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="blocks"](%self.7)
    %38 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="0"](%37)
    %39 : __torch__.transformers.modeling_funnel.FunnelLayer = prim::GetAttr[name="0"](%38)
    %40 : __torch__.transformers.modeling_funnel.FunnelAttentionStructure = prim::GetAttr[name="attention_structure"](%self.7)
    %41 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="cos_dropout"](%40)
    %42 : __torch__.transformers.modeling_funnel.FunnelAttentionStructure = prim::GetAttr[name="attention_structure"](%self.7)
    %43 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="sin_dropout"](%42)
    %attention_mask.2 : Float(17:13, 13:1) = aten::type_as(%attention_mask.1, %2), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:625:0
    %45 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:195:0
    %46 : int = aten::size(%2, %45), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:195:0
    %seq_len.1 : Long() = prim::NumToTensor(%46), scope: __module.funnel/__module.funnel.encoder
    %48 : Scalar = aten::ScalarImplicit(%seq_len.1), scope: __module.funnel/__module.funnel.encoder
    %49 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
    %50 : int = prim::Constant[value=384](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
    %51 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
    %52 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
    %53 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
    %54 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
    %55 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
    %freq_seq.1 : Float(384:1) = aten::arange(%49, %50, %51, %52, %53, %54, %55), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
    %57 : Long() = prim::Constant[value={384}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
    %58 : Float(384:1) = aten::div(%freq_seq.1, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
    %59 : Float() = prim::Constant[value={10000}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
    %60 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
    %61 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
    %62 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
    %63 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
    %64 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
    %65 : Float() = aten::to(%59, %60, %61, %62, %63, %64), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
    %66 : Float() = aten::detach(%65), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
    %67 : Float(384:1) = aten::pow(%66, %58), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
    %68 : Float(384:1) = aten::reciprocal(%67), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
    %69 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
    %inv_freq.1 : Float(384:1) = aten::mul(%68, %69), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
    %71 : Long() = aten::neg(%seq_len.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %72 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %73 : Long() = aten::mul(%71, %72), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %74 : Scalar = aten::ScalarImplicit(%73), scope: __module.funnel/__module.funnel.encoder
    %75 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %76 : Long() = aten::mul(%seq_len.1, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %77 : Scalar = aten::ScalarImplicit(%76), scope: __module.funnel/__module.funnel.encoder
    %78 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %79 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %80 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %81 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %82 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %rel_pos_id.1 : Float(52:1) = aten::arange(%74, %77, %78, %79, %80, %81, %82), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
    %84 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:251:0
    %zero_offset.1 : Long() = aten::mul(%seq_len.1, %84), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:251:0
    %86 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %87 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %88 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %89 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %90 : Float(52:1) = aten::slice(%rel_pos_id.1, %86, %87, %88, %89), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %91 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %92 : Float(52:1, 1:1) = aten::unsqueeze(%90, %91), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %93 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %94 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq.1, %93), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %sinusoid.1 : Float(52:384, 384:1) = aten::mul(%92, %94), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
    %input.3 : Float(52:384, 384:1) = aten::sin(%sinusoid.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:253:0
    %953 : Tensor = prim::CallMethod[name="forward"](%43, %input.3)
    %input.4 : Float(52:384, 384:1) = aten::cos(%sinusoid.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:254:0
    %954 : Tensor = prim::CallMethod[name="forward"](%41, %input.4)
    %100 : Tensor[] = prim::ListConstruct(%953, %954), scope: __module.funnel/__module.funnel.encoder
    %101 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:255:0
    %pos_embed.1 : Float(52:768, 768:1) = aten::cat(%100, %101), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:255:0
    %103 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
    %104 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
    %105 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
    %106 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
    %107 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
    %108 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
    %pos.1 : Float(13:1) = aten::arange(%103, %48, %104, %105, %106, %107, %108), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
    %110 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %111 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %112 : Float() = aten::select(%pos.1, %110, %111), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %115 : Float() = aten::select(%pos.1, %113, %114), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %ref_point.1 : Float() = aten::sub(%112, %115, %116), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %121 : Long() = prim::Constant[value={13}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %122 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %max_dist.1 : Float() = aten::add(%ref_point.1, %121, %122), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %124 : Scalar = aten::ScalarImplicit(%max_dist.1), scope: __module.funnel/__module.funnel.encoder
    %125 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %126 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %127 : Float() = aten::select(%pos.1, %125, %126), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %128 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %129 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %130 : Float() = aten::select(%pos.1, %128, %129), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %min_dist.1 : Float() = aten::sub(%127, %130, %131), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %135 : Float() = aten::sub(%min_dist.1, %133, %134), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %136 : Scalar = aten::ScalarImplicit(%135), scope: __module.funnel/__module.funnel.encoder
    %137 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %138 : int = prim::Constant[value=4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %139 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %140 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %141 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %rel_pos.1 : Long(26:1) = aten::arange(%124, %136, %137, %138, %139, %140, %141), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %143 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %145 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %146 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %147 : Long(26:1) = aten::slice(%rel_pos.1, %143, %144, %145, %146), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %149 : Long(26:1, 1:1) = aten::unsqueeze(%147, %148), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %150 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %rel_pos.2 : Long(26:1, 1:1) = aten::add(%149, %zero_offset.1, %150), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %152 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %153 : int = aten::size(%rel_pos.2, %152), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %154 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder
    %155 : int = aten::Int(%154), scope: __module.funnel/__module.funnel.encoder
    %156 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %157 : int[] = prim::ListConstruct(%155, %156), scope: __module.funnel/__module.funnel.encoder
    %158 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %rel_pos.3 : Long(26:1, 768:0) = aten::expand(%rel_pos.2, %157, %158), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %160 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %161 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %162 : Float(26:768, 768:1) = aten::gather(%pos_embed.1, %160, %rel_pos.3, %161), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %163 : Float(1:1) = prim::Constant[value={-1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %164 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %165 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %166 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %167 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %168 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
    %169 : Float(1:1) = aten::to(%163, %164, %165, %166, %167, %168), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %cls_pos.1 : Float(1:1) = aten::detach(%169), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %171 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %172 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %173 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %174 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %pooled_pos_id.1 : Float(11:1) = aten::slice(%pos.1, %171, %172, %173, %174), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %176 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %177 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %178 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %179 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %180 : Float(6:2) = aten::slice(%pooled_pos_id.1, %176, %177, %178, %179), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %181 : Tensor[] = prim::ListConstruct(%cls_pos.1, %180), scope: __module.funnel/__module.funnel.encoder
    %182 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %pooled_pos.1 : Float(7:1) = aten::cat(%181, %182), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %184 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %185 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %186 : Float() = aten::select(%pooled_pos.1, %184, %185), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %187 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %188 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %189 : Float() = aten::select(%pos.1, %187, %188), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %190 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %ref_point.2 : Float() = aten::sub(%186, %189, %190), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %195 : Long() = prim::Constant[value={14}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %196 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %max_dist.2 : Float() = aten::add(%ref_point.2, %195, %196), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %198 : Scalar = aten::ScalarImplicit(%max_dist.2), scope: __module.funnel/__module.funnel.encoder
    %199 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %200 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %201 : Float() = aten::select(%pooled_pos.1, %199, %200), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %202 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %203 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %204 : Float() = aten::select(%pos.1, %202, %203), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %205 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %min_dist.2 : Float() = aten::sub(%201, %204, %205), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %207 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %208 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %209 : Float() = aten::sub(%min_dist.2, %207, %208), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %210 : Scalar = aten::ScalarImplicit(%209), scope: __module.funnel/__module.funnel.encoder
    %211 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %212 : int = prim::Constant[value=4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %213 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %214 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %215 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %rel_pos.4 : Long(27:1) = aten::arange(%198, %210, %211, %212, %213, %214, %215), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %217 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %218 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %219 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %220 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %221 : Long(27:1) = aten::slice(%rel_pos.4, %217, %218, %219, %220), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %222 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %223 : Long(27:1, 1:1) = aten::unsqueeze(%221, %222), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %224 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %rel_pos.5 : Long(27:1, 1:1) = aten::add(%223, %zero_offset.1, %224), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %226 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %227 : int = aten::size(%rel_pos.5, %226), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %228 : Long() = prim::NumToTensor(%227), scope: __module.funnel/__module.funnel.encoder
    %229 : int = aten::Int(%228), scope: __module.funnel/__module.funnel.encoder
    %230 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %231 : int[] = prim::ListConstruct(%229, %230), scope: __module.funnel/__module.funnel.encoder
    %232 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %rel_pos.6 : Long(27:1, 768:0) = aten::expand(%rel_pos.5, %231, %232), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %234 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
    %235 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
    %236 : Float(27:768, 768:1) = aten::gather(%pos_embed.1, %234, %rel_pos.6, %235), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
    %237 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %238 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %239 : Float() = aten::select(%pooled_pos.1, %237, %238), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %242 : Float() = aten::select(%pooled_pos.1, %240, %241), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %ref_point.3 : Float() = aten::sub(%239, %242, %243), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %248 : Long() = prim::Constant[value={14}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %249 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %max_dist.3 : Float() = aten::add(%ref_point.3, %248, %249), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %251 : Scalar = aten::ScalarImplicit(%max_dist.3), scope: __module.funnel/__module.funnel.encoder
    %252 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %253 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %254 : Float() = aten::select(%pooled_pos.1, %252, %253), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %255 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %256 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %257 : Float() = aten::select(%pooled_pos.1, %255, %256), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %258 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %min_dist.3 : Float() = aten::sub(%254, %257, %258), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %260 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %261 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %262 : Float() = aten::sub(%min_dist.3, %260, %261), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %263 : Scalar = aten::ScalarImplicit(%262), scope: __module.funnel/__module.funnel.encoder
    %264 : int = prim::Constant[value=-2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %265 : int = prim::Constant[value=4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %266 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %267 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %268 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %rel_pos.7 : Long(14:1) = aten::arange(%251, %263, %264, %265, %266, %267, %268), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %270 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %271 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %272 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %274 : Long(14:1) = aten::slice(%rel_pos.7, %270, %271, %272, %273), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %275 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %276 : Long(14:1, 1:1) = aten::unsqueeze(%274, %275), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %277 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %rel_pos.8 : Long(14:1, 1:1) = aten::add(%276, %zero_offset.1, %277), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %279 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %280 : int = aten::size(%rel_pos.8, %279), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %281 : Long() = prim::NumToTensor(%280), scope: __module.funnel/__module.funnel.encoder
    %282 : int = aten::Int(%281), scope: __module.funnel/__module.funnel.encoder
    %283 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %284 : int[] = prim::ListConstruct(%282, %283), scope: __module.funnel/__module.funnel.encoder
    %285 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %rel_pos.9 : Long(14:1, 768:0) = aten::expand(%rel_pos.8, %284, %285), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %287 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %288 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %289 : Float(14:768, 768:1) = aten::gather(%pos_embed.1, %287, %rel_pos.9, %288), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %290 : Float(1:1) = prim::Constant[value={-3}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %291 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %292 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %293 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %294 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %295 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
    %296 : Float(1:1) = aten::to(%290, %291, %292, %293, %294, %295), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %cls_pos.2 : Float(1:1) = aten::detach(%296), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
    %298 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %299 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %300 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %301 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %pooled_pos_id.2 : Float(5:1) = aten::slice(%pooled_pos.1, %298, %299, %300, %301), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
    %303 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %304 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %305 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %306 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %307 : Float(3:2) = aten::slice(%pooled_pos_id.2, %303, %304, %305, %306), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %308 : Tensor[] = prim::ListConstruct(%cls_pos.2, %307), scope: __module.funnel/__module.funnel.encoder
    %309 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %pooled_pos.2 : Float(4:1) = aten::cat(%308, %309), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
    %311 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %312 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %313 : Float() = aten::select(%pooled_pos.2, %311, %312), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %314 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %315 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %316 : Float() = aten::select(%pooled_pos.1, %314, %315), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %317 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %ref_point.4 : Float() = aten::sub(%313, %316, %317), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %322 : Long() = prim::Constant[value={16}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %323 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %max_dist.4 : Float() = aten::add(%ref_point.4, %322, %323), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %325 : Scalar = aten::ScalarImplicit(%max_dist.4), scope: __module.funnel/__module.funnel.encoder
    %326 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %327 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %328 : Float() = aten::select(%pooled_pos.2, %326, %327), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %329 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %330 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %331 : Float() = aten::select(%pooled_pos.1, %329, %330), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %332 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %min_dist.4 : Float() = aten::sub(%328, %331, %332), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %334 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %335 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %336 : Float() = aten::sub(%min_dist.4, %334, %335), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %337 : Scalar = aten::ScalarImplicit(%336), scope: __module.funnel/__module.funnel.encoder
    %338 : int = prim::Constant[value=-2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %339 : int = prim::Constant[value=4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %340 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %341 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %342 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %rel_pos.10 : Long(15:1) = aten::arange(%325, %337, %338, %339, %340, %341, %342), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %344 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %345 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %346 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %347 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %348 : Long(15:1) = aten::slice(%rel_pos.10, %344, %345, %346, %347), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %349 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %350 : Long(15:1, 1:1) = aten::unsqueeze(%348, %349), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %351 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %rel_pos.11 : Long(15:1, 1:1) = aten::add(%350, %zero_offset.1, %351), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
    %353 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %354 : int = aten::size(%rel_pos.11, %353), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %355 : Long() = prim::NumToTensor(%354), scope: __module.funnel/__module.funnel.encoder
    %356 : int = aten::Int(%355), scope: __module.funnel/__module.funnel.encoder
    %357 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %358 : int[] = prim::ListConstruct(%356, %357), scope: __module.funnel/__module.funnel.encoder
    %359 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %rel_pos.12 : Long(15:1, 768:0) = aten::expand(%rel_pos.11, %358, %359), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
    %361 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
    %362 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
    %363 : Float(15:768, 768:1) = aten::gather(%pos_embed.1, %361, %rel_pos.12, %362), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
    %364 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %365 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %366 : Float() = aten::select(%pooled_pos.2, %364, %365), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %367 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %368 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %369 : Float() = aten::select(%pooled_pos.2, %367, %368), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %370 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %ref_point.5 : Float() = aten::sub(%366, %369, %370), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
    %375 : Long() = prim::Constant[value={16}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %376 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %max_dist.5 : Float() = aten::add(%ref_point.5, %375, %376), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
    %378 : Scalar = aten::ScalarImplicit(%max_dist.5), scope: __module.funnel/__module.funnel.encoder
    %379 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %380 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %381 : Float() = aten::select(%pooled_pos.2, %379, %380), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %382 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %383 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %384 : Float() = aten::select(%pooled_pos.2, %382, %383), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %385 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %min_dist.5 : Float() = aten::sub(%381, %384, %385), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
    %387 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %388 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %389 : Float() = aten::sub(%min_dist.5, %387, %388), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %390 : Scalar = aten::ScalarImplicit(%389), scope: __module.funnel/__module.funnel.encoder
    %391 : int = prim::Constant[value=-4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %392 : int = prim::Constant[value=4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %393 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %394 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %395 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %rel_pos.13 : Long(8:1) = aten::arange(%378, %390, %391, %392, %393, %394, %395), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
    %397 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %398 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %399 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %400 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %401 : Long(8:1) = aten::slice(%rel_pos.13, %397, %398, %399, %400), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %402 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %403 : Long(8:1, 1:1) = aten::unsqueeze(%401, %402), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %404 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %rel_pos.14 : Long(8:1, 1:1) = aten::add(%403, %zero_offset.1, %404), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
    %406 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %407 : int = aten::size(%rel_pos.14, %406), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %408 : Long() = prim::NumToTensor(%407), scope: __module.funnel/__module.funnel.encoder
    %409 : int = aten::Int(%408), scope: __module.funnel/__module.funnel.encoder
    %410 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %411 : int[] = prim::ListConstruct(%409, %410), scope: __module.funnel/__module.funnel.encoder
    %412 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %rel_pos.15 : Long(8:1, 768:0) = aten::expand(%rel_pos.14, %411, %412), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
    %414 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %415 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %416 : Float(8:768, 768:1) = aten::gather(%pos_embed.1, %414, %rel_pos.15, %415), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
    %417 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %418 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %419 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %420 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %421 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %417, %418, %419, %420), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %422 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %423 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %424 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %425 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %426 : Long(17:13, 13:1) = aten::slice(%421, %422, %423, %424, %425), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %427 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %428 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%426, %427), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %429 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %430 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %431 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %432 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %433 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %429, %430, %431, %432), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %434 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %435 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%433, %434), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
    %token_type_mat.1 : Bool(17:169, 13:13, 13:1) = aten::eq(%428, %435), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
    %437 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
    %cls_ids.1 : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %437), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
    %439 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %440 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %441 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %442 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %443 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %439, %440, %441, %442), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %444 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %445 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %446 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %447 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %448 : Bool(17:13, 13:1) = aten::slice(%443, %444, %445, %446, %447), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %449 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %450 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%448, %449), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %451 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %452 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %453 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %454 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %455 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %451, %452, %453, %454), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %456 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %457 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%455, %456), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %cls_mat.1 : Bool(17:169, 13:13, 13:1) = aten::__or__(%450, %457), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
    %token_type_mat.2 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat.1, %token_type_mat.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:211:0
    %460 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %461 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %462 : Long() = aten::sub(%seq_len.1, %460, %461), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %463 : int = aten::Int(%462), scope: __module.funnel/__module.funnel.encoder
    %464 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %465 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %466 : Long() = aten::sub(%seq_len.1, %464, %465), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %467 : int = aten::Int(%466), scope: __module.funnel/__module.funnel.encoder
    %468 : int[] = prim::ListConstruct(%463, %467), scope: __module.funnel/__module.funnel.encoder
    %469 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %470 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %471 : Device = prim::Constant[value="cpu"](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %472 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %input.5 : Float(12:12, 12:1) = aten::ones(%468, %469, %470, %471, %472), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
    %474 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
    %475 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
    %476 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
    %477 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
    %478 : int[] = prim::ListConstruct(%474, %475, %476, %477), scope: __module.funnel/__module.funnel.encoder
    %479 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
    %cls_mask.1 : Float(13:13, 13:1) = aten::constant_pad_nd(%input.5, %478, %479), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
    %955 : Tensor = prim::CallMethod[name="forward"](%39, %2, %162, %cls_mask.1, %token_type_mat.2, %attention_mask.2)
    %956 : Tensor = prim::CallMethod[name="forward"](%36, %955, %162, %cls_mask.1, %token_type_mat.2, %attention_mask.2)
    %957 : Tensor = prim::CallMethod[name="forward"](%33, %956, %162, %cls_mask.1, %token_type_mat.2, %attention_mask.2)
    %958 : Tensor = prim::CallMethod[name="forward"](%30, %957, %162, %cls_mask.1, %token_type_mat.2, %attention_mask.2)
    %495 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %496 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %497 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %498 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %499 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %495, %496, %497, %498), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %500 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %501 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %502 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %503 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %504 : Bool(17:169, 1:13, 13:1) = aten::slice(%499, %500, %501, %502, %503), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %505 : Tensor[] = prim::ListConstruct(%504, %token_type_mat.2), scope: __module.funnel/__module.funnel.encoder
    %506 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %tensor.1 : Bool(17:182, 14:13, 13:1) = aten::cat(%505, %506), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %508 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %509 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %510 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %511 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %512 : Bool(17:182, 14:13, 13:1) = aten::slice(%tensor.1, %508, %509, %510, %511), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %513 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %514 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %515 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %516 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %token_type_mat.7 : Bool(17:182, 7:26, 13:1) = aten::slice(%512, %513, %514, %515, %516), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %518 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %519 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %520 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %521 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %522 : Float(1:13, 13:1) = aten::slice(%cls_mask.1, %518, %519, %520, %521), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %523 : Tensor[] = prim::ListConstruct(%522, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder
    %524 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %tensor.2 : Float(14:13, 13:1) = aten::cat(%523, %524), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %526 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %527 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %528 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %529 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %cls_mask.2 : Float(7:26, 13:1) = aten::slice(%tensor.2, %526, %527, %528, %529), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %531 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %532 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %533 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %534 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %535 : Float(17:9984, 13:768, 768:1) = aten::slice(%958, %531, %532, %533, %534), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %536 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %537 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %538 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %539 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %suffix.1 : Float(17:9984, 12:768, 768:1) = aten::slice(%535, %536, %537, %538, %539), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %541 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %542 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %543 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %544 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %545 : Float(17:9984, 13:768, 768:1) = aten::slice(%958, %541, %542, %543, %544), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %546 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %547 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %548 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %549 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %550 : Float(17:9984, 1:768, 768:1) = aten::slice(%545, %546, %547, %548, %549), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %551 : Tensor[] = prim::ListConstruct(%550, %suffix.1), scope: __module.funnel/__module.funnel.encoder
    %552 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %tensor.3 : Float(17:9984, 13:768, 768:1) = aten::cat(%551, %552), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %554 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %555 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %556 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %557 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %558 : Float(17:9984, 13:768, 768:1) = aten::slice(%tensor.3, %554, %555, %556, %557), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %559 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %560 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::unsqueeze(%558, %559), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %561 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %562 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %563 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %564 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %565 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%560, %561, %562, %563, %564), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %566 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %567 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %568 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %569 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %tensor.4 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%565, %566, %567, %568, %569), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %571 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %572 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %573 : int[] = prim::ListConstruct(%571, %572), scope: __module.funnel/__module.funnel.encoder
    %574 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %575 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %576 : int[] = prim::ListConstruct(%574, %575), scope: __module.funnel/__module.funnel.encoder
    %577 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %578 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %579 : int[] = prim::ListConstruct(%577, %578), scope: __module.funnel/__module.funnel.encoder
    %580 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %581 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %582 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
    %tensor.5 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::avg_pool2d(%tensor.4, %573, %576, %579, %580, %581, %582), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %584 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %585 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %586 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %587 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %588 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%tensor.5, %584, %585, %586, %587), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %589 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %590 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %query.4 : Float(17:5376, 7:768, 768:1) = aten::select(%588, %589, %590), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %959 : Tensor = prim::CallMethod[name="forward"](%27, %query.4, %958, %236, %cls_mask.2, %token_type_mat.7, %attention_mask.2)
    %593 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %594 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %595 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %596 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %597 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %593, %594, %595, %596), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %598 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %599 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %600 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %601 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %602 : Bool(17:182, 7:26, 13:1) = aten::slice(%597, %598, %599, %600, %601), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %603 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %604 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %605 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %606 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %607 : Bool(17:182, 7:26, 1:1) = aten::slice(%602, %603, %604, %605, %606), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %608 : Tensor[] = prim::ListConstruct(%607, %token_type_mat.7), scope: __module.funnel/__module.funnel.encoder
    %609 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %tensor.6 : Bool(17:98, 7:14, 14:1) = aten::cat(%608, %609), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %611 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %612 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %613 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %614 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %615 : Bool(17:98, 7:14, 14:1) = aten::slice(%tensor.6, %611, %612, %613, %614), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %616 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %617 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %618 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %619 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %620 : Bool(17:98, 7:14, 14:1) = aten::slice(%615, %616, %617, %618, %619), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %621 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %622 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %623 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %624 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %token_type_mat.9 : Bool(17:98, 7:14, 7:2) = aten::slice(%620, %621, %622, %623, %624), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %626 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %627 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %628 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %629 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %630 : Float(7:26, 13:1) = aten::slice(%cls_mask.2, %626, %627, %628, %629), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %631 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %632 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %633 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %634 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %635 : Float(7:26, 1:1) = aten::slice(%630, %631, %632, %633, %634), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %636 : Tensor[] = prim::ListConstruct(%635, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder
    %637 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %tensor.7 : Float(7:14, 14:1) = aten::cat(%636, %637), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %639 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %640 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %641 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %642 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %643 : Float(7:14, 14:1) = aten::slice(%tensor.7, %639, %640, %641, %642), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %644 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %645 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %646 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %647 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %cls_mask.3 : Float(7:14, 7:2) = aten::slice(%643, %644, %645, %646, %647), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %649 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %650 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %651 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %652 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %653 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %649, %650, %651, %652), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %654 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %655 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %656 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %657 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %suffix.2 : Float(17:13, 12:1) = aten::slice(%653, %654, %655, %656, %657), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %659 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %660 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %661 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %662 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %663 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %659, %660, %661, %662), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %664 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %665 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %666 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %667 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %668 : Float(17:13, 1:1) = aten::slice(%663, %664, %665, %666, %667), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %669 : Tensor[] = prim::ListConstruct(%668, %suffix.2), scope: __module.funnel/__module.funnel.encoder
    %670 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %tensor.8 : Float(17:13, 13:1) = aten::cat(%669, %670), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %672 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %673 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %674 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %675 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %676 : Float(17:13, 13:1) = aten::slice(%tensor.8, %672, %673, %674, %675), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %677 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %678 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%676, %677), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %679 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %680 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %681 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %682 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %683 : Float(17:13, 1:13, 13:1) = aten::slice(%678, %679, %680, %681, %682), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %684 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %tensor.9 : Float(17:13, 1:13, 13:1, 1:1) = aten::unsqueeze(%683, %684), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %input.51 : Float(17:13, 1:13, 13:1, 1:1) = aten::neg(%tensor.9), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
    %687 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %688 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %689 : int[] = prim::ListConstruct(%687, %688), scope: __module.funnel/__module.funnel.encoder
    %690 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %691 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %692 : int[] = prim::ListConstruct(%690, %691), scope: __module.funnel/__module.funnel.encoder
    %693 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %694 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %695 : int[] = prim::ListConstruct(%693, %694), scope: __module.funnel/__module.funnel.encoder
    %696 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %697 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %698 : int[] = prim::ListConstruct(%696, %697), scope: __module.funnel/__module.funnel.encoder
    %699 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %700 : Float(17:7, 1:7, 7:1, 1:1) = aten::max_pool2d(%input.51, %689, %692, %695, %698, %699), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %tensor.10 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%700), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
    %702 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %703 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %704 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %705 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %706 : Float(17:7, 1:7, 7:1, 1:1) = aten::slice(%tensor.10, %702, %703, %704, %705), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %707 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %708 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %709 : Float(17:7, 7:1, 1:1) = aten::select(%706, %707, %708), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %710 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %711 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %712 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %713 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %714 : Float(17:7, 7:1, 1:1) = aten::slice(%709, %710, %711, %712, %713), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %715 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %716 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %attention_mask.3 : Float(17:7, 7:1) = aten::select(%714, %715, %716), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %960 : Tensor = prim::CallMethod[name="forward"](%24, %959, %289, %cls_mask.3, %token_type_mat.9, %attention_mask.3)
    %961 : Tensor = prim::CallMethod[name="forward"](%21, %960, %289, %cls_mask.3, %token_type_mat.9, %attention_mask.3)
    %962 : Tensor = prim::CallMethod[name="forward"](%18, %961, %289, %cls_mask.3, %token_type_mat.9, %attention_mask.3)
    %726 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %727 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %728 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %729 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %730 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %726, %727, %728, %729), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %731 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %732 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %733 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %734 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %735 : Bool(17:98, 1:14, 7:2) = aten::slice(%730, %731, %732, %733, %734), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %736 : Tensor[] = prim::ListConstruct(%735, %token_type_mat.9), scope: __module.funnel/__module.funnel.encoder
    %737 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %tensor.11 : Bool(17:56, 8:7, 7:1) = aten::cat(%736, %737), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %739 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %740 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %741 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %742 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %743 : Bool(17:56, 8:7, 7:1) = aten::slice(%tensor.11, %739, %740, %741, %742), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %744 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %745 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %746 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %747 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %token_type_mat.13 : Bool(17:56, 4:14, 7:1) = aten::slice(%743, %744, %745, %746, %747), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %749 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %750 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %751 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %752 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %753 : Float(1:14, 7:2) = aten::slice(%cls_mask.3, %749, %750, %751, %752), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %754 : Tensor[] = prim::ListConstruct(%753, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder
    %755 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %tensor.12 : Float(8:7, 7:1) = aten::cat(%754, %755), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %757 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %758 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %759 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %760 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %cls_mask.4 : Float(4:14, 7:1) = aten::slice(%tensor.12, %757, %758, %759, %760), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %762 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %763 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %764 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %765 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %766 : Float(17:5376, 7:768, 768:1) = aten::slice(%962, %762, %763, %764, %765), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %767 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %768 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %769 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %770 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %suffix.3 : Float(17:5376, 6:768, 768:1) = aten::slice(%766, %767, %768, %769, %770), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %772 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %773 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %774 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %775 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %776 : Float(17:5376, 7:768, 768:1) = aten::slice(%962, %772, %773, %774, %775), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %777 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %778 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %779 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %780 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %781 : Float(17:5376, 1:768, 768:1) = aten::slice(%776, %777, %778, %779, %780), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %782 : Tensor[] = prim::ListConstruct(%781, %suffix.3), scope: __module.funnel/__module.funnel.encoder
    %783 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %tensor.13 : Float(17:5376, 7:768, 768:1) = aten::cat(%782, %783), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %785 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %786 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %787 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %788 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %789 : Float(17:5376, 7:768, 768:1) = aten::slice(%tensor.13, %785, %786, %787, %788), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %790 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %791 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::unsqueeze(%789, %790), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %792 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %793 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %794 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %795 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %796 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%791, %792, %793, %794, %795), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %797 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %798 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %799 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %800 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %tensor.14 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%796, %797, %798, %799, %800), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
    %802 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %803 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %804 : int[] = prim::ListConstruct(%802, %803), scope: __module.funnel/__module.funnel.encoder
    %805 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %806 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %807 : int[] = prim::ListConstruct(%805, %806), scope: __module.funnel/__module.funnel.encoder
    %808 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %809 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %810 : int[] = prim::ListConstruct(%808, %809), scope: __module.funnel/__module.funnel.encoder
    %811 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %812 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %813 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
    %tensor.15 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::avg_pool2d(%tensor.14, %804, %807, %810, %811, %812, %813), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
    %815 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %816 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %817 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %818 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %819 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::slice(%tensor.15, %815, %816, %817, %818), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %820 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %821 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %query.8 : Float(17:3072, 4:768, 768:1) = aten::select(%819, %820, %821), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
    %963 : Tensor = prim::CallMethod[name="forward"](%15, %query.8, %962, %363, %cls_mask.4, %token_type_mat.13, %attention_mask.3)
    %824 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %825 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %826 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %827 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %828 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %824, %825, %826, %827), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %829 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %830 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %831 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %832 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %833 : Bool(17:56, 4:14, 7:1) = aten::slice(%828, %829, %830, %831, %832), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %834 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %835 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %836 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %837 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %838 : Bool(17:56, 4:14, 1:1) = aten::slice(%833, %834, %835, %836, %837), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %839 : Tensor[] = prim::ListConstruct(%838, %token_type_mat.13), scope: __module.funnel/__module.funnel.encoder
    %840 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %tensor.16 : Bool(17:32, 4:8, 8:1) = aten::cat(%839, %840), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %842 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %843 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %844 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %845 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %846 : Bool(17:32, 4:8, 8:1) = aten::slice(%tensor.16, %842, %843, %844, %845), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %847 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %848 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %849 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %850 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %851 : Bool(17:32, 4:8, 8:1) = aten::slice(%846, %847, %848, %849, %850), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %852 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %853 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %854 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %855 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %token_type_mat.15 : Bool(17:32, 4:8, 4:2) = aten::slice(%851, %852, %853, %854, %855), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %857 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %858 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %859 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %860 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %861 : Float(4:14, 7:1) = aten::slice(%cls_mask.4, %857, %858, %859, %860), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %862 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %863 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %864 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %865 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %866 : Float(4:14, 1:1) = aten::slice(%861, %862, %863, %864, %865), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %867 : Tensor[] = prim::ListConstruct(%866, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder
    %868 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %tensor.17 : Float(4:8, 8:1) = aten::cat(%867, %868), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
    %870 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %871 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %872 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %873 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %874 : Float(4:8, 8:1) = aten::slice(%tensor.17, %870, %871, %872, %873), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %875 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %876 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %877 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %878 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %cls_mask.5 : Float(4:8, 4:2) = aten::slice(%874, %875, %876, %877, %878), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
    %880 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %881 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %882 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %883 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %884 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %880, %881, %882, %883), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %885 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %886 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %887 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %888 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %suffix : Float(17:7, 6:1) = aten::slice(%884, %885, %886, %887, %888), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
    %890 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %891 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %892 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %893 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %894 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %890, %891, %892, %893), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %895 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %896 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %897 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %898 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %899 : Float(17:7, 1:1) = aten::slice(%894, %895, %896, %897, %898), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %900 : Tensor[] = prim::ListConstruct(%899, %suffix), scope: __module.funnel/__module.funnel.encoder
    %901 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %tensor.18 : Float(17:7, 7:1) = aten::cat(%900, %901), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
    %903 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %904 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %905 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %906 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %907 : Float(17:7, 7:1) = aten::slice(%tensor.18, %903, %904, %905, %906), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %908 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %909 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%907, %908), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %910 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %911 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %912 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %913 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %914 : Float(17:7, 1:7, 7:1) = aten::slice(%909, %910, %911, %912, %913), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %915 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %tensor.19 : Float(17:7, 1:7, 7:1, 1:1) = aten::unsqueeze(%914, %915), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
    %input.88 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%tensor.19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
    %918 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %919 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %920 : int[] = prim::ListConstruct(%918, %919), scope: __module.funnel/__module.funnel.encoder
    %921 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %922 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %923 : int[] = prim::ListConstruct(%921, %922), scope: __module.funnel/__module.funnel.encoder
    %924 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %925 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %926 : int[] = prim::ListConstruct(%924, %925), scope: __module.funnel/__module.funnel.encoder
    %927 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %928 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %929 : int[] = prim::ListConstruct(%927, %928), scope: __module.funnel/__module.funnel.encoder
    %930 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %931 : Float(17:4, 1:4, 4:1, 1:1) = aten::max_pool2d(%input.88, %920, %923, %926, %929, %930), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
    %tensor : Float(17:4, 1:4, 4:1, 1:1) = aten::neg(%931), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
    %933 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %934 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %935 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %936 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %937 : Float(17:4, 1:4, 4:1, 1:1) = aten::slice(%tensor, %933, %934, %935, %936), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %938 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %939 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %940 : Float(17:4, 4:1, 1:1) = aten::select(%937, %938, %939), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %941 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %942 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %943 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %944 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %945 : Float(17:4, 4:1, 1:1) = aten::slice(%940, %941, %942, %943, %944), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %946 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %947 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %attention_mask : Float(17:4, 4:1) = aten::select(%945, %946, %947), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
    %964 : Tensor = prim::CallMethod[name="forward"](%12, %963, %416, %cls_mask.5, %token_type_mat.15, %attention_mask)
    %965 : Tensor = prim::CallMethod[name="forward"](%9, %964, %416, %cls_mask.5, %token_type_mat.15, %attention_mask)
    %966 : Tensor = prim::CallMethod[name="forward"](%6, %965, %416, %cls_mask.5, %token_type_mat.15, %attention_mask)
    %952 : (Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:3072, 4:768, 768:1), Float(17:9984, 13:768, 768:1)) = prim::TupleConstruct(%958, %958, %958, %966, %958)
    return (%952)

FunnelEmbeddings.dropout
Dropout._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
    %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
    return (%inputs_embeds)

FunnelEmbeddings.layer_norm
LayerNorm._actual_script_module
  graph(%self.5 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.5)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.5)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
    %input.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%1, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
    return (%input.2)

FunnelEmbeddings.word_embeddings
Embedding._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %3 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %input.1 : Float(17:9984, 13:768, 768:1) = aten::embedding(%2, %input_ids, %3, %4, %5), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    return (%input.1)

FunnelAttentionStructure.cos_dropout
Dropout._actual_script_module
  graph(%self.9 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.4 : Float(52:384, 384:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
    %cos_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.4, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
    return (%cos_embed.1)

FunnelAttentionStructure.sin_dropout
Dropout._actual_script_module
  graph(%self.8 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.3 : Float(52:384, 384:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
    %sin_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.3, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
    return (%sin_embed.1)

ModuleList.*
  module had no methods with graph attrs.

FunnelLayer._actual_script_module
  graph(%self.10 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask.1 : Float(13:13, 13:1),
        %token_type_mat.2 : Bool(17:169, 13:13, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.10)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.10)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.1, %token_type_mat.2, %attention_mask.2)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask.1 : Float(13:13, 13:1),
        %token_type_mat.2 : Bool(17:169, 13:13, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.11)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.11)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.11)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.11)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.11)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.11)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.11)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.11)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.11)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.11)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.11)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.11)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
    %batch_size.1 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %21 : int = aten::Int(%batch_size.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %22 : int = aten::Int(%batch_size.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %23 : int = aten::Int(%batch_size.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %24 : int = aten::Int(%batch_size.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
    %seq_len.2 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %28 : int = aten::Int(%seq_len.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %29 : int = aten::Int(%seq_len.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
    %context_len.1 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %39 : int = aten::Int(%context_len.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %40 : int = aten::Int(%context_len.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %41 : int = aten::Int(%context_len.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %q_head.1 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %59 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
    %q_head.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.1, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.1 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_w_bias.1, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %content_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
    %v.1 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %86 : Float(26:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %v.1, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %positional_attn.1 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.1, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
    %batch_size.2 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %95 : int = aten::Int(%batch_size.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %96 : int = aten::Int(%batch_size.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.1, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
    %n_head.1 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %100 : int = aten::Int(%n_head.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %101 : int = aten::Int(%n_head.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.1, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
    %seq_len.3 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %105 : int = aten::Int(%seq_len.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %106 : int = aten::Int(%seq_len.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.1, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.1 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %110 : int = aten::Int(%max_rel_len.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %positional_attn.2 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.1, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.2, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.3 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.1, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %positional_attn.4 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.3, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.5 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.4, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.6 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.2, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
    %batch_size.3 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %147 : int = aten::Int(%batch_size.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.2, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
    %seq_len.4 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %151 : int = aten::Int(%seq_len.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.2, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
    %context_len.2 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %155 : int = aten::Int(%context_len.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.1 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_s_bias.1, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %162 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.2, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.3 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:371:0
    %diff_token_type.1 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.1 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.3, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.3, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.3, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.3, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.1, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.3, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.3, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.3, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.3, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.1, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.3, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.1, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.1, %positional_attn.6, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
    %attn_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%232, %token_type_attn.2, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %attn_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.1, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %253 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:396:0
    %256 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %attn_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.2, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
    %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.3, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.6)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %attn_vec.1 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
    %input.7 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.1, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.7)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
    %input.9 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.9)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.19 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.19)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.19)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.19)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.19)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.19)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %9 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %11 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %13 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %15 : Float(17:39936, 13:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %17 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %18 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %21 : Float(17:39936, 13:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.11)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
    %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.14)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.15 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.6 : Float(17:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.6, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.17 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.1)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
    %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.9 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.18)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.18)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.10)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.linear.Linear,
        %input.7 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
    %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.7, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
    %input.8 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.8)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
    %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.11 : Float(17:39936, 13:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.11, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.12)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
    %h.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.1)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.14 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.24)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.1)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.20)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.4 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.1 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.4, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.1)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.22 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.22)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.22)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.5 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.5, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.13)

FunnelLayer._actual_script_module
  graph(%self.25 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask.1 : Float(13:13, 13:1),
        %token_type_mat.2 : Bool(17:169, 13:13, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.25)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.25)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.1, %token_type_mat.2, %attention_mask.2)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.26 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask.1 : Float(13:13, 13:1),
        %token_type_mat.2 : Bool(17:169, 13:13, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.26)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.26)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.26)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.26)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.26)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.26)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.26)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.26)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.26)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.26)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.26)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.26)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
    %batch_size.4 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %21 : int = aten::Int(%batch_size.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %22 : int = aten::Int(%batch_size.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %23 : int = aten::Int(%batch_size.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %24 : int = aten::Int(%batch_size.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
    %seq_len.5 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %28 : int = aten::Int(%seq_len.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %29 : int = aten::Int(%seq_len.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
    %context_len.3 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %39 : int = aten::Int(%context_len.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %40 : int = aten::Int(%context_len.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %41 : int = aten::Int(%context_len.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %q_head.3 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %59 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
    %q_head.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.3, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.2 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_w_bias.2, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %content_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
    %v.2 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %86 : Float(26:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %v.2, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %positional_attn.7 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.7, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
    %batch_size.5 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %95 : int = aten::Int(%batch_size.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %96 : int = aten::Int(%batch_size.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.7, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
    %n_head.2 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %100 : int = aten::Int(%n_head.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %101 : int = aten::Int(%n_head.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.7, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
    %seq_len.6 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %105 : int = aten::Int(%seq_len.6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %106 : int = aten::Int(%seq_len.6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.7, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.2 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %110 : int = aten::Int(%max_rel_len.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %positional_attn.8 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.7, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.8, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.9 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.2, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %positional_attn.10 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.9, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.11 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.10, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.12 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.11, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.2, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
    %batch_size.6 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %147 : int = aten::Int(%batch_size.6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.2, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
    %seq_len.7 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %151 : int = aten::Int(%seq_len.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.2, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
    %context_len.4 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %155 : int = aten::Int(%context_len.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.2 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_s_bias.2, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %162 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.4, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.4 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:371:0
    %diff_token_type.2 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.2 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.4, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.4, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.4, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.4, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.2, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.4, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.4, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.4, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.4, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.2, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.4, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.3, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.2, %positional_attn.12, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
    %attn_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%232, %token_type_attn.4, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %attn_score.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.4, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %253 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:396:0
    %256 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %attn_score.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.5, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
    %input.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.6, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.15)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %attn_vec.2 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
    %input.16 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.2, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.16)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
    %input.18 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.18)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.34 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.34)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.34)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.34)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.34)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.34)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %9 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %11 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %13 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %15 : Float(17:39936, 13:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %17 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %18 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %21 : Float(17:39936, 13:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %input.20 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.20)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
    %input.23 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.23)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.15 : Float(17:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.15, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.2)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.28 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.28)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.28)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
    %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.18 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.33)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.33)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.19 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.18, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.19)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.31 : __torch__.torch.nn.modules.linear.Linear,
        %input.16 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.31)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.31)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
    %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.16, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
    %input.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.17)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.27)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
    %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.20 : Float(17:39936, 13:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.20, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.21)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
    %h.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.2)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.39 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.23 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.39)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.39)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.2)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.35)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.35)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.9 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.2 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.9, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.2)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.37)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.37)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.22 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.22)

FunnelLayer._actual_script_module
  graph(%self.40 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask.1 : Float(13:13, 13:1),
        %token_type_mat.2 : Bool(17:169, 13:13, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.40)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.40)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.1, %token_type_mat.2, %attention_mask.2)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.41 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask.1 : Float(13:13, 13:1),
        %token_type_mat.2 : Bool(17:169, 13:13, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.41)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.41)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.41)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.41)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.41)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.41)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.41)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.41)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.41)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.41)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.41)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.41)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
    %batch_size.7 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %21 : int = aten::Int(%batch_size.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %22 : int = aten::Int(%batch_size.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %23 : int = aten::Int(%batch_size.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %24 : int = aten::Int(%batch_size.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
    %seq_len.8 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %28 : int = aten::Int(%seq_len.8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %29 : int = aten::Int(%seq_len.8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
    %context_len.5 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %39 : int = aten::Int(%context_len.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %40 : int = aten::Int(%context_len.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %41 : int = aten::Int(%context_len.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %q_head.5 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %59 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
    %q_head.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.5, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.3 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_w_bias.3, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %content_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
    %v.3 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %86 : Float(26:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %v.3, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %positional_attn.13 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.13, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
    %batch_size.8 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %95 : int = aten::Int(%batch_size.8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %96 : int = aten::Int(%batch_size.8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.13, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
    %n_head.3 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %100 : int = aten::Int(%n_head.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %101 : int = aten::Int(%n_head.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.13, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
    %seq_len.9 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %105 : int = aten::Int(%seq_len.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %106 : int = aten::Int(%seq_len.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.13, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.3 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %110 : int = aten::Int(%max_rel_len.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %positional_attn.14 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.13, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.14, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.15 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.3, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %positional_attn.16 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.15, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.17 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.16, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.18 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.17, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.2, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
    %batch_size.9 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %147 : int = aten::Int(%batch_size.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.2, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
    %seq_len.10 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %151 : int = aten::Int(%seq_len.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.2, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
    %context_len.6 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %155 : int = aten::Int(%context_len.6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.3 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_s_bias.3, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %162 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.6, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.5 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:371:0
    %diff_token_type.3 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.3 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.5, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.5, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.5, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.5, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.3, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.5, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.5, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.5, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.5, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.3, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.5, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.3, %positional_attn.18, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
    %attn_score.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%232, %token_type_attn.6, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %attn_score.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.7, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %253 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:396:0
    %256 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %attn_score.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.8, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
    %input.24 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.9, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.24)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %attn_vec.3 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
    %input.25 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.3, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.25)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
    %input.27 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.27)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.49 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.49)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.49)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.49)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.49)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.49)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %9 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %11 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %13 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %15 : Float(17:39936, 13:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %17 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %18 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %21 : Float(17:39936, 13:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %input.29 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.29)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
    %input.32 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.32)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.24 : Float(17:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.24, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.47 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.3)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.43)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
    %output.11 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.11, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.27 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.28 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.28)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.linear.Linear,
        %input.25 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.46)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.46)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
    %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
    %input.26 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.26)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
    %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.29 : Float(17:39936, 13:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.30 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.29, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.30)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
    %h.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.3)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.32 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.32, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.3)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.50)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.14 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.3 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.14, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.3)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.52)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.52)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.31)

FunnelLayer._actual_script_module
  graph(%self.55 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask.1 : Float(13:13, 13:1),
        %token_type_mat.2 : Bool(17:169, 13:13, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.55)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.55)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.1, %token_type_mat.2, %attention_mask.2)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.56 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask.1 : Float(13:13, 13:1),
        %token_type_mat.2 : Bool(17:169, 13:13, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.56)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.56)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.56)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.56)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.56)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.56)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.56)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.56)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.56)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.56)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.56)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.56)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
    %batch_size.10 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %21 : int = aten::Int(%batch_size.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %22 : int = aten::Int(%batch_size.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %23 : int = aten::Int(%batch_size.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %24 : int = aten::Int(%batch_size.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
    %seq_len.11 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %28 : int = aten::Int(%seq_len.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %29 : int = aten::Int(%seq_len.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
    %context_len.7 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %39 : int = aten::Int(%context_len.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %40 : int = aten::Int(%context_len.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %41 : int = aten::Int(%context_len.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %q_head.7 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %59 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
    %q_head.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.7, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.4 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_w_bias.4, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %content_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
    %v.4 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %86 : Float(26:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %v.4, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %positional_attn.19 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.19, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
    %batch_size.11 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %95 : int = aten::Int(%batch_size.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %96 : int = aten::Int(%batch_size.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.19, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
    %n_head.4 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %100 : int = aten::Int(%n_head.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %101 : int = aten::Int(%n_head.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.19, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
    %seq_len.12 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %105 : int = aten::Int(%seq_len.12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %106 : int = aten::Int(%seq_len.12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.19, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.4 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %110 : int = aten::Int(%max_rel_len.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %positional_attn.20 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.19, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.20, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.21 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.4, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %positional_attn.22 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.21, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.23 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.22, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.24 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.23, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.2, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
    %batch_size.12 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %147 : int = aten::Int(%batch_size.12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.2, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
    %seq_len.13 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %151 : int = aten::Int(%seq_len.13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.2, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
    %context_len.8 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %155 : int = aten::Int(%context_len.8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.4 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_s_bias.4, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %162 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.8, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.6 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:371:0
    %diff_token_type.4 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.4 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.6, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.6, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.6, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.6, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.4, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.6, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.6, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.6, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.6, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.4, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.6, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.7, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.4, %positional_attn.24, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
    %attn_score.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%232, %token_type_attn.8, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %attn_score.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.10, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %253 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:396:0
    %256 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %attn_score.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.11, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
    %input.33 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.12, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.33)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %attn_vec.4 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
    %input.34 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.4, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.34)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
    %input.36 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.36)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.64 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.64)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.64)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.64)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.64)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.64)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %9 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %11 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %13 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %15 : Float(17:39936, 13:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %17 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %18 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %21 : Float(17:39936, 13:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %input.38 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.38)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
    %input.41 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.41)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.33 : Float(17:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.33, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.62 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.4)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.58)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
    %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.36 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.63)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.63)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.37 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.36, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.37)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.61 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.61)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.61)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
    %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.34, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
    %input.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.35)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.59)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
    %output.17 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.17, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.38 : Float(17:39936, 13:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.39 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.38, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.39)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
    %h.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.4)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.41 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.69)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.69)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %hidden.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.41, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%hidden.1)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.19 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.4 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.19, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.4)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.67)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.67)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.40 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.40)

FunnelLayer._actual_script_module
  graph(%self.70 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %query.4 : Float(17:5376, 7:768, 768:1),
        %2 : Float(17:9984, 13:768, 768:1),
        %3 : Float(27:768, 768:1),
        %cls_mask.2 : Float(7:26, 13:1),
        %token_type_mat.7 : Bool(17:182, 7:26, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %7 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.70)
    %8 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.70)
    %11 : Tensor = prim::CallMethod[name="forward"](%8, %query.4, %2, %3, %cls_mask.2, %token_type_mat.7, %attention_mask.2)
    %12 : Tensor = prim::CallMethod[name="forward"](%7, %11)
    return (%12)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.71 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %query.4 : Float(17:5376, 7:768, 768:1),
        %2 : Float(17:9984, 13:768, 768:1),
        %3 : Float(27:768, 768:1),
        %cls_mask.2 : Float(7:26, 13:1),
        %token_type_mat.7 : Bool(17:182, 7:26, 13:1),
        %attention_mask.2 : Float(17:13, 13:1)):
    %7 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.71)
    %8 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.71)
    %9 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.71)
    %10 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.71)
    %11 : Tensor = prim::GetAttr[name="seg_embed"](%self.71)
    %12 : Tensor = prim::GetAttr[name="r_s_bias"](%self.71)
    %13 : Tensor = prim::GetAttr[name="r_kernel"](%self.71)
    %14 : Tensor = prim::GetAttr[name="r_r_bias"](%self.71)
    %15 : Tensor = prim::GetAttr[name="r_w_bias"](%self.71)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.71)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.71)
    %18 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.71)
    %19 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
    %20 : int = aten::size(%query.4, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
    %batch_size.13 : Long() = prim::NumToTensor(%20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %22 : int = aten::Int(%batch_size.13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %23 : int = aten::Int(%batch_size.13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %24 : int = aten::Int(%batch_size.13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %25 : int = aten::Int(%batch_size.13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
    %27 : int = aten::size(%query.4, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
    %seq_len.14 : Long() = prim::NumToTensor(%27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %29 : int = aten::Int(%seq_len.14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %30 : int = aten::Int(%seq_len.14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %37 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
    %38 : int = aten::size(%2, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
    %context_len.9 : Long() = prim::NumToTensor(%38), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %40 : int = aten::Int(%context_len.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %41 : int = aten::Int(%context_len.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %42 : int = aten::Int(%context_len.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %277 : Tensor = prim::CallMethod[name="forward"](%18, %query.4)
    %47 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
    %48 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
    %49 : int[] = prim::ListConstruct(%25, %30, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %q_head.9 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%277, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
    %278 : Tensor = prim::CallMethod[name="forward"](%17, %2)
    %52 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
    %53 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
    %54 : int[] = prim::ListConstruct(%24, %42, %52, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %55 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%278, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
    %279 : Tensor = prim::CallMethod[name="forward"](%16, %2)
    %57 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
    %58 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
    %59 : int[] = prim::ListConstruct(%23, %41, %57, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
    %61 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
    %q_head.10 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.9, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
    %63 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.5 : Float(12:64, 64:1) = aten::mul(%15, %63), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
    %65 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
    %66 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_w_bias.5, %65), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
    %67 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %68 : Tensor[] = prim::ListConstruct(%66, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %content_score.5 : Float(17:1092, 12:91, 7:13, 13:1) = aten::einsum(%67, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %83 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
    %v.5 : Float(12:64, 64:1) = aten::mul(%14, %83), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
    %85 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %86 : Tensor[] = prim::ListConstruct(%3, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %87 : Float(27:768, 12:64, 64:1) = aten::einsum(%85, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %88 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
    %89 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %v.5, %88), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
    %90 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %91 : Tensor[] = prim::ListConstruct(%89, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %positional_attn.25 : Float(17:189, 12:3213, 7:27, 27:1) = aten::einsum(%90, %91), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %93 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
    %94 : int = aten::size(%positional_attn.25, %93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
    %batch_size.14 : Long() = prim::NumToTensor(%94), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %96 : int = aten::Int(%batch_size.14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %97 : int = aten::Int(%batch_size.14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %98 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
    %99 : int = aten::size(%positional_attn.25, %98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
    %n_head.5 : Long() = prim::NumToTensor(%99), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %101 : int = aten::Int(%n_head.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %102 : int = aten::Int(%n_head.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %103 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
    %104 : int = aten::size(%positional_attn.25, %103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
    %seq_len.15 : Long() = prim::NumToTensor(%104), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %106 : int = aten::Int(%seq_len.15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %107 : int = aten::Int(%seq_len.15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %108 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
    %109 : int = aten::size(%positional_attn.25, %108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.5 : Long() = prim::NumToTensor(%109), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %111 : int = aten::Int(%max_rel_len.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %112 : int[] = prim::ListConstruct(%97, %102, %111, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %positional_attn.26 : Float(17:189, 12:3213, 27:7, 7:1) = aten::reshape(%positional_attn.25, %112), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:428:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %117 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %118 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%positional_attn.26, %114, %115, %116, %117), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %122 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %123 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%118, %119, %120, %121, %122), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %127 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %128 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%123, %124, %125, %126, %127), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %132 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.27 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%128, %129, %130, %131, %132), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
    %134 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
    %135 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
    %136 : Long() = aten::sub(%max_rel_len.5, %134, %135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
    %137 : int = aten::Int(%136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %138 : int[] = prim::ListConstruct(%96, %101, %106, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %positional_attn.28 : Float(17:189, 12:3213, 7:25, 25:1) = aten::reshape(%positional_attn.27, %138), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
    %140 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
    %142 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.29 : Float(17:189, 12:3213, 7:25, 13:1) = aten::slice(%positional_attn.28, %140, %141, %40, %142), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.30 : Float(17:189, 12:3213, 7:25, 13:1) = aten::mul_(%positional_attn.29, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:498:0
    %145 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
    %146 : int = aten::size(%token_type_mat.7, %145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
    %batch_size.15 : Long() = prim::NumToTensor(%146), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %148 : int = aten::Int(%batch_size.15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %149 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
    %150 : int = aten::size(%token_type_mat.7, %149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
    %seq_len.16 : Long() = prim::NumToTensor(%150), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %152 : int = aten::Int(%seq_len.16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %153 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
    %154 : int = aten::size(%token_type_mat.7, %153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
    %context_len.10 : Long() = prim::NumToTensor(%154), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %156 : int = aten::Int(%context_len.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %157 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.5 : Float(12:64, 64:1) = aten::mul(%12, %157), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
    %159 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
    %160 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_s_bias.5, %159), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
    %161 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %162 : Tensor[] = prim::ListConstruct(%160, %11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %163 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%161, %162), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %167 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %168 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %164, %165, %166, %167), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %169 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %170 : Bool(17:182, 1:182, 7:26, 13:1) = aten::unsqueeze(%168, %169), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %177 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %178 : int = aten::size(%q_head.10, %177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %179 : Long() = prim::NumToTensor(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %180 : int = aten::Int(%179), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %184 : int[] = prim::ListConstruct(%148, %180, %152, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %185 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.8 : Bool(17:182, 12:0, 7:26, 13:1) = aten::expand(%170, %184, %185), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
    %187 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:371:0
    %188 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:371:0
    %189 : Tensor[] = aten::split(%163, %187, %188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:371:0
    %diff_token_type.5 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.5 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%189), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %192 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %193 : int = aten::size(%token_type_mat.8, %192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %194 : Long() = prim::NumToTensor(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %195 : int = aten::Int(%194), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %196 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %197 : int = aten::size(%token_type_mat.8, %196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %198 : Long() = prim::NumToTensor(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %199 : int = aten::Int(%198), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %200 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %201 : int = aten::size(%token_type_mat.8, %200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %203 : int = aten::Int(%202), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %204 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %205 : int = aten::size(%token_type_mat.8, %204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %207 : int = aten::Int(%206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %208 : int[] = prim::ListConstruct(%195, %199, %203, %207), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %209 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %210 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%same_token_type.5, %208, %209), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %211 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %212 : int = aten::size(%token_type_mat.8, %211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %213 : Long() = prim::NumToTensor(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %214 : int = aten::Int(%213), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %215 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %216 : int = aten::size(%token_type_mat.8, %215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %217 : Long() = prim::NumToTensor(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %218 : int = aten::Int(%217), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %219 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %220 : int = aten::size(%token_type_mat.8, %219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %221 : Long() = prim::NumToTensor(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %222 : int = aten::Int(%221), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %223 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %224 : int = aten::size(%token_type_mat.8, %223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %225 : Long() = prim::NumToTensor(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %226 : int = aten::Int(%225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %227 : int[] = prim::ListConstruct(%214, %218, %222, %226), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %228 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %229 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%diff_token_type.5, %227, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.9 : Float(17:1092, 12:91, 7:13, 13:1) = aten::where(%token_type_mat.8, %210, %229), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.10 : Float(17:1092, 12:91, 7:13, 13:1) = aten::mul_(%token_type_attn.9, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:522:0
    %232 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
    %233 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%content_score.5, %positional_attn.30, %232), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
    %234 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
    %attn_score.13 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%233, %token_type_attn.10, %234), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
    %236 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
    %238 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
    %239 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %attn_score.14 : Float(17:1092, 12:91, 7:13, 13:1) = aten::to(%attn_score.13, %236, %237, %238, %239), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %244 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %245 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %241, %242, %243, %244), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %246 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %247 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%245, %246), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %248 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %249 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%247, %248), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %250 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %252 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %253 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %254 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%249, %250, %251, %252, %253), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:396:0
    %256 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:396:0
    %257 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%254, %255, %256), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:396:0
    %258 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %259 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%257, %258), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %260 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %attn_score.15 : Float(17:1092, 12:91, 7:13, 13:1) = aten::sub(%attn_score.14, %259, %260), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
    %262 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
    %263 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
    %input.42 : Float(17:1092, 12:91, 7:13, 13:1) = aten::softmax(%attn_score.15, %262, %263), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
    %280 : Tensor = prim::CallMethod[name="forward"](%10, %input.42)
    %266 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %267 : Tensor[] = prim::ListConstruct(%280, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %attn_vec.5 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%266, %267), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
    %269 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
    %270 : int[] = prim::ListConstruct(%22, %29, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
    %input.43 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.5, %270), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
    %281 : Tensor = prim::CallMethod[name="forward"](%9, %input.43)
    %282 : Tensor = prim::CallMethod[name="forward"](%8, %281)
    %274 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
    %input.45 : Float(17:5376, 7:768, 768:1) = aten::add(%query.4, %282, %274), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
    %283 : Tensor = prim::CallMethod[name="forward"](%7, %input.45)
    return (%283)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.79 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.79)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.79)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.79)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.79)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.79)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %9 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %11 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %13 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %15 : Float(17:21504, 7:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %17 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %18 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %21 : Float(17:21504, 7:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %input.47 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.47)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
    %input.50 : Float(17:5376, 7:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.50)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.75 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.42 : Float(17:1092, 12:91, 7:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:1092, 12:91, 7:13, 13:1) = aten::dropout(%input.42, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.5)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.73 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.73)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.73)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
    %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.45 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.46 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.45, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.46)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.linear.Linear,
        %input.43 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
    %output.23 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.43, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
    %input.44 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.23, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.44)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.72 : __torch__.torch.nn.modules.linear.Linear,
        %query.4 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.72)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.4, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.74)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.74)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
    %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.81 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.47 : Float(17:21504, 7:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.48 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.47, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.48)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
    %h.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.5)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.50 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.84)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.5 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.50, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.5)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.24 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.5 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.24, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.5)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:21504, 7:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.25 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.49 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.25, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.49)

FunnelLayer._actual_script_module
  graph(%self.85 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:5376, 7:768, 768:1),
        %2 : Float(14:768, 768:1),
        %cls_mask.3 : Float(7:14, 7:2),
        %token_type_mat.9 : Bool(17:98, 7:14, 7:2),
        %attention_mask.3 : Float(17:7, 7:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.85)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.85)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.3, %token_type_mat.9, %attention_mask.3)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.86 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:5376, 7:768, 768:1),
        %2 : Float(14:768, 768:1),
        %cls_mask.3 : Float(7:14, 7:2),
        %token_type_mat.9 : Bool(17:98, 7:14, 7:2),
        %attention_mask.3 : Float(17:7, 7:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.86)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.86)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.86)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.86)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.86)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.86)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.86)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.86)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.86)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.86)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.86)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.86)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
    %batch_size.16 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %21 : int = aten::Int(%batch_size.16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %22 : int = aten::Int(%batch_size.16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %23 : int = aten::Int(%batch_size.16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %24 : int = aten::Int(%batch_size.16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
    %seq_len.17 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %28 : int = aten::Int(%seq_len.17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %29 : int = aten::Int(%seq_len.17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
    %context_len.11 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %39 : int = aten::Int(%context_len.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %40 : int = aten::Int(%context_len.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %41 : int = aten::Int(%context_len.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %q_head.11 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %54 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %59 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
    %q_head.12 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.11, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.6 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_w_bias.6, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %content_score.6 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
    %v.6 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %86 : Float(14:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %v.6, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %positional_attn.31 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.31, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
    %batch_size.17 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %95 : int = aten::Int(%batch_size.17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %96 : int = aten::Int(%batch_size.17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.31, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
    %n_head.6 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %100 : int = aten::Int(%n_head.6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %101 : int = aten::Int(%n_head.6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.31, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
    %seq_len.18 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %105 : int = aten::Int(%seq_len.18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %106 : int = aten::Int(%seq_len.18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.31, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.6 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %110 : int = aten::Int(%max_rel_len.6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %positional_attn.32 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.31, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.32, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.33 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.6, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %positional_attn.34 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.33, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.35 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.34, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.36 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.35, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.9, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
    %batch_size.18 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %147 : int = aten::Int(%batch_size.18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.9, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
    %seq_len.19 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %151 : int = aten::Int(%seq_len.19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.9, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
    %context_len.12 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %155 : int = aten::Int(%context_len.12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.6 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_s_bias.6, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %162 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.12, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.10 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:371:0
    %diff_token_type.6 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.6 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.10, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.10, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.10, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.10, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.6, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.10, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.10, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.10, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.10, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.6, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.11 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.10, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.12 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.11, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.6, %positional_attn.36, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
    %attn_score.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%232, %token_type_attn.12, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %attn_score.17 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.16, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %253 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:396:0
    %256 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %attn_score.18 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.17, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
    %input.52 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.18, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.52)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %attn_vec.6 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
    %input.53 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.6, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.53)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
    %input.55 : Float(17:5376, 7:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.55)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.94 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.94)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.94)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.94)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.94)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.94)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %9 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %11 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %13 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %15 : Float(17:21504, 7:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %17 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %18 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %21 : Float(17:21504, 7:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %input.57 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.57)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
    %input.60 : Float(17:5376, 7:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.60)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.52 : Float(17:588, 12:49, 7:7, 7:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.52, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.6)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.88 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.88)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.88)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
    %output.26 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.26, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.55 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.56 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.55, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.56)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.91 : __torch__.torch.nn.modules.linear.Linear,
        %input.53 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.91)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.91)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
    %output.28 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.53, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
    %input.54 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.28, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.54)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
    %output.27 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.27, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.96 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.57 : Float(17:21504, 7:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.58 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.57, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.58)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
    %h.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.6)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.60 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.99)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.99)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.6 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.60, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.6)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.95 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.95)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.95)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.29 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.6 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.29, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.6)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.97 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:21504, 7:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.97)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.97)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.30 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.59 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.30, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.59)

FunnelLayer._actual_script_module
  graph(%self.100 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:5376, 7:768, 768:1),
        %2 : Float(14:768, 768:1),
        %cls_mask.3 : Float(7:14, 7:2),
        %token_type_mat.9 : Bool(17:98, 7:14, 7:2),
        %attention_mask.3 : Float(17:7, 7:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.100)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.100)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.3, %token_type_mat.9, %attention_mask.3)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.101 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:5376, 7:768, 768:1),
        %2 : Float(14:768, 768:1),
        %cls_mask.3 : Float(7:14, 7:2),
        %token_type_mat.9 : Bool(17:98, 7:14, 7:2),
        %attention_mask.3 : Float(17:7, 7:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.101)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.101)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.101)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.101)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.101)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.101)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.101)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.101)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.101)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.101)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.101)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.101)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
    %batch_size.19 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %21 : int = aten::Int(%batch_size.19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %22 : int = aten::Int(%batch_size.19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %23 : int = aten::Int(%batch_size.19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %24 : int = aten::Int(%batch_size.19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
    %seq_len.20 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %28 : int = aten::Int(%seq_len.20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %29 : int = aten::Int(%seq_len.20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
    %context_len.13 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %39 : int = aten::Int(%context_len.13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %40 : int = aten::Int(%context_len.13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %41 : int = aten::Int(%context_len.13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %q_head.13 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %54 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %59 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
    %q_head.14 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.13, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.7 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_w_bias.7, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %content_score.7 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
    %v.7 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %86 : Float(14:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %v.7, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %positional_attn.37 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.37, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
    %batch_size.20 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %95 : int = aten::Int(%batch_size.20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %96 : int = aten::Int(%batch_size.20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.37, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
    %n_head.7 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %100 : int = aten::Int(%n_head.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %101 : int = aten::Int(%n_head.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.37, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
    %seq_len.21 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %105 : int = aten::Int(%seq_len.21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %106 : int = aten::Int(%seq_len.21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.37, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.7 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %110 : int = aten::Int(%max_rel_len.7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %positional_attn.38 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.37, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.38, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.39 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.7, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %positional_attn.40 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.39, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.41 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.40, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.42 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.41, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.9, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
    %batch_size.21 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %147 : int = aten::Int(%batch_size.21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.9, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
    %seq_len.22 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %151 : int = aten::Int(%seq_len.22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.9, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
    %context_len.14 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %155 : int = aten::Int(%context_len.14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.7 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_s_bias.7, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %162 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.14, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.11 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:371:0
    %diff_token_type.7 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.7 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.11, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.11, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.11, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.11, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.7, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.11, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.11, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.11, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.11, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.7, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.13 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.11, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.14 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.13, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.7, %positional_attn.42, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
    %attn_score.19 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%232, %token_type_attn.14, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %attn_score.20 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.19, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %253 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:396:0
    %256 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %attn_score.21 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.20, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
    %input.61 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.21, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.61)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %attn_vec.7 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
    %input.62 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.7, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.62)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
    %input.64 : Float(17:5376, 7:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.64)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.109 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.109)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.109)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.109)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.109)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.109)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %9 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %11 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %13 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %15 : Float(17:21504, 7:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %17 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %18 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %21 : Float(17:21504, 7:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %input.66 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.66)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
    %input.69 : Float(17:5376, 7:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.69)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.105 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.61 : Float(17:588, 12:49, 7:7, 7:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.61, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.107 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.7)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.103 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.103)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.103)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
    %output.31 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.31, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.64 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.65 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.64, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.65)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.106 : __torch__.torch.nn.modules.linear.Linear,
        %input.62 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.106)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.106)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
    %output.33 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.62, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
    %input.63 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.33, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.63)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
    %output.32 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.32, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.66 : Float(17:21504, 7:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.67 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.66, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.67)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
    %h.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.7)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.7 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.69, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.7)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.34 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.7 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.34, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.7)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:21504, 7:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.112)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.112)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.35 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.68 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.35, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.68)

FunnelLayer._actual_script_module
  graph(%self.115 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:5376, 7:768, 768:1),
        %2 : Float(14:768, 768:1),
        %cls_mask.3 : Float(7:14, 7:2),
        %token_type_mat.9 : Bool(17:98, 7:14, 7:2),
        %attention_mask.3 : Float(17:7, 7:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.115)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.115)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.3, %token_type_mat.9, %attention_mask.3)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.116 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:5376, 7:768, 768:1),
        %2 : Float(14:768, 768:1),
        %cls_mask.3 : Float(7:14, 7:2),
        %token_type_mat.9 : Bool(17:98, 7:14, 7:2),
        %attention_mask.3 : Float(17:7, 7:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.116)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.116)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.116)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.116)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.116)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.116)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.116)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.116)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.116)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.116)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.116)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.116)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
    %batch_size.22 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %21 : int = aten::Int(%batch_size.22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %22 : int = aten::Int(%batch_size.22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %23 : int = aten::Int(%batch_size.22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %24 : int = aten::Int(%batch_size.22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
    %seq_len.23 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %28 : int = aten::Int(%seq_len.23), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %29 : int = aten::Int(%seq_len.23), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
    %context_len.15 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %39 : int = aten::Int(%context_len.15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %40 : int = aten::Int(%context_len.15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %41 : int = aten::Int(%context_len.15), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %q_head.15 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %54 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %59 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
    %q_head.16 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.15, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.8 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_w_bias.8, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %content_score.8 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
    %v.8 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %86 : Float(14:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %v.8, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %positional_attn.43 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.43, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
    %batch_size.23 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %95 : int = aten::Int(%batch_size.23), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %96 : int = aten::Int(%batch_size.23), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.43, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
    %n_head.8 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %100 : int = aten::Int(%n_head.8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %101 : int = aten::Int(%n_head.8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.43, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
    %seq_len.24 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %105 : int = aten::Int(%seq_len.24), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %106 : int = aten::Int(%seq_len.24), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.43, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.8 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %110 : int = aten::Int(%max_rel_len.8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %positional_attn.44 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.43, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.44, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.45 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.8, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %positional_attn.46 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.45, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.47 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.46, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.48 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.47, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.9, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
    %batch_size.24 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %147 : int = aten::Int(%batch_size.24), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.9, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
    %seq_len.25 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %151 : int = aten::Int(%seq_len.25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.9, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
    %context_len.16 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %155 : int = aten::Int(%context_len.16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.8 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_s_bias.8, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %162 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.16, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.12 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:371:0
    %diff_token_type.8 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.8 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.12, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.12, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.12, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.12, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.8, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.12, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.12, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.12, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.12, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.8, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.15 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.12, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.15, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.8, %positional_attn.48, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
    %attn_score.22 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%232, %token_type_attn.16, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %attn_score.23 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.22, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %253 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:396:0
    %256 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %attn_score.24 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.23, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
    %input.70 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.24, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.70)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %attn_vec.8 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
    %input.71 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.8, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.71)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
    %input.73 : Float(17:5376, 7:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.73)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.124 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.124)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.124)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.124)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.124)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.124)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %9 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %11 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %13 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %15 : Float(17:21504, 7:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %17 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %18 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %21 : Float(17:21504, 7:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %input.75 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.75)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
    %input.78 : Float(17:5376, 7:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.78)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.70 : Float(17:588, 12:49, 7:7, 7:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.70, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.8)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.118 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.118)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.118)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
    %output.36 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.36, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.73 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.74 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.73, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.74)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.121 : __torch__.torch.nn.modules.linear.Linear,
        %input.71 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.121)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.121)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
    %output.38 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.71, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
    %input.72 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.38, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.72)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.117 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.117)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.119)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.119)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
    %output.37 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.37, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.126 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.75 : Float(17:21504, 7:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.76 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.75, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.76)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.128 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
    %h.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.8)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.129 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.78 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.129)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.129)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %hidden : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.78, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%hidden)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.125)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.125)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.39 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.8 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.39, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.8)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.127 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:21504, 7:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.127)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.127)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.40 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.77 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.40, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.77)

FunnelLayer._actual_script_module
  graph(%self.130 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %query.8 : Float(17:3072, 4:768, 768:1),
        %2 : Float(17:5376, 7:768, 768:1),
        %3 : Float(15:768, 768:1),
        %cls_mask.4 : Float(4:14, 7:1),
        %token_type_mat.13 : Bool(17:56, 4:14, 7:1),
        %attention_mask.3 : Float(17:7, 7:1)):
    %7 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.130)
    %8 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.130)
    %11 : Tensor = prim::CallMethod[name="forward"](%8, %query.8, %2, %3, %cls_mask.4, %token_type_mat.13, %attention_mask.3)
    %12 : Tensor = prim::CallMethod[name="forward"](%7, %11)
    return (%12)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.131 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %query.8 : Float(17:3072, 4:768, 768:1),
        %2 : Float(17:5376, 7:768, 768:1),
        %3 : Float(15:768, 768:1),
        %cls_mask.4 : Float(4:14, 7:1),
        %token_type_mat.13 : Bool(17:56, 4:14, 7:1),
        %attention_mask.3 : Float(17:7, 7:1)):
    %7 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.131)
    %8 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.131)
    %9 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.131)
    %10 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.131)
    %11 : Tensor = prim::GetAttr[name="seg_embed"](%self.131)
    %12 : Tensor = prim::GetAttr[name="r_s_bias"](%self.131)
    %13 : Tensor = prim::GetAttr[name="r_kernel"](%self.131)
    %14 : Tensor = prim::GetAttr[name="r_r_bias"](%self.131)
    %15 : Tensor = prim::GetAttr[name="r_w_bias"](%self.131)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.131)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.131)
    %18 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.131)
    %19 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
    %20 : int = aten::size(%query.8, %19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
    %batch_size.25 : Long() = prim::NumToTensor(%20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %22 : int = aten::Int(%batch_size.25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %23 : int = aten::Int(%batch_size.25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %24 : int = aten::Int(%batch_size.25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %25 : int = aten::Int(%batch_size.25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
    %27 : int = aten::size(%query.8, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
    %seq_len.26 : Long() = prim::NumToTensor(%27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %29 : int = aten::Int(%seq_len.26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %30 : int = aten::Int(%seq_len.26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %37 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
    %38 : int = aten::size(%2, %37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
    %context_len.17 : Long() = prim::NumToTensor(%38), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %40 : int = aten::Int(%context_len.17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %41 : int = aten::Int(%context_len.17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %42 : int = aten::Int(%context_len.17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %277 : Tensor = prim::CallMethod[name="forward"](%18, %query.8)
    %47 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
    %48 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
    %49 : int[] = prim::ListConstruct(%25, %30, %47, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %q_head.17 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%277, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
    %278 : Tensor = prim::CallMethod[name="forward"](%17, %2)
    %52 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
    %53 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
    %54 : int[] = prim::ListConstruct(%24, %42, %52, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %55 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%278, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
    %279 : Tensor = prim::CallMethod[name="forward"](%16, %2)
    %57 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
    %58 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
    %59 : int[] = prim::ListConstruct(%23, %41, %57, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %60 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
    %61 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
    %q_head.18 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.17, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
    %63 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.9 : Float(12:64, 64:1) = aten::mul(%15, %63), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
    %65 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
    %66 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_w_bias.9, %65), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
    %67 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %68 : Tensor[] = prim::ListConstruct(%66, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %content_score.9 : Float(17:336, 12:28, 4:7, 7:1) = aten::einsum(%67, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %83 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
    %v.9 : Float(12:64, 64:1) = aten::mul(%14, %83), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
    %85 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %86 : Tensor[] = prim::ListConstruct(%3, %13), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %87 : Float(15:768, 12:64, 64:1) = aten::einsum(%85, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %88 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
    %89 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %v.9, %88), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
    %90 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %91 : Tensor[] = prim::ListConstruct(%89, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %positional_attn.49 : Float(17:60, 12:1020, 4:15, 15:1) = aten::einsum(%90, %91), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %93 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
    %94 : int = aten::size(%positional_attn.49, %93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
    %batch_size.26 : Long() = prim::NumToTensor(%94), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %96 : int = aten::Int(%batch_size.26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %97 : int = aten::Int(%batch_size.26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %98 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
    %99 : int = aten::size(%positional_attn.49, %98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
    %n_head.9 : Long() = prim::NumToTensor(%99), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %101 : int = aten::Int(%n_head.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %102 : int = aten::Int(%n_head.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %103 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
    %104 : int = aten::size(%positional_attn.49, %103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
    %seq_len.27 : Long() = prim::NumToTensor(%104), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %106 : int = aten::Int(%seq_len.27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %107 : int = aten::Int(%seq_len.27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %108 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
    %109 : int = aten::size(%positional_attn.49, %108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.9 : Long() = prim::NumToTensor(%109), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %111 : int = aten::Int(%max_rel_len.9), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %112 : int[] = prim::ListConstruct(%97, %102, %111, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %positional_attn.50 : Float(17:60, 12:1020, 15:4, 4:1) = aten::reshape(%positional_attn.49, %112), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:428:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %117 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %118 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%positional_attn.50, %114, %115, %116, %117), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %122 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %123 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%118, %119, %120, %121, %122), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %127 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %128 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%123, %124, %125, %126, %127), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %132 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.51 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%128, %129, %130, %131, %132), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
    %134 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
    %135 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
    %136 : Long() = aten::sub(%max_rel_len.9, %134, %135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
    %137 : int = aten::Int(%136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %138 : int[] = prim::ListConstruct(%96, %101, %106, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %positional_attn.52 : Float(17:60, 12:1020, 4:13, 13:1) = aten::reshape(%positional_attn.51, %138), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
    %140 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
    %142 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.53 : Float(17:60, 12:1020, 4:13, 7:1) = aten::slice(%positional_attn.52, %140, %141, %40, %142), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.54 : Float(17:60, 12:1020, 4:13, 7:1) = aten::mul_(%positional_attn.53, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:498:0
    %145 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
    %146 : int = aten::size(%token_type_mat.13, %145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
    %batch_size.27 : Long() = prim::NumToTensor(%146), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %148 : int = aten::Int(%batch_size.27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %149 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
    %150 : int = aten::size(%token_type_mat.13, %149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
    %seq_len.28 : Long() = prim::NumToTensor(%150), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %152 : int = aten::Int(%seq_len.28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %153 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
    %154 : int = aten::size(%token_type_mat.13, %153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
    %context_len.18 : Long() = prim::NumToTensor(%154), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %156 : int = aten::Int(%context_len.18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %157 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.9 : Float(12:64, 64:1) = aten::mul(%12, %157), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
    %159 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
    %160 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_s_bias.9, %159), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
    %161 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %162 : Tensor[] = prim::ListConstruct(%160, %11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %163 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%161, %162), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %167 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %168 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %164, %165, %166, %167), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %169 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %170 : Bool(17:56, 1:56, 4:14, 7:1) = aten::unsqueeze(%168, %169), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %177 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %178 : int = aten::size(%q_head.18, %177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %179 : Long() = prim::NumToTensor(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %180 : int = aten::Int(%179), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %184 : int[] = prim::ListConstruct(%148, %180, %152, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %185 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.14 : Bool(17:56, 12:0, 4:14, 7:1) = aten::expand(%170, %184, %185), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
    %187 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:371:0
    %188 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:371:0
    %189 : Tensor[] = aten::split(%163, %187, %188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:371:0
    %diff_token_type.9 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.9 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%189), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %192 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %193 : int = aten::size(%token_type_mat.14, %192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %194 : Long() = prim::NumToTensor(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %195 : int = aten::Int(%194), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %196 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %197 : int = aten::size(%token_type_mat.14, %196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %198 : Long() = prim::NumToTensor(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %199 : int = aten::Int(%198), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %200 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %201 : int = aten::size(%token_type_mat.14, %200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %202 : Long() = prim::NumToTensor(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %203 : int = aten::Int(%202), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %204 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %205 : int = aten::size(%token_type_mat.14, %204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %206 : Long() = prim::NumToTensor(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %207 : int = aten::Int(%206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %208 : int[] = prim::ListConstruct(%195, %199, %203, %207), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %209 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %210 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%same_token_type.9, %208, %209), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %211 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %212 : int = aten::size(%token_type_mat.14, %211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %213 : Long() = prim::NumToTensor(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %214 : int = aten::Int(%213), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %215 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %216 : int = aten::size(%token_type_mat.14, %215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %217 : Long() = prim::NumToTensor(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %218 : int = aten::Int(%217), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %219 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %220 : int = aten::size(%token_type_mat.14, %219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %221 : Long() = prim::NumToTensor(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %222 : int = aten::Int(%221), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %223 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %224 : int = aten::size(%token_type_mat.14, %223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %225 : Long() = prim::NumToTensor(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %226 : int = aten::Int(%225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %227 : int[] = prim::ListConstruct(%214, %218, %222, %226), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %228 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %229 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%diff_token_type.9, %227, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.17 : Float(17:336, 12:28, 4:7, 7:1) = aten::where(%token_type_mat.14, %210, %229), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.18 : Float(17:336, 12:28, 4:7, 7:1) = aten::mul_(%token_type_attn.17, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:522:0
    %232 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
    %233 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%content_score.9, %positional_attn.54, %232), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
    %234 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
    %attn_score.25 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%233, %token_type_attn.18, %234), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
    %236 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
    %238 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
    %239 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %attn_score.26 : Float(17:336, 12:28, 4:7, 7:1) = aten::to(%attn_score.25, %236, %237, %238, %239), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %244 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %245 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %241, %242, %243, %244), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %246 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %247 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%245, %246), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %248 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %249 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%247, %248), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %250 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %252 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %253 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %254 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%249, %250, %251, %252, %253), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:396:0
    %256 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:396:0
    %257 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%254, %255, %256), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:396:0
    %258 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %259 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%257, %258), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %260 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %attn_score.27 : Float(17:336, 12:28, 4:7, 7:1) = aten::sub(%attn_score.26, %259, %260), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
    %262 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
    %263 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
    %input.79 : Float(17:336, 12:28, 4:7, 7:1) = aten::softmax(%attn_score.27, %262, %263), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
    %280 : Tensor = prim::CallMethod[name="forward"](%10, %input.79)
    %266 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %267 : Tensor[] = prim::ListConstruct(%280, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %attn_vec.9 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%266, %267), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
    %269 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
    %270 : int[] = prim::ListConstruct(%22, %29, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
    %input.80 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.9, %270), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
    %281 : Tensor = prim::CallMethod[name="forward"](%9, %input.80)
    %282 : Tensor = prim::CallMethod[name="forward"](%8, %281)
    %274 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
    %input.82 : Float(17:3072, 4:768, 768:1) = aten::add(%query.8, %282, %274), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
    %283 : Tensor = prim::CallMethod[name="forward"](%7, %input.82)
    return (%283)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.139 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.139)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.139)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.139)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.139)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.139)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %9 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %11 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %13 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %15 : Float(17:12288, 4:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %17 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %18 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %21 : Float(17:12288, 4:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %input.84 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.84)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
    %input.87 : Float(17:3072, 4:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.87)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.135 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.79 : Float(17:336, 12:28, 4:7, 7:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:336, 12:28, 4:7, 7:1) = aten::dropout(%input.79, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.137 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.9)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.133 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.133)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.133)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
    %output.41 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.41, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.138 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.82 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.138)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.138)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.83 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.82, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.83)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.136 : __torch__.torch.nn.modules.linear.Linear,
        %input.80 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.136)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.136)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
    %output.43 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.80, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
    %input.81 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.43, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.81)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.132 : __torch__.torch.nn.modules.linear.Linear,
        %query.8 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.132)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.8, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:5376, 7:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.134)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.134)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
    %output.42 : Float(17:5376, 7:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.42, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.141 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.84 : Float(17:12288, 4:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.85 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.84, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.85)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.143 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
    %h.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.9)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.144 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.87 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.144)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.144)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.9 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.87, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.9)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.140)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.140)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.44 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.9 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.44, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.9)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.142 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:12288, 4:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.142)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.142)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.45 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.86 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.45, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.86)

FunnelLayer._actual_script_module
  graph(%self.145 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:3072, 4:768, 768:1),
        %2 : Float(8:768, 768:1),
        %cls_mask.5 : Float(4:8, 4:2),
        %token_type_mat.15 : Bool(17:32, 4:8, 4:2),
        %attention_mask : Float(17:4, 4:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.145)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.145)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.5, %token_type_mat.15, %attention_mask)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.146 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:3072, 4:768, 768:1),
        %2 : Float(8:768, 768:1),
        %cls_mask.5 : Float(4:8, 4:2),
        %token_type_mat.15 : Bool(17:32, 4:8, 4:2),
        %attention_mask : Float(17:4, 4:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.146)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.146)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.146)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.146)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.146)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.146)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.146)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.146)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.146)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.146)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.146)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.146)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
    %batch_size.28 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %21 : int = aten::Int(%batch_size.28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %22 : int = aten::Int(%batch_size.28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %23 : int = aten::Int(%batch_size.28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %24 : int = aten::Int(%batch_size.28), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
    %seq_len.29 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %28 : int = aten::Int(%seq_len.29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %29 : int = aten::Int(%seq_len.29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
    %context_len.19 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %39 : int = aten::Int(%context_len.19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %40 : int = aten::Int(%context_len.19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %41 : int = aten::Int(%context_len.19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %q_head.19 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %54 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %59 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
    %q_head.20 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.19, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.10 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_w_bias.10, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %content_score.10 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
    %v.10 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %86 : Float(8:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %v.10, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %positional_attn.55 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.55, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
    %batch_size.29 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %95 : int = aten::Int(%batch_size.29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %96 : int = aten::Int(%batch_size.29), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.55, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
    %n_head.10 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %100 : int = aten::Int(%n_head.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %101 : int = aten::Int(%n_head.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.55, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
    %seq_len.30 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %105 : int = aten::Int(%seq_len.30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %106 : int = aten::Int(%seq_len.30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.55, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.10 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %110 : int = aten::Int(%max_rel_len.10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %positional_attn.56 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.55, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.56, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.57 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.10, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %positional_attn.58 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.57, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.59 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.58, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.60 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.59, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.15, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
    %batch_size.30 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %147 : int = aten::Int(%batch_size.30), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.15, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
    %seq_len.31 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %151 : int = aten::Int(%seq_len.31), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.15, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
    %context_len.20 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %155 : int = aten::Int(%context_len.20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.10 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_s_bias.10, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %162 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.20, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.16 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:371:0
    %diff_token_type.10 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.10 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.16, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.16, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.16, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.16, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.10, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.16, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.16, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.16, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.16, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.10, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.19 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.16, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.20 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.19, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.10, %positional_attn.60, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
    %attn_score.28 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%232, %token_type_attn.20, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %attn_score.29 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.28, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:4, 4:1) = aten::slice(%attention_mask, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %253 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:396:0
    %256 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %attn_score.30 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.29, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
    %input.89 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.30, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.89)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %attn_vec.10 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
    %input.90 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.10, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.90)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
    %input.92 : Float(17:3072, 4:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.92)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.154 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.154)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.154)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.154)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.154)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.154)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %9 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %11 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %13 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %15 : Float(17:12288, 4:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %17 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %18 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %21 : Float(17:12288, 4:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %input.94 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.94)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
    %input.97 : Float(17:3072, 4:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.97)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.150 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.89 : Float(17:192, 12:16, 4:4, 4:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.89, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.152 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.10)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.148 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.148)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.148)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
    %output.46 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.46, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.153 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.92 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.153)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.153)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.93 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.92, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.93)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.151 : __torch__.torch.nn.modules.linear.Linear,
        %input.90 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.151)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.151)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
    %output.48 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.90, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
    %input.91 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.48, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.91)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.147 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.147)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.149 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.149)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.149)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
    %output.47 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.47, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.156 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.94 : Float(17:12288, 4:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.95 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.94, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.95)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.158 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
    %h.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.10)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.159 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.97 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.159)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.159)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.10 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.97, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.10)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.155)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.155)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.49 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.10 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.49, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.10)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.157 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:12288, 4:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.157)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.157)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.50 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.96 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.50, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.96)

FunnelLayer._actual_script_module
  graph(%self.160 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:3072, 4:768, 768:1),
        %2 : Float(8:768, 768:1),
        %cls_mask.5 : Float(4:8, 4:2),
        %token_type_mat.15 : Bool(17:32, 4:8, 4:2),
        %attention_mask : Float(17:4, 4:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.160)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.160)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.5, %token_type_mat.15, %attention_mask)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.161 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:3072, 4:768, 768:1),
        %2 : Float(8:768, 768:1),
        %cls_mask.5 : Float(4:8, 4:2),
        %token_type_mat.15 : Bool(17:32, 4:8, 4:2),
        %attention_mask : Float(17:4, 4:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.161)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.161)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.161)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.161)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.161)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.161)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.161)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.161)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.161)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.161)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.161)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.161)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
    %batch_size.31 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %21 : int = aten::Int(%batch_size.31), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %22 : int = aten::Int(%batch_size.31), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %23 : int = aten::Int(%batch_size.31), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %24 : int = aten::Int(%batch_size.31), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
    %seq_len.32 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %28 : int = aten::Int(%seq_len.32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %29 : int = aten::Int(%seq_len.32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
    %context_len.21 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %39 : int = aten::Int(%context_len.21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %40 : int = aten::Int(%context_len.21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %41 : int = aten::Int(%context_len.21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %q_head.21 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %54 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %59 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
    %q_head.22 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.21, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.11 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_w_bias.11, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %content_score.11 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
    %v.11 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %86 : Float(8:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %v.11, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %positional_attn.61 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.61, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
    %batch_size.32 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %95 : int = aten::Int(%batch_size.32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %96 : int = aten::Int(%batch_size.32), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.61, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
    %n_head.11 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %100 : int = aten::Int(%n_head.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %101 : int = aten::Int(%n_head.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.61, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
    %seq_len.33 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %105 : int = aten::Int(%seq_len.33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %106 : int = aten::Int(%seq_len.33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.61, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.11 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %110 : int = aten::Int(%max_rel_len.11), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %positional_attn.62 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.61, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.62, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.63 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.11, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %positional_attn.64 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.63, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.65 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.64, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.66 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.65, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.15, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
    %batch_size.33 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %147 : int = aten::Int(%batch_size.33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.15, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
    %seq_len.34 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %151 : int = aten::Int(%seq_len.34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.15, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
    %context_len.22 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %155 : int = aten::Int(%context_len.22), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.11 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_s_bias.11, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %162 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.22, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.17 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:371:0
    %diff_token_type.11 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.11 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.17, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.17, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.17, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.17, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.11, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.17, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.17, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.17, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.17, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.11, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.21 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.17, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.22 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.21, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.11, %positional_attn.66, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
    %attn_score.31 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%232, %token_type_attn.22, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %attn_score.32 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.31, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:4, 4:1) = aten::slice(%attention_mask, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %253 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:396:0
    %256 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %attn_score.33 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.32, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
    %input.98 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.33, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.98)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %attn_vec.11 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
    %input.99 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.11, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.99)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
    %input.101 : Float(17:3072, 4:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.101)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.169 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.169)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.169)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.169)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.169)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.169)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %9 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %11 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %13 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %15 : Float(17:12288, 4:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %17 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %18 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %21 : Float(17:12288, 4:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %input.103 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.103)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
    %input.106 : Float(17:3072, 4:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.106)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.165 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.98 : Float(17:192, 12:16, 4:4, 4:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.98, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.11)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.163 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.163)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.163)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
    %output.51 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.51, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.168 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.101 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.168)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.168)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.102 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.101, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.102)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.166 : __torch__.torch.nn.modules.linear.Linear,
        %input.99 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.166)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.166)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
    %output.53 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.99, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
    %input.100 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.53, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.100)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.162 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.162)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.164 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.164)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.164)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
    %output.52 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.52, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.171 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.103 : Float(17:12288, 4:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.104 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.103, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.104)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.173 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
    %h.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.11)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.174 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.106 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.174)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.174)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query.11 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.106, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query.11)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.170 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.170)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.170)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.54 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.11 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.54, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.11)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.172 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:12288, 4:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.172)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.172)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.55 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.105 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.55, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.105)

FunnelLayer._actual_script_module
  graph(%self.175 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:3072, 4:768, 768:1),
        %2 : Float(8:768, 768:1),
        %cls_mask.5 : Float(4:8, 4:2),
        %token_type_mat.15 : Bool(17:32, 4:8, 4:2),
        %attention_mask : Float(17:4, 4:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.175)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.175)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask.5, %token_type_mat.15, %attention_mask)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.176 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:3072, 4:768, 768:1),
        %2 : Float(8:768, 768:1),
        %cls_mask.5 : Float(4:8, 4:2),
        %token_type_mat.15 : Bool(17:32, 4:8, 4:2),
        %attention_mask : Float(17:4, 4:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.176)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.176)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.176)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.176)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.176)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.176)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.176)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.176)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.176)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.176)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.176)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.176)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
    %batch_size.34 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %21 : int = aten::Int(%batch_size.34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %22 : int = aten::Int(%batch_size.34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %23 : int = aten::Int(%batch_size.34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %24 : int = aten::Int(%batch_size.34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
    %seq_len.35 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %28 : int = aten::Int(%seq_len.35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %29 : int = aten::Int(%seq_len.35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
    %context_len.23 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %39 : int = aten::Int(%context_len.23), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %40 : int = aten::Int(%context_len.23), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %41 : int = aten::Int(%context_len.23), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %q_head.23 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %54 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %59 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
    %q_head.24 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.23, %60), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.12 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_w_bias.12, %64), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %content_score.12 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
    %v.12 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %86 : Float(8:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %v.12, %87), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %positional_attn.67 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.67, %92), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
    %batch_size.35 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %95 : int = aten::Int(%batch_size.35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %96 : int = aten::Int(%batch_size.35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.67, %97), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
    %n_head.12 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %100 : int = aten::Int(%n_head.12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %101 : int = aten::Int(%n_head.12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.67, %102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
    %seq_len.36 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %105 : int = aten::Int(%seq_len.36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %106 : int = aten::Int(%seq_len.36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.67, %107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.12 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %110 : int = aten::Int(%max_rel_len.12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %positional_attn.68 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.67, %111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.68, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.69 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.12, %133, %134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %positional_attn.70 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.69, %137), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.71 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.70, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.72 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.71, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.15, %144), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
    %batch_size.36 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %147 : int = aten::Int(%batch_size.36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.15, %148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
    %seq_len.37 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %151 : int = aten::Int(%seq_len.37), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.15, %152), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
    %context_len.24 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %155 : int = aten::Int(%context_len.24), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.12 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_s_bias.12, %158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %162 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.24, %176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.18 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:371:0
    %diff_token_type.12 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.12 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.18, %191), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.18, %195), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.18, %199), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.18, %203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.12, %207, %208), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.18, %210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.18, %214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.18, %218), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.18, %222), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.12, %226, %227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.23 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.18, %209, %228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.24 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.23, %cls_mask.5), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.12, %positional_attn.72, %231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
    %attn_score.34 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%232, %token_type_attn.24, %233), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %attn_score.35 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.34, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %244 : Float(17:4, 4:1) = aten::slice(%attention_mask, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %246 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %248 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %253 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:396:0
    %256 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %attn_score.36 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.35, %258, %259), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
    %input.107 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.36, %261, %262), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.107)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %attn_vec.12 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
    %input.108 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.12, %269), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.108)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
    %input.110 : Float(17:3072, 4:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.110)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.184 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.184)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.184)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.184)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.184)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.184)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %9 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %11 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %13 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %15 : Float(17:12288, 4:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %17 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %18 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %21 : Float(17:12288, 4:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %input.112 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.112)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
    %input.115 : Float(17:3072, 4:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.115)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.180 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.107 : Float(17:192, 12:16, 4:4, 4:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.107, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.182 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.12)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.178)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.178)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
    %output.56 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.56, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.183 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.110 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.183)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.183)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.111 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.110, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.111)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.181 : __torch__.torch.nn.modules.linear.Linear,
        %input.108 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.181)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.181)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
    %output.58 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.108, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
    %input.109 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.58, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.109)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.177 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.177)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.179 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.179)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.179)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
    %output.57 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.57, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.186 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.112 : Float(17:12288, 4:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.113 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.112, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.113)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.188 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
    %h.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.12)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.115 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.189)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.189)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    %x.13 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.115, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%x.13)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.185 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3072, 4:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.185)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.185)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.59 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.12 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.59, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.12)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.187 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:12288, 4:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.187)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.187)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.60 : Float(17:3072, 4:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.114 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.60, %2, %6), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.114)

FunnelAttentionStructure.cos_dropout
Dropout._actual_script_module
  graph(%self.192 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.118 : Float(52:384, 384:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
    %cos_embed : Float(52:384, 384:1) = aten::dropout(%input.118, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
    return (%cos_embed)

FunnelAttentionStructure.sin_dropout
Dropout._actual_script_module
  graph(%self.191 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.117 : Float(52:384, 384:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
    %sin_embed : Float(52:384, 384:1) = aten::dropout(%input.117, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
    return (%sin_embed)

FunnelLayer._actual_script_module
  graph(%self.193 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %input_embeds : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask : Float(13:13, 13:1),
        %token_type_mat.20 : Bool(17:169, 13:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.193)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.193)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %input_embeds, %2, %cls_mask, %token_type_mat.20, %attention_mask.1)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.194 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %input_embeds : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask : Float(13:13, 13:1),
        %token_type_mat.20 : Bool(17:169, 13:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.194)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.194)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.194)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.194)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.194)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.194)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.194)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.194)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.194)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.194)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.194)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.194)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%input_embeds, %18), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
    %batch_size.37 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %21 : int = aten::Int(%batch_size.37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %22 : int = aten::Int(%batch_size.37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %23 : int = aten::Int(%batch_size.37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %24 : int = aten::Int(%batch_size.37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%input_embeds, %25), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
    %seq_len.39 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %28 : int = aten::Int(%seq_len.39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %29 : int = aten::Int(%seq_len.39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%input_embeds, %36), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:531:0
    %context_len.25 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %39 : int = aten::Int(%context_len.25), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %40 : int = aten::Int(%context_len.25), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %41 : int = aten::Int(%context_len.25), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %input_embeds)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %q_head.25 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %input_embeds)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %input_embeds)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %59 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:540:0
    %q_head.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.25, %60), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias.13 : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_w_bias.13, %64), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %content_score.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:486:0
    %v.13 : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %86 : Float(26:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %v.13, %87), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %positional_attn.73 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.73, %92), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
    %batch_size.38 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %95 : int = aten::Int(%batch_size.38), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %96 : int = aten::Int(%batch_size.38), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.73, %97), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
    %n_head.13 : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %100 : int = aten::Int(%n_head.13), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %101 : int = aten::Int(%n_head.13), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.73, %102), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
    %seq_len.40 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %105 : int = aten::Int(%seq_len.40), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %106 : int = aten::Int(%seq_len.40), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.73, %107), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len.13 : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %110 : int = aten::Int(%max_rel_len.13), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %positional_attn.74 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.73, %111), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.74, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.75 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len.13, %133, %134), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %positional_attn.76 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.75, %137), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.77 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.76, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.78 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.77, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.20, %144), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
    %batch_size.39 : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %147 : int = aten::Int(%batch_size.39), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.20, %148), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
    %seq_len.41 : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %151 : int = aten::Int(%seq_len.41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.20, %152), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
    %context_len.26 : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %155 : int = aten::Int(%context_len.26), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias.13 : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_s_bias.13, %158), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %162 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head.26, %176), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat.21 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:371:0
    %diff_token_type.13 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.13 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat.21, %191), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat.21, %195), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat.21, %199), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat.21, %203), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.13, %207, %208), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat.21, %210), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat.21, %214), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat.21, %218), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat.21, %222), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.13, %226, %227), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.25 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.21, %209, %228), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.25, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.13, %positional_attn.78, %231), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
    %attn_score.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%232, %token_type_attn.26, %233), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %attn_score.38 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.37, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %244 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %246 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %248 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %253 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:396:0
    %256 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %attn_score.39 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.38, %258, %259), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:558:0
    %input.120 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.39, %261, %262), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.120)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %attn_vec.13 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention
    %input.121 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.13, %269), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.121)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:568:0
    %input.123 : Float(17:9984, 13:768, 768:1) = aten::add(%input_embeds, %281, %273), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.123)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.202 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.202)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.202)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.202)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.202)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.202)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %9 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %11 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %13 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %15 : Float(17:39936, 13:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %17 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %18 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %21 : Float(17:39936, 13:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %input.125 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.125)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/modeling_funnel.py:588:0
    %input.128 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.128)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.198 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.120 : Float(17:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.120, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.200 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out.13)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.196 : __torch__.torch.nn.modules.linear.Linear,
        %input_embeds : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.196)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.196)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
    %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.201 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.123 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.201)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.201)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.124 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.123, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.124)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.199 : __torch__.torch.nn.modules.linear.Linear,
        %input.121 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.199)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.199)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
    %output.65 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.121, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1678:0
    %input.122 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.65, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.122)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.195 : __torch__.torch.nn.modules.linear.Linear,
        %input_embeds : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.195)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.197 : __torch__.torch.nn.modules.linear.Linear,
        %input_embeds : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.197)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.197)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
    %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.attention/__module.funnel.decoder.layers.0.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.204 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.125 : Float(17:39936, 13:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.126 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.125, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.126)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.206 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.dropout # torch/nn/functional.py:973:0
    %h.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.dropout # torch/nn/functional.py:973:0
    return (%h.13)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.207 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.128 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.207)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.207)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    %query : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.128, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%query)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.203 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.203)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.203)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.66 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x.15 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.66, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x.15)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.205 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.205)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.205)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.127 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.0/__module.funnel.decoder.layers.0.ffn/__module.funnel.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.127)

FunnelLayer._actual_script_module
  graph(%self.208 : __torch__.transformers.modeling_funnel.FunnelLayer,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask : Float(13:13, 13:1),
        %token_type_mat.20 : Bool(17:169, 13:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %6 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%self.208)
    %7 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%self.208)
    %10 : Tensor = prim::CallMethod[name="forward"](%7, %1, %2, %cls_mask, %token_type_mat.20, %attention_mask.1)
    %11 : Tensor = prim::CallMethod[name="forward"](%6, %10)
    return (%11)

FunnelLayer.attention
FunnelRelMultiheadAttention._actual_script_module
  graph(%self.209 : __torch__.transformers.modeling_funnel.FunnelRelMultiheadAttention,
        %1 : Float(17:9984, 13:768, 768:1),
        %2 : Float(26:768, 768:1),
        %cls_mask : Float(13:13, 13:1),
        %token_type_mat.20 : Bool(17:169, 13:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.209)
    %7 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="hidden_dropout"](%self.209)
    %8 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="post_proj"](%self.209)
    %9 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="attention_dropout"](%self.209)
    %10 : Tensor = prim::GetAttr[name="seg_embed"](%self.209)
    %11 : Tensor = prim::GetAttr[name="r_s_bias"](%self.209)
    %12 : Tensor = prim::GetAttr[name="r_kernel"](%self.209)
    %13 : Tensor = prim::GetAttr[name="r_r_bias"](%self.209)
    %14 : Tensor = prim::GetAttr[name="r_w_bias"](%self.209)
    %15 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_head"](%self.209)
    %16 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_head"](%self.209)
    %17 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_head"](%self.209)
    %18 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
    %19 : int = aten::size(%1, %18), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
    %batch_size.40 : Long() = prim::NumToTensor(%19), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %21 : int = aten::Int(%batch_size.40), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %22 : int = aten::Int(%batch_size.40), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %23 : int = aten::Int(%batch_size.40), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %24 : int = aten::Int(%batch_size.40), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %25 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
    %26 : int = aten::size(%1, %25), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
    %seq_len.42 : Long() = prim::NumToTensor(%26), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %28 : int = aten::Int(%seq_len.42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %29 : int = aten::Int(%seq_len.42), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %36 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:531:0
    %37 : int = aten::size(%1, %36), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:531:0
    %context_len.27 : Long() = prim::NumToTensor(%37), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %39 : int = aten::Int(%context_len.27), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %40 : int = aten::Int(%context_len.27), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %41 : int = aten::Int(%context_len.27), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %276 : Tensor = prim::CallMethod[name="forward"](%17, %1)
    %46 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:535:0
    %47 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:535:0
    %48 : int[] = prim::ListConstruct(%24, %29, %46, %47), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %q_head.27 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%276, %48), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:535:0
    %277 : Tensor = prim::CallMethod[name="forward"](%16, %1)
    %51 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:537:0
    %52 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:537:0
    %53 : int[] = prim::ListConstruct(%23, %41, %51, %52), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%277, %53), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:537:0
    %278 : Tensor = prim::CallMethod[name="forward"](%15, %1)
    %56 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:538:0
    %57 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:538:0
    %58 : int[] = prim::ListConstruct(%22, %40, %56, %57), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %59 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%278, %58), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:538:0
    %60 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:540:0
    %q_head : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.27, %60), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:540:0
    %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:542:0
    %r_w_bias : Float(12:64, 64:1) = aten::mul(%14, %62), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:542:0
    %64 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:544:0
    %65 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_w_bias, %64), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:544:0
    %66 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %67 : Tensor[] = prim::ListConstruct(%65, %54), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %content_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%66, %67), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %82 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:486:0
    %v : Float(12:64, 64:1) = aten::mul(%13, %82), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:486:0
    %84 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %85 : Tensor[] = prim::ListConstruct(%2, %12), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %86 : Float(26:768, 12:64, 64:1) = aten::einsum(%84, %85), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %87 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:493:0
    %88 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %v, %87), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:493:0
    %89 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %90 : Tensor[] = prim::ListConstruct(%88, %86), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %positional_attn.79 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%89, %90), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %92 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
    %93 : int = aten::size(%positional_attn.79, %92), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
    %batch_size.41 : Long() = prim::NumToTensor(%93), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %95 : int = aten::Int(%batch_size.41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %96 : int = aten::Int(%batch_size.41), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %97 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
    %98 : int = aten::size(%positional_attn.79, %97), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
    %n_head : Long() = prim::NumToTensor(%98), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %100 : int = aten::Int(%n_head), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %101 : int = aten::Int(%n_head), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %102 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
    %103 : int = aten::size(%positional_attn.79, %102), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
    %seq_len.43 : Long() = prim::NumToTensor(%103), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %105 : int = aten::Int(%seq_len.43), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %106 : int = aten::Int(%seq_len.43), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %107 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
    %108 : int = aten::size(%positional_attn.79, %107), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
    %max_rel_len : Long() = prim::NumToTensor(%108), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %110 : int = aten::Int(%max_rel_len), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %111 : int[] = prim::ListConstruct(%96, %101, %110, %106), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %positional_attn.80 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.79, %111), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:428:0
    %113 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %114 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %115 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %116 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %117 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.80, %113, %114, %115, %116), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %118 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %119 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %120 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %121 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %122 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%117, %118, %119, %120, %121), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %123 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %124 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %125 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %126 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %127 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%122, %123, %124, %125, %126), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %128 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %129 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %130 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %131 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %positional_attn.81 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%127, %128, %129, %130, %131), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
    %133 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
    %134 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
    %135 : Long() = aten::sub(%max_rel_len, %133, %134), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
    %136 : int = aten::Int(%135), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %137 : int[] = prim::ListConstruct(%95, %100, %105, %136), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %positional_attn.82 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.81, %137), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
    %139 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:431:0
    %140 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:431:0
    %141 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:431:0
    %positional_attn.83 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.82, %139, %140, %39, %141), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:431:0
    %positional_attn : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.83, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:498:0
    %144 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
    %145 : int = aten::size(%token_type_mat.20, %144), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
    %batch_size : Long() = prim::NumToTensor(%145), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %147 : int = aten::Int(%batch_size), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %148 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
    %149 : int = aten::size(%token_type_mat.20, %148), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
    %seq_len : Long() = prim::NumToTensor(%149), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %151 : int = aten::Int(%seq_len), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %152 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
    %153 : int = aten::size(%token_type_mat.20, %152), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
    %context_len : Long() = prim::NumToTensor(%153), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %155 : int = aten::Int(%context_len), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %156 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:508:0
    %r_s_bias : Float(12:64, 64:1) = aten::mul(%11, %156), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:508:0
    %158 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:511:0
    %159 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_s_bias, %158), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:511:0
    %160 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %161 : Tensor[] = prim::ListConstruct(%159, %10), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %162 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%160, %161), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %163 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %164 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %165 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %166 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %167 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %163, %164, %165, %166), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %168 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %169 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%167, %168), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %176 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %177 : int = aten::size(%q_head, %176), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %178 : Long() = prim::NumToTensor(%177), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %179 : int = aten::Int(%178), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %183 : int[] = prim::ListConstruct(%147, %179, %151, %155), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %184 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %token_type_mat : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%169, %183, %184), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
    %186 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:371:0
    %187 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:371:0
    %188 : Tensor[] = aten::split(%162, %186, %187), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:371:0
    %diff_token_type : Float(17:26, 12:442, 13:2, 1:1), %same_token_type : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%188), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %191 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %192 : int = aten::size(%token_type_mat, %191), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %193 : Long() = prim::NumToTensor(%192), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %194 : int = aten::Int(%193), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %195 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %196 : int = aten::size(%token_type_mat, %195), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %197 : Long() = prim::NumToTensor(%196), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %198 : int = aten::Int(%197), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %199 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %200 : int = aten::size(%token_type_mat, %199), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %201 : Long() = prim::NumToTensor(%200), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %202 : int = aten::Int(%201), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %203 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %204 : int = aten::size(%token_type_mat, %203), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %205 : Long() = prim::NumToTensor(%204), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %206 : int = aten::Int(%205), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %207 : int[] = prim::ListConstruct(%194, %198, %202, %206), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %208 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %209 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type, %207, %208), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %210 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %211 : int = aten::size(%token_type_mat, %210), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %212 : Long() = prim::NumToTensor(%211), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %213 : int = aten::Int(%212), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %214 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %215 : int = aten::size(%token_type_mat, %214), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %216 : Long() = prim::NumToTensor(%215), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %217 : int = aten::Int(%216), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %218 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %219 : int = aten::size(%token_type_mat, %218), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %220 : Long() = prim::NumToTensor(%219), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %221 : int = aten::Int(%220), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %222 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %223 : int = aten::size(%token_type_mat, %222), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %224 : Long() = prim::NumToTensor(%223), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %225 : int = aten::Int(%224), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %226 : int[] = prim::ListConstruct(%213, %217, %221, %225), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %227 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %228 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type, %226, %227), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
    %token_type_attn.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat, %209, %228), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:517:0
    %token_type_attn : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.27, %cls_mask), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:522:0
    %231 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
    %232 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score, %positional_attn, %231), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
    %233 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
    %attn_score.40 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%232, %token_type_attn, %233), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
    %235 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:553:0
    %236 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:553:0
    %237 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:553:0
    %238 : None = prim::Constant(), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %attn_score.41 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.40, %235, %236, %237, %238), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:553:0
    %240 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %241 : int = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %242 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %243 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %244 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %240, %241, %242, %243), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %245 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %246 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%244, %245), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %247 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %248 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%246, %247), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %249 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %250 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %251 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %252 : None = prim::Constant(), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %253 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%248, %249, %250, %251, %252), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %254 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:396:0
    %255 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:396:0
    %256 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%253, %254, %255), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/tensor.py:396:0
    %257 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %258 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%256, %257), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %259 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %attn_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.41, %258, %259), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
    %261 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:558:0
    %262 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:558:0
    %input.129 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score, %261, %262), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:558:0
    %279 : Tensor = prim::CallMethod[name="forward"](%9, %input.129)
    %265 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %266 : Tensor[] = prim::ListConstruct(%279, %59), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %attn_vec : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%265, %266), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # torch/functional.py:327:0
    %268 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:565:0
    %269 : int[] = prim::ListConstruct(%21, %28, %268), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention
    %input.130 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec, %269), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:565:0
    %280 : Tensor = prim::CallMethod[name="forward"](%8, %input.130)
    %281 : Tensor = prim::CallMethod[name="forward"](%7, %280)
    %273 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:568:0
    %input.132 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %281, %273), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention # transformers/modeling_funnel.py:568:0
    %282 : Tensor = prim::CallMethod[name="forward"](%6, %input.132)
    return (%282)

FunnelLayer.ffn
FunnelPositionwiseFFN._actual_script_module
  graph(%self.217 : __torch__.transformers.modeling_funnel.FunnelPositionwiseFFN,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.217)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.217)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_2"](%self.217)
    %5 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="activation_dropout"](%self.217)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear_1"](%self.217)
    %29 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %8 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %9 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%29, %8), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %10 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %11 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%29, %10), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %12 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %13 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%11, %12), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %14 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %15 : Float(17:39936, 13:3072, 3072:1) = aten::add(%29, %13, %14), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %16 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %17 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%15, %16), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %18 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%17), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %19 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %20 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %21 : Float(17:39936, 13:3072, 3072:1) = aten::add(%18, %19, %20), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %input.134 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%9, %21), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/activations.py:30:0
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %input.134)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31)
    %26 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/modeling_funnel.py:588:0
    %input.137 : Float(17:9984, 13:768, 768:1) = aten::add(%1, %32, %26), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn # transformers/modeling_funnel.py:588:0
    %33 : Tensor = prim::CallMethod[name="forward"](%2, %input.137)
    return (%33)

FunnelRelMultiheadAttention.attention_dropout
Dropout._actual_script_module
  graph(%self.213 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.129 : Float(17:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.attention_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.attention_dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.129, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.attention_dropout # torch/nn/functional.py:973:0
    return (%4)

FunnelRelMultiheadAttention.hidden_dropout
Dropout._actual_script_module
  graph(%self.215 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    %attn_out : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.hidden_dropout # torch/nn/functional.py:973:0
    return (%attn_out)

FunnelRelMultiheadAttention.k_head
Linear._actual_script_module
  graph(%self.211 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.211)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.211)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
    %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.k_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelRelMultiheadAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.216 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.132 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.216)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.216)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm # torch/nn/functional.py:2048:0
    %input.133 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.132, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.layer_norm # torch/nn/functional.py:2048:0
    return (%input.133)

FunnelRelMultiheadAttention.post_proj
Linear._actual_script_module
  graph(%self.214 : __torch__.torch.nn.modules.linear.Linear,
        %input.130 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.214)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.214)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
    %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.130, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1678:0
    %input.131 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1678:0
    return (%input.131)

FunnelRelMultiheadAttention.q_head
Linear._actual_script_module
  graph(%self.210 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.210)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
    %4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
    return (%4)

FunnelRelMultiheadAttention.v_head
Linear._actual_script_module
  graph(%self.212 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.212)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.212)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
    %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1678:0
    %7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.attention/__module.funnel.decoder.layers.1.attention.v_head # torch/nn/functional.py:1678:0
    return (%7)

FunnelPositionwiseFFN.activation_dropout
Dropout._actual_script_module
  graph(%self.219 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.134 : Float(17:39936, 13:3072, 3072:1)):
    %2 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    %input.135 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.134, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.activation_dropout # torch/nn/functional.py:973:0
    return (%input.135)

FunnelPositionwiseFFN.dropout
Dropout._actual_script_module
  graph(%self.221 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.dropout # torch/nn/functional.py:973:0
    %h : Float(17:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.dropout # torch/nn/functional.py:973:0
    return (%h)

FunnelPositionwiseFFN.layer_norm
LayerNorm._actual_script_module
  graph(%self.222 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.137 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.222)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.222)
    %4 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm
    %6 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    %input : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.137, %5, %3, %2, %6, %7), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.layer_norm # torch/nn/functional.py:2048:0
    return (%input)

FunnelPositionwiseFFN.linear_1
Linear._actual_script_module
  graph(%self.218 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.218)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.218)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
    %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1678:0
    %x : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1678:0
    return (%x)

FunnelPositionwiseFFN.linear_2
Linear._actual_script_module
  graph(%self.220 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.220)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.220)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
    %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1678:0
    %input.136 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %2, %6), scope: __module.funnel/__module.funnel.decoder/__module.funnel.decoder.layers.1/__module.funnel.decoder.layers.1.ffn/__module.funnel.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1678:0
    return (%input.136)

