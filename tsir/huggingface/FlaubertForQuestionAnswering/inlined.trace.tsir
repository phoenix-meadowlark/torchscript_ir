graph(%self.1 : __torch__.transformers.modeling_flaubert.FlaubertForQuestionAnswering,
      %input_ids : Long(17:13, 13:1),
      %padding_mask : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_utils.SQuADHead = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_flaubert.___torch_mangle_251.FlaubertModel = prim::GetAttr[name="transformer"](%self.1)
  %13 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %14 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %15 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %16 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %17 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %18 : None = prim::Constant(), scope: __module.transformer
  %19 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
  %21 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %22 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %23 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %24 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %25 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %26 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %27 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %28 : int = prim::Constant[value=4](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %29 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %30 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %31 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %32 : __torch__.torch.nn.modules.normalization.___torch_mangle_249.LayerNorm = prim::GetAttr[name="11"](%31)
  %33 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %34 : __torch__.transformers.modeling_xlm.___torch_mangle_236.TransformerFFN = prim::GetAttr[name="11"](%33)
  %35 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %36 : __torch__.torch.nn.modules.normalization.___torch_mangle_199.LayerNorm = prim::GetAttr[name="11"](%35)
  %37 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %38 : __torch__.transformers.modeling_xlm.___torch_mangle_186.MultiHeadAttention = prim::GetAttr[name="11"](%37)
  %39 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %40 : __torch__.torch.nn.modules.normalization.___torch_mangle_248.LayerNorm = prim::GetAttr[name="10"](%39)
  %41 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %42 : __torch__.transformers.modeling_xlm.___torch_mangle_233.TransformerFFN = prim::GetAttr[name="10"](%41)
  %43 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %44 : __torch__.torch.nn.modules.normalization.___torch_mangle_198.LayerNorm = prim::GetAttr[name="10"](%43)
  %45 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %46 : __torch__.transformers.modeling_xlm.___torch_mangle_181.MultiHeadAttention = prim::GetAttr[name="10"](%45)
  %47 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %48 : __torch__.torch.nn.modules.normalization.___torch_mangle_247.LayerNorm = prim::GetAttr[name="9"](%47)
  %49 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %50 : __torch__.transformers.modeling_xlm.___torch_mangle_230.TransformerFFN = prim::GetAttr[name="9"](%49)
  %51 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %52 : __torch__.torch.nn.modules.normalization.___torch_mangle_197.LayerNorm = prim::GetAttr[name="9"](%51)
  %53 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %54 : __torch__.transformers.modeling_xlm.___torch_mangle_176.MultiHeadAttention = prim::GetAttr[name="9"](%53)
  %55 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %56 : __torch__.torch.nn.modules.normalization.___torch_mangle_246.LayerNorm = prim::GetAttr[name="8"](%55)
  %57 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %58 : __torch__.transformers.modeling_xlm.___torch_mangle_227.TransformerFFN = prim::GetAttr[name="8"](%57)
  %59 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %60 : __torch__.torch.nn.modules.normalization.___torch_mangle_196.LayerNorm = prim::GetAttr[name="8"](%59)
  %61 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %62 : __torch__.transformers.modeling_xlm.___torch_mangle_171.MultiHeadAttention = prim::GetAttr[name="8"](%61)
  %63 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %64 : __torch__.torch.nn.modules.normalization.___torch_mangle_245.LayerNorm = prim::GetAttr[name="7"](%63)
  %65 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %66 : __torch__.transformers.modeling_xlm.___torch_mangle_224.TransformerFFN = prim::GetAttr[name="7"](%65)
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %68 : __torch__.torch.nn.modules.normalization.___torch_mangle_195.LayerNorm = prim::GetAttr[name="7"](%67)
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %70 : __torch__.transformers.modeling_xlm.___torch_mangle_166.MultiHeadAttention = prim::GetAttr[name="7"](%69)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %72 : __torch__.torch.nn.modules.normalization.___torch_mangle_244.LayerNorm = prim::GetAttr[name="6"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %74 : __torch__.transformers.modeling_xlm.___torch_mangle_221.TransformerFFN = prim::GetAttr[name="6"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %76 : __torch__.torch.nn.modules.normalization.___torch_mangle_194.LayerNorm = prim::GetAttr[name="6"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %78 : __torch__.transformers.modeling_xlm.___torch_mangle_161.MultiHeadAttention = prim::GetAttr[name="6"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %80 : __torch__.torch.nn.modules.normalization.___torch_mangle_243.LayerNorm = prim::GetAttr[name="5"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %82 : __torch__.transformers.modeling_xlm.___torch_mangle_218.TransformerFFN = prim::GetAttr[name="5"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %84 : __torch__.torch.nn.modules.normalization.___torch_mangle_193.LayerNorm = prim::GetAttr[name="5"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %86 : __torch__.transformers.modeling_xlm.___torch_mangle_156.MultiHeadAttention = prim::GetAttr[name="5"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %88 : __torch__.torch.nn.modules.normalization.___torch_mangle_242.LayerNorm = prim::GetAttr[name="4"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %90 : __torch__.transformers.modeling_xlm.___torch_mangle_215.TransformerFFN = prim::GetAttr[name="4"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %92 : __torch__.torch.nn.modules.normalization.___torch_mangle_192.LayerNorm = prim::GetAttr[name="4"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %94 : __torch__.transformers.modeling_xlm.___torch_mangle_151.MultiHeadAttention = prim::GetAttr[name="4"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %96 : __torch__.torch.nn.modules.normalization.___torch_mangle_241.LayerNorm = prim::GetAttr[name="3"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %98 : __torch__.transformers.modeling_xlm.___torch_mangle_212.TransformerFFN = prim::GetAttr[name="3"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %100 : __torch__.torch.nn.modules.normalization.___torch_mangle_191.LayerNorm = prim::GetAttr[name="3"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %102 : __torch__.transformers.modeling_xlm.___torch_mangle_146.MultiHeadAttention = prim::GetAttr[name="3"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %104 : __torch__.torch.nn.modules.normalization.___torch_mangle_240.LayerNorm = prim::GetAttr[name="2"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %106 : __torch__.transformers.modeling_xlm.___torch_mangle_209.TransformerFFN = prim::GetAttr[name="2"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %108 : __torch__.torch.nn.modules.normalization.___torch_mangle_190.LayerNorm = prim::GetAttr[name="2"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %110 : __torch__.transformers.modeling_xlm.___torch_mangle_141.MultiHeadAttention = prim::GetAttr[name="2"](%109)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %112 : __torch__.torch.nn.modules.normalization.___torch_mangle_239.LayerNorm = prim::GetAttr[name="1"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %114 : __torch__.transformers.modeling_xlm.___torch_mangle_206.TransformerFFN = prim::GetAttr[name="1"](%113)
  %115 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %116 : __torch__.torch.nn.modules.normalization.___torch_mangle_189.LayerNorm = prim::GetAttr[name="1"](%115)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %118 : __torch__.transformers.modeling_xlm.___torch_mangle_136.MultiHeadAttention = prim::GetAttr[name="1"](%117)
  %119 : __torch__.torch.nn.modules.container.___torch_mangle_250.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %120 : __torch__.torch.nn.modules.normalization.___torch_mangle_238.LayerNorm = prim::GetAttr[name="0"](%119)
  %121 : __torch__.torch.nn.modules.container.___torch_mangle_237.ModuleList = prim::GetAttr[name="ffns"](%4)
  %122 : __torch__.transformers.modeling_xlm.___torch_mangle_203.TransformerFFN = prim::GetAttr[name="0"](%121)
  %123 : __torch__.torch.nn.modules.container.___torch_mangle_200.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %124 : __torch__.torch.nn.modules.normalization.___torch_mangle_188.LayerNorm = prim::GetAttr[name="0"](%123)
  %125 : __torch__.torch.nn.modules.container.___torch_mangle_187.ModuleList = prim::GetAttr[name="attentions"](%4)
  %126 : __torch__.transformers.modeling_xlm.___torch_mangle_131.MultiHeadAttention = prim::GetAttr[name="0"](%125)
  %127 : __torch__.torch.nn.modules.normalization.___torch_mangle_126.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%4)
  %128 : __torch__.torch.nn.modules.sparse.___torch_mangle_124.Embedding = prim::GetAttr[name="position_embeddings"](%4)
  %129 : __torch__.torch.nn.modules.sparse.___torch_mangle_125.Embedding = prim::GetAttr[name="embeddings"](%4)
  %130 : int = aten::size(%input_ids, %30), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %131 : int = aten::size(%input_ids, %29), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %position_ids : Long(13:1) = aten::arange(%131, %28, %30, %27, %26), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %133 : Long(1:13, 13:1) = aten::unsqueeze(%position_ids, %30), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
  %134 : int[] = prim::ListConstruct(%130, %131), scope: __module.transformer
  %input.1 : Long(17:0, 13:1) = aten::expand(%133, %134, %26), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
  %136 : Tensor = prim::GetAttr[name="weight"](%129)
  %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%136, %input_ids, %25, %26, %26), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %138 : Tensor = prim::GetAttr[name="weight"](%128)
  %139 : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%138, %input.1, %24, %26, %26), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %140 : Float(17:26624, 13:2048, 2048:1) = aten::expand_as(%139, %inputs_embeds), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
  %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%inputs_embeds, %140, %29), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
  %142 : Tensor = prim::GetAttr[name="bias"](%127)
  %143 : Tensor = prim::GetAttr[name="weight"](%127)
  %144 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm_emb
  %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %144, %143, %142, %22, %23), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.3, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %147 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %148 : Float(17:13, 13:1, 1:1) = aten::to(%147, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %148), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %150 : __torch__.torch.nn.modules.linear.___torch_mangle_130.Linear = prim::GetAttr[name="out_lin"](%126)
  %151 : __torch__.torch.nn.modules.linear.___torch_mangle_129.Linear = prim::GetAttr[name="v_lin"](%126)
  %152 : __torch__.torch.nn.modules.linear.___torch_mangle_128.Linear = prim::GetAttr[name="k_lin"](%126)
  %153 : __torch__.torch.nn.modules.linear.___torch_mangle_127.Linear = prim::GetAttr[name="q_lin"](%126)
  %154 : int = aten::size(%input.4, %30), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %155 : int = aten::size(%input.4, %29), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %156 : Tensor = prim::GetAttr[name="bias"](%153)
  %157 : Tensor = prim::GetAttr[name="weight"](%153)
  %158 : Float(2048:1, 2048:2048) = aten::t(%157), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %158), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %156, %29), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1678:0
  %161 : int[] = prim::ListConstruct(%154, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.0
  %162 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.1, %161), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%162, %29, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %164 : Tensor = prim::GetAttr[name="bias"](%152)
  %165 : Tensor = prim::GetAttr[name="weight"](%152)
  %166 : Float(2048:1, 2048:2048) = aten::t(%165), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %166), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %164, %29), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1678:0
  %169 : int[] = prim::ListConstruct(%154, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.0
  %170 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.2, %169), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%170, %29, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %172 : Tensor = prim::GetAttr[name="bias"](%151)
  %173 : Tensor = prim::GetAttr[name="weight"](%151)
  %174 : Float(2048:1, 2048:2048) = aten::t(%173), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %174), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %172, %29), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1678:0
  %177 : int[] = prim::ListConstruct(%154, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.0
  %178 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.3, %177), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%178, %29, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %15), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %181 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %25, %16), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %181), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %183 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.0 # torch/tensor.py:22:0
  %184 : int[] = prim::ListConstruct(%154, %29, %29, %155), scope: __module.transformer/__module.transformer.attentions.0
  %185 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%183, %184), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%185, %scores.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %17), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
  %189 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %24, %18), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:1498:0
  %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%189, %20, %26), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
  %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:200:0
  %192 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %29, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %193 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%192, %30), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %194 : int[] = prim::ListConstruct(%154, %24, %21), scope: __module.transformer/__module.transformer.attentions.0
  %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%193, %194), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %196 : Tensor = prim::GetAttr[name="bias"](%150)
  %197 : Tensor = prim::GetAttr[name="weight"](%150)
  %198 : Float(2048:1, 2048:2048) = aten::t(%197), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %198), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %196, %29), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1678:0
  %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.8, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %203 : Tensor = prim::GetAttr[name="bias"](%124)
  %204 : Tensor = prim::GetAttr[name="weight"](%124)
  %205 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.0
  %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %205, %204, %203, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
  %207 : __torch__.torch.nn.modules.linear.___torch_mangle_202.Linear = prim::GetAttr[name="lin2"](%122)
  %208 : __torch__.torch.nn.modules.linear.___torch_mangle_201.Linear = prim::GetAttr[name="lin1"](%122)
  %209 : Tensor = prim::GetAttr[name="bias"](%208)
  %210 : Tensor = prim::GetAttr[name="weight"](%208)
  %211 : Float(2048:1, 8192:2048) = aten::t(%210), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.1, %211), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %209, %29), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1678:0
  %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.10), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:1369:0
  %215 : Tensor = prim::GetAttr[name="bias"](%207)
  %216 : Tensor = prim::GetAttr[name="weight"](%207)
  %217 : Float(8192:1, 2048:8192) = aten::t(%216), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %217), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %215, %29), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1678:0
  %220 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.12, %20, %26), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
  %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.1, %220, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %222 : Tensor = prim::GetAttr[name="bias"](%120)
  %223 : Tensor = prim::GetAttr[name="weight"](%120)
  %224 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.0
  %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %224, %223, %222, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
  %226 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %227 : Float(17:13, 13:1, 1:1) = aten::to(%226, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.2, %227), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %229 : __torch__.torch.nn.modules.linear.___torch_mangle_135.Linear = prim::GetAttr[name="out_lin"](%118)
  %230 : __torch__.torch.nn.modules.linear.___torch_mangle_134.Linear = prim::GetAttr[name="v_lin"](%118)
  %231 : __torch__.torch.nn.modules.linear.___torch_mangle_133.Linear = prim::GetAttr[name="k_lin"](%118)
  %232 : __torch__.torch.nn.modules.linear.___torch_mangle_132.Linear = prim::GetAttr[name="q_lin"](%118)
  %233 : int = aten::size(%input.14, %30), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %234 : int = aten::size(%input.14, %29), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %235 : Tensor = prim::GetAttr[name="bias"](%232)
  %236 : Tensor = prim::GetAttr[name="weight"](%232)
  %237 : Float(2048:1, 2048:2048) = aten::t(%236), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %237), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %235, %29), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1678:0
  %240 : int[] = prim::ListConstruct(%233, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.1
  %241 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.5, %240), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%241, %29, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %243 : Tensor = prim::GetAttr[name="bias"](%231)
  %244 : Tensor = prim::GetAttr[name="weight"](%231)
  %245 : Float(2048:1, 2048:2048) = aten::t(%244), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %245), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %243, %29), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1678:0
  %248 : int[] = prim::ListConstruct(%233, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.1
  %249 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.6, %248), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%249, %29, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %251 : Tensor = prim::GetAttr[name="bias"](%230)
  %252 : Tensor = prim::GetAttr[name="weight"](%230)
  %253 : Float(2048:1, 2048:2048) = aten::t(%252), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %253), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %251, %29), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1678:0
  %256 : int[] = prim::ListConstruct(%233, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.1
  %257 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.7, %256), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%257, %29, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %15), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:188:0
  %260 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %25, %16), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %260), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %262 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.1 # torch/tensor.py:22:0
  %263 : int[] = prim::ListConstruct(%233, %29, %29, %234), scope: __module.transformer/__module.transformer.attentions.1
  %264 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%262, %263), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%264, %scores.3), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %17), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:191:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
  %268 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %24, %18), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:1498:0
  %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%268, %20, %26), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
  %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:200:0
  %271 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %29, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %272 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%271, %30), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %273 : int[] = prim::ListConstruct(%233, %24, %21), scope: __module.transformer/__module.transformer.attentions.1
  %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%272, %273), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %275 : Tensor = prim::GetAttr[name="bias"](%229)
  %276 : Tensor = prim::GetAttr[name="weight"](%229)
  %277 : Float(2048:1, 2048:2048) = aten::t(%276), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %277), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %275, %29), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1678:0
  %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.18, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %282 : Tensor = prim::GetAttr[name="bias"](%116)
  %283 : Tensor = prim::GetAttr[name="weight"](%116)
  %284 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.1
  %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %284, %283, %282, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
  %286 : __torch__.torch.nn.modules.linear.___torch_mangle_205.Linear = prim::GetAttr[name="lin2"](%114)
  %287 : __torch__.torch.nn.modules.linear.___torch_mangle_204.Linear = prim::GetAttr[name="lin1"](%114)
  %288 : Tensor = prim::GetAttr[name="bias"](%287)
  %289 : Tensor = prim::GetAttr[name="weight"](%287)
  %290 : Float(2048:1, 8192:2048) = aten::t(%289), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.2, %290), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %288, %29), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1678:0
  %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.20), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:1369:0
  %294 : Tensor = prim::GetAttr[name="bias"](%286)
  %295 : Tensor = prim::GetAttr[name="weight"](%286)
  %296 : Float(8192:1, 2048:8192) = aten::t(%295), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %296), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %294, %29), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1678:0
  %299 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.22, %20, %26), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
  %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.2, %299, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %301 : Tensor = prim::GetAttr[name="bias"](%112)
  %302 : Tensor = prim::GetAttr[name="weight"](%112)
  %303 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.1
  %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %303, %302, %301, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
  %305 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %306 : Float(17:13, 13:1, 1:1) = aten::to(%305, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.3, %306), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %308 : __torch__.torch.nn.modules.linear.___torch_mangle_140.Linear = prim::GetAttr[name="out_lin"](%110)
  %309 : __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear = prim::GetAttr[name="v_lin"](%110)
  %310 : __torch__.torch.nn.modules.linear.___torch_mangle_138.Linear = prim::GetAttr[name="k_lin"](%110)
  %311 : __torch__.torch.nn.modules.linear.___torch_mangle_137.Linear = prim::GetAttr[name="q_lin"](%110)
  %312 : int = aten::size(%input.24, %30), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %313 : int = aten::size(%input.24, %29), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %314 : Tensor = prim::GetAttr[name="bias"](%311)
  %315 : Tensor = prim::GetAttr[name="weight"](%311)
  %316 : Float(2048:1, 2048:2048) = aten::t(%315), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %316), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %314, %29), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1678:0
  %319 : int[] = prim::ListConstruct(%312, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.2
  %320 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.9, %319), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%320, %29, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %322 : Tensor = prim::GetAttr[name="bias"](%310)
  %323 : Tensor = prim::GetAttr[name="weight"](%310)
  %324 : Float(2048:1, 2048:2048) = aten::t(%323), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %324), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %322, %29), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1678:0
  %327 : int[] = prim::ListConstruct(%312, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.2
  %328 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.10, %327), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%328, %29, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %330 : Tensor = prim::GetAttr[name="bias"](%309)
  %331 : Tensor = prim::GetAttr[name="weight"](%309)
  %332 : Float(2048:1, 2048:2048) = aten::t(%331), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %332), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %330, %29), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1678:0
  %335 : int[] = prim::ListConstruct(%312, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.2
  %336 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.11, %335), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%336, %29, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %15), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:188:0
  %339 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %25, %16), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %339), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %341 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.2 # torch/tensor.py:22:0
  %342 : int[] = prim::ListConstruct(%312, %29, %29, %313), scope: __module.transformer/__module.transformer.attentions.2
  %343 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%341, %342), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%343, %scores.5), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %17), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:191:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
  %347 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %24, %18), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:1498:0
  %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%347, %20, %26), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
  %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:200:0
  %350 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %29, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %351 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%350, %30), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %352 : int[] = prim::ListConstruct(%312, %24, %21), scope: __module.transformer/__module.transformer.attentions.2
  %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%351, %352), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %354 : Tensor = prim::GetAttr[name="bias"](%308)
  %355 : Tensor = prim::GetAttr[name="weight"](%308)
  %356 : Float(2048:1, 2048:2048) = aten::t(%355), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %356), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %354, %29), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1678:0
  %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.28, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %361 : Tensor = prim::GetAttr[name="bias"](%108)
  %362 : Tensor = prim::GetAttr[name="weight"](%108)
  %363 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.2
  %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %363, %362, %361, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
  %365 : __torch__.torch.nn.modules.linear.___torch_mangle_208.Linear = prim::GetAttr[name="lin2"](%106)
  %366 : __torch__.torch.nn.modules.linear.___torch_mangle_207.Linear = prim::GetAttr[name="lin1"](%106)
  %367 : Tensor = prim::GetAttr[name="bias"](%366)
  %368 : Tensor = prim::GetAttr[name="weight"](%366)
  %369 : Float(2048:1, 8192:2048) = aten::t(%368), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.3, %369), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %367, %29), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1678:0
  %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.30), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:1369:0
  %373 : Tensor = prim::GetAttr[name="bias"](%365)
  %374 : Tensor = prim::GetAttr[name="weight"](%365)
  %375 : Float(8192:1, 2048:8192) = aten::t(%374), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %375), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %373, %29), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1678:0
  %378 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.32, %20, %26), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
  %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.3, %378, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %380 : Tensor = prim::GetAttr[name="bias"](%104)
  %381 : Tensor = prim::GetAttr[name="weight"](%104)
  %382 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.2
  %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %382, %381, %380, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
  %384 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %385 : Float(17:13, 13:1, 1:1) = aten::to(%384, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.4, %385), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %387 : __torch__.torch.nn.modules.linear.___torch_mangle_145.Linear = prim::GetAttr[name="out_lin"](%102)
  %388 : __torch__.torch.nn.modules.linear.___torch_mangle_144.Linear = prim::GetAttr[name="v_lin"](%102)
  %389 : __torch__.torch.nn.modules.linear.___torch_mangle_143.Linear = prim::GetAttr[name="k_lin"](%102)
  %390 : __torch__.torch.nn.modules.linear.___torch_mangle_142.Linear = prim::GetAttr[name="q_lin"](%102)
  %391 : int = aten::size(%input.34, %30), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %392 : int = aten::size(%input.34, %29), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %393 : Tensor = prim::GetAttr[name="bias"](%390)
  %394 : Tensor = prim::GetAttr[name="weight"](%390)
  %395 : Float(2048:1, 2048:2048) = aten::t(%394), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %395), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %393, %29), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1678:0
  %398 : int[] = prim::ListConstruct(%391, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.3
  %399 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.13, %398), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%399, %29, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %401 : Tensor = prim::GetAttr[name="bias"](%389)
  %402 : Tensor = prim::GetAttr[name="weight"](%389)
  %403 : Float(2048:1, 2048:2048) = aten::t(%402), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %403), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %401, %29), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1678:0
  %406 : int[] = prim::ListConstruct(%391, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.3
  %407 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.14, %406), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%407, %29, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %409 : Tensor = prim::GetAttr[name="bias"](%388)
  %410 : Tensor = prim::GetAttr[name="weight"](%388)
  %411 : Float(2048:1, 2048:2048) = aten::t(%410), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %411), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %409, %29), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1678:0
  %414 : int[] = prim::ListConstruct(%391, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.3
  %415 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.15, %414), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%415, %29, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %15), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:188:0
  %418 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %25, %16), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %418), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %420 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.3 # torch/tensor.py:22:0
  %421 : int[] = prim::ListConstruct(%391, %29, %29, %392), scope: __module.transformer/__module.transformer.attentions.3
  %422 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%420, %421), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%422, %scores.7), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %17), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:191:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
  %426 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %24, %18), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:1498:0
  %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%426, %20, %26), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
  %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:200:0
  %429 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %29, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %430 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%429, %30), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %431 : int[] = prim::ListConstruct(%391, %24, %21), scope: __module.transformer/__module.transformer.attentions.3
  %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%430, %431), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %433 : Tensor = prim::GetAttr[name="bias"](%387)
  %434 : Tensor = prim::GetAttr[name="weight"](%387)
  %435 : Float(2048:1, 2048:2048) = aten::t(%434), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %435), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %433, %29), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1678:0
  %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.38, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %440 : Tensor = prim::GetAttr[name="bias"](%100)
  %441 : Tensor = prim::GetAttr[name="weight"](%100)
  %442 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.3
  %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %442, %441, %440, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
  %444 : __torch__.torch.nn.modules.linear.___torch_mangle_211.Linear = prim::GetAttr[name="lin2"](%98)
  %445 : __torch__.torch.nn.modules.linear.___torch_mangle_210.Linear = prim::GetAttr[name="lin1"](%98)
  %446 : Tensor = prim::GetAttr[name="bias"](%445)
  %447 : Tensor = prim::GetAttr[name="weight"](%445)
  %448 : Float(2048:1, 8192:2048) = aten::t(%447), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.4, %448), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %446, %29), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1678:0
  %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.40), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:1369:0
  %452 : Tensor = prim::GetAttr[name="bias"](%444)
  %453 : Tensor = prim::GetAttr[name="weight"](%444)
  %454 : Float(8192:1, 2048:8192) = aten::t(%453), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %454), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %452, %29), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1678:0
  %457 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.42, %20, %26), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
  %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.4, %457, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %459 : Tensor = prim::GetAttr[name="bias"](%96)
  %460 : Tensor = prim::GetAttr[name="weight"](%96)
  %461 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.3
  %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %461, %460, %459, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
  %463 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %464 : Float(17:13, 13:1, 1:1) = aten::to(%463, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.5, %464), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %466 : __torch__.torch.nn.modules.linear.___torch_mangle_150.Linear = prim::GetAttr[name="out_lin"](%94)
  %467 : __torch__.torch.nn.modules.linear.___torch_mangle_149.Linear = prim::GetAttr[name="v_lin"](%94)
  %468 : __torch__.torch.nn.modules.linear.___torch_mangle_148.Linear = prim::GetAttr[name="k_lin"](%94)
  %469 : __torch__.torch.nn.modules.linear.___torch_mangle_147.Linear = prim::GetAttr[name="q_lin"](%94)
  %470 : int = aten::size(%input.44, %30), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %471 : int = aten::size(%input.44, %29), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %472 : Tensor = prim::GetAttr[name="bias"](%469)
  %473 : Tensor = prim::GetAttr[name="weight"](%469)
  %474 : Float(2048:1, 2048:2048) = aten::t(%473), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %474), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %472, %29), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1678:0
  %477 : int[] = prim::ListConstruct(%470, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.4
  %478 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.17, %477), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%478, %29, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %480 : Tensor = prim::GetAttr[name="bias"](%468)
  %481 : Tensor = prim::GetAttr[name="weight"](%468)
  %482 : Float(2048:1, 2048:2048) = aten::t(%481), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %482), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %480, %29), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1678:0
  %485 : int[] = prim::ListConstruct(%470, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.4
  %486 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.18, %485), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%486, %29, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %488 : Tensor = prim::GetAttr[name="bias"](%467)
  %489 : Tensor = prim::GetAttr[name="weight"](%467)
  %490 : Float(2048:1, 2048:2048) = aten::t(%489), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %490), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %488, %29), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1678:0
  %493 : int[] = prim::ListConstruct(%470, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.4
  %494 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.19, %493), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%494, %29, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %15), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:188:0
  %497 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %25, %16), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %497), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %499 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.4 # torch/tensor.py:22:0
  %500 : int[] = prim::ListConstruct(%470, %29, %29, %471), scope: __module.transformer/__module.transformer.attentions.4
  %501 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%499, %500), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%501, %scores.9), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %17), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:191:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
  %505 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %24, %18), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:1498:0
  %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%505, %20, %26), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
  %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:200:0
  %508 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %29, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %509 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%508, %30), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %510 : int[] = prim::ListConstruct(%470, %24, %21), scope: __module.transformer/__module.transformer.attentions.4
  %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%509, %510), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %512 : Tensor = prim::GetAttr[name="bias"](%466)
  %513 : Tensor = prim::GetAttr[name="weight"](%466)
  %514 : Float(2048:1, 2048:2048) = aten::t(%513), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %514), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %512, %29), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1678:0
  %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.48, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %519 : Tensor = prim::GetAttr[name="bias"](%92)
  %520 : Tensor = prim::GetAttr[name="weight"](%92)
  %521 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.4
  %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %521, %520, %519, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
  %523 : __torch__.torch.nn.modules.linear.___torch_mangle_214.Linear = prim::GetAttr[name="lin2"](%90)
  %524 : __torch__.torch.nn.modules.linear.___torch_mangle_213.Linear = prim::GetAttr[name="lin1"](%90)
  %525 : Tensor = prim::GetAttr[name="bias"](%524)
  %526 : Tensor = prim::GetAttr[name="weight"](%524)
  %527 : Float(2048:1, 8192:2048) = aten::t(%526), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.5, %527), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %525, %29), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1678:0
  %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.50), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:1369:0
  %531 : Tensor = prim::GetAttr[name="bias"](%523)
  %532 : Tensor = prim::GetAttr[name="weight"](%523)
  %533 : Float(8192:1, 2048:8192) = aten::t(%532), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %533), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %531, %29), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1678:0
  %536 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.52, %20, %26), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
  %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.5, %536, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %538 : Tensor = prim::GetAttr[name="bias"](%88)
  %539 : Tensor = prim::GetAttr[name="weight"](%88)
  %540 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.4
  %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %540, %539, %538, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
  %542 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %543 : Float(17:13, 13:1, 1:1) = aten::to(%542, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.6, %543), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %545 : __torch__.torch.nn.modules.linear.___torch_mangle_155.Linear = prim::GetAttr[name="out_lin"](%86)
  %546 : __torch__.torch.nn.modules.linear.___torch_mangle_154.Linear = prim::GetAttr[name="v_lin"](%86)
  %547 : __torch__.torch.nn.modules.linear.___torch_mangle_153.Linear = prim::GetAttr[name="k_lin"](%86)
  %548 : __torch__.torch.nn.modules.linear.___torch_mangle_152.Linear = prim::GetAttr[name="q_lin"](%86)
  %549 : int = aten::size(%input.54, %30), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %550 : int = aten::size(%input.54, %29), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %551 : Tensor = prim::GetAttr[name="bias"](%548)
  %552 : Tensor = prim::GetAttr[name="weight"](%548)
  %553 : Float(2048:1, 2048:2048) = aten::t(%552), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %553), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %551, %29), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1678:0
  %556 : int[] = prim::ListConstruct(%549, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.5
  %557 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.21, %556), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%557, %29, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %559 : Tensor = prim::GetAttr[name="bias"](%547)
  %560 : Tensor = prim::GetAttr[name="weight"](%547)
  %561 : Float(2048:1, 2048:2048) = aten::t(%560), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %561), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %559, %29), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1678:0
  %564 : int[] = prim::ListConstruct(%549, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.5
  %565 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.22, %564), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%565, %29, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %567 : Tensor = prim::GetAttr[name="bias"](%546)
  %568 : Tensor = prim::GetAttr[name="weight"](%546)
  %569 : Float(2048:1, 2048:2048) = aten::t(%568), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %569), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %567, %29), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1678:0
  %572 : int[] = prim::ListConstruct(%549, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.5
  %573 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.23, %572), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%573, %29, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %15), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:188:0
  %576 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %25, %16), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %576), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %578 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.5 # torch/tensor.py:22:0
  %579 : int[] = prim::ListConstruct(%549, %29, %29, %550), scope: __module.transformer/__module.transformer.attentions.5
  %580 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%578, %579), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%580, %scores.11), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %17), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:191:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
  %584 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %24, %18), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:1498:0
  %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%584, %20, %26), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
  %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:200:0
  %587 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %29, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %588 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%587, %30), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %589 : int[] = prim::ListConstruct(%549, %24, %21), scope: __module.transformer/__module.transformer.attentions.5
  %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%588, %589), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %591 : Tensor = prim::GetAttr[name="bias"](%545)
  %592 : Tensor = prim::GetAttr[name="weight"](%545)
  %593 : Float(2048:1, 2048:2048) = aten::t(%592), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %593), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %591, %29), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1678:0
  %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.58, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %598 : Tensor = prim::GetAttr[name="bias"](%84)
  %599 : Tensor = prim::GetAttr[name="weight"](%84)
  %600 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.5
  %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %600, %599, %598, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
  %602 : __torch__.torch.nn.modules.linear.___torch_mangle_217.Linear = prim::GetAttr[name="lin2"](%82)
  %603 : __torch__.torch.nn.modules.linear.___torch_mangle_216.Linear = prim::GetAttr[name="lin1"](%82)
  %604 : Tensor = prim::GetAttr[name="bias"](%603)
  %605 : Tensor = prim::GetAttr[name="weight"](%603)
  %606 : Float(2048:1, 8192:2048) = aten::t(%605), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.6, %606), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %604, %29), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1678:0
  %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.60), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:1369:0
  %610 : Tensor = prim::GetAttr[name="bias"](%602)
  %611 : Tensor = prim::GetAttr[name="weight"](%602)
  %612 : Float(8192:1, 2048:8192) = aten::t(%611), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %612), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %610, %29), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1678:0
  %615 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.62, %20, %26), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
  %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.6, %615, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %617 : Tensor = prim::GetAttr[name="bias"](%80)
  %618 : Tensor = prim::GetAttr[name="weight"](%80)
  %619 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.5
  %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %619, %618, %617, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
  %621 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %622 : Float(17:13, 13:1, 1:1) = aten::to(%621, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.7, %622), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %624 : __torch__.torch.nn.modules.linear.___torch_mangle_160.Linear = prim::GetAttr[name="out_lin"](%78)
  %625 : __torch__.torch.nn.modules.linear.___torch_mangle_159.Linear = prim::GetAttr[name="v_lin"](%78)
  %626 : __torch__.torch.nn.modules.linear.___torch_mangle_158.Linear = prim::GetAttr[name="k_lin"](%78)
  %627 : __torch__.torch.nn.modules.linear.___torch_mangle_157.Linear = prim::GetAttr[name="q_lin"](%78)
  %628 : int = aten::size(%input.64, %30), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %629 : int = aten::size(%input.64, %29), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %630 : Tensor = prim::GetAttr[name="bias"](%627)
  %631 : Tensor = prim::GetAttr[name="weight"](%627)
  %632 : Float(2048:1, 2048:2048) = aten::t(%631), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %632), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %630, %29), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1678:0
  %635 : int[] = prim::ListConstruct(%628, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.6
  %636 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.25, %635), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%636, %29, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %638 : Tensor = prim::GetAttr[name="bias"](%626)
  %639 : Tensor = prim::GetAttr[name="weight"](%626)
  %640 : Float(2048:1, 2048:2048) = aten::t(%639), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %640), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %638, %29), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1678:0
  %643 : int[] = prim::ListConstruct(%628, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.6
  %644 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.26, %643), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%644, %29, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %646 : Tensor = prim::GetAttr[name="bias"](%625)
  %647 : Tensor = prim::GetAttr[name="weight"](%625)
  %648 : Float(2048:1, 2048:2048) = aten::t(%647), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %648), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %646, %29), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1678:0
  %651 : int[] = prim::ListConstruct(%628, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.6
  %652 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.27, %651), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%652, %29, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %15), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:188:0
  %655 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %25, %16), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %655), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %657 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.6 # torch/tensor.py:22:0
  %658 : int[] = prim::ListConstruct(%628, %29, %29, %629), scope: __module.transformer/__module.transformer.attentions.6
  %659 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%657, %658), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%659, %scores.13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %17), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:191:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
  %663 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %24, %18), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:1498:0
  %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%663, %20, %26), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
  %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:200:0
  %666 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %29, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %667 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%666, %30), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %668 : int[] = prim::ListConstruct(%628, %24, %21), scope: __module.transformer/__module.transformer.attentions.6
  %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%667, %668), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %670 : Tensor = prim::GetAttr[name="bias"](%624)
  %671 : Tensor = prim::GetAttr[name="weight"](%624)
  %672 : Float(2048:1, 2048:2048) = aten::t(%671), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %672), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %670, %29), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1678:0
  %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.68, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %677 : Tensor = prim::GetAttr[name="bias"](%76)
  %678 : Tensor = prim::GetAttr[name="weight"](%76)
  %679 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.6
  %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %679, %678, %677, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
  %681 : __torch__.torch.nn.modules.linear.___torch_mangle_220.Linear = prim::GetAttr[name="lin2"](%74)
  %682 : __torch__.torch.nn.modules.linear.___torch_mangle_219.Linear = prim::GetAttr[name="lin1"](%74)
  %683 : Tensor = prim::GetAttr[name="bias"](%682)
  %684 : Tensor = prim::GetAttr[name="weight"](%682)
  %685 : Float(2048:1, 8192:2048) = aten::t(%684), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.7, %685), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %683, %29), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1678:0
  %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:1369:0
  %689 : Tensor = prim::GetAttr[name="bias"](%681)
  %690 : Tensor = prim::GetAttr[name="weight"](%681)
  %691 : Float(8192:1, 2048:8192) = aten::t(%690), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %691), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %689, %29), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1678:0
  %694 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.72, %20, %26), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
  %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.7, %694, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %696 : Tensor = prim::GetAttr[name="bias"](%72)
  %697 : Tensor = prim::GetAttr[name="weight"](%72)
  %698 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.6
  %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %698, %697, %696, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
  %700 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %701 : Float(17:13, 13:1, 1:1) = aten::to(%700, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.8, %701), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %703 : __torch__.torch.nn.modules.linear.___torch_mangle_165.Linear = prim::GetAttr[name="out_lin"](%70)
  %704 : __torch__.torch.nn.modules.linear.___torch_mangle_164.Linear = prim::GetAttr[name="v_lin"](%70)
  %705 : __torch__.torch.nn.modules.linear.___torch_mangle_163.Linear = prim::GetAttr[name="k_lin"](%70)
  %706 : __torch__.torch.nn.modules.linear.___torch_mangle_162.Linear = prim::GetAttr[name="q_lin"](%70)
  %707 : int = aten::size(%input.74, %30), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %708 : int = aten::size(%input.74, %29), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %709 : Tensor = prim::GetAttr[name="bias"](%706)
  %710 : Tensor = prim::GetAttr[name="weight"](%706)
  %711 : Float(2048:1, 2048:2048) = aten::t(%710), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %711), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %709, %29), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1678:0
  %714 : int[] = prim::ListConstruct(%707, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.7
  %715 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.29, %714), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%715, %29, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %717 : Tensor = prim::GetAttr[name="bias"](%705)
  %718 : Tensor = prim::GetAttr[name="weight"](%705)
  %719 : Float(2048:1, 2048:2048) = aten::t(%718), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %719), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %717, %29), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1678:0
  %722 : int[] = prim::ListConstruct(%707, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.7
  %723 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.30, %722), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%723, %29, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %725 : Tensor = prim::GetAttr[name="bias"](%704)
  %726 : Tensor = prim::GetAttr[name="weight"](%704)
  %727 : Float(2048:1, 2048:2048) = aten::t(%726), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %727), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %725, %29), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1678:0
  %730 : int[] = prim::ListConstruct(%707, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.7
  %731 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.31, %730), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%731, %29, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:188:0
  %734 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %25, %16), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %734), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %736 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.7 # torch/tensor.py:22:0
  %737 : int[] = prim::ListConstruct(%707, %29, %29, %708), scope: __module.transformer/__module.transformer.attentions.7
  %738 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%736, %737), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%738, %scores.15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %17), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:191:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
  %742 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %24, %18), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:1498:0
  %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%742, %20, %26), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
  %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:200:0
  %745 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %29, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %746 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%745, %30), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %747 : int[] = prim::ListConstruct(%707, %24, %21), scope: __module.transformer/__module.transformer.attentions.7
  %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%746, %747), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %749 : Tensor = prim::GetAttr[name="bias"](%703)
  %750 : Tensor = prim::GetAttr[name="weight"](%703)
  %751 : Float(2048:1, 2048:2048) = aten::t(%750), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %751), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %749, %29), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1678:0
  %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.78, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %756 : Tensor = prim::GetAttr[name="bias"](%68)
  %757 : Tensor = prim::GetAttr[name="weight"](%68)
  %758 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.7
  %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %758, %757, %756, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
  %760 : __torch__.torch.nn.modules.linear.___torch_mangle_223.Linear = prim::GetAttr[name="lin2"](%66)
  %761 : __torch__.torch.nn.modules.linear.___torch_mangle_222.Linear = prim::GetAttr[name="lin1"](%66)
  %762 : Tensor = prim::GetAttr[name="bias"](%761)
  %763 : Tensor = prim::GetAttr[name="weight"](%761)
  %764 : Float(2048:1, 8192:2048) = aten::t(%763), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.8, %764), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %762, %29), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1678:0
  %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.80), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:1369:0
  %768 : Tensor = prim::GetAttr[name="bias"](%760)
  %769 : Tensor = prim::GetAttr[name="weight"](%760)
  %770 : Float(8192:1, 2048:8192) = aten::t(%769), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %770), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %768, %29), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1678:0
  %773 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.82, %20, %26), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
  %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.8, %773, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %775 : Tensor = prim::GetAttr[name="bias"](%64)
  %776 : Tensor = prim::GetAttr[name="weight"](%64)
  %777 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.7
  %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %777, %776, %775, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
  %779 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %780 : Float(17:13, 13:1, 1:1) = aten::to(%779, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.9, %780), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %782 : __torch__.torch.nn.modules.linear.___torch_mangle_170.Linear = prim::GetAttr[name="out_lin"](%62)
  %783 : __torch__.torch.nn.modules.linear.___torch_mangle_169.Linear = prim::GetAttr[name="v_lin"](%62)
  %784 : __torch__.torch.nn.modules.linear.___torch_mangle_168.Linear = prim::GetAttr[name="k_lin"](%62)
  %785 : __torch__.torch.nn.modules.linear.___torch_mangle_167.Linear = prim::GetAttr[name="q_lin"](%62)
  %786 : int = aten::size(%input.84, %30), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %787 : int = aten::size(%input.84, %29), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %788 : Tensor = prim::GetAttr[name="bias"](%785)
  %789 : Tensor = prim::GetAttr[name="weight"](%785)
  %790 : Float(2048:1, 2048:2048) = aten::t(%789), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %790), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %788, %29), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1678:0
  %793 : int[] = prim::ListConstruct(%786, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.8
  %794 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.33, %793), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%794, %29, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %796 : Tensor = prim::GetAttr[name="bias"](%784)
  %797 : Tensor = prim::GetAttr[name="weight"](%784)
  %798 : Float(2048:1, 2048:2048) = aten::t(%797), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %798), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %796, %29), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1678:0
  %801 : int[] = prim::ListConstruct(%786, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.8
  %802 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.34, %801), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%802, %29, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %804 : Tensor = prim::GetAttr[name="bias"](%783)
  %805 : Tensor = prim::GetAttr[name="weight"](%783)
  %806 : Float(2048:1, 2048:2048) = aten::t(%805), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %806), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %804, %29), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1678:0
  %809 : int[] = prim::ListConstruct(%786, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.8
  %810 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.35, %809), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%810, %29, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %15), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:188:0
  %813 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %25, %16), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %813), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %815 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.8 # torch/tensor.py:22:0
  %816 : int[] = prim::ListConstruct(%786, %29, %29, %787), scope: __module.transformer/__module.transformer.attentions.8
  %817 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%815, %816), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%817, %scores.17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:191:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
  %821 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %24, %18), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:1498:0
  %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%821, %20, %26), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
  %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:200:0
  %824 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %29, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %825 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%824, %30), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %826 : int[] = prim::ListConstruct(%786, %24, %21), scope: __module.transformer/__module.transformer.attentions.8
  %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%825, %826), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %828 : Tensor = prim::GetAttr[name="bias"](%782)
  %829 : Tensor = prim::GetAttr[name="weight"](%782)
  %830 : Float(2048:1, 2048:2048) = aten::t(%829), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %830), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %828, %29), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1678:0
  %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.88, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %835 : Tensor = prim::GetAttr[name="bias"](%60)
  %836 : Tensor = prim::GetAttr[name="weight"](%60)
  %837 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.8
  %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %837, %836, %835, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
  %839 : __torch__.torch.nn.modules.linear.___torch_mangle_226.Linear = prim::GetAttr[name="lin2"](%58)
  %840 : __torch__.torch.nn.modules.linear.___torch_mangle_225.Linear = prim::GetAttr[name="lin1"](%58)
  %841 : Tensor = prim::GetAttr[name="bias"](%840)
  %842 : Tensor = prim::GetAttr[name="weight"](%840)
  %843 : Float(2048:1, 8192:2048) = aten::t(%842), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.9, %843), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %841, %29), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1678:0
  %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.90), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:1369:0
  %847 : Tensor = prim::GetAttr[name="bias"](%839)
  %848 : Tensor = prim::GetAttr[name="weight"](%839)
  %849 : Float(8192:1, 2048:8192) = aten::t(%848), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %849), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %847, %29), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1678:0
  %852 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.92, %20, %26), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
  %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.9, %852, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %854 : Tensor = prim::GetAttr[name="bias"](%56)
  %855 : Tensor = prim::GetAttr[name="weight"](%56)
  %856 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.8
  %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %856, %855, %854, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
  %858 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %859 : Float(17:13, 13:1, 1:1) = aten::to(%858, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.10, %859), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %861 : __torch__.torch.nn.modules.linear.___torch_mangle_175.Linear = prim::GetAttr[name="out_lin"](%54)
  %862 : __torch__.torch.nn.modules.linear.___torch_mangle_174.Linear = prim::GetAttr[name="v_lin"](%54)
  %863 : __torch__.torch.nn.modules.linear.___torch_mangle_173.Linear = prim::GetAttr[name="k_lin"](%54)
  %864 : __torch__.torch.nn.modules.linear.___torch_mangle_172.Linear = prim::GetAttr[name="q_lin"](%54)
  %865 : int = aten::size(%input.94, %30), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %866 : int = aten::size(%input.94, %29), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %867 : Tensor = prim::GetAttr[name="bias"](%864)
  %868 : Tensor = prim::GetAttr[name="weight"](%864)
  %869 : Float(2048:1, 2048:2048) = aten::t(%868), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %869), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %867, %29), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1678:0
  %872 : int[] = prim::ListConstruct(%865, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.9
  %873 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.37, %872), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%873, %29, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %875 : Tensor = prim::GetAttr[name="bias"](%863)
  %876 : Tensor = prim::GetAttr[name="weight"](%863)
  %877 : Float(2048:1, 2048:2048) = aten::t(%876), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %877), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %875, %29), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1678:0
  %880 : int[] = prim::ListConstruct(%865, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.9
  %881 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.38, %880), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%881, %29, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %883 : Tensor = prim::GetAttr[name="bias"](%862)
  %884 : Tensor = prim::GetAttr[name="weight"](%862)
  %885 : Float(2048:1, 2048:2048) = aten::t(%884), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %885), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %883, %29), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1678:0
  %888 : int[] = prim::ListConstruct(%865, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.9
  %889 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.39, %888), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%889, %29, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %15), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:188:0
  %892 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %25, %16), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %892), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %894 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.9 # torch/tensor.py:22:0
  %895 : int[] = prim::ListConstruct(%865, %29, %29, %866), scope: __module.transformer/__module.transformer.attentions.9
  %896 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%894, %895), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%896, %scores.19), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %17), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:191:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
  %900 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %24, %18), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:1498:0
  %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%900, %20, %26), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
  %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:200:0
  %903 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %29, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %904 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%903, %30), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %905 : int[] = prim::ListConstruct(%865, %24, %21), scope: __module.transformer/__module.transformer.attentions.9
  %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%904, %905), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %907 : Tensor = prim::GetAttr[name="bias"](%861)
  %908 : Tensor = prim::GetAttr[name="weight"](%861)
  %909 : Float(2048:1, 2048:2048) = aten::t(%908), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %909), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %907, %29), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1678:0
  %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.98, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %914 : Tensor = prim::GetAttr[name="bias"](%52)
  %915 : Tensor = prim::GetAttr[name="weight"](%52)
  %916 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.9
  %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %916, %915, %914, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
  %918 : __torch__.torch.nn.modules.linear.___torch_mangle_229.Linear = prim::GetAttr[name="lin2"](%50)
  %919 : __torch__.torch.nn.modules.linear.___torch_mangle_228.Linear = prim::GetAttr[name="lin1"](%50)
  %920 : Tensor = prim::GetAttr[name="bias"](%919)
  %921 : Tensor = prim::GetAttr[name="weight"](%919)
  %922 : Float(2048:1, 8192:2048) = aten::t(%921), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.10, %922), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %920, %29), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1678:0
  %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.100), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:1369:0
  %926 : Tensor = prim::GetAttr[name="bias"](%918)
  %927 : Tensor = prim::GetAttr[name="weight"](%918)
  %928 : Float(8192:1, 2048:8192) = aten::t(%927), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %928), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %926, %29), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1678:0
  %931 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.102, %20, %26), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
  %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.10, %931, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %933 : Tensor = prim::GetAttr[name="bias"](%48)
  %934 : Tensor = prim::GetAttr[name="weight"](%48)
  %935 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.9
  %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %935, %934, %933, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
  %937 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %938 : Float(17:13, 13:1, 1:1) = aten::to(%937, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.11, %938), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %940 : __torch__.torch.nn.modules.linear.___torch_mangle_180.Linear = prim::GetAttr[name="out_lin"](%46)
  %941 : __torch__.torch.nn.modules.linear.___torch_mangle_179.Linear = prim::GetAttr[name="v_lin"](%46)
  %942 : __torch__.torch.nn.modules.linear.___torch_mangle_178.Linear = prim::GetAttr[name="k_lin"](%46)
  %943 : __torch__.torch.nn.modules.linear.___torch_mangle_177.Linear = prim::GetAttr[name="q_lin"](%46)
  %944 : int = aten::size(%input.104, %30), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %945 : int = aten::size(%input.104, %29), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %946 : Tensor = prim::GetAttr[name="bias"](%943)
  %947 : Tensor = prim::GetAttr[name="weight"](%943)
  %948 : Float(2048:1, 2048:2048) = aten::t(%947), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %948), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %946, %29), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1678:0
  %951 : int[] = prim::ListConstruct(%944, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.10
  %952 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.41, %951), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%952, %29, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %954 : Tensor = prim::GetAttr[name="bias"](%942)
  %955 : Tensor = prim::GetAttr[name="weight"](%942)
  %956 : Float(2048:1, 2048:2048) = aten::t(%955), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %956), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %954, %29), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1678:0
  %959 : int[] = prim::ListConstruct(%944, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.10
  %960 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.42, %959), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%960, %29, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %962 : Tensor = prim::GetAttr[name="bias"](%941)
  %963 : Tensor = prim::GetAttr[name="weight"](%941)
  %964 : Float(2048:1, 2048:2048) = aten::t(%963), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %964), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %962, %29), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1678:0
  %967 : int[] = prim::ListConstruct(%944, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.10
  %968 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.43, %967), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%968, %29, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %15), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:188:0
  %971 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %25, %16), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %971), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %973 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.10 # torch/tensor.py:22:0
  %974 : int[] = prim::ListConstruct(%944, %29, %29, %945), scope: __module.transformer/__module.transformer.attentions.10
  %975 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%973, %974), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%975, %scores.21), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %17), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:191:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
  %979 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %24, %18), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:1498:0
  %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%979, %20, %26), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
  %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:200:0
  %982 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %29, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %983 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%982, %30), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %984 : int[] = prim::ListConstruct(%944, %24, %21), scope: __module.transformer/__module.transformer.attentions.10
  %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%983, %984), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %986 : Tensor = prim::GetAttr[name="bias"](%940)
  %987 : Tensor = prim::GetAttr[name="weight"](%940)
  %988 : Float(2048:1, 2048:2048) = aten::t(%987), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %988), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %986, %29), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1678:0
  %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.108, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %993 : Tensor = prim::GetAttr[name="bias"](%44)
  %994 : Tensor = prim::GetAttr[name="weight"](%44)
  %995 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.10
  %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %995, %994, %993, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
  %997 : __torch__.torch.nn.modules.linear.___torch_mangle_232.Linear = prim::GetAttr[name="lin2"](%42)
  %998 : __torch__.torch.nn.modules.linear.___torch_mangle_231.Linear = prim::GetAttr[name="lin1"](%42)
  %999 : Tensor = prim::GetAttr[name="bias"](%998)
  %1000 : Tensor = prim::GetAttr[name="weight"](%998)
  %1001 : Float(2048:1, 8192:2048) = aten::t(%1000), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.11, %1001), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %999, %29), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1678:0
  %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.110), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:1369:0
  %1005 : Tensor = prim::GetAttr[name="bias"](%997)
  %1006 : Tensor = prim::GetAttr[name="weight"](%997)
  %1007 : Float(8192:1, 2048:8192) = aten::t(%1006), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %1007), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %1005, %29), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1678:0
  %1010 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.112, %20, %26), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
  %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.11, %1010, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %1012 : Tensor = prim::GetAttr[name="bias"](%40)
  %1013 : Tensor = prim::GetAttr[name="weight"](%40)
  %1014 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.10
  %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %1014, %1013, %1012, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1016 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1017 : Float(17:13, 13:1, 1:1) = aten::to(%1016, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.12, %1017), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1019 : __torch__.torch.nn.modules.linear.___torch_mangle_185.Linear = prim::GetAttr[name="out_lin"](%38)
  %1020 : __torch__.torch.nn.modules.linear.___torch_mangle_184.Linear = prim::GetAttr[name="v_lin"](%38)
  %1021 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name="k_lin"](%38)
  %1022 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name="q_lin"](%38)
  %1023 : int = aten::size(%input.114, %30), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1024 : int = aten::size(%input.114, %29), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1025 : Tensor = prim::GetAttr[name="bias"](%1022)
  %1026 : Tensor = prim::GetAttr[name="weight"](%1022)
  %1027 : Float(2048:1, 2048:2048) = aten::t(%1026), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1027), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %1025, %29), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1678:0
  %1030 : int[] = prim::ListConstruct(%1023, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.11
  %1031 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.45, %1030), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1031, %29, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1033 : Tensor = prim::GetAttr[name="bias"](%1021)
  %1034 : Tensor = prim::GetAttr[name="weight"](%1021)
  %1035 : Float(2048:1, 2048:2048) = aten::t(%1034), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1035), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %1033, %29), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1678:0
  %1038 : int[] = prim::ListConstruct(%1023, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.11
  %1039 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.46, %1038), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1039, %29, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1041 : Tensor = prim::GetAttr[name="bias"](%1020)
  %1042 : Tensor = prim::GetAttr[name="weight"](%1020)
  %1043 : Float(2048:1, 2048:2048) = aten::t(%1042), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1043), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %1041, %29), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1678:0
  %1046 : int[] = prim::ListConstruct(%1023, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.11
  %1047 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.47, %1046), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1047, %29, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %15), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:188:0
  %1050 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %25, %16), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %1050), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %1052 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %30), scope: __module.transformer/__module.transformer.attentions.11 # torch/tensor.py:22:0
  %1053 : int[] = prim::ListConstruct(%1023, %29, %29, %1024), scope: __module.transformer/__module.transformer.attentions.11
  %1054 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1052, %1053), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1054, %scores.23), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %17), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:191:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
  %1058 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %24, %18), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:1498:0
  %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1058, %20, %26), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
  %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:200:0
  %1061 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %29, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1062 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1061, %30), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1063 : int[] = prim::ListConstruct(%1023, %24, %21), scope: __module.transformer/__module.transformer.attentions.11
  %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1062, %1063), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1065 : Tensor = prim::GetAttr[name="bias"](%1019)
  %1066 : Tensor = prim::GetAttr[name="weight"](%1019)
  %1067 : Float(2048:1, 2048:2048) = aten::t(%1066), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %1067), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %1065, %29), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1678:0
  %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.118, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %29), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %1072 : Tensor = prim::GetAttr[name="bias"](%36)
  %1073 : Tensor = prim::GetAttr[name="weight"](%36)
  %1074 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.11
  %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %1074, %1073, %1072, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1076 : __torch__.torch.nn.modules.linear.___torch_mangle_235.Linear = prim::GetAttr[name="lin2"](%34)
  %1077 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name="lin1"](%34)
  %1078 : Tensor = prim::GetAttr[name="bias"](%1077)
  %1079 : Tensor = prim::GetAttr[name="weight"](%1077)
  %1080 : Float(2048:1, 8192:2048) = aten::t(%1079), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor, %1080), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %1078, %29), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1678:0
  %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.120), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:1369:0
  %1084 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1085 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1086 : Float(8192:1, 2048:8192) = aten::t(%1085), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %output.72 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %1086), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.72, %1084, %29), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1678:0
  %1089 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.122, %20, %26), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
  %input.123 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor, %1089, %29), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %1091 : Tensor = prim::GetAttr[name="bias"](%32)
  %1092 : Tensor = prim::GetAttr[name="weight"](%32)
  %1093 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.11
  %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.123, %1093, %1092, %1091, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1095 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1096 : Float(17:13, 13:1, 1:1) = aten::to(%1095, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.124 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor, %1096), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1098 : int = prim::Constant[value=0](), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %1099 : int = prim::Constant[value=9223372036854775807](), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %1100 : str = prim::Constant[value="blh,bl->bh"](), scope: __module.qa_outputs # torch/functional.py:327:0
  %1101 : int = prim::Constant[value=25](), scope: __module.qa_outputs # transformers/modeling_utils.py:1395:0
  %1102 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %1103 : int = prim::Constant[value=2048](), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %1104 : int = prim::Constant[value=-2](), scope: __module.qa_outputs # transformers/modeling_utils.py:1382:0
  %1105 : bool = prim::Constant[value=0](), scope: __module.qa_outputs # transformers/modeling_utils.py:1381:0
  %1106 : bool = prim::Constant[value=1](), scope: __module.qa_outputs # transformers/modeling_utils.py:1378:0
  %1107 : int = prim::Constant[value=5](), scope: __module.qa_outputs # transformers/modeling_utils.py:1378:0
  %1108 : None = prim::Constant(), scope: __module.qa_outputs
  %1109 : int = prim::Constant[value=2](), scope: __module.qa_outputs # transformers/modeling_utils.py:1375:0
  %1110 : int = prim::Constant[value=1](), scope: __module.qa_outputs/__module.qa_outputs.start_logits/__module.qa_outputs.start_logits.dense # torch/nn/functional.py:1678:0
  %1111 : int = prim::Constant[value=-1](), scope: __module.qa_outputs/__module.qa_outputs.start_logits # transformers/modeling_utils.py:1126:0
  %1112 : __torch__.transformers.modeling_utils.PoolerAnswerClass = prim::GetAttr[name="answer_class"](%3)
  %1113 : __torch__.transformers.modeling_utils.PoolerEndLogits = prim::GetAttr[name="end_logits"](%3)
  %1114 : __torch__.transformers.modeling_utils.PoolerStartLogits = prim::GetAttr[name="start_logits"](%3)
  %1115 : __torch__.torch.nn.modules.linear.___torch_mangle_252.Linear = prim::GetAttr[name="dense"](%1114)
  %1116 : Tensor = prim::GetAttr[name="bias"](%1115)
  %1117 : Tensor = prim::GetAttr[name="weight"](%1115)
  %1118 : Float(2048:1, 1:2048) = aten::t(%1117), scope: __module.qa_outputs/__module.qa_outputs.start_logits/__module.qa_outputs.start_logits.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:13, 13:1, 1:1) = aten::matmul(%input.124, %1118), scope: __module.qa_outputs/__module.qa_outputs.start_logits/__module.qa_outputs.start_logits.dense # torch/nn/functional.py:1676:0
  %1120 : Float(17:13, 13:1, 1:1) = aten::add_(%output.73, %1116, %1110), scope: __module.qa_outputs/__module.qa_outputs.start_logits/__module.qa_outputs.start_logits.dense # torch/nn/functional.py:1678:0
  %input.125 : Float(17:13, 13:1) = aten::squeeze(%1120, %1111), scope: __module.qa_outputs/__module.qa_outputs.start_logits # transformers/modeling_utils.py:1126:0
  %1122 : int = aten::size(%input.124, %1110), scope: __module.qa_outputs # transformers/modeling_utils.py:1375:0
  %1123 : int = aten::size(%input.124, %1109), scope: __module.qa_outputs # transformers/modeling_utils.py:1375:0
  %start_log_probs : Float(17:13, 13:1) = aten::softmax(%input.125, %1111, %1108), scope: __module.qa_outputs # torch/nn/functional.py:1498:0
  %1125 : Float(17:5, 5:1), %start_top_index : Long(17:5, 5:1) = aten::topk(%start_log_probs, %1107, %1111, %1106, %1106), scope: __module.qa_outputs # transformers/modeling_utils.py:1378:0
  %1127 : Long(17:5, 5:1, 1:1) = aten::unsqueeze(%start_top_index, %1111), scope: __module.qa_outputs # transformers/modeling_utils.py:1381:0
  %1128 : int[] = prim::ListConstruct(%1111, %1111, %1123), scope: __module.qa_outputs
  %start_top_index_exp : Long(17:5, 5:1, 2048:0) = aten::expand(%1127, %1128, %1105), scope: __module.qa_outputs # transformers/modeling_utils.py:1381:0
  %start_states.1 : Float(17:10240, 5:2048, 2048:1) = aten::gather(%input.124, %1104, %start_top_index_exp, %1105), scope: __module.qa_outputs # transformers/modeling_utils.py:1382:0
  %1131 : Float(17:10240, 1:10240, 5:2048, 2048:1) = aten::unsqueeze(%start_states.1, %1110), scope: __module.qa_outputs # transformers/modeling_utils.py:1383:0
  %1132 : int[] = prim::ListConstruct(%1111, %1122, %1111, %1111), scope: __module.qa_outputs
  %start_states.2 : Float(17:10240, 13:0, 5:2048, 2048:1) = aten::expand(%1131, %1132, %1105), scope: __module.qa_outputs # transformers/modeling_utils.py:1383:0
  %1134 : Float(17:26624, 13:2048, 1:2048, 2048:1) = aten::unsqueeze(%input.124, %1109), scope: __module.qa_outputs # transformers/modeling_utils.py:1385:0
  %hidden_states : Float(17:26624, 13:2048, 5:0, 2048:1) = aten::expand_as(%1134, %start_states.2), scope: __module.qa_outputs # transformers/modeling_utils.py:1385:0
  %1136 : __torch__.torch.nn.modules.linear.___torch_mangle_255.Linear = prim::GetAttr[name="dense_1"](%1113)
  %1137 : __torch__.torch.nn.modules.normalization.___torch_mangle_254.LayerNorm = prim::GetAttr[name="LayerNorm"](%1113)
  %1138 : __torch__.torch.nn.modules.linear.___torch_mangle_253.Linear = prim::GetAttr[name="dense_0"](%1113)
  %1139 : Tensor[] = prim::ListConstruct(%hidden_states, %start_states.2), scope: __module.qa_outputs/__module.qa_outputs.end_logits
  %input.126 : Float(17:266240, 13:20480, 5:4096, 4096:1) = aten::cat(%1139, %1111), scope: __module.qa_outputs/__module.qa_outputs.end_logits # transformers/modeling_utils.py:1190:0
  %1141 : Tensor = prim::GetAttr[name="bias"](%1138)
  %1142 : Tensor = prim::GetAttr[name="weight"](%1138)
  %1143 : Float(4096:1, 2048:4096) = aten::t(%1142), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_0 # torch/nn/functional.py:1676:0
  %output.74 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::matmul(%input.126, %1143), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_0 # torch/nn/functional.py:1676:0
  %input.127 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::add_(%output.74, %1141, %1110), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_0 # torch/nn/functional.py:1678:0
  %input.128 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::tanh(%input.127), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.activation # torch/nn/modules/activation.py:350:0
  %1147 : Tensor = prim::GetAttr[name="bias"](%1137)
  %1148 : Tensor = prim::GetAttr[name="weight"](%1137)
  %1149 : int[] = prim::ListConstruct(%1103), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.LayerNorm
  %input.129 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::layer_norm(%input.128, %1149, %1148, %1147, %1102, %1106), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %1151 : Tensor = prim::GetAttr[name="bias"](%1136)
  %1152 : Tensor = prim::GetAttr[name="weight"](%1136)
  %1153 : Float(2048:1, 1:2048) = aten::t(%1152), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_1 # torch/nn/functional.py:1676:0
  %output : Float(17:65, 13:5, 5:1, 1:1) = aten::matmul(%input.129, %1153), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_1 # torch/nn/functional.py:1676:0
  %1155 : Float(17:65, 13:5, 5:1, 1:1) = aten::add_(%output, %1151, %1110), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_1 # torch/nn/functional.py:1678:0
  %input.130 : Float(17:65, 13:5, 5:1) = aten::squeeze(%1155, %1111), scope: __module.qa_outputs/__module.qa_outputs.end_logits # transformers/modeling_utils.py:1193:0
  %end_log_probs : Float(17:65, 13:5, 5:1) = aten::softmax(%input.130, %1110, %1108), scope: __module.qa_outputs # torch/nn/functional.py:1498:0
  %end_top_log_probs : Float(17:25, 5:5, 5:1), %end_top_index : Long(17:25, 5:5, 5:1) = aten::topk(%end_log_probs, %1107, %1110, %1106, %1106), scope: __module.qa_outputs # transformers/modeling_utils.py:1392:0
  %1160 : int[] = prim::ListConstruct(%1111, %1101), scope: __module.qa_outputs
  %1161 : Float(17:25, 25:1) = aten::view(%end_top_log_probs, %1160), scope: __module.qa_outputs # transformers/modeling_utils.py:1395:0
  %1162 : int[] = prim::ListConstruct(%1111, %1101), scope: __module.qa_outputs
  %1163 : Long(17:25, 25:1) = aten::view(%end_top_index, %1162), scope: __module.qa_outputs # transformers/modeling_utils.py:1396:0
  %1164 : Tensor[] = prim::ListConstruct(%input.124, %start_log_probs), scope: __module.qa_outputs
  %start_states : Float(17:2048, 2048:1) = aten::einsum(%1100, %1164), scope: __module.qa_outputs # torch/functional.py:327:0
  %1166 : __torch__.torch.nn.modules.linear.___torch_mangle_258.Linear = prim::GetAttr[name="dense_1"](%1112)
  %1167 : __torch__.torch.nn.modules.linear.___torch_mangle_256.Linear = prim::GetAttr[name="dense_0"](%1112)
  %1168 : Float(17:26624, 13:2048, 2048:1) = aten::slice(%input.124, %1098, %1098, %1099, %1110), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %1169 : Float(17:26624, 2048:1) = aten::select(%1168, %1110, %1111), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %cls_token_state : Float(17:26624, 2048:1) = aten::slice(%1169, %1110, %1098, %1099, %1110), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %1171 : Tensor[] = prim::ListConstruct(%start_states, %cls_token_state), scope: __module.qa_outputs/__module.qa_outputs.answer_class
  %input.131 : Float(17:4096, 4096:1) = aten::cat(%1171, %1111), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1260:0
  %1173 : Tensor = prim::GetAttr[name="bias"](%1167)
  %1174 : Tensor = prim::GetAttr[name="weight"](%1167)
  %1175 : Float(4096:1, 2048:4096) = aten::t(%1174), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.dense_0 # torch/nn/functional.py:1674:0
  %input.132 : Float(17:2048, 2048:1) = aten::addmm(%1173, %input.131, %1175, %1110, %1110), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.dense_0 # torch/nn/functional.py:1674:0
  %input : Float(17:2048, 2048:1) = aten::tanh(%input.132), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.activation # torch/nn/modules/activation.py:350:0
  %1178 : Tensor = prim::GetAttr[name="weight"](%1166)
  %1179 : Float(2048:1, 1:2048) = aten::t(%1178), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.dense_1 # torch/nn/functional.py:1676:0
  %1180 : Float(17:1, 1:1) = aten::matmul(%input, %1179), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.dense_1 # torch/nn/functional.py:1676:0
  %1181 : Float(17:1) = aten::squeeze(%1180, %1111), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1262:0
  %1182 : (Float(17:5, 5:1), Long(17:5, 5:1), Float(17:25, 25:1), Long(17:25, 25:1), Float(17:1)) = prim::TupleConstruct(%1125, %start_top_index, %1161, %1163, %1181)
  %7 : Float(17:5, 5:1), %8 : Long(17:5, 5:1), %9 : Float(17:25, 25:1), %10 : Long(17:25, 25:1), %11 : Float(17:1) = prim::TupleUnpack(%1182)
  %12 : (Float(17:5, 5:1), Long(17:5, 5:1), Float(17:25, 25:1), Long(17:25, 25:1), Float(17:1)) = prim::TupleConstruct(%7, %8, %9, %10, %11)
  return (%12)
