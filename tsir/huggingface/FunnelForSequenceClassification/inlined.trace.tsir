graph(%self.1 : __torch__.transformers.modeling_funnel.FunnelForSequenceClassification,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_funnel.___torch_mangle_3909.FunnelClassificationHead = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.transformers.modeling_funnel.___torch_mangle_3905.FunnelBaseModel = prim::GetAttr[name="funnel"](%self.1)
  %16 : int = prim::Constant[value=384](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %17 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %18 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %19 : Long() = prim::Constant[value={384}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %20 : Float() = prim::Constant[value={10000}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %21 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
  %22 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %23 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %24 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %25 : Long() = prim::Constant[value={13}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %26 : Float(1:1) = prim::Constant[value={-1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %27 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %28 : Long() = prim::Constant[value={14}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %29 : int = prim::Constant[value=-2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %30 : Float(1:1) = prim::Constant[value={-3}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %31 : Long() = prim::Constant[value={16}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %32 : int = prim::Constant[value=-4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %33 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %34 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %35 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %36 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %37 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %38 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %39 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %40 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %41 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %42 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %43 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %44 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %45 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %46 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %47 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %48 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %49 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %50 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %51 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %52 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %53 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %54 : bool = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:945:0
  %55 : Device = prim::Constant[value="cpu"](), scope: __module.funnel # transformers/modeling_funnel.py:945:0
  %56 : int = prim::Constant[value=4](), scope: __module.funnel # transformers/modeling_funnel.py:945:0
  %57 : int = prim::Constant[value=1](), scope: __module.funnel # transformers/modeling_funnel.py:934:0
  %58 : int = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:934:0
  %59 : __torch__.transformers.modeling_funnel.___torch_mangle_3904.FunnelEncoder = prim::GetAttr[name="encoder"](%4)
  %60 : __torch__.transformers.modeling_funnel.___torch_mangle_3716.FunnelEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %61 : int = aten::size(%input_ids, %58), scope: __module.funnel # transformers/modeling_funnel.py:934:0
  %62 : int = aten::size(%input_ids, %57), scope: __module.funnel # transformers/modeling_funnel.py:934:0
  %63 : int[] = prim::ListConstruct(%61, %62), scope: __module.funnel
  %token_type_ids : Long(17:13, 13:1) = aten::zeros(%63, %56, %58, %55, %54), scope: __module.funnel # transformers/modeling_funnel.py:945:0
  %65 : __torch__.torch.nn.modules.normalization.___torch_mangle_3714.LayerNorm = prim::GetAttr[name="layer_norm"](%60)
  %66 : __torch__.torch.nn.modules.sparse.___torch_mangle_3713.Embedding = prim::GetAttr[name="word_embeddings"](%60)
  %67 : Tensor = prim::GetAttr[name="weight"](%66)
  %input.1 : Float(17:9984, 13:768, 768:1) = aten::embedding(%67, %input_ids, %49, %54, %54), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %69 : Tensor = prim::GetAttr[name="bias"](%65)
  %70 : Tensor = prim::GetAttr[name="weight"](%65)
  %71 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm
  %input.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.1, %71, %70, %69, %51, %50), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.2, %53, %54), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_3902.ModuleList = prim::GetAttr[name="2"](%74)
  %76 : __torch__.transformers.modeling_funnel.___torch_mangle_3901.FunnelLayer = prim::GetAttr[name="3"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_3902.ModuleList = prim::GetAttr[name="2"](%77)
  %79 : __torch__.transformers.modeling_funnel.___torch_mangle_3886.FunnelLayer = prim::GetAttr[name="2"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_3902.ModuleList = prim::GetAttr[name="2"](%80)
  %82 : __torch__.transformers.modeling_funnel.___torch_mangle_3871.FunnelLayer = prim::GetAttr[name="1"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_3902.ModuleList = prim::GetAttr[name="2"](%83)
  %85 : __torch__.transformers.modeling_funnel.___torch_mangle_3856.FunnelLayer = prim::GetAttr[name="0"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_3841.ModuleList = prim::GetAttr[name="1"](%86)
  %88 : __torch__.transformers.modeling_funnel.___torch_mangle_3840.FunnelLayer = prim::GetAttr[name="3"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_3841.ModuleList = prim::GetAttr[name="1"](%89)
  %91 : __torch__.transformers.modeling_funnel.___torch_mangle_3825.FunnelLayer = prim::GetAttr[name="2"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_3841.ModuleList = prim::GetAttr[name="1"](%92)
  %94 : __torch__.transformers.modeling_funnel.___torch_mangle_3810.FunnelLayer = prim::GetAttr[name="1"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_3841.ModuleList = prim::GetAttr[name="1"](%95)
  %97 : __torch__.transformers.modeling_funnel.___torch_mangle_3795.FunnelLayer = prim::GetAttr[name="0"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_3780.ModuleList = prim::GetAttr[name="0"](%98)
  %100 : __torch__.transformers.modeling_funnel.___torch_mangle_3779.FunnelLayer = prim::GetAttr[name="3"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_3780.ModuleList = prim::GetAttr[name="0"](%101)
  %103 : __torch__.transformers.modeling_funnel.___torch_mangle_3764.FunnelLayer = prim::GetAttr[name="2"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_3780.ModuleList = prim::GetAttr[name="0"](%104)
  %106 : __torch__.transformers.modeling_funnel.___torch_mangle_3749.FunnelLayer = prim::GetAttr[name="1"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_3903.ModuleList = prim::GetAttr[name="blocks"](%59)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_3780.ModuleList = prim::GetAttr[name="0"](%107)
  %109 : __torch__.transformers.modeling_funnel.___torch_mangle_3734.FunnelLayer = prim::GetAttr[name="0"](%108)
  %attention_mask.2 : Float(17:13, 13:1) = aten::type_as(%attention_mask.1, %inputs_embeds), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:625:0
  %111 : int = aten::size(%inputs_embeds, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:195:0
  %seq_len.1 : Long() = prim::NumToTensor(%111), scope: __module.funnel/__module.funnel.encoder
  %freq_seq : Float(384:1) = aten::arange(%58, %16, %17, %18, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %114 : Float(384:1) = aten::div(%freq_seq, %19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %115 : Float() = aten::to(%20, %55, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %116 : Float() = aten::detach(%115), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %117 : Float(384:1) = aten::pow(%116, %114), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %118 : Float(384:1) = aten::reciprocal(%117), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %inv_freq : Float(384:1) = aten::mul(%118, %22), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %120 : Long() = aten::neg(%seq_len.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %121 : Long() = aten::mul(%120, %23), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %122 : Scalar = aten::ScalarImplicit(%121), scope: __module.funnel/__module.funnel.encoder
  %123 : Long() = aten::mul(%seq_len.1, %23), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %124 : Scalar = aten::ScalarImplicit(%123), scope: __module.funnel/__module.funnel.encoder
  %rel_pos_id : Float(52:1) = aten::arange(%122, %124, %17, %18, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %zero_offset : Long() = aten::mul(%seq_len.1, %23), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:251:0
  %127 : Float(52:1) = aten::slice(%rel_pos_id, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %128 : Float(52:1, 1:1) = aten::unsqueeze(%127, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %129 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %sinusoid : Float(52:384, 384:1) = aten::mul(%128, %129), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %input.3 : Float(52:384, 384:1) = aten::sin(%sinusoid), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:253:0
  %sin_embed : Float(52:384, 384:1) = aten::dropout(%input.3, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.4 : Float(52:384, 384:1) = aten::cos(%sinusoid), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:254:0
  %cos_embed : Float(52:384, 384:1) = aten::dropout(%input.4, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %135 : Tensor[] = prim::ListConstruct(%sin_embed, %cos_embed), scope: __module.funnel/__module.funnel.encoder
  %pos_embed : Float(52:768, 768:1) = aten::cat(%135, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:255:0
  %pos : Float(13:1) = aten::arange(%58, %111, %57, %18, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
  %138 : Float() = aten::select(%pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %139 : Float() = aten::select(%pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.1 : Float() = aten::sub(%138, %139, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.1 : Float() = aten::add(%ref_point.1, %25, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %142 : Scalar = aten::ScalarImplicit(%max_dist.1), scope: __module.funnel/__module.funnel.encoder
  %143 : Float() = aten::select(%pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %144 : Float() = aten::select(%pos, %58, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.1 : Float() = aten::sub(%143, %144, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %146 : Float() = aten::sub(%min_dist.1, %22, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %147 : Scalar = aten::ScalarImplicit(%146), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.1 : Long(26:1) = aten::arange(%142, %147, %49, %56, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %149 : Long(26:1) = aten::slice(%rel_pos.1, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %150 : Long(26:1, 1:1) = aten::unsqueeze(%149, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.2 : Long(26:1, 1:1) = aten::add(%150, %zero_offset, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %152 : int = aten::size(%rel_pos.2, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %153 : int[] = prim::ListConstruct(%152, %52), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.3 : Long(26:1, 768:0) = aten::expand(%rel_pos.2, %153, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %155 : Float(26:768, 768:1) = aten::gather(%pos_embed, %58, %rel_pos.3, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %156 : Float(1:1) = aten::to(%26, %55, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.1 : Float(1:1) = aten::detach(%156), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.1 : Float(11:1) = aten::slice(%pos, %58, %57, %49, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %159 : Float(6:2) = aten::slice(%pooled_pos_id.1, %58, %58, %24, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %160 : Tensor[] = prim::ListConstruct(%cls_pos.1, %159), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos.1 : Float(7:1) = aten::cat(%160, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %162 : Float() = aten::select(%pooled_pos.1, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %163 : Float() = aten::select(%pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.2 : Float() = aten::sub(%162, %163, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.2 : Float() = aten::add(%ref_point.2, %28, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %166 : Scalar = aten::ScalarImplicit(%max_dist.2), scope: __module.funnel/__module.funnel.encoder
  %167 : Float() = aten::select(%pooled_pos.1, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %168 : Float() = aten::select(%pos, %58, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.2 : Float() = aten::sub(%167, %168, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %170 : Float() = aten::sub(%min_dist.2, %22, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %171 : Scalar = aten::ScalarImplicit(%170), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.4 : Long(27:1) = aten::arange(%166, %171, %49, %56, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %173 : Long(27:1) = aten::slice(%rel_pos.4, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %174 : Long(27:1, 1:1) = aten::unsqueeze(%173, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.5 : Long(27:1, 1:1) = aten::add(%174, %zero_offset, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %176 : int = aten::size(%rel_pos.5, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %177 : int[] = prim::ListConstruct(%176, %52), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.6 : Long(27:1, 768:0) = aten::expand(%rel_pos.5, %177, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %179 : Float(27:768, 768:1) = aten::gather(%pos_embed, %58, %rel_pos.6, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %180 : Float() = aten::select(%pooled_pos.1, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %181 : Float() = aten::select(%pooled_pos.1, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.3 : Float() = aten::sub(%180, %181, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.3 : Float() = aten::add(%ref_point.3, %28, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %184 : Scalar = aten::ScalarImplicit(%max_dist.3), scope: __module.funnel/__module.funnel.encoder
  %185 : Float() = aten::select(%pooled_pos.1, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %186 : Float() = aten::select(%pooled_pos.1, %58, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.3 : Float() = aten::sub(%185, %186, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %188 : Float() = aten::sub(%min_dist.3, %22, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %189 : Scalar = aten::ScalarImplicit(%188), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.7 : Long(14:1) = aten::arange(%184, %189, %29, %56, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %191 : Long(14:1) = aten::slice(%rel_pos.7, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %192 : Long(14:1, 1:1) = aten::unsqueeze(%191, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.8 : Long(14:1, 1:1) = aten::add(%192, %zero_offset, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %194 : int = aten::size(%rel_pos.8, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %195 : int[] = prim::ListConstruct(%194, %52), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.9 : Long(14:1, 768:0) = aten::expand(%rel_pos.8, %195, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %197 : Float(14:768, 768:1) = aten::gather(%pos_embed, %58, %rel_pos.9, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %198 : Float(1:1) = aten::to(%30, %55, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos : Float(1:1) = aten::detach(%198), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id : Float(5:1) = aten::slice(%pooled_pos.1, %58, %57, %49, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %201 : Float(3:2) = aten::slice(%pooled_pos_id, %58, %58, %24, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %202 : Tensor[] = prim::ListConstruct(%cls_pos, %201), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos : Float(4:1) = aten::cat(%202, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %204 : Float() = aten::select(%pooled_pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %205 : Float() = aten::select(%pooled_pos.1, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.4 : Float() = aten::sub(%204, %205, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.4 : Float() = aten::add(%ref_point.4, %31, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %208 : Scalar = aten::ScalarImplicit(%max_dist.4), scope: __module.funnel/__module.funnel.encoder
  %209 : Float() = aten::select(%pooled_pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %210 : Float() = aten::select(%pooled_pos.1, %58, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.4 : Float() = aten::sub(%209, %210, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %212 : Float() = aten::sub(%min_dist.4, %22, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %213 : Scalar = aten::ScalarImplicit(%212), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.10 : Long(15:1) = aten::arange(%208, %213, %29, %56, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %215 : Long(15:1) = aten::slice(%rel_pos.10, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %216 : Long(15:1, 1:1) = aten::unsqueeze(%215, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.11 : Long(15:1, 1:1) = aten::add(%216, %zero_offset, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %218 : int = aten::size(%rel_pos.11, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %219 : int[] = prim::ListConstruct(%218, %52), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.12 : Long(15:1, 768:0) = aten::expand(%rel_pos.11, %219, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %221 : Float(15:768, 768:1) = aten::gather(%pos_embed, %58, %rel_pos.12, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %222 : Float() = aten::select(%pooled_pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %223 : Float() = aten::select(%pooled_pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point : Float() = aten::sub(%222, %223, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist : Float() = aten::add(%ref_point, %31, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %226 : Scalar = aten::ScalarImplicit(%max_dist), scope: __module.funnel/__module.funnel.encoder
  %227 : Float() = aten::select(%pooled_pos, %58, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %228 : Float() = aten::select(%pooled_pos, %58, %49), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist : Float() = aten::sub(%227, %228, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %230 : Float() = aten::sub(%min_dist, %22, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %231 : Scalar = aten::ScalarImplicit(%230), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.13 : Long(8:1) = aten::arange(%226, %231, %32, %56, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %233 : Long(8:1) = aten::slice(%rel_pos.13, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %234 : Long(8:1, 1:1) = aten::unsqueeze(%233, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.14 : Long(8:1, 1:1) = aten::add(%234, %zero_offset, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %236 : int = aten::size(%rel_pos.14, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %237 : int[] = prim::ListConstruct(%236, %52), scope: __module.funnel/__module.funnel.encoder
  %rel_pos : Long(8:1, 768:0) = aten::expand(%rel_pos.14, %237, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %239 : Float(8:768, 768:1) = aten::gather(%pos_embed, %58, %rel_pos, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %240 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %241 : Long(17:13, 13:1) = aten::slice(%240, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %242 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%241, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %243 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %244 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%243, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.1 : Bool(17:169, 13:13, 13:1) = aten::eq(%242, %244), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %cls_ids : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %27), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %247 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %248 : Bool(17:13, 13:1) = aten::slice(%247, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %249 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%248, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %250 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %251 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%250, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %cls_mat : Bool(17:169, 13:13, 13:1) = aten::__or__(%249, %251), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.2 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat, %token_type_mat.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:211:0
  %254 : Long() = aten::sub(%seq_len.1, %22, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %255 : int = aten::Int(%254), scope: __module.funnel/__module.funnel.encoder
  %256 : Long() = aten::sub(%seq_len.1, %22, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %257 : int = aten::Int(%256), scope: __module.funnel/__module.funnel.encoder
  %258 : int[] = prim::ListConstruct(%255, %257), scope: __module.funnel/__module.funnel.encoder
  %input.5 : Float(12:12, 12:1) = aten::ones(%258, %18, %58, %55, %54), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %260 : int[] = prim::ListConstruct(%57, %58, %57, %58), scope: __module.funnel/__module.funnel.encoder
  %cls_mask.1 : Float(13:13, 13:1) = aten::constant_pad_nd(%input.5, %260, %58), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
  %262 : __torch__.transformers.modeling_funnel.___torch_mangle_3733.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%109)
  %263 : __torch__.transformers.modeling_funnel.___torch_mangle_3727.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%109)
  %264 : __torch__.torch.nn.modules.normalization.___torch_mangle_3726.LayerNorm = prim::GetAttr[name="layer_norm"](%263)
  %265 : __torch__.torch.nn.modules.linear.___torch_mangle_3725.Linear = prim::GetAttr[name="post_proj"](%263)
  %266 : Tensor = prim::GetAttr[name="seg_embed"](%263)
  %267 : Tensor = prim::GetAttr[name="r_s_bias"](%263)
  %268 : Tensor = prim::GetAttr[name="r_kernel"](%263)
  %269 : Tensor = prim::GetAttr[name="r_r_bias"](%263)
  %270 : Tensor = prim::GetAttr[name="r_w_bias"](%263)
  %271 : __torch__.torch.nn.modules.linear.___torch_mangle_3724.Linear = prim::GetAttr[name="v_head"](%263)
  %272 : __torch__.torch.nn.modules.linear.___torch_mangle_3723.Linear = prim::GetAttr[name="k_head"](%263)
  %273 : __torch__.torch.nn.modules.linear.___torch_mangle_3722.Linear = prim::GetAttr[name="q_head"](%263)
  %274 : int = aten::size(%inputs_embeds, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %275 : int = aten::size(%inputs_embeds, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %276 : int = aten::size(%inputs_embeds, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
  %277 : Tensor = prim::GetAttr[name="weight"](%273)
  %278 : Float(768:1, 768:768) = aten::t(%277), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %279 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %278), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %280 : int[] = prim::ListConstruct(%274, %275, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %q_head.1 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%279, %280), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %282 : Tensor = prim::GetAttr[name="bias"](%272)
  %283 : Tensor = prim::GetAttr[name="weight"](%272)
  %284 : Float(768:1, 768:768) = aten::t(%283), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %284), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %286 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %282, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
  %287 : int[] = prim::ListConstruct(%274, %276, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %288 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%286, %287), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
  %289 : Tensor = prim::GetAttr[name="bias"](%271)
  %290 : Tensor = prim::GetAttr[name="weight"](%271)
  %291 : Float(768:1, 768:768) = aten::t(%290), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %291), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %293 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %289, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
  %294 : int[] = prim::ListConstruct(%274, %276, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %295 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%293, %294), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.1, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.1 : Float(12:64, 64:1) = aten::mul(%270, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
  %298 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_w_bias.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
  %299 : Tensor[] = prim::ListConstruct(%298, %288), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %content_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%42, %299), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %v.1 : Float(12:64, 64:1) = aten::mul(%269, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
  %302 : Tensor[] = prim::ListConstruct(%155, %268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %303 : Float(26:768, 12:64, 64:1) = aten::einsum(%43, %302), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %304 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %v.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
  %305 : Tensor[] = prim::ListConstruct(%304, %303), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.1 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%44, %305), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %307 : int = aten::size(%positional_attn.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %308 : int = aten::size(%positional_attn.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %309 : int = aten::size(%positional_attn.1, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %310 : int = aten::size(%positional_attn.1, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.1 : Long() = prim::NumToTensor(%310), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %312 : int[] = prim::ListConstruct(%307, %308, %310, %309), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.2 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.1, %312), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:428:0
  %314 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %315 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%314, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %316 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%315, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.3 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%316, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %318 : Long() = aten::sub(%max_rel_len.1, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %319 : int = aten::Int(%318), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %320 : int[] = prim::ListConstruct(%307, %308, %309, %319), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.4 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.3, %320), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.5 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.4, %45, %58, %276, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.6 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:498:0
  %324 : int = aten::size(%token_type_mat.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %325 : int = aten::size(%token_type_mat.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %326 : int = aten::size(%token_type_mat.2, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.1 : Float(12:64, 64:1) = aten::mul(%267, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
  %328 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_s_bias.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
  %329 : Tensor[] = prim::ListConstruct(%328, %266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %330 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%46, %329), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %331 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %332 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%331, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %333 : int = aten::size(%q_head.2, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %334 : int[] = prim::ListConstruct(%324, %333, %325, %326), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %token_type_mat.3 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%332, %334, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %336 : Tensor[] = aten::split(%330, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:371:0
  %diff_token_type.1 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.1 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%336), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %339 : int = aten::size(%token_type_mat.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %340 : int = aten::size(%token_type_mat.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %341 : int = aten::size(%token_type_mat.3, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %342 : int = aten::size(%token_type_mat.3, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %343 : int[] = prim::ListConstruct(%339, %340, %341, %342), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %344 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.1, %343, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %345 : int = aten::size(%token_type_mat.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %346 : int = aten::size(%token_type_mat.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %347 : int = aten::size(%token_type_mat.3, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %348 : int = aten::size(%token_type_mat.3, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %349 : int[] = prim::ListConstruct(%345, %346, %347, %348), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %350 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.1, %349, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.3, %344, %350), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.1, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:522:0
  %353 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.1, %positional_attn.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%353, %token_type_attn.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.1, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
  %356 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %357 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%356, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %358 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%357, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %359 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%358, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %360 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%359, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:396:0
  %361 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%360, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.2, %361, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.3, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
  %364 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.6, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %365 : Tensor[] = prim::ListConstruct(%364, %295), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %attn_vec.1 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%48, %365), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %367 : int[] = prim::ListConstruct(%274, %275, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %input.7 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.1, %367), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
  %369 : Tensor = prim::GetAttr[name="bias"](%265)
  %370 : Tensor = prim::GetAttr[name="weight"](%265)
  %371 : Float(768:1, 768:768) = aten::t(%370), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.7, %371), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %369, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.8, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %attn_out.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
  %376 : Tensor = prim::GetAttr[name="bias"](%264)
  %377 : Tensor = prim::GetAttr[name="weight"](%264)
  %378 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %378, %377, %376, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %380 : __torch__.torch.nn.modules.normalization.___torch_mangle_3732.LayerNorm = prim::GetAttr[name="layer_norm"](%262)
  %381 : __torch__.torch.nn.modules.linear.___torch_mangle_3730.Linear = prim::GetAttr[name="linear_2"](%262)
  %382 : __torch__.torch.nn.modules.linear.___torch_mangle_3728.Linear = prim::GetAttr[name="linear_1"](%262)
  %383 : Tensor = prim::GetAttr[name="bias"](%382)
  %384 : Tensor = prim::GetAttr[name="weight"](%382)
  %385 : Float(768:1, 3072:768) = aten::t(%384), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.10, %385), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.1 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.4, %383, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %388 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.1, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %389 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.1, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %390 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%389, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %391 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.1, %390, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %392 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%391, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %393 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%392), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %394 : Float(17:39936, 13:3072, 3072:1) = aten::add(%393, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%388, %394), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.11, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %397 : Tensor = prim::GetAttr[name="bias"](%381)
  %398 : Tensor = prim::GetAttr[name="weight"](%381)
  %399 : Float(3072:1, 768:3072) = aten::t(%398), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %399), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.5, %397, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%input.10, %h.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
  %404 : Tensor = prim::GetAttr[name="bias"](%380)
  %405 : Tensor = prim::GetAttr[name="weight"](%380)
  %406 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm
  %query.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %406, %405, %404, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %408 : __torch__.transformers.modeling_funnel.___torch_mangle_3748.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%106)
  %409 : __torch__.transformers.modeling_funnel.___torch_mangle_3742.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%106)
  %410 : __torch__.torch.nn.modules.normalization.___torch_mangle_3741.LayerNorm = prim::GetAttr[name="layer_norm"](%409)
  %411 : __torch__.torch.nn.modules.linear.___torch_mangle_3740.Linear = prim::GetAttr[name="post_proj"](%409)
  %412 : Tensor = prim::GetAttr[name="seg_embed"](%409)
  %413 : Tensor = prim::GetAttr[name="r_s_bias"](%409)
  %414 : Tensor = prim::GetAttr[name="r_kernel"](%409)
  %415 : Tensor = prim::GetAttr[name="r_r_bias"](%409)
  %416 : Tensor = prim::GetAttr[name="r_w_bias"](%409)
  %417 : __torch__.torch.nn.modules.linear.___torch_mangle_3739.Linear = prim::GetAttr[name="v_head"](%409)
  %418 : __torch__.torch.nn.modules.linear.___torch_mangle_3738.Linear = prim::GetAttr[name="k_head"](%409)
  %419 : __torch__.torch.nn.modules.linear.___torch_mangle_3737.Linear = prim::GetAttr[name="q_head"](%409)
  %420 : int = aten::size(%query.1, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %421 : int = aten::size(%query.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %422 : int = aten::size(%query.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
  %423 : Tensor = prim::GetAttr[name="weight"](%419)
  %424 : Float(768:1, 768:768) = aten::t(%423), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %425 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %424), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %426 : int[] = prim::ListConstruct(%420, %421, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %q_head.3 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%425, %426), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
  %428 : Tensor = prim::GetAttr[name="bias"](%418)
  %429 : Tensor = prim::GetAttr[name="weight"](%418)
  %430 : Float(768:1, 768:768) = aten::t(%429), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %430), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %432 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %428, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
  %433 : int[] = prim::ListConstruct(%420, %422, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %434 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%432, %433), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
  %435 : Tensor = prim::GetAttr[name="bias"](%417)
  %436 : Tensor = prim::GetAttr[name="weight"](%417)
  %437 : Float(768:1, 768:768) = aten::t(%436), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %437), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %439 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %435, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
  %440 : int[] = prim::ListConstruct(%420, %422, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %441 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%439, %440), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.3, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.2 : Float(12:64, 64:1) = aten::mul(%416, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
  %444 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_w_bias.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
  %445 : Tensor[] = prim::ListConstruct(%444, %434), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %content_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%42, %445), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %v.2 : Float(12:64, 64:1) = aten::mul(%415, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
  %448 : Tensor[] = prim::ListConstruct(%155, %414), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %449 : Float(26:768, 12:64, 64:1) = aten::einsum(%43, %448), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %450 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %v.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
  %451 : Tensor[] = prim::ListConstruct(%450, %449), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.7 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%44, %451), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %453 : int = aten::size(%positional_attn.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %454 : int = aten::size(%positional_attn.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %455 : int = aten::size(%positional_attn.7, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %456 : int = aten::size(%positional_attn.7, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.2 : Long() = prim::NumToTensor(%456), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %458 : int[] = prim::ListConstruct(%453, %454, %456, %455), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.8 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.7, %458), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:428:0
  %460 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.8, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %461 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%460, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %462 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%461, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.9 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%462, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %464 : Long() = aten::sub(%max_rel_len.2, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %465 : int = aten::Int(%464), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %466 : int[] = prim::ListConstruct(%453, %454, %455, %465), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.10 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.9, %466), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.11 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.10, %45, %58, %422, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.12 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.11, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:498:0
  %470 : int = aten::size(%token_type_mat.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %471 : int = aten::size(%token_type_mat.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %472 : int = aten::size(%token_type_mat.2, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.2 : Float(12:64, 64:1) = aten::mul(%413, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
  %474 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_s_bias.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
  %475 : Tensor[] = prim::ListConstruct(%474, %412), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %476 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%46, %475), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %477 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %478 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%477, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %479 : int = aten::size(%q_head.4, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %480 : int[] = prim::ListConstruct(%470, %479, %471, %472), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %token_type_mat.4 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%478, %480, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %482 : Tensor[] = aten::split(%476, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:371:0
  %diff_token_type.2 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.2 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%482), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %485 : int = aten::size(%token_type_mat.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %486 : int = aten::size(%token_type_mat.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %487 : int = aten::size(%token_type_mat.4, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %488 : int = aten::size(%token_type_mat.4, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %489 : int[] = prim::ListConstruct(%485, %486, %487, %488), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %490 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.2, %489, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %491 : int = aten::size(%token_type_mat.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %492 : int = aten::size(%token_type_mat.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %493 : int = aten::size(%token_type_mat.4, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %494 : int = aten::size(%token_type_mat.4, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %495 : int[] = prim::ListConstruct(%491, %492, %493, %494), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %496 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.2, %495, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.4, %490, %496), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.3, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:522:0
  %499 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.2, %positional_attn.12, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%499, %token_type_attn.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.4, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
  %502 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %503 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%502, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %504 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%503, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %505 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%504, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %506 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%505, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:396:0
  %507 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%506, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.5, %507, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %input.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.6, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
  %510 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.15, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %511 : Tensor[] = prim::ListConstruct(%510, %441), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %attn_vec.2 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%48, %511), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %513 : int[] = prim::ListConstruct(%420, %421, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %input.16 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.2, %513), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
  %515 : Tensor = prim::GetAttr[name="bias"](%411)
  %516 : Tensor = prim::GetAttr[name="weight"](%411)
  %517 : Float(768:1, 768:768) = aten::t(%516), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.16, %517), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %515, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.17, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::add(%query.1, %attn_out.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
  %522 : Tensor = prim::GetAttr[name="bias"](%410)
  %523 : Tensor = prim::GetAttr[name="weight"](%410)
  %524 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.18, %524, %523, %522, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %526 : __torch__.torch.nn.modules.normalization.___torch_mangle_3747.LayerNorm = prim::GetAttr[name="layer_norm"](%408)
  %527 : __torch__.torch.nn.modules.linear.___torch_mangle_3745.Linear = prim::GetAttr[name="linear_2"](%408)
  %528 : __torch__.torch.nn.modules.linear.___torch_mangle_3743.Linear = prim::GetAttr[name="linear_1"](%408)
  %529 : Tensor = prim::GetAttr[name="bias"](%528)
  %530 : Tensor = prim::GetAttr[name="weight"](%528)
  %531 : Float(768:1, 3072:768) = aten::t(%530), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.9 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.19, %531), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.2 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.9, %529, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %534 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.2, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %535 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.2, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %536 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%535, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %537 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.2, %536, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %538 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%537, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %539 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%538), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %540 : Float(17:39936, 13:3072, 3072:1) = aten::add(%539, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.20 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%534, %540), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.20, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %543 : Tensor = prim::GetAttr[name="bias"](%527)
  %544 : Tensor = prim::GetAttr[name="weight"](%527)
  %545 : Float(3072:1, 768:3072) = aten::t(%544), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.21, %545), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %543, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.22, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add(%input.19, %h.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
  %550 : Tensor = prim::GetAttr[name="bias"](%526)
  %551 : Tensor = prim::GetAttr[name="weight"](%526)
  %552 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm
  %query.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %552, %551, %550, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %554 : __torch__.transformers.modeling_funnel.___torch_mangle_3763.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%103)
  %555 : __torch__.transformers.modeling_funnel.___torch_mangle_3757.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%103)
  %556 : __torch__.torch.nn.modules.normalization.___torch_mangle_3756.LayerNorm = prim::GetAttr[name="layer_norm"](%555)
  %557 : __torch__.torch.nn.modules.linear.___torch_mangle_3755.Linear = prim::GetAttr[name="post_proj"](%555)
  %558 : Tensor = prim::GetAttr[name="seg_embed"](%555)
  %559 : Tensor = prim::GetAttr[name="r_s_bias"](%555)
  %560 : Tensor = prim::GetAttr[name="r_kernel"](%555)
  %561 : Tensor = prim::GetAttr[name="r_r_bias"](%555)
  %562 : Tensor = prim::GetAttr[name="r_w_bias"](%555)
  %563 : __torch__.torch.nn.modules.linear.___torch_mangle_3754.Linear = prim::GetAttr[name="v_head"](%555)
  %564 : __torch__.torch.nn.modules.linear.___torch_mangle_3753.Linear = prim::GetAttr[name="k_head"](%555)
  %565 : __torch__.torch.nn.modules.linear.___torch_mangle_3752.Linear = prim::GetAttr[name="q_head"](%555)
  %566 : int = aten::size(%query.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %567 : int = aten::size(%query.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %568 : int = aten::size(%query.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
  %569 : Tensor = prim::GetAttr[name="weight"](%565)
  %570 : Float(768:1, 768:768) = aten::t(%569), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %571 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %570), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %572 : int[] = prim::ListConstruct(%566, %567, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %q_head.5 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%571, %572), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
  %574 : Tensor = prim::GetAttr[name="bias"](%564)
  %575 : Tensor = prim::GetAttr[name="weight"](%564)
  %576 : Float(768:1, 768:768) = aten::t(%575), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.11 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %576), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %578 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.11, %574, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
  %579 : int[] = prim::ListConstruct(%566, %568, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %580 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%578, %579), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
  %581 : Tensor = prim::GetAttr[name="bias"](%563)
  %582 : Tensor = prim::GetAttr[name="weight"](%563)
  %583 : Float(768:1, 768:768) = aten::t(%582), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %583), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %585 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %581, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
  %586 : int[] = prim::ListConstruct(%566, %568, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %587 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%585, %586), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.5, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.3 : Float(12:64, 64:1) = aten::mul(%562, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
  %590 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_w_bias.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
  %591 : Tensor[] = prim::ListConstruct(%590, %580), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %content_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%42, %591), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %v.3 : Float(12:64, 64:1) = aten::mul(%561, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
  %594 : Tensor[] = prim::ListConstruct(%155, %560), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %595 : Float(26:768, 12:64, 64:1) = aten::einsum(%43, %594), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %596 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %v.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
  %597 : Tensor[] = prim::ListConstruct(%596, %595), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.13 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%44, %597), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %599 : int = aten::size(%positional_attn.13, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %600 : int = aten::size(%positional_attn.13, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %601 : int = aten::size(%positional_attn.13, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %602 : int = aten::size(%positional_attn.13, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.3 : Long() = prim::NumToTensor(%602), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %604 : int[] = prim::ListConstruct(%599, %600, %602, %601), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.14 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.13, %604), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:428:0
  %606 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.14, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %607 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%606, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %608 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%607, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.15 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%608, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %610 : Long() = aten::sub(%max_rel_len.3, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %611 : int = aten::Int(%610), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %612 : int[] = prim::ListConstruct(%599, %600, %601, %611), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.16 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.15, %612), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.17 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.16, %45, %58, %568, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.18 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.17, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:498:0
  %616 : int = aten::size(%token_type_mat.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %617 : int = aten::size(%token_type_mat.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %618 : int = aten::size(%token_type_mat.2, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.3 : Float(12:64, 64:1) = aten::mul(%559, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
  %620 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_s_bias.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
  %621 : Tensor[] = prim::ListConstruct(%620, %558), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %622 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%46, %621), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %623 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %624 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%623, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %625 : int = aten::size(%q_head.6, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %626 : int[] = prim::ListConstruct(%616, %625, %617, %618), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %token_type_mat.5 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%624, %626, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %628 : Tensor[] = aten::split(%622, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:371:0
  %diff_token_type.3 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.3 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%628), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %631 : int = aten::size(%token_type_mat.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %632 : int = aten::size(%token_type_mat.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %633 : int = aten::size(%token_type_mat.5, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %634 : int = aten::size(%token_type_mat.5, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %635 : int[] = prim::ListConstruct(%631, %632, %633, %634), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %636 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.3, %635, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %637 : int = aten::size(%token_type_mat.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %638 : int = aten::size(%token_type_mat.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %639 : int = aten::size(%token_type_mat.5, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %640 : int = aten::size(%token_type_mat.5, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %641 : int[] = prim::ListConstruct(%637, %638, %639, %640), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %642 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.3, %641, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.5, %636, %642), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:522:0
  %645 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.3, %positional_attn.18, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%645, %token_type_attn.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.7, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
  %648 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %649 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%648, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %650 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%649, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %651 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%650, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %652 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%651, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:396:0
  %653 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%652, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.8, %653, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %input.24 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.9, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
  %656 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.24, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %657 : Tensor[] = prim::ListConstruct(%656, %587), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %attn_vec.3 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%48, %657), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %659 : int[] = prim::ListConstruct(%566, %567, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.3, %659), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
  %661 : Tensor = prim::GetAttr[name="bias"](%557)
  %662 : Tensor = prim::GetAttr[name="weight"](%557)
  %663 : Float(768:1, 768:768) = aten::t(%662), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %663), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.26 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %661, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.26, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.27 : Float(17:9984, 13:768, 768:1) = aten::add(%query.2, %attn_out.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
  %668 : Tensor = prim::GetAttr[name="bias"](%556)
  %669 : Tensor = prim::GetAttr[name="weight"](%556)
  %670 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %670, %669, %668, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %672 : __torch__.torch.nn.modules.normalization.___torch_mangle_3762.LayerNorm = prim::GetAttr[name="layer_norm"](%554)
  %673 : __torch__.torch.nn.modules.linear.___torch_mangle_3760.Linear = prim::GetAttr[name="linear_2"](%554)
  %674 : __torch__.torch.nn.modules.linear.___torch_mangle_3758.Linear = prim::GetAttr[name="linear_1"](%554)
  %675 : Tensor = prim::GetAttr[name="bias"](%674)
  %676 : Tensor = prim::GetAttr[name="weight"](%674)
  %677 : Float(768:1, 3072:768) = aten::t(%676), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.14 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.28, %677), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.3 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.14, %675, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %680 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.3, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %681 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.3, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %682 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%681, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %683 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.3, %682, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %684 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%683, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %685 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%684), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %686 : Float(17:39936, 13:3072, 3072:1) = aten::add(%685, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.29 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%680, %686), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.30 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.29, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %689 : Tensor = prim::GetAttr[name="bias"](%673)
  %690 : Tensor = prim::GetAttr[name="weight"](%673)
  %691 : Float(3072:1, 768:3072) = aten::t(%690), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.30, %691), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %689, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.31, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.32 : Float(17:9984, 13:768, 768:1) = aten::add(%input.28, %h.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
  %696 : Tensor = prim::GetAttr[name="bias"](%672)
  %697 : Tensor = prim::GetAttr[name="weight"](%672)
  %698 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm
  %query.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.32, %698, %697, %696, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %700 : __torch__.transformers.modeling_funnel.___torch_mangle_3778.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%100)
  %701 : __torch__.transformers.modeling_funnel.___torch_mangle_3772.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%100)
  %702 : __torch__.torch.nn.modules.normalization.___torch_mangle_3771.LayerNorm = prim::GetAttr[name="layer_norm"](%701)
  %703 : __torch__.torch.nn.modules.linear.___torch_mangle_3770.Linear = prim::GetAttr[name="post_proj"](%701)
  %704 : Tensor = prim::GetAttr[name="seg_embed"](%701)
  %705 : Tensor = prim::GetAttr[name="r_s_bias"](%701)
  %706 : Tensor = prim::GetAttr[name="r_kernel"](%701)
  %707 : Tensor = prim::GetAttr[name="r_r_bias"](%701)
  %708 : Tensor = prim::GetAttr[name="r_w_bias"](%701)
  %709 : __torch__.torch.nn.modules.linear.___torch_mangle_3769.Linear = prim::GetAttr[name="v_head"](%701)
  %710 : __torch__.torch.nn.modules.linear.___torch_mangle_3768.Linear = prim::GetAttr[name="k_head"](%701)
  %711 : __torch__.torch.nn.modules.linear.___torch_mangle_3767.Linear = prim::GetAttr[name="q_head"](%701)
  %712 : int = aten::size(%query.3, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %713 : int = aten::size(%query.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %714 : int = aten::size(%query.3, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
  %715 : Tensor = prim::GetAttr[name="weight"](%711)
  %716 : Float(768:1, 768:768) = aten::t(%715), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %717 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %716), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %718 : int[] = prim::ListConstruct(%712, %713, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %q_head.7 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%717, %718), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
  %720 : Tensor = prim::GetAttr[name="bias"](%710)
  %721 : Tensor = prim::GetAttr[name="weight"](%710)
  %722 : Float(768:1, 768:768) = aten::t(%721), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %722), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %724 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %720, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
  %725 : int[] = prim::ListConstruct(%712, %714, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %726 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%724, %725), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
  %727 : Tensor = prim::GetAttr[name="bias"](%709)
  %728 : Tensor = prim::GetAttr[name="weight"](%709)
  %729 : Float(768:1, 768:768) = aten::t(%728), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.17 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %729), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %731 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.17, %727, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
  %732 : int[] = prim::ListConstruct(%712, %714, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %733 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%731, %732), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.7, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.4 : Float(12:64, 64:1) = aten::mul(%708, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
  %736 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_w_bias.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
  %737 : Tensor[] = prim::ListConstruct(%736, %726), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %content_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%42, %737), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %v.4 : Float(12:64, 64:1) = aten::mul(%707, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
  %740 : Tensor[] = prim::ListConstruct(%155, %706), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %741 : Float(26:768, 12:64, 64:1) = aten::einsum(%43, %740), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %742 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %v.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
  %743 : Tensor[] = prim::ListConstruct(%742, %741), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.19 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%44, %743), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %745 : int = aten::size(%positional_attn.19, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %746 : int = aten::size(%positional_attn.19, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %747 : int = aten::size(%positional_attn.19, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %748 : int = aten::size(%positional_attn.19, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.4 : Long() = prim::NumToTensor(%748), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %750 : int[] = prim::ListConstruct(%745, %746, %748, %747), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.20 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.19, %750), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:428:0
  %752 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.20, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %753 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%752, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %754 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%753, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.21 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%754, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %756 : Long() = aten::sub(%max_rel_len.4, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %757 : int = aten::Int(%756), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %758 : int[] = prim::ListConstruct(%745, %746, %747, %757), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.22 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.21, %758), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.23 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.22, %45, %58, %714, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.24 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.23, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:498:0
  %762 : int = aten::size(%token_type_mat.2, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %763 : int = aten::size(%token_type_mat.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %764 : int = aten::size(%token_type_mat.2, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.4 : Float(12:64, 64:1) = aten::mul(%705, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
  %766 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_s_bias.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
  %767 : Tensor[] = prim::ListConstruct(%766, %704), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %768 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%46, %767), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %769 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %770 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%769, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %771 : int = aten::size(%q_head.8, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %772 : int[] = prim::ListConstruct(%762, %771, %763, %764), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %token_type_mat.6 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%770, %772, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %774 : Tensor[] = aten::split(%768, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:371:0
  %diff_token_type.4 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.4 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%774), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %777 : int = aten::size(%token_type_mat.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %778 : int = aten::size(%token_type_mat.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %779 : int = aten::size(%token_type_mat.6, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %780 : int = aten::size(%token_type_mat.6, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %781 : int[] = prim::ListConstruct(%777, %778, %779, %780), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %782 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.4, %781, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %783 : int = aten::size(%token_type_mat.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %784 : int = aten::size(%token_type_mat.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %785 : int = aten::size(%token_type_mat.6, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %786 : int = aten::size(%token_type_mat.6, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %787 : int[] = prim::ListConstruct(%783, %784, %785, %786), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %788 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.4, %787, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.6, %782, %788), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.7, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:522:0
  %791 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.4, %positional_attn.24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%791, %token_type_attn.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.10, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
  %794 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %795 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%794, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %796 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%795, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %797 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%796, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %798 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%797, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:396:0
  %799 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%798, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.11, %799, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %input.33 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.12, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
  %802 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.33, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %803 : Tensor[] = prim::ListConstruct(%802, %733), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %attn_vec.4 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%48, %803), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %805 : int[] = prim::ListConstruct(%712, %713, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.4, %805), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
  %807 : Tensor = prim::GetAttr[name="bias"](%703)
  %808 : Tensor = prim::GetAttr[name="weight"](%703)
  %809 : Float(768:1, 768:768) = aten::t(%808), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.34, %809), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %807, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.35, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.36 : Float(17:9984, 13:768, 768:1) = aten::add(%query.3, %attn_out.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
  %814 : Tensor = prim::GetAttr[name="bias"](%702)
  %815 : Tensor = prim::GetAttr[name="weight"](%702)
  %816 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm
  %input.37 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.36, %816, %815, %814, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %818 : __torch__.torch.nn.modules.normalization.___torch_mangle_3777.LayerNorm = prim::GetAttr[name="layer_norm"](%700)
  %819 : __torch__.torch.nn.modules.linear.___torch_mangle_3775.Linear = prim::GetAttr[name="linear_2"](%700)
  %820 : __torch__.torch.nn.modules.linear.___torch_mangle_3773.Linear = prim::GetAttr[name="linear_1"](%700)
  %821 : Tensor = prim::GetAttr[name="bias"](%820)
  %822 : Tensor = prim::GetAttr[name="weight"](%820)
  %823 : Float(768:1, 3072:768) = aten::t(%822), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.37, %823), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.4 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.19, %821, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %826 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.4, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %827 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.4, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %828 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%827, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %829 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.4, %828, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %830 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%829, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %831 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%830), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %832 : Float(17:39936, 13:3072, 3072:1) = aten::add(%831, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.38 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%826, %832), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.39 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.38, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %835 : Tensor = prim::GetAttr[name="bias"](%819)
  %836 : Tensor = prim::GetAttr[name="weight"](%819)
  %837 : Float(3072:1, 768:3072) = aten::t(%836), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.39, %837), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %835, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.40, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:9984, 13:768, 768:1) = aten::add(%input.37, %h.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
  %842 : Tensor = prim::GetAttr[name="bias"](%818)
  %843 : Tensor = prim::GetAttr[name="weight"](%818)
  %844 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm
  %hidden.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.41, %844, %843, %842, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %846 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %847 : Bool(17:169, 1:13, 13:1) = aten::slice(%846, %57, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %848 : Tensor[] = prim::ListConstruct(%847, %token_type_mat.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.1 : Bool(17:182, 14:13, 13:1) = aten::cat(%848, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %850 : Bool(17:182, 14:13, 13:1) = aten::slice(%tensor.1, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.7 : Bool(17:182, 7:26, 13:1) = aten::slice(%850, %57, %58, %49, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %852 : Float(1:13, 13:1) = aten::slice(%cls_mask.1, %58, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %853 : Tensor[] = prim::ListConstruct(%852, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.2 : Float(14:13, 13:1) = aten::cat(%853, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.2 : Float(7:26, 13:1) = aten::slice(%tensor.2, %58, %58, %49, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %856 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.1 : Float(17:9984, 12:768, 768:1) = aten::slice(%856, %57, %58, %49, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %858 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %859 : Float(17:9984, 1:768, 768:1) = aten::slice(%858, %57, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %860 : Tensor[] = prim::ListConstruct(%859, %suffix.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.3 : Float(17:9984, 13:768, 768:1) = aten::cat(%860, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %862 : Float(17:9984, 13:768, 768:1) = aten::slice(%tensor.3, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %863 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::unsqueeze(%862, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %864 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%863, %27, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.4 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%864, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %866 : int[] = prim::ListConstruct(%27, %57), scope: __module.funnel/__module.funnel.encoder
  %867 : int[] = prim::ListConstruct(%27, %57), scope: __module.funnel/__module.funnel.encoder
  %868 : int[] = prim::ListConstruct(%58, %58), scope: __module.funnel/__module.funnel.encoder
  %tensor.5 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::avg_pool2d(%tensor.4, %866, %867, %868, %50, %50, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %870 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%tensor.5, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.4 : Float(17:5376, 7:768, 768:1) = aten::select(%870, %57, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %872 : __torch__.transformers.modeling_funnel.___torch_mangle_3794.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%97)
  %873 : __torch__.transformers.modeling_funnel.___torch_mangle_3788.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%97)
  %874 : __torch__.torch.nn.modules.normalization.___torch_mangle_3787.LayerNorm = prim::GetAttr[name="layer_norm"](%873)
  %875 : __torch__.torch.nn.modules.linear.___torch_mangle_3786.Linear = prim::GetAttr[name="post_proj"](%873)
  %876 : Tensor = prim::GetAttr[name="seg_embed"](%873)
  %877 : Tensor = prim::GetAttr[name="r_s_bias"](%873)
  %878 : Tensor = prim::GetAttr[name="r_kernel"](%873)
  %879 : Tensor = prim::GetAttr[name="r_r_bias"](%873)
  %880 : Tensor = prim::GetAttr[name="r_w_bias"](%873)
  %881 : __torch__.torch.nn.modules.linear.___torch_mangle_3785.Linear = prim::GetAttr[name="v_head"](%873)
  %882 : __torch__.torch.nn.modules.linear.___torch_mangle_3784.Linear = prim::GetAttr[name="k_head"](%873)
  %883 : __torch__.torch.nn.modules.linear.___torch_mangle_3783.Linear = prim::GetAttr[name="q_head"](%873)
  %884 : int = aten::size(%query.4, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %885 : int = aten::size(%query.4, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %886 : int = aten::size(%hidden.1, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
  %887 : Tensor = prim::GetAttr[name="weight"](%883)
  %888 : Float(768:1, 768:768) = aten::t(%887), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %889 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.4, %888), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %890 : int[] = prim::ListConstruct(%884, %885, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %q_head.9 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%889, %890), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
  %892 : Tensor = prim::GetAttr[name="bias"](%882)
  %893 : Tensor = prim::GetAttr[name="weight"](%882)
  %894 : Float(768:1, 768:768) = aten::t(%893), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %894), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %896 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %892, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
  %897 : int[] = prim::ListConstruct(%884, %886, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %898 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%896, %897), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
  %899 : Tensor = prim::GetAttr[name="bias"](%881)
  %900 : Tensor = prim::GetAttr[name="weight"](%881)
  %901 : Float(768:1, 768:768) = aten::t(%900), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %901), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %903 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %899, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
  %904 : int[] = prim::ListConstruct(%884, %886, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %905 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%903, %904), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.10 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.9, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.5 : Float(12:64, 64:1) = aten::mul(%880, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
  %908 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_w_bias.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
  %909 : Tensor[] = prim::ListConstruct(%908, %898), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %content_score.5 : Float(17:1092, 12:91, 7:13, 13:1) = aten::einsum(%42, %909), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %v.5 : Float(12:64, 64:1) = aten::mul(%879, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
  %912 : Tensor[] = prim::ListConstruct(%179, %878), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %913 : Float(27:768, 12:64, 64:1) = aten::einsum(%43, %912), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %914 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %v.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
  %915 : Tensor[] = prim::ListConstruct(%914, %913), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.25 : Float(17:189, 12:3213, 7:27, 27:1) = aten::einsum(%44, %915), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %917 : int = aten::size(%positional_attn.25, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %918 : int = aten::size(%positional_attn.25, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %919 : int = aten::size(%positional_attn.25, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %920 : int = aten::size(%positional_attn.25, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.5 : Long() = prim::NumToTensor(%920), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %922 : int[] = prim::ListConstruct(%917, %918, %920, %919), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.26 : Float(17:189, 12:3213, 27:7, 7:1) = aten::reshape(%positional_attn.25, %922), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:428:0
  %924 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%positional_attn.26, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %925 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%924, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %926 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%925, %27, %27, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.27 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%926, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %928 : Long() = aten::sub(%max_rel_len.5, %23, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %929 : int = aten::Int(%928), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %930 : int[] = prim::ListConstruct(%917, %918, %919, %929), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.28 : Float(17:189, 12:3213, 7:25, 25:1) = aten::reshape(%positional_attn.27, %930), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.29 : Float(17:189, 12:3213, 7:25, 13:1) = aten::slice(%positional_attn.28, %45, %58, %886, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.30 : Float(17:189, 12:3213, 7:25, 13:1) = aten::mul_(%positional_attn.29, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:498:0
  %934 : int = aten::size(%token_type_mat.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %935 : int = aten::size(%token_type_mat.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %936 : int = aten::size(%token_type_mat.7, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.5 : Float(12:64, 64:1) = aten::mul(%877, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
  %938 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_s_bias.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
  %939 : Tensor[] = prim::ListConstruct(%938, %876), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %940 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%46, %939), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %941 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %942 : Bool(17:182, 1:182, 7:26, 13:1) = aten::unsqueeze(%941, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %943 : int = aten::size(%q_head.10, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %944 : int[] = prim::ListConstruct(%934, %943, %935, %936), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %token_type_mat.8 : Bool(17:182, 12:0, 7:26, 13:1) = aten::expand(%942, %944, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %946 : Tensor[] = aten::split(%940, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:371:0
  %diff_token_type.5 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.5 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%946), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %949 : int = aten::size(%token_type_mat.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %950 : int = aten::size(%token_type_mat.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %951 : int = aten::size(%token_type_mat.8, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %952 : int = aten::size(%token_type_mat.8, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %953 : int[] = prim::ListConstruct(%949, %950, %951, %952), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %954 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%same_token_type.5, %953, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %955 : int = aten::size(%token_type_mat.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %956 : int = aten::size(%token_type_mat.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %957 : int = aten::size(%token_type_mat.8, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %958 : int = aten::size(%token_type_mat.8, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %959 : int[] = prim::ListConstruct(%955, %956, %957, %958), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %960 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%diff_token_type.5, %959, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.9 : Float(17:1092, 12:91, 7:13, 13:1) = aten::where(%token_type_mat.8, %954, %960), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.10 : Float(17:1092, 12:91, 7:13, 13:1) = aten::mul_(%token_type_attn.9, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:522:0
  %963 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%content_score.5, %positional_attn.30, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.13 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%963, %token_type_attn.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.14 : Float(17:1092, 12:91, 7:13, 13:1) = aten::to(%attn_score.13, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
  %966 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %967 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%966, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %968 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%967, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %969 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%968, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %970 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%969, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:396:0
  %971 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%970, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.15 : Float(17:1092, 12:91, 7:13, 13:1) = aten::sub(%attn_score.14, %971, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %input.42 : Float(17:1092, 12:91, 7:13, 13:1) = aten::softmax(%attn_score.15, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
  %974 : Float(17:1092, 12:91, 7:13, 13:1) = aten::dropout(%input.42, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %975 : Tensor[] = prim::ListConstruct(%974, %905), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %attn_vec.5 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%48, %975), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %977 : int[] = prim::ListConstruct(%884, %885, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %input.43 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.5, %977), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
  %979 : Tensor = prim::GetAttr[name="bias"](%875)
  %980 : Tensor = prim::GetAttr[name="weight"](%875)
  %981 : Float(768:1, 768:768) = aten::t(%980), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.23 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.43, %981), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.44 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.23, %979, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.44, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:5376, 7:768, 768:1) = aten::add(%query.4, %attn_out.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
  %986 : Tensor = prim::GetAttr[name="bias"](%874)
  %987 : Tensor = prim::GetAttr[name="weight"](%874)
  %988 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm
  %input.46 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.45, %988, %987, %986, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %990 : __torch__.torch.nn.modules.normalization.___torch_mangle_3793.LayerNorm = prim::GetAttr[name="layer_norm"](%872)
  %991 : __torch__.torch.nn.modules.linear.___torch_mangle_3791.Linear = prim::GetAttr[name="linear_2"](%872)
  %992 : __torch__.torch.nn.modules.linear.___torch_mangle_3789.Linear = prim::GetAttr[name="linear_1"](%872)
  %993 : Tensor = prim::GetAttr[name="bias"](%992)
  %994 : Tensor = prim::GetAttr[name="weight"](%992)
  %995 : Float(768:1, 3072:768) = aten::t(%994), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.46, %995), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.5 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.24, %993, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %998 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.5, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %999 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.5, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1000 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%999, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1001 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.5, %1000, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1002 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1001, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1003 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1002), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1004 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1003, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.47 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%998, %1004), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.48 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.47, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1007 : Tensor = prim::GetAttr[name="bias"](%991)
  %1008 : Tensor = prim::GetAttr[name="weight"](%991)
  %1009 : Float(3072:1, 768:3072) = aten::t(%1008), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.25 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.48, %1009), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.49 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.25, %1007, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.49, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:5376, 7:768, 768:1) = aten::add(%input.46, %h.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
  %1014 : Tensor = prim::GetAttr[name="bias"](%990)
  %1015 : Tensor = prim::GetAttr[name="weight"](%990)
  %1016 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm
  %query.5 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.50, %1016, %1015, %1014, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1018 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1019 : Bool(17:182, 7:26, 13:1) = aten::slice(%1018, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1020 : Bool(17:182, 7:26, 1:1) = aten::slice(%1019, %27, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1021 : Tensor[] = prim::ListConstruct(%1020, %token_type_mat.7), scope: __module.funnel/__module.funnel.encoder
  %tensor.6 : Bool(17:98, 7:14, 14:1) = aten::cat(%1021, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1023 : Bool(17:98, 7:14, 14:1) = aten::slice(%tensor.6, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1024 : Bool(17:98, 7:14, 14:1) = aten::slice(%1023, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.9 : Bool(17:98, 7:14, 7:2) = aten::slice(%1024, %27, %58, %49, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1026 : Float(7:26, 13:1) = aten::slice(%cls_mask.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1027 : Float(7:26, 1:1) = aten::slice(%1026, %57, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1028 : Tensor[] = prim::ListConstruct(%1027, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.7 : Float(7:14, 14:1) = aten::cat(%1028, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1030 : Float(7:14, 14:1) = aten::slice(%tensor.7, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.3 : Float(7:14, 7:2) = aten::slice(%1030, %57, %58, %49, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1032 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.2 : Float(17:13, 12:1) = aten::slice(%1032, %57, %58, %49, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1034 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1035 : Float(17:13, 1:1) = aten::slice(%1034, %57, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1036 : Tensor[] = prim::ListConstruct(%1035, %suffix.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.8 : Float(17:13, 13:1) = aten::cat(%1036, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1038 : Float(17:13, 13:1) = aten::slice(%tensor.8, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1039 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%1038, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1040 : Float(17:13, 1:13, 13:1) = aten::slice(%1039, %27, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.9 : Float(17:13, 1:13, 13:1, 1:1) = aten::unsqueeze(%1040, %45), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.51 : Float(17:13, 1:13, 13:1, 1:1) = aten::neg(%tensor.9), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1043 : int[] = prim::ListConstruct(%27, %57), scope: __module.funnel/__module.funnel.encoder
  %1044 : int[] = prim::ListConstruct(%27, %57), scope: __module.funnel/__module.funnel.encoder
  %1045 : int[] = prim::ListConstruct(%58, %58), scope: __module.funnel/__module.funnel.encoder
  %1046 : int[] = prim::ListConstruct(%57, %57), scope: __module.funnel/__module.funnel.encoder
  %1047 : Float(17:7, 1:7, 7:1, 1:1) = aten::max_pool2d(%input.51, %1043, %1044, %1045, %1046, %50), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor.10 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%1047), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1049 : Float(17:7, 1:7, 7:1, 1:1) = aten::slice(%tensor.10, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1050 : Float(17:7, 7:1, 1:1) = aten::select(%1049, %57, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1051 : Float(17:7, 7:1, 1:1) = aten::slice(%1050, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask.3 : Float(17:7, 7:1) = aten::select(%1051, %27, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1053 : __torch__.transformers.modeling_funnel.___torch_mangle_3809.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%94)
  %1054 : __torch__.transformers.modeling_funnel.___torch_mangle_3803.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%94)
  %1055 : __torch__.torch.nn.modules.normalization.___torch_mangle_3802.LayerNorm = prim::GetAttr[name="layer_norm"](%1054)
  %1056 : __torch__.torch.nn.modules.linear.___torch_mangle_3801.Linear = prim::GetAttr[name="post_proj"](%1054)
  %1057 : Tensor = prim::GetAttr[name="seg_embed"](%1054)
  %1058 : Tensor = prim::GetAttr[name="r_s_bias"](%1054)
  %1059 : Tensor = prim::GetAttr[name="r_kernel"](%1054)
  %1060 : Tensor = prim::GetAttr[name="r_r_bias"](%1054)
  %1061 : Tensor = prim::GetAttr[name="r_w_bias"](%1054)
  %1062 : __torch__.torch.nn.modules.linear.___torch_mangle_3800.Linear = prim::GetAttr[name="v_head"](%1054)
  %1063 : __torch__.torch.nn.modules.linear.___torch_mangle_3799.Linear = prim::GetAttr[name="k_head"](%1054)
  %1064 : __torch__.torch.nn.modules.linear.___torch_mangle_3798.Linear = prim::GetAttr[name="q_head"](%1054)
  %1065 : int = aten::size(%query.5, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1066 : int = aten::size(%query.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1067 : int = aten::size(%query.5, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
  %1068 : Tensor = prim::GetAttr[name="weight"](%1064)
  %1069 : Float(768:1, 768:768) = aten::t(%1068), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1070 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1069), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1071 : int[] = prim::ListConstruct(%1065, %1066, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %q_head.11 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1070, %1071), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
  %1073 : Tensor = prim::GetAttr[name="bias"](%1063)
  %1074 : Tensor = prim::GetAttr[name="weight"](%1063)
  %1075 : Float(768:1, 768:768) = aten::t(%1074), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.26 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1075), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %1077 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.26, %1073, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
  %1078 : int[] = prim::ListConstruct(%1065, %1067, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1079 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1077, %1078), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
  %1080 : Tensor = prim::GetAttr[name="bias"](%1062)
  %1081 : Tensor = prim::GetAttr[name="weight"](%1062)
  %1082 : Float(768:1, 768:768) = aten::t(%1081), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.27 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1082), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %1084 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.27, %1080, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
  %1085 : int[] = prim::ListConstruct(%1065, %1067, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1086 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1084, %1085), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.12 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.11, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.6 : Float(12:64, 64:1) = aten::mul(%1061, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
  %1089 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_w_bias.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
  %1090 : Tensor[] = prim::ListConstruct(%1089, %1079), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %content_score.6 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%42, %1090), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %v.6 : Float(12:64, 64:1) = aten::mul(%1060, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
  %1093 : Tensor[] = prim::ListConstruct(%197, %1059), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1094 : Float(14:768, 12:64, 64:1) = aten::einsum(%43, %1093), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1095 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %v.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
  %1096 : Tensor[] = prim::ListConstruct(%1095, %1094), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.31 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%44, %1096), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1098 : int = aten::size(%positional_attn.31, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1099 : int = aten::size(%positional_attn.31, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1100 : int = aten::size(%positional_attn.31, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1101 : int = aten::size(%positional_attn.31, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.6 : Long() = prim::NumToTensor(%1101), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1103 : int[] = prim::ListConstruct(%1098, %1099, %1101, %1100), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.32 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.31, %1103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:428:0
  %1105 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.32, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1106 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1105, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1107 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1106, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.33 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1107, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1109 : Long() = aten::sub(%max_rel_len.6, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %1110 : int = aten::Int(%1109), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1111 : int[] = prim::ListConstruct(%1098, %1099, %1100, %1110), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.34 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.33, %1111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.35 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.34, %45, %58, %1067, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.36 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.35, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:498:0
  %1115 : int = aten::size(%token_type_mat.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1116 : int = aten::size(%token_type_mat.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1117 : int = aten::size(%token_type_mat.9, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.6 : Float(12:64, 64:1) = aten::mul(%1058, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
  %1119 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_s_bias.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
  %1120 : Tensor[] = prim::ListConstruct(%1119, %1057), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1121 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%46, %1120), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1122 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1123 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1122, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1124 : int = aten::size(%q_head.12, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1125 : int[] = prim::ListConstruct(%1115, %1124, %1116, %1117), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %token_type_mat.10 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1123, %1125, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1127 : Tensor[] = aten::split(%1121, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:371:0
  %diff_token_type.6 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.6 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1127), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1130 : int = aten::size(%token_type_mat.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1131 : int = aten::size(%token_type_mat.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1132 : int = aten::size(%token_type_mat.10, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1133 : int = aten::size(%token_type_mat.10, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1134 : int[] = prim::ListConstruct(%1130, %1131, %1132, %1133), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1135 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.6, %1134, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1136 : int = aten::size(%token_type_mat.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1137 : int = aten::size(%token_type_mat.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1138 : int = aten::size(%token_type_mat.10, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1139 : int = aten::size(%token_type_mat.10, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1140 : int[] = prim::ListConstruct(%1136, %1137, %1138, %1139), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1141 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.6, %1140, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.11 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.10, %1135, %1141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.12 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.11, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:522:0
  %1144 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.6, %positional_attn.36, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1144, %token_type_attn.12, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.17 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.16, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
  %1147 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1148 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1147, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1149 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1148, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1150 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1149, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1151 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1150, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:396:0
  %1152 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1151, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.18 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.17, %1152, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %input.52 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.18, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
  %1155 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.52, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1156 : Tensor[] = prim::ListConstruct(%1155, %1086), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %attn_vec.6 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%48, %1156), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1158 : int[] = prim::ListConstruct(%1065, %1066, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %input.53 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.6, %1158), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
  %1160 : Tensor = prim::GetAttr[name="bias"](%1056)
  %1161 : Tensor = prim::GetAttr[name="weight"](%1056)
  %1162 : Float(768:1, 768:768) = aten::t(%1161), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.53, %1162), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.54 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.28, %1160, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.54, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:5376, 7:768, 768:1) = aten::add(%query.5, %attn_out.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
  %1167 : Tensor = prim::GetAttr[name="bias"](%1055)
  %1168 : Tensor = prim::GetAttr[name="weight"](%1055)
  %1169 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm
  %input.56 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.55, %1169, %1168, %1167, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1171 : __torch__.torch.nn.modules.normalization.___torch_mangle_3808.LayerNorm = prim::GetAttr[name="layer_norm"](%1053)
  %1172 : __torch__.torch.nn.modules.linear.___torch_mangle_3806.Linear = prim::GetAttr[name="linear_2"](%1053)
  %1173 : __torch__.torch.nn.modules.linear.___torch_mangle_3804.Linear = prim::GetAttr[name="linear_1"](%1053)
  %1174 : Tensor = prim::GetAttr[name="bias"](%1173)
  %1175 : Tensor = prim::GetAttr[name="weight"](%1173)
  %1176 : Float(768:1, 3072:768) = aten::t(%1175), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.56, %1176), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.6 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.29, %1174, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1179 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.6, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1180 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.6, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1181 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1180, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1182 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.6, %1181, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1183 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1182, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1184 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1183), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1185 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1184, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.57 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1179, %1185), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.58 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.57, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1188 : Tensor = prim::GetAttr[name="bias"](%1172)
  %1189 : Tensor = prim::GetAttr[name="weight"](%1172)
  %1190 : Float(3072:1, 768:3072) = aten::t(%1189), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.58, %1190), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.59 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.30, %1188, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.59, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:5376, 7:768, 768:1) = aten::add(%input.56, %h.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
  %1195 : Tensor = prim::GetAttr[name="bias"](%1171)
  %1196 : Tensor = prim::GetAttr[name="weight"](%1171)
  %1197 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm
  %query.6 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.60, %1197, %1196, %1195, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1199 : __torch__.transformers.modeling_funnel.___torch_mangle_3824.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%91)
  %1200 : __torch__.transformers.modeling_funnel.___torch_mangle_3818.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%91)
  %1201 : __torch__.torch.nn.modules.normalization.___torch_mangle_3817.LayerNorm = prim::GetAttr[name="layer_norm"](%1200)
  %1202 : __torch__.torch.nn.modules.linear.___torch_mangle_3816.Linear = prim::GetAttr[name="post_proj"](%1200)
  %1203 : Tensor = prim::GetAttr[name="seg_embed"](%1200)
  %1204 : Tensor = prim::GetAttr[name="r_s_bias"](%1200)
  %1205 : Tensor = prim::GetAttr[name="r_kernel"](%1200)
  %1206 : Tensor = prim::GetAttr[name="r_r_bias"](%1200)
  %1207 : Tensor = prim::GetAttr[name="r_w_bias"](%1200)
  %1208 : __torch__.torch.nn.modules.linear.___torch_mangle_3815.Linear = prim::GetAttr[name="v_head"](%1200)
  %1209 : __torch__.torch.nn.modules.linear.___torch_mangle_3814.Linear = prim::GetAttr[name="k_head"](%1200)
  %1210 : __torch__.torch.nn.modules.linear.___torch_mangle_3813.Linear = prim::GetAttr[name="q_head"](%1200)
  %1211 : int = aten::size(%query.6, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1212 : int = aten::size(%query.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1213 : int = aten::size(%query.6, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
  %1214 : Tensor = prim::GetAttr[name="weight"](%1210)
  %1215 : Float(768:1, 768:768) = aten::t(%1214), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1216 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1215), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1217 : int[] = prim::ListConstruct(%1211, %1212, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %q_head.13 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1216, %1217), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
  %1219 : Tensor = prim::GetAttr[name="bias"](%1209)
  %1220 : Tensor = prim::GetAttr[name="weight"](%1209)
  %1221 : Float(768:1, 768:768) = aten::t(%1220), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.31 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1221), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %1223 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.31, %1219, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
  %1224 : int[] = prim::ListConstruct(%1211, %1213, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1225 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1223, %1224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
  %1226 : Tensor = prim::GetAttr[name="bias"](%1208)
  %1227 : Tensor = prim::GetAttr[name="weight"](%1208)
  %1228 : Float(768:1, 768:768) = aten::t(%1227), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.32 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1228), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %1230 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.32, %1226, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
  %1231 : int[] = prim::ListConstruct(%1211, %1213, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1232 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1230, %1231), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.14 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.13, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.7 : Float(12:64, 64:1) = aten::mul(%1207, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
  %1235 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_w_bias.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
  %1236 : Tensor[] = prim::ListConstruct(%1235, %1225), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %content_score.7 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%42, %1236), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %v.7 : Float(12:64, 64:1) = aten::mul(%1206, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
  %1239 : Tensor[] = prim::ListConstruct(%197, %1205), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1240 : Float(14:768, 12:64, 64:1) = aten::einsum(%43, %1239), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1241 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %v.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
  %1242 : Tensor[] = prim::ListConstruct(%1241, %1240), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.37 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%44, %1242), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1244 : int = aten::size(%positional_attn.37, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1245 : int = aten::size(%positional_attn.37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1246 : int = aten::size(%positional_attn.37, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1247 : int = aten::size(%positional_attn.37, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.7 : Long() = prim::NumToTensor(%1247), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1249 : int[] = prim::ListConstruct(%1244, %1245, %1247, %1246), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.38 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.37, %1249), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:428:0
  %1251 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.38, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1252 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1251, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1253 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1252, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.39 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1253, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1255 : Long() = aten::sub(%max_rel_len.7, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %1256 : int = aten::Int(%1255), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1257 : int[] = prim::ListConstruct(%1244, %1245, %1246, %1256), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.40 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.39, %1257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.41 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.40, %45, %58, %1213, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.42 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.41, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:498:0
  %1261 : int = aten::size(%token_type_mat.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1262 : int = aten::size(%token_type_mat.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1263 : int = aten::size(%token_type_mat.9, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.7 : Float(12:64, 64:1) = aten::mul(%1204, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
  %1265 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_s_bias.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
  %1266 : Tensor[] = prim::ListConstruct(%1265, %1203), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1267 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%46, %1266), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1268 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1269 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1268, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1270 : int = aten::size(%q_head.14, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1271 : int[] = prim::ListConstruct(%1261, %1270, %1262, %1263), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %token_type_mat.11 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1269, %1271, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1273 : Tensor[] = aten::split(%1267, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:371:0
  %diff_token_type.7 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.7 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1273), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1276 : int = aten::size(%token_type_mat.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1277 : int = aten::size(%token_type_mat.11, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1278 : int = aten::size(%token_type_mat.11, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1279 : int = aten::size(%token_type_mat.11, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1280 : int[] = prim::ListConstruct(%1276, %1277, %1278, %1279), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1281 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.7, %1280, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1282 : int = aten::size(%token_type_mat.11, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1283 : int = aten::size(%token_type_mat.11, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1284 : int = aten::size(%token_type_mat.11, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1285 : int = aten::size(%token_type_mat.11, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1286 : int[] = prim::ListConstruct(%1282, %1283, %1284, %1285), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1287 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.7, %1286, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.13 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.11, %1281, %1287), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.14 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.13, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:522:0
  %1290 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.7, %positional_attn.42, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.19 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1290, %token_type_attn.14, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.20 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.19, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
  %1293 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1294 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1293, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1295 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1294, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1296 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1295, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1297 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1296, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:396:0
  %1298 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1297, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.21 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.20, %1298, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %input.61 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.21, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
  %1301 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.61, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1302 : Tensor[] = prim::ListConstruct(%1301, %1232), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %attn_vec.7 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%48, %1302), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1304 : int[] = prim::ListConstruct(%1211, %1212, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %input.62 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.7, %1304), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
  %1306 : Tensor = prim::GetAttr[name="bias"](%1202)
  %1307 : Tensor = prim::GetAttr[name="weight"](%1202)
  %1308 : Float(768:1, 768:768) = aten::t(%1307), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.62, %1308), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.63 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.33, %1306, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.63, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:5376, 7:768, 768:1) = aten::add(%query.6, %attn_out.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
  %1313 : Tensor = prim::GetAttr[name="bias"](%1201)
  %1314 : Tensor = prim::GetAttr[name="weight"](%1201)
  %1315 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm
  %input.65 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.64, %1315, %1314, %1313, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1317 : __torch__.torch.nn.modules.normalization.___torch_mangle_3823.LayerNorm = prim::GetAttr[name="layer_norm"](%1199)
  %1318 : __torch__.torch.nn.modules.linear.___torch_mangle_3821.Linear = prim::GetAttr[name="linear_2"](%1199)
  %1319 : __torch__.torch.nn.modules.linear.___torch_mangle_3819.Linear = prim::GetAttr[name="linear_1"](%1199)
  %1320 : Tensor = prim::GetAttr[name="bias"](%1319)
  %1321 : Tensor = prim::GetAttr[name="weight"](%1319)
  %1322 : Float(768:1, 3072:768) = aten::t(%1321), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.65, %1322), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.7 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.34, %1320, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1325 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.7, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1326 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.7, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1327 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1326, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1328 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.7, %1327, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1329 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1328, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1330 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1329), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1331 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1330, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.66 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1325, %1331), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.67 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.66, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1334 : Tensor = prim::GetAttr[name="bias"](%1318)
  %1335 : Tensor = prim::GetAttr[name="weight"](%1318)
  %1336 : Float(3072:1, 768:3072) = aten::t(%1335), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.67, %1336), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.68 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.35, %1334, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.68, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:5376, 7:768, 768:1) = aten::add(%input.65, %h.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
  %1341 : Tensor = prim::GetAttr[name="bias"](%1317)
  %1342 : Tensor = prim::GetAttr[name="weight"](%1317)
  %1343 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm
  %query.7 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.69, %1343, %1342, %1341, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1345 : __torch__.transformers.modeling_funnel.___torch_mangle_3839.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%88)
  %1346 : __torch__.transformers.modeling_funnel.___torch_mangle_3833.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%88)
  %1347 : __torch__.torch.nn.modules.normalization.___torch_mangle_3832.LayerNorm = prim::GetAttr[name="layer_norm"](%1346)
  %1348 : __torch__.torch.nn.modules.linear.___torch_mangle_3831.Linear = prim::GetAttr[name="post_proj"](%1346)
  %1349 : Tensor = prim::GetAttr[name="seg_embed"](%1346)
  %1350 : Tensor = prim::GetAttr[name="r_s_bias"](%1346)
  %1351 : Tensor = prim::GetAttr[name="r_kernel"](%1346)
  %1352 : Tensor = prim::GetAttr[name="r_r_bias"](%1346)
  %1353 : Tensor = prim::GetAttr[name="r_w_bias"](%1346)
  %1354 : __torch__.torch.nn.modules.linear.___torch_mangle_3830.Linear = prim::GetAttr[name="v_head"](%1346)
  %1355 : __torch__.torch.nn.modules.linear.___torch_mangle_3829.Linear = prim::GetAttr[name="k_head"](%1346)
  %1356 : __torch__.torch.nn.modules.linear.___torch_mangle_3828.Linear = prim::GetAttr[name="q_head"](%1346)
  %1357 : int = aten::size(%query.7, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1358 : int = aten::size(%query.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1359 : int = aten::size(%query.7, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
  %1360 : Tensor = prim::GetAttr[name="weight"](%1356)
  %1361 : Float(768:1, 768:768) = aten::t(%1360), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1362 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1361), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1363 : int[] = prim::ListConstruct(%1357, %1358, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %q_head.15 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1362, %1363), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
  %1365 : Tensor = prim::GetAttr[name="bias"](%1355)
  %1366 : Tensor = prim::GetAttr[name="weight"](%1355)
  %1367 : Float(768:1, 768:768) = aten::t(%1366), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.36 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1367), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %1369 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.36, %1365, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
  %1370 : int[] = prim::ListConstruct(%1357, %1359, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1371 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1369, %1370), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
  %1372 : Tensor = prim::GetAttr[name="bias"](%1354)
  %1373 : Tensor = prim::GetAttr[name="weight"](%1354)
  %1374 : Float(768:1, 768:768) = aten::t(%1373), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.37 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1374), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %1376 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.37, %1372, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
  %1377 : int[] = prim::ListConstruct(%1357, %1359, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1378 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1376, %1377), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.16 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.15, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.8 : Float(12:64, 64:1) = aten::mul(%1353, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
  %1381 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_w_bias.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
  %1382 : Tensor[] = prim::ListConstruct(%1381, %1371), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %content_score.8 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%42, %1382), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %v.8 : Float(12:64, 64:1) = aten::mul(%1352, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
  %1385 : Tensor[] = prim::ListConstruct(%197, %1351), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1386 : Float(14:768, 12:64, 64:1) = aten::einsum(%43, %1385), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1387 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %v.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
  %1388 : Tensor[] = prim::ListConstruct(%1387, %1386), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.43 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%44, %1388), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1390 : int = aten::size(%positional_attn.43, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1391 : int = aten::size(%positional_attn.43, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1392 : int = aten::size(%positional_attn.43, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1393 : int = aten::size(%positional_attn.43, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.8 : Long() = prim::NumToTensor(%1393), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1395 : int[] = prim::ListConstruct(%1390, %1391, %1393, %1392), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.44 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.43, %1395), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:428:0
  %1397 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.44, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1398 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1397, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1399 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1398, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.45 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1399, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1401 : Long() = aten::sub(%max_rel_len.8, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %1402 : int = aten::Int(%1401), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1403 : int[] = prim::ListConstruct(%1390, %1391, %1392, %1402), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.46 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.45, %1403), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.47 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.46, %45, %58, %1359, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.48 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.47, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:498:0
  %1407 : int = aten::size(%token_type_mat.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1408 : int = aten::size(%token_type_mat.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1409 : int = aten::size(%token_type_mat.9, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.8 : Float(12:64, 64:1) = aten::mul(%1350, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
  %1411 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_s_bias.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
  %1412 : Tensor[] = prim::ListConstruct(%1411, %1349), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1413 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%46, %1412), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1414 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1415 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1414, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1416 : int = aten::size(%q_head.16, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1417 : int[] = prim::ListConstruct(%1407, %1416, %1408, %1409), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %token_type_mat.12 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1415, %1417, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1419 : Tensor[] = aten::split(%1413, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:371:0
  %diff_token_type.8 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.8 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1419), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1422 : int = aten::size(%token_type_mat.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1423 : int = aten::size(%token_type_mat.12, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1424 : int = aten::size(%token_type_mat.12, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1425 : int = aten::size(%token_type_mat.12, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1426 : int[] = prim::ListConstruct(%1422, %1423, %1424, %1425), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1427 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.8, %1426, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1428 : int = aten::size(%token_type_mat.12, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1429 : int = aten::size(%token_type_mat.12, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1430 : int = aten::size(%token_type_mat.12, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1431 : int = aten::size(%token_type_mat.12, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1432 : int[] = prim::ListConstruct(%1428, %1429, %1430, %1431), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1433 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.8, %1432, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.15 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.12, %1427, %1433), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.15, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:522:0
  %1436 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.8, %positional_attn.48, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.22 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1436, %token_type_attn.16, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.23 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.22, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
  %1439 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1440 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1439, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1441 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1440, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1442 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1441, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1443 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1442, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:396:0
  %1444 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1443, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.24 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.23, %1444, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %input.70 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.24, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
  %1447 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.70, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %1448 : Tensor[] = prim::ListConstruct(%1447, %1378), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %attn_vec.8 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%48, %1448), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1450 : int[] = prim::ListConstruct(%1357, %1358, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %input.71 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.8, %1450), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
  %1452 : Tensor = prim::GetAttr[name="bias"](%1348)
  %1453 : Tensor = prim::GetAttr[name="weight"](%1348)
  %1454 : Float(768:1, 768:768) = aten::t(%1453), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.71, %1454), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.72 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.38, %1452, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.72, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:5376, 7:768, 768:1) = aten::add(%query.7, %attn_out.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
  %1459 : Tensor = prim::GetAttr[name="bias"](%1347)
  %1460 : Tensor = prim::GetAttr[name="weight"](%1347)
  %1461 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm
  %input.74 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.73, %1461, %1460, %1459, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %1463 : __torch__.torch.nn.modules.normalization.___torch_mangle_3838.LayerNorm = prim::GetAttr[name="layer_norm"](%1345)
  %1464 : __torch__.torch.nn.modules.linear.___torch_mangle_3836.Linear = prim::GetAttr[name="linear_2"](%1345)
  %1465 : __torch__.torch.nn.modules.linear.___torch_mangle_3834.Linear = prim::GetAttr[name="linear_1"](%1345)
  %1466 : Tensor = prim::GetAttr[name="bias"](%1465)
  %1467 : Tensor = prim::GetAttr[name="weight"](%1465)
  %1468 : Float(768:1, 3072:768) = aten::t(%1467), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.39 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.74, %1468), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.8 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.39, %1466, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1471 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.8, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1472 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.8, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1473 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1472, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1474 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.8, %1473, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1475 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1474, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1476 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1475), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1477 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1476, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.75 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1471, %1477), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.76 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.75, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1480 : Tensor = prim::GetAttr[name="bias"](%1464)
  %1481 : Tensor = prim::GetAttr[name="weight"](%1464)
  %1482 : Float(3072:1, 768:3072) = aten::t(%1481), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.40 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.76, %1482), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.77 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.40, %1480, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.77, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:5376, 7:768, 768:1) = aten::add(%input.74, %h.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
  %1487 : Tensor = prim::GetAttr[name="bias"](%1463)
  %1488 : Tensor = prim::GetAttr[name="weight"](%1463)
  %1489 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm
  %hidden.2 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.78, %1489, %1488, %1487, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1491 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1492 : Bool(17:98, 1:14, 7:2) = aten::slice(%1491, %57, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1493 : Tensor[] = prim::ListConstruct(%1492, %token_type_mat.9), scope: __module.funnel/__module.funnel.encoder
  %tensor.11 : Bool(17:56, 8:7, 7:1) = aten::cat(%1493, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1495 : Bool(17:56, 8:7, 7:1) = aten::slice(%tensor.11, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.13 : Bool(17:56, 4:14, 7:1) = aten::slice(%1495, %57, %58, %49, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1497 : Float(1:14, 7:2) = aten::slice(%cls_mask.3, %58, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1498 : Tensor[] = prim::ListConstruct(%1497, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.12 : Float(8:7, 7:1) = aten::cat(%1498, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.4 : Float(4:14, 7:1) = aten::slice(%tensor.12, %58, %58, %49, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1501 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.3 : Float(17:5376, 6:768, 768:1) = aten::slice(%1501, %57, %58, %49, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1503 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden.2, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1504 : Float(17:5376, 1:768, 768:1) = aten::slice(%1503, %57, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1505 : Tensor[] = prim::ListConstruct(%1504, %suffix.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.13 : Float(17:5376, 7:768, 768:1) = aten::cat(%1505, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1507 : Float(17:5376, 7:768, 768:1) = aten::slice(%tensor.13, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1508 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::unsqueeze(%1507, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1509 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1508, %27, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.14 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1509, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1511 : int[] = prim::ListConstruct(%27, %57), scope: __module.funnel/__module.funnel.encoder
  %1512 : int[] = prim::ListConstruct(%27, %57), scope: __module.funnel/__module.funnel.encoder
  %1513 : int[] = prim::ListConstruct(%58, %58), scope: __module.funnel/__module.funnel.encoder
  %tensor.15 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::avg_pool2d(%tensor.14, %1511, %1512, %1513, %50, %50, %21), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %1515 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::slice(%tensor.15, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.8 : Float(17:3072, 4:768, 768:1) = aten::select(%1515, %57, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %1517 : __torch__.transformers.modeling_funnel.___torch_mangle_3855.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%85)
  %1518 : __torch__.transformers.modeling_funnel.___torch_mangle_3849.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%85)
  %1519 : __torch__.torch.nn.modules.normalization.___torch_mangle_3848.LayerNorm = prim::GetAttr[name="layer_norm"](%1518)
  %1520 : __torch__.torch.nn.modules.linear.___torch_mangle_3847.Linear = prim::GetAttr[name="post_proj"](%1518)
  %1521 : Tensor = prim::GetAttr[name="seg_embed"](%1518)
  %1522 : Tensor = prim::GetAttr[name="r_s_bias"](%1518)
  %1523 : Tensor = prim::GetAttr[name="r_kernel"](%1518)
  %1524 : Tensor = prim::GetAttr[name="r_r_bias"](%1518)
  %1525 : Tensor = prim::GetAttr[name="r_w_bias"](%1518)
  %1526 : __torch__.torch.nn.modules.linear.___torch_mangle_3846.Linear = prim::GetAttr[name="v_head"](%1518)
  %1527 : __torch__.torch.nn.modules.linear.___torch_mangle_3845.Linear = prim::GetAttr[name="k_head"](%1518)
  %1528 : __torch__.torch.nn.modules.linear.___torch_mangle_3844.Linear = prim::GetAttr[name="q_head"](%1518)
  %1529 : int = aten::size(%query.8, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1530 : int = aten::size(%query.8, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1531 : int = aten::size(%hidden.2, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
  %1532 : Tensor = prim::GetAttr[name="weight"](%1528)
  %1533 : Float(768:1, 768:768) = aten::t(%1532), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1534 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.8, %1533), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1535 : int[] = prim::ListConstruct(%1529, %1530, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %q_head.17 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1534, %1535), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
  %1537 : Tensor = prim::GetAttr[name="bias"](%1527)
  %1538 : Tensor = prim::GetAttr[name="weight"](%1527)
  %1539 : Float(768:1, 768:768) = aten::t(%1538), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.41 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden.2, %1539), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %1541 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.41, %1537, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
  %1542 : int[] = prim::ListConstruct(%1529, %1531, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1543 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1541, %1542), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
  %1544 : Tensor = prim::GetAttr[name="bias"](%1526)
  %1545 : Tensor = prim::GetAttr[name="weight"](%1526)
  %1546 : Float(768:1, 768:768) = aten::t(%1545), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.42 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden.2, %1546), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %1548 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.42, %1544, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
  %1549 : int[] = prim::ListConstruct(%1529, %1531, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1550 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1548, %1549), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.18 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.17, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.9 : Float(12:64, 64:1) = aten::mul(%1525, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
  %1553 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_w_bias.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
  %1554 : Tensor[] = prim::ListConstruct(%1553, %1543), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %content_score.9 : Float(17:336, 12:28, 4:7, 7:1) = aten::einsum(%42, %1554), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %v.9 : Float(12:64, 64:1) = aten::mul(%1524, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
  %1557 : Tensor[] = prim::ListConstruct(%221, %1523), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1558 : Float(15:768, 12:64, 64:1) = aten::einsum(%43, %1557), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1559 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %v.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
  %1560 : Tensor[] = prim::ListConstruct(%1559, %1558), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.49 : Float(17:60, 12:1020, 4:15, 15:1) = aten::einsum(%44, %1560), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1562 : int = aten::size(%positional_attn.49, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1563 : int = aten::size(%positional_attn.49, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1564 : int = aten::size(%positional_attn.49, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1565 : int = aten::size(%positional_attn.49, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.9 : Long() = prim::NumToTensor(%1565), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1567 : int[] = prim::ListConstruct(%1562, %1563, %1565, %1564), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.50 : Float(17:60, 12:1020, 15:4, 4:1) = aten::reshape(%positional_attn.49, %1567), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:428:0
  %1569 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%positional_attn.50, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1570 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%1569, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1571 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1570, %27, %27, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.51 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1571, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1573 : Long() = aten::sub(%max_rel_len.9, %23, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %1574 : int = aten::Int(%1573), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1575 : int[] = prim::ListConstruct(%1562, %1563, %1564, %1574), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.52 : Float(17:60, 12:1020, 4:13, 13:1) = aten::reshape(%positional_attn.51, %1575), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.53 : Float(17:60, 12:1020, 4:13, 7:1) = aten::slice(%positional_attn.52, %45, %58, %1531, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.54 : Float(17:60, 12:1020, 4:13, 7:1) = aten::mul_(%positional_attn.53, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:498:0
  %1579 : int = aten::size(%token_type_mat.13, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1580 : int = aten::size(%token_type_mat.13, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1581 : int = aten::size(%token_type_mat.13, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.9 : Float(12:64, 64:1) = aten::mul(%1522, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
  %1583 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_s_bias.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
  %1584 : Tensor[] = prim::ListConstruct(%1583, %1521), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1585 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%46, %1584), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1586 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1587 : Bool(17:56, 1:56, 4:14, 7:1) = aten::unsqueeze(%1586, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1588 : int = aten::size(%q_head.18, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1589 : int[] = prim::ListConstruct(%1579, %1588, %1580, %1581), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %token_type_mat.14 : Bool(17:56, 12:0, 4:14, 7:1) = aten::expand(%1587, %1589, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1591 : Tensor[] = aten::split(%1585, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:371:0
  %diff_token_type.9 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.9 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1591), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1594 : int = aten::size(%token_type_mat.14, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1595 : int = aten::size(%token_type_mat.14, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1596 : int = aten::size(%token_type_mat.14, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1597 : int = aten::size(%token_type_mat.14, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1598 : int[] = prim::ListConstruct(%1594, %1595, %1596, %1597), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1599 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%same_token_type.9, %1598, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1600 : int = aten::size(%token_type_mat.14, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1601 : int = aten::size(%token_type_mat.14, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1602 : int = aten::size(%token_type_mat.14, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1603 : int = aten::size(%token_type_mat.14, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1604 : int[] = prim::ListConstruct(%1600, %1601, %1602, %1603), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1605 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%diff_token_type.9, %1604, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.17 : Float(17:336, 12:28, 4:7, 7:1) = aten::where(%token_type_mat.14, %1599, %1605), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.18 : Float(17:336, 12:28, 4:7, 7:1) = aten::mul_(%token_type_attn.17, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:522:0
  %1608 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%content_score.9, %positional_attn.54, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.25 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%1608, %token_type_attn.18, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.26 : Float(17:336, 12:28, 4:7, 7:1) = aten::to(%attn_score.25, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
  %1611 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1612 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1611, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1613 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1612, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1614 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1613, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1615 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1614, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:396:0
  %1616 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1615, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.27 : Float(17:336, 12:28, 4:7, 7:1) = aten::sub(%attn_score.26, %1616, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %input.79 : Float(17:336, 12:28, 4:7, 7:1) = aten::softmax(%attn_score.27, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
  %1619 : Float(17:336, 12:28, 4:7, 7:1) = aten::dropout(%input.79, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %1620 : Tensor[] = prim::ListConstruct(%1619, %1550), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %attn_vec.9 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%48, %1620), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1622 : int[] = prim::ListConstruct(%1529, %1530, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %input.80 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.9, %1622), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
  %1624 : Tensor = prim::GetAttr[name="bias"](%1520)
  %1625 : Tensor = prim::GetAttr[name="weight"](%1520)
  %1626 : Float(768:1, 768:768) = aten::t(%1625), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.80, %1626), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.81 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.43, %1624, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.81, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.82 : Float(17:3072, 4:768, 768:1) = aten::add(%query.8, %attn_out.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
  %1631 : Tensor = prim::GetAttr[name="bias"](%1519)
  %1632 : Tensor = prim::GetAttr[name="weight"](%1519)
  %1633 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm
  %input.83 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.82, %1633, %1632, %1631, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1635 : __torch__.torch.nn.modules.normalization.___torch_mangle_3854.LayerNorm = prim::GetAttr[name="layer_norm"](%1517)
  %1636 : __torch__.torch.nn.modules.linear.___torch_mangle_3852.Linear = prim::GetAttr[name="linear_2"](%1517)
  %1637 : __torch__.torch.nn.modules.linear.___torch_mangle_3850.Linear = prim::GetAttr[name="linear_1"](%1517)
  %1638 : Tensor = prim::GetAttr[name="bias"](%1637)
  %1639 : Tensor = prim::GetAttr[name="weight"](%1637)
  %1640 : Float(768:1, 3072:768) = aten::t(%1639), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.44 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.83, %1640), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.9 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.44, %1638, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1643 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.9, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1644 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.9, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1645 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1644, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1646 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.9, %1645, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1647 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1646, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1648 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1647), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1649 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1648, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.84 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1643, %1649), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.85 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.84, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1652 : Tensor = prim::GetAttr[name="bias"](%1636)
  %1653 : Tensor = prim::GetAttr[name="weight"](%1636)
  %1654 : Float(3072:1, 768:3072) = aten::t(%1653), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.85, %1654), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.86 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.45, %1652, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.86, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:3072, 4:768, 768:1) = aten::add(%input.83, %h.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
  %1659 : Tensor = prim::GetAttr[name="bias"](%1635)
  %1660 : Tensor = prim::GetAttr[name="weight"](%1635)
  %1661 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm
  %query.9 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.87, %1661, %1660, %1659, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1663 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1664 : Bool(17:56, 4:14, 7:1) = aten::slice(%1663, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1665 : Bool(17:56, 4:14, 1:1) = aten::slice(%1664, %27, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1666 : Tensor[] = prim::ListConstruct(%1665, %token_type_mat.13), scope: __module.funnel/__module.funnel.encoder
  %tensor.16 : Bool(17:32, 4:8, 8:1) = aten::cat(%1666, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1668 : Bool(17:32, 4:8, 8:1) = aten::slice(%tensor.16, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1669 : Bool(17:32, 4:8, 8:1) = aten::slice(%1668, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.15 : Bool(17:32, 4:8, 4:2) = aten::slice(%1669, %27, %58, %49, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1671 : Float(4:14, 7:1) = aten::slice(%cls_mask.4, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1672 : Float(4:14, 1:1) = aten::slice(%1671, %57, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1673 : Tensor[] = prim::ListConstruct(%1672, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder
  %tensor.17 : Float(4:8, 8:1) = aten::cat(%1673, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1675 : Float(4:8, 8:1) = aten::slice(%tensor.17, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask : Float(4:8, 4:2) = aten::slice(%1675, %57, %58, %49, %27), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1677 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix : Float(17:7, 6:1) = aten::slice(%1677, %57, %58, %49, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1679 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1680 : Float(17:7, 1:1) = aten::slice(%1679, %57, %58, %57, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1681 : Tensor[] = prim::ListConstruct(%1680, %suffix), scope: __module.funnel/__module.funnel.encoder
  %tensor.18 : Float(17:7, 7:1) = aten::cat(%1681, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1683 : Float(17:7, 7:1) = aten::slice(%tensor.18, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1684 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1683, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1685 : Float(17:7, 1:7, 7:1) = aten::slice(%1684, %27, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.19 : Float(17:7, 1:7, 7:1, 1:1) = aten::unsqueeze(%1685, %45), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.88 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%tensor.19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1688 : int[] = prim::ListConstruct(%27, %57), scope: __module.funnel/__module.funnel.encoder
  %1689 : int[] = prim::ListConstruct(%27, %57), scope: __module.funnel/__module.funnel.encoder
  %1690 : int[] = prim::ListConstruct(%58, %58), scope: __module.funnel/__module.funnel.encoder
  %1691 : int[] = prim::ListConstruct(%57, %57), scope: __module.funnel/__module.funnel.encoder
  %1692 : Float(17:4, 1:4, 4:1, 1:1) = aten::max_pool2d(%input.88, %1688, %1689, %1690, %1691, %50), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor : Float(17:4, 1:4, 4:1, 1:1) = aten::neg(%1692), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1694 : Float(17:4, 1:4, 4:1, 1:1) = aten::slice(%tensor, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1695 : Float(17:4, 4:1, 1:1) = aten::select(%1694, %57, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1696 : Float(17:4, 4:1, 1:1) = aten::slice(%1695, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask : Float(17:4, 4:1) = aten::select(%1696, %27, %58), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1698 : __torch__.transformers.modeling_funnel.___torch_mangle_3870.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%82)
  %1699 : __torch__.transformers.modeling_funnel.___torch_mangle_3864.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%82)
  %1700 : __torch__.torch.nn.modules.normalization.___torch_mangle_3863.LayerNorm = prim::GetAttr[name="layer_norm"](%1699)
  %1701 : __torch__.torch.nn.modules.linear.___torch_mangle_3862.Linear = prim::GetAttr[name="post_proj"](%1699)
  %1702 : Tensor = prim::GetAttr[name="seg_embed"](%1699)
  %1703 : Tensor = prim::GetAttr[name="r_s_bias"](%1699)
  %1704 : Tensor = prim::GetAttr[name="r_kernel"](%1699)
  %1705 : Tensor = prim::GetAttr[name="r_r_bias"](%1699)
  %1706 : Tensor = prim::GetAttr[name="r_w_bias"](%1699)
  %1707 : __torch__.torch.nn.modules.linear.___torch_mangle_3861.Linear = prim::GetAttr[name="v_head"](%1699)
  %1708 : __torch__.torch.nn.modules.linear.___torch_mangle_3860.Linear = prim::GetAttr[name="k_head"](%1699)
  %1709 : __torch__.torch.nn.modules.linear.___torch_mangle_3859.Linear = prim::GetAttr[name="q_head"](%1699)
  %1710 : int = aten::size(%query.9, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1711 : int = aten::size(%query.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1712 : int = aten::size(%query.9, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
  %1713 : Tensor = prim::GetAttr[name="weight"](%1709)
  %1714 : Float(768:1, 768:768) = aten::t(%1713), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1715 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1714), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1716 : int[] = prim::ListConstruct(%1710, %1711, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %q_head.19 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1715, %1716), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
  %1718 : Tensor = prim::GetAttr[name="bias"](%1708)
  %1719 : Tensor = prim::GetAttr[name="weight"](%1708)
  %1720 : Float(768:1, 768:768) = aten::t(%1719), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1720), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %1722 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.46, %1718, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
  %1723 : int[] = prim::ListConstruct(%1710, %1712, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1724 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1722, %1723), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
  %1725 : Tensor = prim::GetAttr[name="bias"](%1707)
  %1726 : Tensor = prim::GetAttr[name="weight"](%1707)
  %1727 : Float(768:1, 768:768) = aten::t(%1726), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1727), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %1729 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.47, %1725, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
  %1730 : int[] = prim::ListConstruct(%1710, %1712, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1731 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1729, %1730), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.20 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.19, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.10 : Float(12:64, 64:1) = aten::mul(%1706, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
  %1734 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_w_bias.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
  %1735 : Tensor[] = prim::ListConstruct(%1734, %1724), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %content_score.10 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%42, %1735), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %v.10 : Float(12:64, 64:1) = aten::mul(%1705, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
  %1738 : Tensor[] = prim::ListConstruct(%239, %1704), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1739 : Float(8:768, 12:64, 64:1) = aten::einsum(%43, %1738), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1740 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %v.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
  %1741 : Tensor[] = prim::ListConstruct(%1740, %1739), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.55 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%44, %1741), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1743 : int = aten::size(%positional_attn.55, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1744 : int = aten::size(%positional_attn.55, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1745 : int = aten::size(%positional_attn.55, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1746 : int = aten::size(%positional_attn.55, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.10 : Long() = prim::NumToTensor(%1746), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1748 : int[] = prim::ListConstruct(%1743, %1744, %1746, %1745), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.56 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.55, %1748), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:428:0
  %1750 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.56, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1751 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1750, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1752 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1751, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.57 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1752, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1754 : Long() = aten::sub(%max_rel_len.10, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %1755 : int = aten::Int(%1754), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1756 : int[] = prim::ListConstruct(%1743, %1744, %1745, %1755), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.58 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.57, %1756), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.59 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.58, %45, %58, %1712, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.60 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.59, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:498:0
  %1760 : int = aten::size(%token_type_mat.15, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1761 : int = aten::size(%token_type_mat.15, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1762 : int = aten::size(%token_type_mat.15, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.10 : Float(12:64, 64:1) = aten::mul(%1703, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
  %1764 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_s_bias.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
  %1765 : Tensor[] = prim::ListConstruct(%1764, %1702), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1766 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%46, %1765), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1767 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1768 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1767, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1769 : int = aten::size(%q_head.20, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1770 : int[] = prim::ListConstruct(%1760, %1769, %1761, %1762), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %token_type_mat.16 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1768, %1770, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1772 : Tensor[] = aten::split(%1766, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:371:0
  %diff_token_type.10 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.10 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1772), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1775 : int = aten::size(%token_type_mat.16, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1776 : int = aten::size(%token_type_mat.16, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1777 : int = aten::size(%token_type_mat.16, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1778 : int = aten::size(%token_type_mat.16, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1779 : int[] = prim::ListConstruct(%1775, %1776, %1777, %1778), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1780 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.10, %1779, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1781 : int = aten::size(%token_type_mat.16, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1782 : int = aten::size(%token_type_mat.16, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1783 : int = aten::size(%token_type_mat.16, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1784 : int = aten::size(%token_type_mat.16, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1785 : int[] = prim::ListConstruct(%1781, %1782, %1783, %1784), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1786 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.10, %1785, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.19 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.16, %1780, %1786), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.20 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.19, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:522:0
  %1789 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.10, %positional_attn.60, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.28 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1789, %token_type_attn.20, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.29 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.28, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
  %1792 : Float(17:4, 4:1) = aten::slice(%attention_mask, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1793 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1792, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1794 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1793, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1795 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1794, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1796 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1795, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:396:0
  %1797 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1796, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.30 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.29, %1797, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %input.89 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.30, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
  %1800 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.89, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1801 : Tensor[] = prim::ListConstruct(%1800, %1731), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %attn_vec.10 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%48, %1801), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1803 : int[] = prim::ListConstruct(%1710, %1711, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %input.90 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.10, %1803), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
  %1805 : Tensor = prim::GetAttr[name="bias"](%1701)
  %1806 : Tensor = prim::GetAttr[name="weight"](%1701)
  %1807 : Float(768:1, 768:768) = aten::t(%1806), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.48 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.90, %1807), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.91 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.48, %1805, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.91, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.92 : Float(17:3072, 4:768, 768:1) = aten::add(%query.9, %attn_out.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
  %1812 : Tensor = prim::GetAttr[name="bias"](%1700)
  %1813 : Tensor = prim::GetAttr[name="weight"](%1700)
  %1814 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm
  %input.93 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.92, %1814, %1813, %1812, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1816 : __torch__.torch.nn.modules.normalization.___torch_mangle_3869.LayerNorm = prim::GetAttr[name="layer_norm"](%1698)
  %1817 : __torch__.torch.nn.modules.linear.___torch_mangle_3867.Linear = prim::GetAttr[name="linear_2"](%1698)
  %1818 : __torch__.torch.nn.modules.linear.___torch_mangle_3865.Linear = prim::GetAttr[name="linear_1"](%1698)
  %1819 : Tensor = prim::GetAttr[name="bias"](%1818)
  %1820 : Tensor = prim::GetAttr[name="weight"](%1818)
  %1821 : Float(768:1, 3072:768) = aten::t(%1820), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.93, %1821), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.10 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.49, %1819, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1824 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.10, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1825 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.10, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1826 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1825, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1827 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.10, %1826, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1828 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1827, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1829 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1828), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1830 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1829, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.94 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1824, %1830), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.95 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.94, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1833 : Tensor = prim::GetAttr[name="bias"](%1817)
  %1834 : Tensor = prim::GetAttr[name="weight"](%1817)
  %1835 : Float(3072:1, 768:3072) = aten::t(%1834), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.95, %1835), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.96 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.50, %1833, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.96, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.97 : Float(17:3072, 4:768, 768:1) = aten::add(%input.93, %h.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
  %1840 : Tensor = prim::GetAttr[name="bias"](%1816)
  %1841 : Tensor = prim::GetAttr[name="weight"](%1816)
  %1842 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm
  %query.10 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.97, %1842, %1841, %1840, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1844 : __torch__.transformers.modeling_funnel.___torch_mangle_3885.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%79)
  %1845 : __torch__.transformers.modeling_funnel.___torch_mangle_3879.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%79)
  %1846 : __torch__.torch.nn.modules.normalization.___torch_mangle_3878.LayerNorm = prim::GetAttr[name="layer_norm"](%1845)
  %1847 : __torch__.torch.nn.modules.linear.___torch_mangle_3877.Linear = prim::GetAttr[name="post_proj"](%1845)
  %1848 : Tensor = prim::GetAttr[name="seg_embed"](%1845)
  %1849 : Tensor = prim::GetAttr[name="r_s_bias"](%1845)
  %1850 : Tensor = prim::GetAttr[name="r_kernel"](%1845)
  %1851 : Tensor = prim::GetAttr[name="r_r_bias"](%1845)
  %1852 : Tensor = prim::GetAttr[name="r_w_bias"](%1845)
  %1853 : __torch__.torch.nn.modules.linear.___torch_mangle_3876.Linear = prim::GetAttr[name="v_head"](%1845)
  %1854 : __torch__.torch.nn.modules.linear.___torch_mangle_3875.Linear = prim::GetAttr[name="k_head"](%1845)
  %1855 : __torch__.torch.nn.modules.linear.___torch_mangle_3874.Linear = prim::GetAttr[name="q_head"](%1845)
  %1856 : int = aten::size(%query.10, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1857 : int = aten::size(%query.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1858 : int = aten::size(%query.10, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
  %1859 : Tensor = prim::GetAttr[name="weight"](%1855)
  %1860 : Float(768:1, 768:768) = aten::t(%1859), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1861 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1860), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1862 : int[] = prim::ListConstruct(%1856, %1857, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %q_head.21 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1861, %1862), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
  %1864 : Tensor = prim::GetAttr[name="bias"](%1854)
  %1865 : Tensor = prim::GetAttr[name="weight"](%1854)
  %1866 : Float(768:1, 768:768) = aten::t(%1865), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1866), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %1868 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.51, %1864, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
  %1869 : int[] = prim::ListConstruct(%1856, %1858, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1870 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1868, %1869), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
  %1871 : Tensor = prim::GetAttr[name="bias"](%1853)
  %1872 : Tensor = prim::GetAttr[name="weight"](%1853)
  %1873 : Float(768:1, 768:768) = aten::t(%1872), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1873), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %1875 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.52, %1871, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
  %1876 : int[] = prim::ListConstruct(%1856, %1858, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1877 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1875, %1876), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.22 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.21, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.11 : Float(12:64, 64:1) = aten::mul(%1852, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
  %1880 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_w_bias.11, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
  %1881 : Tensor[] = prim::ListConstruct(%1880, %1870), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %content_score.11 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%42, %1881), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %v.11 : Float(12:64, 64:1) = aten::mul(%1851, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
  %1884 : Tensor[] = prim::ListConstruct(%239, %1850), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1885 : Float(8:768, 12:64, 64:1) = aten::einsum(%43, %1884), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1886 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %v.11, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
  %1887 : Tensor[] = prim::ListConstruct(%1886, %1885), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.61 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%44, %1887), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1889 : int = aten::size(%positional_attn.61, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1890 : int = aten::size(%positional_attn.61, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1891 : int = aten::size(%positional_attn.61, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1892 : int = aten::size(%positional_attn.61, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.11 : Long() = prim::NumToTensor(%1892), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1894 : int[] = prim::ListConstruct(%1889, %1890, %1892, %1891), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.62 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.61, %1894), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:428:0
  %1896 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.62, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1897 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1896, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1898 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1897, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.63 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1898, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1900 : Long() = aten::sub(%max_rel_len.11, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %1901 : int = aten::Int(%1900), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1902 : int[] = prim::ListConstruct(%1889, %1890, %1891, %1901), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.64 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.63, %1902), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.65 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.64, %45, %58, %1858, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.66 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.65, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:498:0
  %1906 : int = aten::size(%token_type_mat.15, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1907 : int = aten::size(%token_type_mat.15, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1908 : int = aten::size(%token_type_mat.15, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.11 : Float(12:64, 64:1) = aten::mul(%1849, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
  %1910 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_s_bias.11, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
  %1911 : Tensor[] = prim::ListConstruct(%1910, %1848), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1912 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%46, %1911), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1913 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1914 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1913, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1915 : int = aten::size(%q_head.22, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1916 : int[] = prim::ListConstruct(%1906, %1915, %1907, %1908), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %token_type_mat.17 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1914, %1916, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1918 : Tensor[] = aten::split(%1912, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:371:0
  %diff_token_type.11 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.11 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1918), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1921 : int = aten::size(%token_type_mat.17, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1922 : int = aten::size(%token_type_mat.17, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1923 : int = aten::size(%token_type_mat.17, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1924 : int = aten::size(%token_type_mat.17, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1925 : int[] = prim::ListConstruct(%1921, %1922, %1923, %1924), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1926 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.11, %1925, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1927 : int = aten::size(%token_type_mat.17, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1928 : int = aten::size(%token_type_mat.17, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1929 : int = aten::size(%token_type_mat.17, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1930 : int = aten::size(%token_type_mat.17, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1931 : int[] = prim::ListConstruct(%1927, %1928, %1929, %1930), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1932 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.11, %1931, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.21 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.17, %1926, %1932), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.22 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.21, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:522:0
  %1935 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.11, %positional_attn.66, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.31 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1935, %token_type_attn.22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.32 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.31, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
  %1938 : Float(17:4, 4:1) = aten::slice(%attention_mask, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1939 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1938, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1940 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1939, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1941 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1940, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1942 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1941, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:396:0
  %1943 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1942, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.33 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.32, %1943, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %input.98 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.33, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
  %1946 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.98, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1947 : Tensor[] = prim::ListConstruct(%1946, %1877), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %attn_vec.11 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%48, %1947), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1949 : int[] = prim::ListConstruct(%1856, %1857, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %input.99 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.11, %1949), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
  %1951 : Tensor = prim::GetAttr[name="bias"](%1847)
  %1952 : Tensor = prim::GetAttr[name="weight"](%1847)
  %1953 : Float(768:1, 768:768) = aten::t(%1952), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.99, %1953), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.53, %1951, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.100, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3072, 4:768, 768:1) = aten::add(%query.10, %attn_out.11, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
  %1958 : Tensor = prim::GetAttr[name="bias"](%1846)
  %1959 : Tensor = prim::GetAttr[name="weight"](%1846)
  %1960 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm
  %input.102 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.101, %1960, %1959, %1958, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1962 : __torch__.torch.nn.modules.normalization.___torch_mangle_3884.LayerNorm = prim::GetAttr[name="layer_norm"](%1844)
  %1963 : __torch__.torch.nn.modules.linear.___torch_mangle_3882.Linear = prim::GetAttr[name="linear_2"](%1844)
  %1964 : __torch__.torch.nn.modules.linear.___torch_mangle_3880.Linear = prim::GetAttr[name="linear_1"](%1844)
  %1965 : Tensor = prim::GetAttr[name="bias"](%1964)
  %1966 : Tensor = prim::GetAttr[name="weight"](%1964)
  %1967 : Float(768:1, 3072:768) = aten::t(%1966), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.102, %1967), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.11 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.54, %1965, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1970 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.11, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1971 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.11, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1972 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1971, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1973 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.11, %1972, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1974 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1973, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1975 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1974), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1976 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1975, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.103 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1970, %1976), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.104 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.103, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1979 : Tensor = prim::GetAttr[name="bias"](%1963)
  %1980 : Tensor = prim::GetAttr[name="weight"](%1963)
  %1981 : Float(3072:1, 768:3072) = aten::t(%1980), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.104, %1981), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.105 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.55, %1979, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.105, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.106 : Float(17:3072, 4:768, 768:1) = aten::add(%input.102, %h.11, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
  %1986 : Tensor = prim::GetAttr[name="bias"](%1962)
  %1987 : Tensor = prim::GetAttr[name="weight"](%1962)
  %1988 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm
  %query : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.106, %1988, %1987, %1986, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1990 : __torch__.transformers.modeling_funnel.___torch_mangle_3900.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%76)
  %1991 : __torch__.transformers.modeling_funnel.___torch_mangle_3894.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%76)
  %1992 : __torch__.torch.nn.modules.normalization.___torch_mangle_3893.LayerNorm = prim::GetAttr[name="layer_norm"](%1991)
  %1993 : __torch__.torch.nn.modules.linear.___torch_mangle_3892.Linear = prim::GetAttr[name="post_proj"](%1991)
  %1994 : Tensor = prim::GetAttr[name="seg_embed"](%1991)
  %1995 : Tensor = prim::GetAttr[name="r_s_bias"](%1991)
  %1996 : Tensor = prim::GetAttr[name="r_kernel"](%1991)
  %1997 : Tensor = prim::GetAttr[name="r_r_bias"](%1991)
  %1998 : Tensor = prim::GetAttr[name="r_w_bias"](%1991)
  %1999 : __torch__.torch.nn.modules.linear.___torch_mangle_3891.Linear = prim::GetAttr[name="v_head"](%1991)
  %2000 : __torch__.torch.nn.modules.linear.___torch_mangle_3890.Linear = prim::GetAttr[name="k_head"](%1991)
  %2001 : __torch__.torch.nn.modules.linear.___torch_mangle_3889.Linear = prim::GetAttr[name="q_head"](%1991)
  %2002 : int = aten::size(%query, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2003 : int = aten::size(%query, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2004 : int = aten::size(%query, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
  %2005 : Tensor = prim::GetAttr[name="weight"](%2001)
  %2006 : Float(768:1, 768:768) = aten::t(%2005), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2007 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query, %2006), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2008 : int[] = prim::ListConstruct(%2002, %2003, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %q_head.23 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2007, %2008), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
  %2010 : Tensor = prim::GetAttr[name="bias"](%2000)
  %2011 : Tensor = prim::GetAttr[name="weight"](%2000)
  %2012 : Float(768:1, 768:768) = aten::t(%2011), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query, %2012), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %2014 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.56, %2010, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
  %2015 : int[] = prim::ListConstruct(%2002, %2004, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2016 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2014, %2015), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
  %2017 : Tensor = prim::GetAttr[name="bias"](%1999)
  %2018 : Tensor = prim::GetAttr[name="weight"](%1999)
  %2019 : Float(768:1, 768:768) = aten::t(%2018), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query, %2019), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %2021 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.57, %2017, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
  %2022 : int[] = prim::ListConstruct(%2002, %2004, %39, %40), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2023 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2021, %2022), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
  %q_head : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.23, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias : Float(12:64, 64:1) = aten::mul(%1998, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
  %2026 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %r_w_bias, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
  %2027 : Tensor[] = prim::ListConstruct(%2026, %2016), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %content_score : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%42, %2027), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %v : Float(12:64, 64:1) = aten::mul(%1997, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
  %2030 : Tensor[] = prim::ListConstruct(%239, %1996), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2031 : Float(8:768, 12:64, 64:1) = aten::einsum(%43, %2030), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2032 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %v, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
  %2033 : Tensor[] = prim::ListConstruct(%2032, %2031), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.67 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%44, %2033), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2035 : int = aten::size(%positional_attn.67, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2036 : int = aten::size(%positional_attn.67, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2037 : int = aten::size(%positional_attn.67, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2038 : int = aten::size(%positional_attn.67, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len : Long() = prim::NumToTensor(%2038), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2040 : int[] = prim::ListConstruct(%2035, %2036, %2038, %2037), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.68 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.67, %2040), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:428:0
  %2042 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.68, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2043 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%2042, %57, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2044 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2043, %27, %57, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.69 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2044, %45, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2046 : Long() = aten::sub(%max_rel_len, %22, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %2047 : int = aten::Int(%2046), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2048 : int[] = prim::ListConstruct(%2035, %2036, %2037, %2047), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.70 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.69, %2048), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.71 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.70, %45, %58, %2004, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.71, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:498:0
  %2052 : int = aten::size(%token_type_mat.15, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2053 : int = aten::size(%token_type_mat.15, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2054 : int = aten::size(%token_type_mat.15, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias : Float(12:64, 64:1) = aten::mul(%1995, %41), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
  %2056 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %r_s_bias, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
  %2057 : Tensor[] = prim::ListConstruct(%2056, %1994), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2058 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%46, %2057), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2059 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2060 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%2059, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2061 : int = aten::size(%q_head, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2062 : int[] = prim::ListConstruct(%2052, %2061, %2053, %2054), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %token_type_mat : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%2060, %2062, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2064 : Tensor[] = aten::split(%2058, %57, %49), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:371:0
  %diff_token_type : Float(17:8, 12:136, 4:2, 1:1), %same_token_type : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%2064), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2067 : int = aten::size(%token_type_mat, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2068 : int = aten::size(%token_type_mat, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2069 : int = aten::size(%token_type_mat, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2070 : int = aten::size(%token_type_mat, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2071 : int[] = prim::ListConstruct(%2067, %2068, %2069, %2070), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2072 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type, %2071, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2073 : int = aten::size(%token_type_mat, %58), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2074 : int = aten::size(%token_type_mat, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2075 : int = aten::size(%token_type_mat, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2076 : int = aten::size(%token_type_mat, %45), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2077 : int[] = prim::ListConstruct(%2073, %2074, %2075, %2076), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2078 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type, %2077, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.23 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat, %2072, %2078), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.23, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:522:0
  %2081 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score, %positional_attn, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.34 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%2081, %token_type_attn, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.35 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.34, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
  %2084 : Float(17:4, 4:1) = aten::slice(%attention_mask, %58, %58, %24, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2085 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%2084, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2086 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%2085, %27), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2087 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%2086, %18, %54, %54, %21), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2088 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%2087, %57, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:396:0
  %2089 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%2088, %47), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.35, %2089, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %input.107 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score, %49, %18), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
  %2092 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.107, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %2093 : Tensor[] = prim::ListConstruct(%2092, %2023), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %attn_vec : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%48, %2093), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2095 : int[] = prim::ListConstruct(%2002, %2003, %52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %input.108 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec, %2095), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
  %2097 : Tensor = prim::GetAttr[name="bias"](%1993)
  %2098 : Tensor = prim::GetAttr[name="weight"](%1993)
  %2099 : Float(768:1, 768:768) = aten::t(%2098), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.108, %2099), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.109 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.58, %2097, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.109, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:3072, 4:768, 768:1) = aten::add(%query, %attn_out, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
  %2104 : Tensor = prim::GetAttr[name="bias"](%1992)
  %2105 : Tensor = prim::GetAttr[name="weight"](%1992)
  %2106 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm
  %input.111 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.110, %2106, %2105, %2104, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %2108 : __torch__.torch.nn.modules.normalization.___torch_mangle_3899.LayerNorm = prim::GetAttr[name="layer_norm"](%1990)
  %2109 : __torch__.torch.nn.modules.linear.___torch_mangle_3897.Linear = prim::GetAttr[name="linear_2"](%1990)
  %2110 : __torch__.torch.nn.modules.linear.___torch_mangle_3895.Linear = prim::GetAttr[name="linear_1"](%1990)
  %2111 : Tensor = prim::GetAttr[name="bias"](%2110)
  %2112 : Tensor = prim::GetAttr[name="weight"](%2110)
  %2113 : Float(768:1, 3072:768) = aten::t(%2112), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.111, %2113), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.59, %2111, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2116 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x, %33), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2117 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x, %34), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2118 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2117, %35), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2119 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x, %2118, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2120 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2119, %36), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2121 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%2120), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2122 : Float(17:12288, 4:3072, 3072:1) = aten::add(%2121, %37, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.112 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2116, %2122), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.113 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.112, %38, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2125 : Tensor = prim::GetAttr[name="bias"](%2109)
  %2126 : Tensor = prim::GetAttr[name="weight"](%2109)
  %2127 : Float(3072:1, 768:3072) = aten::t(%2126), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.113, %2127), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3072, 4:768, 768:1) = aten::add_(%output, %2125, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.114, %53, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3072, 4:768, 768:1) = aten::add(%input.111, %h, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
  %2132 : Tensor = prim::GetAttr[name="bias"](%2108)
  %2133 : Tensor = prim::GetAttr[name="weight"](%2108)
  %2134 : int[] = prim::ListConstruct(%52), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm
  %last_hidden_state : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.115, %2134, %2133, %2132, %51, %50), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %6 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:1275:0
  %7 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:1275:0
  %8 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_funnel.py:1275:0
  %9 : int = prim::Constant[value=1]() # transformers/modeling_funnel.py:1275:0
  %10 : Float(17:3072, 4:768, 768:1) = aten::slice(%last_hidden_state, %6, %7, %8, %9) # transformers/modeling_funnel.py:1275:0
  %11 : int = prim::Constant[value=1]() # transformers/modeling_funnel.py:1275:0
  %12 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:1275:0
  %input.116 : Float(17:3072, 768:1) = aten::select(%10, %11, %12) # transformers/modeling_funnel.py:1275:0
  %2136 : float = prim::Constant[value=0.10000000000000001](), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %2137 : bool = prim::Constant[value=0](), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %2138 : int = prim::Constant[value=1](), scope: __module.classifier/__module.classifier.linear_hidden # torch/nn/functional.py:1674:0
  %2139 : __torch__.torch.nn.modules.linear.___torch_mangle_3908.Linear = prim::GetAttr[name="linear_out"](%3)
  %2140 : __torch__.torch.nn.modules.linear.___torch_mangle_3906.Linear = prim::GetAttr[name="linear_hidden"](%3)
  %2141 : Tensor = prim::GetAttr[name="bias"](%2140)
  %2142 : Tensor = prim::GetAttr[name="weight"](%2140)
  %2143 : Float(768:1, 768:768) = aten::t(%2142), scope: __module.classifier/__module.classifier.linear_hidden # torch/nn/functional.py:1674:0
  %hidden : Float(17:768, 768:1) = aten::addmm(%2141, %input.116, %2143, %2138, %2138), scope: __module.classifier/__module.classifier.linear_hidden # torch/nn/functional.py:1674:0
  %input.117 : Float(17:768, 768:1) = aten::tanh(%hidden), scope: __module.classifier # transformers/modeling_funnel.py:791:0
  %input : Float(17:768, 768:1) = aten::dropout(%input.117, %2136, %2137), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %2147 : Tensor = prim::GetAttr[name="bias"](%2139)
  %2148 : Tensor = prim::GetAttr[name="weight"](%2139)
  %2149 : Float(768:1, 2:768) = aten::t(%2148), scope: __module.classifier/__module.classifier.linear_out # torch/nn/functional.py:1674:0
  %2150 : Float(17:2, 2:1) = aten::addmm(%2147, %input, %2149, %2138, %2138), scope: __module.classifier/__module.classifier.linear_out # torch/nn/functional.py:1674:0
  %15 : (Float(17:2, 2:1)) = prim::TupleConstruct(%2150)
  return (%15)
