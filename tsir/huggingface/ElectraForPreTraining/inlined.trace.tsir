graph(%self.1 : __torch__.transformers.modeling_electra.ElectraForPreTraining,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_electra.ElectraDiscriminatorPredictions = prim::GetAttr[name="discriminator_predictions"](%self.1)
  %4 : __torch__.transformers.modeling_electra.___torch_mangle_16056.ElectraModel = prim::GetAttr[name="electra"](%self.1)
  %8 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
  %9 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
  %10 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %11 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %12 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %13 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %14 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %15 : int = prim::Constant[value=128](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %16 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
  %17 : Double() = prim::Constant[value={-10000}](), scope: __module.electra # transformers/modeling_utils.py:258:0
  %18 : float = prim::Constant[value=1.](), scope: __module.electra # torch/tensor.py:396:0
  %19 : None = prim::Constant(), scope: __module.electra
  %20 : int = prim::Constant[value=6](), scope: __module.electra # transformers/modeling_utils.py:257:0
  %21 : int = prim::Constant[value=3](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %22 : int = prim::Constant[value=2](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %23 : int = prim::Constant[value=9223372036854775807](), scope: __module.electra # transformers/modeling_utils.py:244:0
  %24 : bool = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %25 : Device = prim::Constant[value="cpu"](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %26 : int = prim::Constant[value=4](), scope: __module.electra # transformers/modeling_electra.py:735:0
  %27 : int = prim::Constant[value=1](), scope: __module.electra # transformers/modeling_electra.py:724:0
  %28 : int = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:724:0
  %29 : __torch__.transformers.modeling_electra.___torch_mangle_16055.ElectraEncoder = prim::GetAttr[name="encoder"](%4)
  %30 : __torch__.torch.nn.modules.linear.___torch_mangle_15849.Linear = prim::GetAttr[name="embeddings_project"](%4)
  %31 : __torch__.transformers.modeling_electra.___torch_mangle_15848.ElectraEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %32 : int = aten::size(%input_ids, %28), scope: __module.electra # transformers/modeling_electra.py:724:0
  %33 : int = aten::size(%input_ids, %27), scope: __module.electra # transformers/modeling_electra.py:724:0
  %34 : int[] = prim::ListConstruct(%32, %33), scope: __module.electra
  %input.2 : Long(17:13, 13:1) = aten::zeros(%34, %26, %28, %25, %24), scope: __module.electra # transformers/modeling_electra.py:735:0
  %36 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %28, %28, %23, %27), scope: __module.electra # transformers/modeling_utils.py:244:0
  %37 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%36, %27), scope: __module.electra # transformers/modeling_utils.py:244:0
  %38 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%37, %22), scope: __module.electra # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%38, %21, %28, %23, %27), scope: __module.electra # transformers/modeling_utils.py:244:0
  %40 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %20, %24, %24, %19), scope: __module.electra # transformers/modeling_utils.py:257:0
  %41 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%40, %18, %27), scope: __module.electra # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%41, %17), scope: __module.electra # transformers/modeling_utils.py:258:0
  %43 : __torch__.torch.nn.modules.normalization.___torch_mangle_15846.LayerNorm = prim::GetAttr[name="LayerNorm"](%31)
  %44 : __torch__.torch.nn.modules.sparse.___torch_mangle_15845.Embedding = prim::GetAttr[name="token_type_embeddings"](%31)
  %45 : __torch__.torch.nn.modules.sparse.___torch_mangle_15844.Embedding = prim::GetAttr[name="position_embeddings"](%31)
  %46 : __torch__.torch.nn.modules.sparse.___torch_mangle_15843.Embedding = prim::GetAttr[name="word_embeddings"](%31)
  %47 : Tensor = prim::GetAttr[name="position_ids"](%31)
  %48 : int = aten::size(%input_ids, %27), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:173:0
  %49 : Long(1:512, 512:1) = aten::slice(%47, %28, %28, %23, %27), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%49, %27, %28, %48, %27), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
  %51 : Tensor = prim::GetAttr[name="weight"](%46)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%51, %input_ids, %28, %24, %24), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %53 : Tensor = prim::GetAttr[name="weight"](%45)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%53, %input.1, %12, %24, %24), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %55 : Tensor = prim::GetAttr[name="weight"](%44)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%55, %input.2, %12, %24, %24), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %57 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %27), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%57, %token_type_embeddings, %27), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
  %59 : Tensor = prim::GetAttr[name="bias"](%43)
  %60 : Tensor = prim::GetAttr[name="weight"](%43)
  %61 : int[] = prim::ListConstruct(%15), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %61, %60, %59, %14, %13), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %16, %24), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
  %64 : Tensor = prim::GetAttr[name="bias"](%30)
  %65 : Tensor = prim::GetAttr[name="weight"](%30)
  %66 : Float(128:1, 256:128) = aten::t(%65), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1676:0
  %output.1 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.5, %66), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1676:0
  %input.6 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.1, %64, %27), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1678:0
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %70 : __torch__.transformers.modeling_electra.___torch_mangle_16053.ElectraLayer = prim::GetAttr[name="11"](%69)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %72 : __torch__.transformers.modeling_electra.___torch_mangle_16036.ElectraLayer = prim::GetAttr[name="10"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %74 : __torch__.transformers.modeling_electra.___torch_mangle_16019.ElectraLayer = prim::GetAttr[name="9"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %76 : __torch__.transformers.modeling_electra.___torch_mangle_16002.ElectraLayer = prim::GetAttr[name="8"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %78 : __torch__.transformers.modeling_electra.___torch_mangle_15985.ElectraLayer = prim::GetAttr[name="7"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %80 : __torch__.transformers.modeling_electra.___torch_mangle_15968.ElectraLayer = prim::GetAttr[name="6"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %82 : __torch__.transformers.modeling_electra.___torch_mangle_15951.ElectraLayer = prim::GetAttr[name="5"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %84 : __torch__.transformers.modeling_electra.___torch_mangle_15934.ElectraLayer = prim::GetAttr[name="4"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %86 : __torch__.transformers.modeling_electra.___torch_mangle_15917.ElectraLayer = prim::GetAttr[name="3"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %88 : __torch__.transformers.modeling_electra.___torch_mangle_15900.ElectraLayer = prim::GetAttr[name="2"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %90 : __torch__.transformers.modeling_electra.___torch_mangle_15883.ElectraLayer = prim::GetAttr[name="1"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_16054.ModuleList = prim::GetAttr[name="layer"](%29)
  %92 : __torch__.transformers.modeling_electra.___torch_mangle_15866.ElectraLayer = prim::GetAttr[name="0"](%91)
  %93 : __torch__.transformers.modeling_electra.___torch_mangle_15865.ElectraOutput = prim::GetAttr[name="output"](%92)
  %94 : __torch__.transformers.modeling_electra.___torch_mangle_15861.ElectraIntermediate = prim::GetAttr[name="intermediate"](%92)
  %95 : __torch__.transformers.modeling_electra.___torch_mangle_15859.ElectraAttention = prim::GetAttr[name="attention"](%92)
  %96 : __torch__.transformers.modeling_electra.___torch_mangle_15858.ElectraSelfOutput = prim::GetAttr[name="output"](%95)
  %97 : __torch__.transformers.modeling_electra.___torch_mangle_15854.ElectraSelfAttention = prim::GetAttr[name="self"](%95)
  %98 : __torch__.torch.nn.modules.linear.___torch_mangle_15852.Linear = prim::GetAttr[name="value"](%97)
  %99 : __torch__.torch.nn.modules.linear.___torch_mangle_15851.Linear = prim::GetAttr[name="key"](%97)
  %100 : __torch__.torch.nn.modules.linear.___torch_mangle_15850.Linear = prim::GetAttr[name="query"](%97)
  %101 : Tensor = prim::GetAttr[name="bias"](%100)
  %102 : Tensor = prim::GetAttr[name="weight"](%100)
  %103 : Float(256:1, 256:256) = aten::t(%102), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %103), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.2, %101, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %106 : Tensor = prim::GetAttr[name="bias"](%99)
  %107 : Tensor = prim::GetAttr[name="weight"](%99)
  %108 : Float(256:1, 256:256) = aten::t(%107), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.3, %106, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %111 : Tensor = prim::GetAttr[name="bias"](%98)
  %112 : Tensor = prim::GetAttr[name="weight"](%98)
  %113 : Float(256:1, 256:256) = aten::t(%112), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.6, %113), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.4, %111, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %116 : int = aten::size(%x.1, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %117 : int = aten::size(%x.1, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %118 : int[] = prim::ListConstruct(%116, %117, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.1, %118), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %120 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.2, %120), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %122 : int = aten::size(%x.3, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %123 : int = aten::size(%x.3, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %124 : int[] = prim::ListConstruct(%122, %123, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.3, %124), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %126 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.4, %126), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %128 : int = aten::size(%x.5, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %129 : int = aten::size(%x.5, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
  %130 : int[] = prim::ListConstruct(%128, %129, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %x.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.5, %130), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
  %132 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.6, %132), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
  %134 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.1, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %134), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.1, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
  %input.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:252:0
  %input.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.7, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.8, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:265:0
  %141 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %142 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.1, %141), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%142, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
  %144 : int = aten::size(%context_layer.2, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
  %145 : int = aten::size(%context_layer.2, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
  %146 : int[] = prim::ListConstruct(%144, %145, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
  %input.9 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.2, %146), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
  %148 : __torch__.torch.nn.modules.normalization.___torch_mangle_15856.LayerNorm = prim::GetAttr[name="LayerNorm"](%96)
  %149 : __torch__.torch.nn.modules.linear.___torch_mangle_15855.Linear = prim::GetAttr[name="dense"](%96)
  %150 : Tensor = prim::GetAttr[name="bias"](%149)
  %151 : Tensor = prim::GetAttr[name="weight"](%149)
  %152 : Float(256:1, 256:256) = aten::t(%151), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.9, %152), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.10 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.5, %150, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.10, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.1, %input.6, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output # transformers/modeling_electra.py:286:0
  %157 : Tensor = prim::GetAttr[name="bias"](%148)
  %158 : Tensor = prim::GetAttr[name="weight"](%148)
  %159 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.11, %159, %158, %157, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %161 : __torch__.torch.nn.modules.linear.___torch_mangle_15860.Linear = prim::GetAttr[name="dense"](%94)
  %162 : Tensor = prim::GetAttr[name="bias"](%161)
  %163 : Tensor = prim::GetAttr[name="weight"](%161)
  %164 : Float(256:1, 1024:256) = aten::t(%163), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.1, %164), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.12 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.6, %162, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.13 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %168 : __torch__.torch.nn.modules.normalization.___torch_mangle_15863.LayerNorm = prim::GetAttr[name="LayerNorm"](%93)
  %169 : __torch__.torch.nn.modules.linear.___torch_mangle_15862.Linear = prim::GetAttr[name="dense"](%93)
  %170 : Tensor = prim::GetAttr[name="bias"](%169)
  %171 : Tensor = prim::GetAttr[name="weight"](%169)
  %172 : Float(1024:1, 256:1024) = aten::t(%171), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.7 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.13, %172), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.14 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.7, %170, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.14, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.2, %input_tensor.1, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output # transformers/modeling_electra.py:365:0
  %177 : Tensor = prim::GetAttr[name="bias"](%168)
  %178 : Tensor = prim::GetAttr[name="weight"](%168)
  %179 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm
  %input.16 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.15, %179, %178, %177, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %181 : __torch__.transformers.modeling_electra.___torch_mangle_15882.ElectraOutput = prim::GetAttr[name="output"](%90)
  %182 : __torch__.transformers.modeling_electra.___torch_mangle_15878.ElectraIntermediate = prim::GetAttr[name="intermediate"](%90)
  %183 : __torch__.transformers.modeling_electra.___torch_mangle_15876.ElectraAttention = prim::GetAttr[name="attention"](%90)
  %184 : __torch__.transformers.modeling_electra.___torch_mangle_15875.ElectraSelfOutput = prim::GetAttr[name="output"](%183)
  %185 : __torch__.transformers.modeling_electra.___torch_mangle_15871.ElectraSelfAttention = prim::GetAttr[name="self"](%183)
  %186 : __torch__.torch.nn.modules.linear.___torch_mangle_15869.Linear = prim::GetAttr[name="value"](%185)
  %187 : __torch__.torch.nn.modules.linear.___torch_mangle_15868.Linear = prim::GetAttr[name="key"](%185)
  %188 : __torch__.torch.nn.modules.linear.___torch_mangle_15867.Linear = prim::GetAttr[name="query"](%185)
  %189 : Tensor = prim::GetAttr[name="bias"](%188)
  %190 : Tensor = prim::GetAttr[name="weight"](%188)
  %191 : Float(256:1, 256:256) = aten::t(%190), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.8 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %191), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.8, %189, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %194 : Tensor = prim::GetAttr[name="bias"](%187)
  %195 : Tensor = prim::GetAttr[name="weight"](%187)
  %196 : Float(256:1, 256:256) = aten::t(%195), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.9 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %196), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.9, %194, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %199 : Tensor = prim::GetAttr[name="bias"](%186)
  %200 : Tensor = prim::GetAttr[name="weight"](%186)
  %201 : Float(256:1, 256:256) = aten::t(%200), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.10 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.16, %201), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.10, %199, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %204 : int = aten::size(%x.7, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %205 : int = aten::size(%x.7, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %206 : int[] = prim::ListConstruct(%204, %205, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.7, %206), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %208 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.8, %208), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %210 : int = aten::size(%x.9, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %211 : int = aten::size(%x.9, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %212 : int[] = prim::ListConstruct(%210, %211, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.9, %212), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %214 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.10, %214), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %216 : int = aten::size(%x.11, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %217 : int = aten::size(%x.11, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
  %218 : int[] = prim::ListConstruct(%216, %217, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %x.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.11, %218), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
  %220 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.12, %220), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
  %222 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.2, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %222), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.3, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:249:0
  %input.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:252:0
  %input.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.17, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.18, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:265:0
  %229 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %230 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.3, %229), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%230, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
  %232 : int = aten::size(%context_layer.4, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
  %233 : int = aten::size(%context_layer.4, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
  %234 : int[] = prim::ListConstruct(%232, %233, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
  %input.19 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.4, %234), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:269:0
  %236 : __torch__.torch.nn.modules.normalization.___torch_mangle_15873.LayerNorm = prim::GetAttr[name="LayerNorm"](%184)
  %237 : __torch__.torch.nn.modules.linear.___torch_mangle_15872.Linear = prim::GetAttr[name="dense"](%184)
  %238 : Tensor = prim::GetAttr[name="bias"](%237)
  %239 : Tensor = prim::GetAttr[name="weight"](%237)
  %240 : Float(256:1, 256:256) = aten::t(%239), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.19, %240), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.20 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.11, %238, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.20, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.21 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.3, %input.16, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output # transformers/modeling_electra.py:286:0
  %245 : Tensor = prim::GetAttr[name="bias"](%236)
  %246 : Tensor = prim::GetAttr[name="weight"](%236)
  %247 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.21, %247, %246, %245, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %249 : __torch__.torch.nn.modules.linear.___torch_mangle_15877.Linear = prim::GetAttr[name="dense"](%182)
  %250 : Tensor = prim::GetAttr[name="bias"](%249)
  %251 : Tensor = prim::GetAttr[name="weight"](%249)
  %252 : Float(256:1, 1024:256) = aten::t(%251), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.2, %252), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.22 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.12, %250, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.23 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %256 : __torch__.torch.nn.modules.normalization.___torch_mangle_15880.LayerNorm = prim::GetAttr[name="LayerNorm"](%181)
  %257 : __torch__.torch.nn.modules.linear.___torch_mangle_15879.Linear = prim::GetAttr[name="dense"](%181)
  %258 : Tensor = prim::GetAttr[name="bias"](%257)
  %259 : Tensor = prim::GetAttr[name="weight"](%257)
  %260 : Float(1024:1, 256:1024) = aten::t(%259), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.13 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.23, %260), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.24 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.13, %258, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.24, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.25 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.4, %input_tensor.2, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output # transformers/modeling_electra.py:365:0
  %265 : Tensor = prim::GetAttr[name="bias"](%256)
  %266 : Tensor = prim::GetAttr[name="weight"](%256)
  %267 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm
  %input.26 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.25, %267, %266, %265, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %269 : __torch__.transformers.modeling_electra.___torch_mangle_15899.ElectraOutput = prim::GetAttr[name="output"](%88)
  %270 : __torch__.transformers.modeling_electra.___torch_mangle_15895.ElectraIntermediate = prim::GetAttr[name="intermediate"](%88)
  %271 : __torch__.transformers.modeling_electra.___torch_mangle_15893.ElectraAttention = prim::GetAttr[name="attention"](%88)
  %272 : __torch__.transformers.modeling_electra.___torch_mangle_15892.ElectraSelfOutput = prim::GetAttr[name="output"](%271)
  %273 : __torch__.transformers.modeling_electra.___torch_mangle_15888.ElectraSelfAttention = prim::GetAttr[name="self"](%271)
  %274 : __torch__.torch.nn.modules.linear.___torch_mangle_15886.Linear = prim::GetAttr[name="value"](%273)
  %275 : __torch__.torch.nn.modules.linear.___torch_mangle_15885.Linear = prim::GetAttr[name="key"](%273)
  %276 : __torch__.torch.nn.modules.linear.___torch_mangle_15884.Linear = prim::GetAttr[name="query"](%273)
  %277 : Tensor = prim::GetAttr[name="bias"](%276)
  %278 : Tensor = prim::GetAttr[name="weight"](%276)
  %279 : Float(256:1, 256:256) = aten::t(%278), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.14 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %279), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.14, %277, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %282 : Tensor = prim::GetAttr[name="bias"](%275)
  %283 : Tensor = prim::GetAttr[name="weight"](%275)
  %284 : Float(256:1, 256:256) = aten::t(%283), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.15 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %284), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.15, %282, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %287 : Tensor = prim::GetAttr[name="bias"](%274)
  %288 : Tensor = prim::GetAttr[name="weight"](%274)
  %289 : Float(256:1, 256:256) = aten::t(%288), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.16 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.26, %289), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.16, %287, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %292 : int = aten::size(%x.13, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %293 : int = aten::size(%x.13, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %294 : int[] = prim::ListConstruct(%292, %293, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.13, %294), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %296 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.14, %296), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %298 : int = aten::size(%x.15, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %299 : int = aten::size(%x.15, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %300 : int[] = prim::ListConstruct(%298, %299, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.15, %300), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %302 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.16, %302), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %304 : int = aten::size(%x.17, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %305 : int = aten::size(%x.17, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
  %306 : int[] = prim::ListConstruct(%304, %305, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %x.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.17, %306), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
  %308 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.18, %308), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
  %310 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.3, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %310), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.5, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:249:0
  %input.27 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:252:0
  %input.28 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.27, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.28, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:265:0
  %317 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %318 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.5, %317), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%318, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
  %320 : int = aten::size(%context_layer.6, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
  %321 : int = aten::size(%context_layer.6, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
  %322 : int[] = prim::ListConstruct(%320, %321, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
  %input.29 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.6, %322), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:269:0
  %324 : __torch__.torch.nn.modules.normalization.___torch_mangle_15890.LayerNorm = prim::GetAttr[name="LayerNorm"](%272)
  %325 : __torch__.torch.nn.modules.linear.___torch_mangle_15889.Linear = prim::GetAttr[name="dense"](%272)
  %326 : Tensor = prim::GetAttr[name="bias"](%325)
  %327 : Tensor = prim::GetAttr[name="weight"](%325)
  %328 : Float(256:1, 256:256) = aten::t(%327), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.29, %328), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.30 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.17, %326, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.30, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.5, %input.26, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output # transformers/modeling_electra.py:286:0
  %333 : Tensor = prim::GetAttr[name="bias"](%324)
  %334 : Tensor = prim::GetAttr[name="weight"](%324)
  %335 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.31, %335, %334, %333, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %337 : __torch__.torch.nn.modules.linear.___torch_mangle_15894.Linear = prim::GetAttr[name="dense"](%270)
  %338 : Tensor = prim::GetAttr[name="bias"](%337)
  %339 : Tensor = prim::GetAttr[name="weight"](%337)
  %340 : Float(256:1, 1024:256) = aten::t(%339), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.3, %340), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.32 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.18, %338, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.33 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %344 : __torch__.torch.nn.modules.normalization.___torch_mangle_15897.LayerNorm = prim::GetAttr[name="LayerNorm"](%269)
  %345 : __torch__.torch.nn.modules.linear.___torch_mangle_15896.Linear = prim::GetAttr[name="dense"](%269)
  %346 : Tensor = prim::GetAttr[name="bias"](%345)
  %347 : Tensor = prim::GetAttr[name="weight"](%345)
  %348 : Float(1024:1, 256:1024) = aten::t(%347), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.19 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.33, %348), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.34 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.19, %346, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.34, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.35 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.6, %input_tensor.3, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output # transformers/modeling_electra.py:365:0
  %353 : Tensor = prim::GetAttr[name="bias"](%344)
  %354 : Tensor = prim::GetAttr[name="weight"](%344)
  %355 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm
  %input.36 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.35, %355, %354, %353, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %357 : __torch__.transformers.modeling_electra.___torch_mangle_15916.ElectraOutput = prim::GetAttr[name="output"](%86)
  %358 : __torch__.transformers.modeling_electra.___torch_mangle_15912.ElectraIntermediate = prim::GetAttr[name="intermediate"](%86)
  %359 : __torch__.transformers.modeling_electra.___torch_mangle_15910.ElectraAttention = prim::GetAttr[name="attention"](%86)
  %360 : __torch__.transformers.modeling_electra.___torch_mangle_15909.ElectraSelfOutput = prim::GetAttr[name="output"](%359)
  %361 : __torch__.transformers.modeling_electra.___torch_mangle_15905.ElectraSelfAttention = prim::GetAttr[name="self"](%359)
  %362 : __torch__.torch.nn.modules.linear.___torch_mangle_15903.Linear = prim::GetAttr[name="value"](%361)
  %363 : __torch__.torch.nn.modules.linear.___torch_mangle_15902.Linear = prim::GetAttr[name="key"](%361)
  %364 : __torch__.torch.nn.modules.linear.___torch_mangle_15901.Linear = prim::GetAttr[name="query"](%361)
  %365 : Tensor = prim::GetAttr[name="bias"](%364)
  %366 : Tensor = prim::GetAttr[name="weight"](%364)
  %367 : Float(256:1, 256:256) = aten::t(%366), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.20 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %367), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.20, %365, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %370 : Tensor = prim::GetAttr[name="bias"](%363)
  %371 : Tensor = prim::GetAttr[name="weight"](%363)
  %372 : Float(256:1, 256:256) = aten::t(%371), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.21 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %372), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.21, %370, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %375 : Tensor = prim::GetAttr[name="bias"](%362)
  %376 : Tensor = prim::GetAttr[name="weight"](%362)
  %377 : Float(256:1, 256:256) = aten::t(%376), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.22 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.36, %377), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.22, %375, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %380 : int = aten::size(%x.19, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %381 : int = aten::size(%x.19, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %382 : int[] = prim::ListConstruct(%380, %381, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.19, %382), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %384 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.20, %384), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %386 : int = aten::size(%x.21, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %387 : int = aten::size(%x.21, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %388 : int[] = prim::ListConstruct(%386, %387, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.21, %388), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %390 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.22, %390), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %392 : int = aten::size(%x.23, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %393 : int = aten::size(%x.23, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
  %394 : int[] = prim::ListConstruct(%392, %393, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %x.24 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.23, %394), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
  %396 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.24, %396), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
  %398 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.4, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %398), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.7, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:249:0
  %input.37 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:252:0
  %input.38 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.37, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.38, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:265:0
  %405 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %406 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.7, %405), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%406, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
  %408 : int = aten::size(%context_layer.8, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
  %409 : int = aten::size(%context_layer.8, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
  %410 : int[] = prim::ListConstruct(%408, %409, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
  %input.39 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.8, %410), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:269:0
  %412 : __torch__.torch.nn.modules.normalization.___torch_mangle_15907.LayerNorm = prim::GetAttr[name="LayerNorm"](%360)
  %413 : __torch__.torch.nn.modules.linear.___torch_mangle_15906.Linear = prim::GetAttr[name="dense"](%360)
  %414 : Tensor = prim::GetAttr[name="bias"](%413)
  %415 : Tensor = prim::GetAttr[name="weight"](%413)
  %416 : Float(256:1, 256:256) = aten::t(%415), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.39, %416), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.40 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.23, %414, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.40, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.7, %input.36, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output # transformers/modeling_electra.py:286:0
  %421 : Tensor = prim::GetAttr[name="bias"](%412)
  %422 : Tensor = prim::GetAttr[name="weight"](%412)
  %423 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.41, %423, %422, %421, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %425 : __torch__.torch.nn.modules.linear.___torch_mangle_15911.Linear = prim::GetAttr[name="dense"](%358)
  %426 : Tensor = prim::GetAttr[name="bias"](%425)
  %427 : Tensor = prim::GetAttr[name="weight"](%425)
  %428 : Float(256:1, 1024:256) = aten::t(%427), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.4, %428), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.42 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.24, %426, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.43 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.42), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %432 : __torch__.torch.nn.modules.normalization.___torch_mangle_15914.LayerNorm = prim::GetAttr[name="LayerNorm"](%357)
  %433 : __torch__.torch.nn.modules.linear.___torch_mangle_15913.Linear = prim::GetAttr[name="dense"](%357)
  %434 : Tensor = prim::GetAttr[name="bias"](%433)
  %435 : Tensor = prim::GetAttr[name="weight"](%433)
  %436 : Float(1024:1, 256:1024) = aten::t(%435), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.25 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.43, %436), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.44 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.25, %434, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.44, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.8, %input_tensor.4, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output # transformers/modeling_electra.py:365:0
  %441 : Tensor = prim::GetAttr[name="bias"](%432)
  %442 : Tensor = prim::GetAttr[name="weight"](%432)
  %443 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm
  %input.46 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.45, %443, %442, %441, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %445 : __torch__.transformers.modeling_electra.___torch_mangle_15933.ElectraOutput = prim::GetAttr[name="output"](%84)
  %446 : __torch__.transformers.modeling_electra.___torch_mangle_15929.ElectraIntermediate = prim::GetAttr[name="intermediate"](%84)
  %447 : __torch__.transformers.modeling_electra.___torch_mangle_15927.ElectraAttention = prim::GetAttr[name="attention"](%84)
  %448 : __torch__.transformers.modeling_electra.___torch_mangle_15926.ElectraSelfOutput = prim::GetAttr[name="output"](%447)
  %449 : __torch__.transformers.modeling_electra.___torch_mangle_15922.ElectraSelfAttention = prim::GetAttr[name="self"](%447)
  %450 : __torch__.torch.nn.modules.linear.___torch_mangle_15920.Linear = prim::GetAttr[name="value"](%449)
  %451 : __torch__.torch.nn.modules.linear.___torch_mangle_15919.Linear = prim::GetAttr[name="key"](%449)
  %452 : __torch__.torch.nn.modules.linear.___torch_mangle_15918.Linear = prim::GetAttr[name="query"](%449)
  %453 : Tensor = prim::GetAttr[name="bias"](%452)
  %454 : Tensor = prim::GetAttr[name="weight"](%452)
  %455 : Float(256:1, 256:256) = aten::t(%454), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.26 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %455), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.26, %453, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %458 : Tensor = prim::GetAttr[name="bias"](%451)
  %459 : Tensor = prim::GetAttr[name="weight"](%451)
  %460 : Float(256:1, 256:256) = aten::t(%459), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.27 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %460), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.27, %458, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %463 : Tensor = prim::GetAttr[name="bias"](%450)
  %464 : Tensor = prim::GetAttr[name="weight"](%450)
  %465 : Float(256:1, 256:256) = aten::t(%464), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.28 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.46, %465), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.28, %463, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %468 : int = aten::size(%x.25, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %469 : int = aten::size(%x.25, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %470 : int[] = prim::ListConstruct(%468, %469, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.26 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.25, %470), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %472 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.26, %472), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %474 : int = aten::size(%x.27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %475 : int = aten::size(%x.27, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %476 : int[] = prim::ListConstruct(%474, %475, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.28 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.27, %476), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %478 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.28, %478), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %480 : int = aten::size(%x.29, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %481 : int = aten::size(%x.29, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
  %482 : int[] = prim::ListConstruct(%480, %481, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %x.30 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.29, %482), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
  %484 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.30, %484), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
  %486 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.5, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %486), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.9, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:249:0
  %input.47 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:252:0
  %input.48 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.47, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.48, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:265:0
  %493 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %494 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.9, %493), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%494, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
  %496 : int = aten::size(%context_layer.10, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
  %497 : int = aten::size(%context_layer.10, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
  %498 : int[] = prim::ListConstruct(%496, %497, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
  %input.49 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.10, %498), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:269:0
  %500 : __torch__.torch.nn.modules.normalization.___torch_mangle_15924.LayerNorm = prim::GetAttr[name="LayerNorm"](%448)
  %501 : __torch__.torch.nn.modules.linear.___torch_mangle_15923.Linear = prim::GetAttr[name="dense"](%448)
  %502 : Tensor = prim::GetAttr[name="bias"](%501)
  %503 : Tensor = prim::GetAttr[name="weight"](%501)
  %504 : Float(256:1, 256:256) = aten::t(%503), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.49, %504), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.50 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.29, %502, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.50, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.9, %input.46, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output # transformers/modeling_electra.py:286:0
  %509 : Tensor = prim::GetAttr[name="bias"](%500)
  %510 : Tensor = prim::GetAttr[name="weight"](%500)
  %511 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.51, %511, %510, %509, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %513 : __torch__.torch.nn.modules.linear.___torch_mangle_15928.Linear = prim::GetAttr[name="dense"](%446)
  %514 : Tensor = prim::GetAttr[name="bias"](%513)
  %515 : Tensor = prim::GetAttr[name="weight"](%513)
  %516 : Float(256:1, 1024:256) = aten::t(%515), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.5, %516), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.52 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.30, %514, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.53 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %520 : __torch__.torch.nn.modules.normalization.___torch_mangle_15931.LayerNorm = prim::GetAttr[name="LayerNorm"](%445)
  %521 : __torch__.torch.nn.modules.linear.___torch_mangle_15930.Linear = prim::GetAttr[name="dense"](%445)
  %522 : Tensor = prim::GetAttr[name="bias"](%521)
  %523 : Tensor = prim::GetAttr[name="weight"](%521)
  %524 : Float(1024:1, 256:1024) = aten::t(%523), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.31 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.53, %524), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.54 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.31, %522, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.54, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.10, %input_tensor.5, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output # transformers/modeling_electra.py:365:0
  %529 : Tensor = prim::GetAttr[name="bias"](%520)
  %530 : Tensor = prim::GetAttr[name="weight"](%520)
  %531 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm
  %input.56 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.55, %531, %530, %529, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %533 : __torch__.transformers.modeling_electra.___torch_mangle_15950.ElectraOutput = prim::GetAttr[name="output"](%82)
  %534 : __torch__.transformers.modeling_electra.___torch_mangle_15946.ElectraIntermediate = prim::GetAttr[name="intermediate"](%82)
  %535 : __torch__.transformers.modeling_electra.___torch_mangle_15944.ElectraAttention = prim::GetAttr[name="attention"](%82)
  %536 : __torch__.transformers.modeling_electra.___torch_mangle_15943.ElectraSelfOutput = prim::GetAttr[name="output"](%535)
  %537 : __torch__.transformers.modeling_electra.___torch_mangle_15939.ElectraSelfAttention = prim::GetAttr[name="self"](%535)
  %538 : __torch__.torch.nn.modules.linear.___torch_mangle_15937.Linear = prim::GetAttr[name="value"](%537)
  %539 : __torch__.torch.nn.modules.linear.___torch_mangle_15936.Linear = prim::GetAttr[name="key"](%537)
  %540 : __torch__.torch.nn.modules.linear.___torch_mangle_15935.Linear = prim::GetAttr[name="query"](%537)
  %541 : Tensor = prim::GetAttr[name="bias"](%540)
  %542 : Tensor = prim::GetAttr[name="weight"](%540)
  %543 : Float(256:1, 256:256) = aten::t(%542), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %543), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.32, %541, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %546 : Tensor = prim::GetAttr[name="bias"](%539)
  %547 : Tensor = prim::GetAttr[name="weight"](%539)
  %548 : Float(256:1, 256:256) = aten::t(%547), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %548), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.33, %546, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %551 : Tensor = prim::GetAttr[name="bias"](%538)
  %552 : Tensor = prim::GetAttr[name="weight"](%538)
  %553 : Float(256:1, 256:256) = aten::t(%552), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.56, %553), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.34, %551, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %556 : int = aten::size(%x.31, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %557 : int = aten::size(%x.31, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %558 : int[] = prim::ListConstruct(%556, %557, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.32 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.31, %558), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %560 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.32, %560), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %562 : int = aten::size(%x.33, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %563 : int = aten::size(%x.33, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %564 : int[] = prim::ListConstruct(%562, %563, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.34 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.33, %564), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %566 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.34, %566), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %568 : int = aten::size(%x.35, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %569 : int = aten::size(%x.35, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
  %570 : int[] = prim::ListConstruct(%568, %569, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %x.36 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.35, %570), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
  %572 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.36, %572), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
  %574 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.6, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %574), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.12 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.11, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:249:0
  %input.57 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:252:0
  %input.58 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.57, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.58, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:265:0
  %581 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %582 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.11, %581), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%582, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
  %584 : int = aten::size(%context_layer.12, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
  %585 : int = aten::size(%context_layer.12, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
  %586 : int[] = prim::ListConstruct(%584, %585, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
  %input.59 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.12, %586), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:269:0
  %588 : __torch__.torch.nn.modules.normalization.___torch_mangle_15941.LayerNorm = prim::GetAttr[name="LayerNorm"](%536)
  %589 : __torch__.torch.nn.modules.linear.___torch_mangle_15940.Linear = prim::GetAttr[name="dense"](%536)
  %590 : Tensor = prim::GetAttr[name="bias"](%589)
  %591 : Tensor = prim::GetAttr[name="weight"](%589)
  %592 : Float(256:1, 256:256) = aten::t(%591), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.59, %592), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.60 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.35, %590, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.60, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.61 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.11, %input.56, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output # transformers/modeling_electra.py:286:0
  %597 : Tensor = prim::GetAttr[name="bias"](%588)
  %598 : Tensor = prim::GetAttr[name="weight"](%588)
  %599 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.61, %599, %598, %597, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %601 : __torch__.torch.nn.modules.linear.___torch_mangle_15945.Linear = prim::GetAttr[name="dense"](%534)
  %602 : Tensor = prim::GetAttr[name="bias"](%601)
  %603 : Tensor = prim::GetAttr[name="weight"](%601)
  %604 : Float(256:1, 1024:256) = aten::t(%603), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.6, %604), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.62 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.36, %602, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.63 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.62), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %608 : __torch__.torch.nn.modules.normalization.___torch_mangle_15948.LayerNorm = prim::GetAttr[name="LayerNorm"](%533)
  %609 : __torch__.torch.nn.modules.linear.___torch_mangle_15947.Linear = prim::GetAttr[name="dense"](%533)
  %610 : Tensor = prim::GetAttr[name="bias"](%609)
  %611 : Tensor = prim::GetAttr[name="weight"](%609)
  %612 : Float(1024:1, 256:1024) = aten::t(%611), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.37 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.63, %612), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.64 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.37, %610, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.64, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.12, %input_tensor.6, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output # transformers/modeling_electra.py:365:0
  %617 : Tensor = prim::GetAttr[name="bias"](%608)
  %618 : Tensor = prim::GetAttr[name="weight"](%608)
  %619 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm
  %input.66 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.65, %619, %618, %617, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %621 : __torch__.transformers.modeling_electra.___torch_mangle_15967.ElectraOutput = prim::GetAttr[name="output"](%80)
  %622 : __torch__.transformers.modeling_electra.___torch_mangle_15963.ElectraIntermediate = prim::GetAttr[name="intermediate"](%80)
  %623 : __torch__.transformers.modeling_electra.___torch_mangle_15961.ElectraAttention = prim::GetAttr[name="attention"](%80)
  %624 : __torch__.transformers.modeling_electra.___torch_mangle_15960.ElectraSelfOutput = prim::GetAttr[name="output"](%623)
  %625 : __torch__.transformers.modeling_electra.___torch_mangle_15956.ElectraSelfAttention = prim::GetAttr[name="self"](%623)
  %626 : __torch__.torch.nn.modules.linear.___torch_mangle_15954.Linear = prim::GetAttr[name="value"](%625)
  %627 : __torch__.torch.nn.modules.linear.___torch_mangle_15953.Linear = prim::GetAttr[name="key"](%625)
  %628 : __torch__.torch.nn.modules.linear.___torch_mangle_15952.Linear = prim::GetAttr[name="query"](%625)
  %629 : Tensor = prim::GetAttr[name="bias"](%628)
  %630 : Tensor = prim::GetAttr[name="weight"](%628)
  %631 : Float(256:1, 256:256) = aten::t(%630), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.38 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %631), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.38, %629, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %634 : Tensor = prim::GetAttr[name="bias"](%627)
  %635 : Tensor = prim::GetAttr[name="weight"](%627)
  %636 : Float(256:1, 256:256) = aten::t(%635), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.39 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %636), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.39, %634, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %639 : Tensor = prim::GetAttr[name="bias"](%626)
  %640 : Tensor = prim::GetAttr[name="weight"](%626)
  %641 : Float(256:1, 256:256) = aten::t(%640), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.40 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.66, %641), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.40, %639, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %644 : int = aten::size(%x.37, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %645 : int = aten::size(%x.37, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %646 : int[] = prim::ListConstruct(%644, %645, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.38 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.37, %646), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %648 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.38, %648), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %650 : int = aten::size(%x.39, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %651 : int = aten::size(%x.39, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %652 : int[] = prim::ListConstruct(%650, %651, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.40 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.39, %652), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %654 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.40, %654), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %656 : int = aten::size(%x.41, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %657 : int = aten::size(%x.41, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
  %658 : int[] = prim::ListConstruct(%656, %657, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %x.42 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.41, %658), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
  %660 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.42, %660), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
  %662 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.7, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.13 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %662), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.14 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.13, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:249:0
  %input.67 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:252:0
  %input.68 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.67, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.68, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:265:0
  %669 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %670 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.13, %669), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%670, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
  %672 : int = aten::size(%context_layer.14, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
  %673 : int = aten::size(%context_layer.14, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
  %674 : int[] = prim::ListConstruct(%672, %673, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
  %input.69 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.14, %674), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:269:0
  %676 : __torch__.torch.nn.modules.normalization.___torch_mangle_15958.LayerNorm = prim::GetAttr[name="LayerNorm"](%624)
  %677 : __torch__.torch.nn.modules.linear.___torch_mangle_15957.Linear = prim::GetAttr[name="dense"](%624)
  %678 : Tensor = prim::GetAttr[name="bias"](%677)
  %679 : Tensor = prim::GetAttr[name="weight"](%677)
  %680 : Float(256:1, 256:256) = aten::t(%679), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.69, %680), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.70 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.41, %678, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.70, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.71 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.13, %input.66, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output # transformers/modeling_electra.py:286:0
  %685 : Tensor = prim::GetAttr[name="bias"](%676)
  %686 : Tensor = prim::GetAttr[name="weight"](%676)
  %687 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.71, %687, %686, %685, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %689 : __torch__.torch.nn.modules.linear.___torch_mangle_15962.Linear = prim::GetAttr[name="dense"](%622)
  %690 : Tensor = prim::GetAttr[name="bias"](%689)
  %691 : Tensor = prim::GetAttr[name="weight"](%689)
  %692 : Float(256:1, 1024:256) = aten::t(%691), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.7, %692), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.72 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.42, %690, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.73 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.72), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %696 : __torch__.torch.nn.modules.normalization.___torch_mangle_15965.LayerNorm = prim::GetAttr[name="LayerNorm"](%621)
  %697 : __torch__.torch.nn.modules.linear.___torch_mangle_15964.Linear = prim::GetAttr[name="dense"](%621)
  %698 : Tensor = prim::GetAttr[name="bias"](%697)
  %699 : Tensor = prim::GetAttr[name="weight"](%697)
  %700 : Float(1024:1, 256:1024) = aten::t(%699), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.73, %700), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.74 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.43, %698, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.74, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.75 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.14, %input_tensor.7, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output # transformers/modeling_electra.py:365:0
  %705 : Tensor = prim::GetAttr[name="bias"](%696)
  %706 : Tensor = prim::GetAttr[name="weight"](%696)
  %707 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm
  %input.76 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.75, %707, %706, %705, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %709 : __torch__.transformers.modeling_electra.___torch_mangle_15984.ElectraOutput = prim::GetAttr[name="output"](%78)
  %710 : __torch__.transformers.modeling_electra.___torch_mangle_15980.ElectraIntermediate = prim::GetAttr[name="intermediate"](%78)
  %711 : __torch__.transformers.modeling_electra.___torch_mangle_15978.ElectraAttention = prim::GetAttr[name="attention"](%78)
  %712 : __torch__.transformers.modeling_electra.___torch_mangle_15977.ElectraSelfOutput = prim::GetAttr[name="output"](%711)
  %713 : __torch__.transformers.modeling_electra.___torch_mangle_15973.ElectraSelfAttention = prim::GetAttr[name="self"](%711)
  %714 : __torch__.torch.nn.modules.linear.___torch_mangle_15971.Linear = prim::GetAttr[name="value"](%713)
  %715 : __torch__.torch.nn.modules.linear.___torch_mangle_15970.Linear = prim::GetAttr[name="key"](%713)
  %716 : __torch__.torch.nn.modules.linear.___torch_mangle_15969.Linear = prim::GetAttr[name="query"](%713)
  %717 : Tensor = prim::GetAttr[name="bias"](%716)
  %718 : Tensor = prim::GetAttr[name="weight"](%716)
  %719 : Float(256:1, 256:256) = aten::t(%718), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.44 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %719), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.44, %717, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %722 : Tensor = prim::GetAttr[name="bias"](%715)
  %723 : Tensor = prim::GetAttr[name="weight"](%715)
  %724 : Float(256:1, 256:256) = aten::t(%723), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %724), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.45, %722, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %727 : Tensor = prim::GetAttr[name="bias"](%714)
  %728 : Tensor = prim::GetAttr[name="weight"](%714)
  %729 : Float(256:1, 256:256) = aten::t(%728), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.76, %729), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.46, %727, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %732 : int = aten::size(%x.43, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %733 : int = aten::size(%x.43, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %734 : int[] = prim::ListConstruct(%732, %733, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.44 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.43, %734), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %736 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.44, %736), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %738 : int = aten::size(%x.45, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %739 : int = aten::size(%x.45, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %740 : int[] = prim::ListConstruct(%738, %739, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.46 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.45, %740), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %742 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.46, %742), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %744 : int = aten::size(%x.47, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %745 : int = aten::size(%x.47, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
  %746 : int[] = prim::ListConstruct(%744, %745, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %x.48 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.47, %746), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
  %748 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.48, %748), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
  %750 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.8, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.15 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %750), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.16 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.15, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:249:0
  %input.77 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:252:0
  %input.78 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.77, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.78, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:265:0
  %757 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %758 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.15, %757), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%758, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
  %760 : int = aten::size(%context_layer.16, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
  %761 : int = aten::size(%context_layer.16, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
  %762 : int[] = prim::ListConstruct(%760, %761, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
  %input.79 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.16, %762), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:269:0
  %764 : __torch__.torch.nn.modules.normalization.___torch_mangle_15975.LayerNorm = prim::GetAttr[name="LayerNorm"](%712)
  %765 : __torch__.torch.nn.modules.linear.___torch_mangle_15974.Linear = prim::GetAttr[name="dense"](%712)
  %766 : Tensor = prim::GetAttr[name="bias"](%765)
  %767 : Tensor = prim::GetAttr[name="weight"](%765)
  %768 : Float(256:1, 256:256) = aten::t(%767), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.79, %768), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.80 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.47, %766, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.80, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.81 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.15, %input.76, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output # transformers/modeling_electra.py:286:0
  %773 : Tensor = prim::GetAttr[name="bias"](%764)
  %774 : Tensor = prim::GetAttr[name="weight"](%764)
  %775 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.81, %775, %774, %773, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %777 : __torch__.torch.nn.modules.linear.___torch_mangle_15979.Linear = prim::GetAttr[name="dense"](%710)
  %778 : Tensor = prim::GetAttr[name="bias"](%777)
  %779 : Tensor = prim::GetAttr[name="weight"](%777)
  %780 : Float(256:1, 1024:256) = aten::t(%779), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.8, %780), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.82 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.48, %778, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.83 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %784 : __torch__.torch.nn.modules.normalization.___torch_mangle_15982.LayerNorm = prim::GetAttr[name="LayerNorm"](%709)
  %785 : __torch__.torch.nn.modules.linear.___torch_mangle_15981.Linear = prim::GetAttr[name="dense"](%709)
  %786 : Tensor = prim::GetAttr[name="bias"](%785)
  %787 : Tensor = prim::GetAttr[name="weight"](%785)
  %788 : Float(1024:1, 256:1024) = aten::t(%787), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.49 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.83, %788), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.84 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.49, %786, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.84, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.85 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.16, %input_tensor.8, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output # transformers/modeling_electra.py:365:0
  %793 : Tensor = prim::GetAttr[name="bias"](%784)
  %794 : Tensor = prim::GetAttr[name="weight"](%784)
  %795 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm
  %input.86 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.85, %795, %794, %793, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %797 : __torch__.transformers.modeling_electra.___torch_mangle_16001.ElectraOutput = prim::GetAttr[name="output"](%76)
  %798 : __torch__.transformers.modeling_electra.___torch_mangle_15997.ElectraIntermediate = prim::GetAttr[name="intermediate"](%76)
  %799 : __torch__.transformers.modeling_electra.___torch_mangle_15995.ElectraAttention = prim::GetAttr[name="attention"](%76)
  %800 : __torch__.transformers.modeling_electra.___torch_mangle_15994.ElectraSelfOutput = prim::GetAttr[name="output"](%799)
  %801 : __torch__.transformers.modeling_electra.___torch_mangle_15990.ElectraSelfAttention = prim::GetAttr[name="self"](%799)
  %802 : __torch__.torch.nn.modules.linear.___torch_mangle_15988.Linear = prim::GetAttr[name="value"](%801)
  %803 : __torch__.torch.nn.modules.linear.___torch_mangle_15987.Linear = prim::GetAttr[name="key"](%801)
  %804 : __torch__.torch.nn.modules.linear.___torch_mangle_15986.Linear = prim::GetAttr[name="query"](%801)
  %805 : Tensor = prim::GetAttr[name="bias"](%804)
  %806 : Tensor = prim::GetAttr[name="weight"](%804)
  %807 : Float(256:1, 256:256) = aten::t(%806), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %807), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.50, %805, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %810 : Tensor = prim::GetAttr[name="bias"](%803)
  %811 : Tensor = prim::GetAttr[name="weight"](%803)
  %812 : Float(256:1, 256:256) = aten::t(%811), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %812), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.51, %810, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %815 : Tensor = prim::GetAttr[name="bias"](%802)
  %816 : Tensor = prim::GetAttr[name="weight"](%802)
  %817 : Float(256:1, 256:256) = aten::t(%816), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.86, %817), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.52, %815, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %820 : int = aten::size(%x.49, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %821 : int = aten::size(%x.49, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %822 : int[] = prim::ListConstruct(%820, %821, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.50 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.49, %822), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %824 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.50, %824), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %826 : int = aten::size(%x.51, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %827 : int = aten::size(%x.51, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %828 : int[] = prim::ListConstruct(%826, %827, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.52 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.51, %828), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %830 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.52, %830), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %832 : int = aten::size(%x.53, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %833 : int = aten::size(%x.53, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
  %834 : int[] = prim::ListConstruct(%832, %833, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %x.54 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.53, %834), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
  %836 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.54, %836), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
  %838 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.9, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %838), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.17, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:249:0
  %input.87 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:252:0
  %input.88 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.87, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.88, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:265:0
  %845 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %846 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.17, %845), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%846, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
  %848 : int = aten::size(%context_layer.18, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
  %849 : int = aten::size(%context_layer.18, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
  %850 : int[] = prim::ListConstruct(%848, %849, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
  %input.89 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.18, %850), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:269:0
  %852 : __torch__.torch.nn.modules.normalization.___torch_mangle_15992.LayerNorm = prim::GetAttr[name="LayerNorm"](%800)
  %853 : __torch__.torch.nn.modules.linear.___torch_mangle_15991.Linear = prim::GetAttr[name="dense"](%800)
  %854 : Tensor = prim::GetAttr[name="bias"](%853)
  %855 : Tensor = prim::GetAttr[name="weight"](%853)
  %856 : Float(256:1, 256:256) = aten::t(%855), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.89, %856), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.90 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.53, %854, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.90, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.91 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.17, %input.86, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output # transformers/modeling_electra.py:286:0
  %861 : Tensor = prim::GetAttr[name="bias"](%852)
  %862 : Tensor = prim::GetAttr[name="weight"](%852)
  %863 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.91, %863, %862, %861, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %865 : __torch__.torch.nn.modules.linear.___torch_mangle_15996.Linear = prim::GetAttr[name="dense"](%798)
  %866 : Tensor = prim::GetAttr[name="bias"](%865)
  %867 : Tensor = prim::GetAttr[name="weight"](%865)
  %868 : Float(256:1, 1024:256) = aten::t(%867), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.9, %868), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.92 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.54, %866, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.93 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %872 : __torch__.torch.nn.modules.normalization.___torch_mangle_15999.LayerNorm = prim::GetAttr[name="LayerNorm"](%797)
  %873 : __torch__.torch.nn.modules.linear.___torch_mangle_15998.Linear = prim::GetAttr[name="dense"](%797)
  %874 : Tensor = prim::GetAttr[name="bias"](%873)
  %875 : Tensor = prim::GetAttr[name="weight"](%873)
  %876 : Float(1024:1, 256:1024) = aten::t(%875), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.93, %876), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.94 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.55, %874, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.94, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.95 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.18, %input_tensor.9, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output # transformers/modeling_electra.py:365:0
  %881 : Tensor = prim::GetAttr[name="bias"](%872)
  %882 : Tensor = prim::GetAttr[name="weight"](%872)
  %883 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm
  %input.96 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.95, %883, %882, %881, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %885 : __torch__.transformers.modeling_electra.___torch_mangle_16018.ElectraOutput = prim::GetAttr[name="output"](%74)
  %886 : __torch__.transformers.modeling_electra.___torch_mangle_16014.ElectraIntermediate = prim::GetAttr[name="intermediate"](%74)
  %887 : __torch__.transformers.modeling_electra.___torch_mangle_16012.ElectraAttention = prim::GetAttr[name="attention"](%74)
  %888 : __torch__.transformers.modeling_electra.___torch_mangle_16011.ElectraSelfOutput = prim::GetAttr[name="output"](%887)
  %889 : __torch__.transformers.modeling_electra.___torch_mangle_16007.ElectraSelfAttention = prim::GetAttr[name="self"](%887)
  %890 : __torch__.torch.nn.modules.linear.___torch_mangle_16005.Linear = prim::GetAttr[name="value"](%889)
  %891 : __torch__.torch.nn.modules.linear.___torch_mangle_16004.Linear = prim::GetAttr[name="key"](%889)
  %892 : __torch__.torch.nn.modules.linear.___torch_mangle_16003.Linear = prim::GetAttr[name="query"](%889)
  %893 : Tensor = prim::GetAttr[name="bias"](%892)
  %894 : Tensor = prim::GetAttr[name="weight"](%892)
  %895 : Float(256:1, 256:256) = aten::t(%894), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %895), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.56, %893, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %898 : Tensor = prim::GetAttr[name="bias"](%891)
  %899 : Tensor = prim::GetAttr[name="weight"](%891)
  %900 : Float(256:1, 256:256) = aten::t(%899), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %900), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.57, %898, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %903 : Tensor = prim::GetAttr[name="bias"](%890)
  %904 : Tensor = prim::GetAttr[name="weight"](%890)
  %905 : Float(256:1, 256:256) = aten::t(%904), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.96, %905), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.58, %903, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %908 : int = aten::size(%x.55, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %909 : int = aten::size(%x.55, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %910 : int[] = prim::ListConstruct(%908, %909, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.56 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.55, %910), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %912 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.56, %912), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %914 : int = aten::size(%x.57, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %915 : int = aten::size(%x.57, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %916 : int[] = prim::ListConstruct(%914, %915, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.58 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.57, %916), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %918 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.58, %918), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %920 : int = aten::size(%x.59, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %921 : int = aten::size(%x.59, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
  %922 : int[] = prim::ListConstruct(%920, %921, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %x.60 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.59, %922), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
  %924 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.60, %924), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
  %926 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.10, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.19 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %926), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.20 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.19, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:249:0
  %input.97 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:252:0
  %input.98 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.97, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.98, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:265:0
  %933 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %934 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.19, %933), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%934, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
  %936 : int = aten::size(%context_layer.20, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
  %937 : int = aten::size(%context_layer.20, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
  %938 : int[] = prim::ListConstruct(%936, %937, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
  %input.99 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.20, %938), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:269:0
  %940 : __torch__.torch.nn.modules.normalization.___torch_mangle_16009.LayerNorm = prim::GetAttr[name="LayerNorm"](%888)
  %941 : __torch__.torch.nn.modules.linear.___torch_mangle_16008.Linear = prim::GetAttr[name="dense"](%888)
  %942 : Tensor = prim::GetAttr[name="bias"](%941)
  %943 : Tensor = prim::GetAttr[name="weight"](%941)
  %944 : Float(256:1, 256:256) = aten::t(%943), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.99, %944), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.59, %942, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.100, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.19, %input.96, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output # transformers/modeling_electra.py:286:0
  %949 : Tensor = prim::GetAttr[name="bias"](%940)
  %950 : Tensor = prim::GetAttr[name="weight"](%940)
  %951 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.101, %951, %950, %949, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %953 : __torch__.torch.nn.modules.linear.___torch_mangle_16013.Linear = prim::GetAttr[name="dense"](%886)
  %954 : Tensor = prim::GetAttr[name="bias"](%953)
  %955 : Tensor = prim::GetAttr[name="weight"](%953)
  %956 : Float(256:1, 1024:256) = aten::t(%955), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.10, %956), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.102 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.60, %954, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.103 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.102), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %960 : __torch__.torch.nn.modules.normalization.___torch_mangle_16016.LayerNorm = prim::GetAttr[name="LayerNorm"](%885)
  %961 : __torch__.torch.nn.modules.linear.___torch_mangle_16015.Linear = prim::GetAttr[name="dense"](%885)
  %962 : Tensor = prim::GetAttr[name="bias"](%961)
  %963 : Tensor = prim::GetAttr[name="weight"](%961)
  %964 : Float(1024:1, 256:1024) = aten::t(%963), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.61 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.103, %964), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.104 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.61, %962, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.104, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.20, %input_tensor.10, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output # transformers/modeling_electra.py:365:0
  %969 : Tensor = prim::GetAttr[name="bias"](%960)
  %970 : Tensor = prim::GetAttr[name="weight"](%960)
  %971 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm
  %input.106 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.105, %971, %970, %969, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %973 : __torch__.transformers.modeling_electra.___torch_mangle_16035.ElectraOutput = prim::GetAttr[name="output"](%72)
  %974 : __torch__.transformers.modeling_electra.___torch_mangle_16031.ElectraIntermediate = prim::GetAttr[name="intermediate"](%72)
  %975 : __torch__.transformers.modeling_electra.___torch_mangle_16029.ElectraAttention = prim::GetAttr[name="attention"](%72)
  %976 : __torch__.transformers.modeling_electra.___torch_mangle_16028.ElectraSelfOutput = prim::GetAttr[name="output"](%975)
  %977 : __torch__.transformers.modeling_electra.___torch_mangle_16024.ElectraSelfAttention = prim::GetAttr[name="self"](%975)
  %978 : __torch__.torch.nn.modules.linear.___torch_mangle_16022.Linear = prim::GetAttr[name="value"](%977)
  %979 : __torch__.torch.nn.modules.linear.___torch_mangle_16021.Linear = prim::GetAttr[name="key"](%977)
  %980 : __torch__.torch.nn.modules.linear.___torch_mangle_16020.Linear = prim::GetAttr[name="query"](%977)
  %981 : Tensor = prim::GetAttr[name="bias"](%980)
  %982 : Tensor = prim::GetAttr[name="weight"](%980)
  %983 : Float(256:1, 256:256) = aten::t(%982), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.62 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %983), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.62, %981, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %986 : Tensor = prim::GetAttr[name="bias"](%979)
  %987 : Tensor = prim::GetAttr[name="weight"](%979)
  %988 : Float(256:1, 256:256) = aten::t(%987), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.63 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %988), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.63, %986, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %991 : Tensor = prim::GetAttr[name="bias"](%978)
  %992 : Tensor = prim::GetAttr[name="weight"](%978)
  %993 : Float(256:1, 256:256) = aten::t(%992), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.64 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.106, %993), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.64, %991, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %996 : int = aten::size(%x.61, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %997 : int = aten::size(%x.61, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %998 : int[] = prim::ListConstruct(%996, %997, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.62 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.61, %998), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1000 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.62, %1000), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1002 : int = aten::size(%x.63, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1003 : int = aten::size(%x.63, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1004 : int[] = prim::ListConstruct(%1002, %1003, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.64 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.63, %1004), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1006 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.64, %1006), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1008 : int = aten::size(%x.65, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1009 : int = aten::size(%x.65, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
  %1010 : int[] = prim::ListConstruct(%1008, %1009, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %x.66 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.65, %1010), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
  %1012 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.66, %1012), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
  %1014 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.11, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.21 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1014), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.22 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.21, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:249:0
  %input.107 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:252:0
  %input.108 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.107, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.108, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:265:0
  %1021 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %1022 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.21, %1021), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
  %context_layer.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%1022, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
  %1024 : int = aten::size(%context_layer.22, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
  %1025 : int = aten::size(%context_layer.22, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
  %1026 : int[] = prim::ListConstruct(%1024, %1025, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
  %input.109 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.22, %1026), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:269:0
  %1028 : __torch__.torch.nn.modules.normalization.___torch_mangle_16026.LayerNorm = prim::GetAttr[name="LayerNorm"](%976)
  %1029 : __torch__.torch.nn.modules.linear.___torch_mangle_16025.Linear = prim::GetAttr[name="dense"](%976)
  %1030 : Tensor = prim::GetAttr[name="bias"](%1029)
  %1031 : Tensor = prim::GetAttr[name="weight"](%1029)
  %1032 : Float(256:1, 256:256) = aten::t(%1031), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.109, %1032), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.110 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.65, %1030, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.110, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.111 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.21, %input.106, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output # transformers/modeling_electra.py:286:0
  %1037 : Tensor = prim::GetAttr[name="bias"](%1028)
  %1038 : Tensor = prim::GetAttr[name="weight"](%1028)
  %1039 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.111, %1039, %1038, %1037, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1041 : __torch__.torch.nn.modules.linear.___torch_mangle_16030.Linear = prim::GetAttr[name="dense"](%974)
  %1042 : Tensor = prim::GetAttr[name="bias"](%1041)
  %1043 : Tensor = prim::GetAttr[name="weight"](%1041)
  %1044 : Float(256:1, 1024:256) = aten::t(%1043), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor.11, %1044), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.112 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.66, %1042, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.113 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.112), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1048 : __torch__.torch.nn.modules.normalization.___torch_mangle_16033.LayerNorm = prim::GetAttr[name="LayerNorm"](%973)
  %1049 : __torch__.torch.nn.modules.linear.___torch_mangle_16032.Linear = prim::GetAttr[name="dense"](%973)
  %1050 : Tensor = prim::GetAttr[name="bias"](%1049)
  %1051 : Tensor = prim::GetAttr[name="weight"](%1049)
  %1052 : Float(1024:1, 256:1024) = aten::t(%1051), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.67 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.113, %1052), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.67, %1050, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.114, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.22, %input_tensor.11, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output # transformers/modeling_electra.py:365:0
  %1057 : Tensor = prim::GetAttr[name="bias"](%1048)
  %1058 : Tensor = prim::GetAttr[name="weight"](%1048)
  %1059 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm
  %input.116 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.115, %1059, %1058, %1057, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1061 : __torch__.transformers.modeling_electra.___torch_mangle_16052.ElectraOutput = prim::GetAttr[name="output"](%70)
  %1062 : __torch__.transformers.modeling_electra.___torch_mangle_16048.ElectraIntermediate = prim::GetAttr[name="intermediate"](%70)
  %1063 : __torch__.transformers.modeling_electra.___torch_mangle_16046.ElectraAttention = prim::GetAttr[name="attention"](%70)
  %1064 : __torch__.transformers.modeling_electra.___torch_mangle_16045.ElectraSelfOutput = prim::GetAttr[name="output"](%1063)
  %1065 : __torch__.transformers.modeling_electra.___torch_mangle_16041.ElectraSelfAttention = prim::GetAttr[name="self"](%1063)
  %1066 : __torch__.torch.nn.modules.linear.___torch_mangle_16039.Linear = prim::GetAttr[name="value"](%1065)
  %1067 : __torch__.torch.nn.modules.linear.___torch_mangle_16038.Linear = prim::GetAttr[name="key"](%1065)
  %1068 : __torch__.torch.nn.modules.linear.___torch_mangle_16037.Linear = prim::GetAttr[name="query"](%1065)
  %1069 : Tensor = prim::GetAttr[name="bias"](%1068)
  %1070 : Tensor = prim::GetAttr[name="weight"](%1068)
  %1071 : Float(256:1, 256:256) = aten::t(%1070), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.68 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1071), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.68, %1069, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1074 : Tensor = prim::GetAttr[name="bias"](%1067)
  %1075 : Tensor = prim::GetAttr[name="weight"](%1067)
  %1076 : Float(256:1, 256:256) = aten::t(%1075), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.69 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1076), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.69, %1074, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1079 : Tensor = prim::GetAttr[name="bias"](%1066)
  %1080 : Tensor = prim::GetAttr[name="weight"](%1066)
  %1081 : Float(256:1, 256:256) = aten::t(%1080), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.70 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.116, %1081), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.70, %1079, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1084 : int = aten::size(%x.67, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1085 : int = aten::size(%x.67, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1086 : int[] = prim::ListConstruct(%1084, %1085, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x.68 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.67, %1086), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1088 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %query_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.68, %1088), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1090 : int = aten::size(%x.69, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1091 : int = aten::size(%x.69, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1092 : int[] = prim::ListConstruct(%1090, %1091, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x.70 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.69, %1092), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1094 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %key_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.70, %1094), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1096 : int = aten::size(%x.71, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1097 : int = aten::size(%x.71, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
  %1098 : int[] = prim::ListConstruct(%1096, %1097, %26, %11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %x : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%x.71, %1098), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
  %1100 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %value_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x, %1100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
  %1102 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer, %12, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores.23 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer, %1102), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
  %attention_scores : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.23, %9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:249:0
  %input.117 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:252:0
  %input.118 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.117, %12, %19), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.118, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:265:0
  %1109 : int[] = prim::ListConstruct(%28, %22, %27, %21), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %1110 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.23, %1109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
  %context_layer : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%1110, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
  %1112 : int = aten::size(%context_layer, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
  %1113 : int = aten::size(%context_layer, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
  %1114 : int[] = prim::ListConstruct(%1112, %1113, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
  %input.119 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer, %1114), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:269:0
  %1116 : __torch__.torch.nn.modules.normalization.___torch_mangle_16043.LayerNorm = prim::GetAttr[name="LayerNorm"](%1064)
  %1117 : __torch__.torch.nn.modules.linear.___torch_mangle_16042.Linear = prim::GetAttr[name="dense"](%1064)
  %1118 : Tensor = prim::GetAttr[name="bias"](%1117)
  %1119 : Tensor = prim::GetAttr[name="weight"](%1117)
  %1120 : Float(256:1, 256:256) = aten::t(%1119), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.119, %1120), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.120 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.71, %1118, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.120, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.121 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states.23, %input.116, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output # transformers/modeling_electra.py:286:0
  %1125 : Tensor = prim::GetAttr[name="bias"](%1116)
  %1126 : Tensor = prim::GetAttr[name="weight"](%1116)
  %1127 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.121, %1127, %1126, %1125, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1129 : __torch__.torch.nn.modules.linear.___torch_mangle_16047.Linear = prim::GetAttr[name="dense"](%1062)
  %1130 : Tensor = prim::GetAttr[name="bias"](%1129)
  %1131 : Tensor = prim::GetAttr[name="weight"](%1129)
  %1132 : Float(256:1, 1024:256) = aten::t(%1131), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input_tensor, %1132), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.122 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.72, %1130, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.123 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%input.122), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1136 : __torch__.torch.nn.modules.normalization.___torch_mangle_16050.LayerNorm = prim::GetAttr[name="LayerNorm"](%1061)
  %1137 : __torch__.torch.nn.modules.linear.___torch_mangle_16049.Linear = prim::GetAttr[name="dense"](%1061)
  %1138 : Tensor = prim::GetAttr[name="bias"](%1137)
  %1139 : Tensor = prim::GetAttr[name="weight"](%1137)
  %1140 : Float(1024:1, 256:1024) = aten::t(%1139), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.123, %1140), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.124 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.73, %1138, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:3328, 13:256, 256:1) = aten::dropout(%input.124, %16, %24), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.125 : Float(17:3328, 13:256, 256:1) = aten::add(%hidden_states, %input_tensor, %27), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output # transformers/modeling_electra.py:365:0
  %1145 : Tensor = prim::GetAttr[name="bias"](%1136)
  %1146 : Tensor = prim::GetAttr[name="weight"](%1136)
  %1147 : int[] = prim::ListConstruct(%8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm
  %input.126 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.125, %1147, %1146, %1145, %14, %13), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1149 : int = prim::Constant[value=1](), scope: __module.discriminator_predictions/__module.discriminator_predictions.dense # torch/nn/functional.py:1678:0
  %1150 : __torch__.torch.nn.modules.linear.___torch_mangle_16058.Linear = prim::GetAttr[name="dense_prediction"](%3)
  %1151 : __torch__.torch.nn.modules.linear.___torch_mangle_16057.Linear = prim::GetAttr[name="dense"](%3)
  %1152 : Tensor = prim::GetAttr[name="bias"](%1151)
  %1153 : Tensor = prim::GetAttr[name="weight"](%1151)
  %1154 : Float(256:1, 256:256) = aten::t(%1153), scope: __module.discriminator_predictions/__module.discriminator_predictions.dense # torch/nn/functional.py:1676:0
  %output.74 : Float(17:3328, 13:256, 256:1) = aten::matmul(%input.126, %1154), scope: __module.discriminator_predictions/__module.discriminator_predictions.dense # torch/nn/functional.py:1676:0
  %input.127 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.74, %1152, %1149), scope: __module.discriminator_predictions/__module.discriminator_predictions.dense # torch/nn/functional.py:1678:0
  %input : Float(17:3328, 13:256, 256:1) = aten::gelu(%input.127), scope: __module.discriminator_predictions # torch/nn/functional.py:1369:0
  %1158 : Tensor = prim::GetAttr[name="bias"](%1150)
  %1159 : Tensor = prim::GetAttr[name="weight"](%1150)
  %1160 : Float(256:1, 1:256) = aten::t(%1159), scope: __module.discriminator_predictions/__module.discriminator_predictions.dense_prediction # torch/nn/functional.py:1676:0
  %output : Float(17:13, 13:1, 1:1) = aten::matmul(%input, %1160), scope: __module.discriminator_predictions/__module.discriminator_predictions.dense_prediction # torch/nn/functional.py:1676:0
  %1162 : Float(17:13, 13:1, 1:1) = aten::add_(%output, %1158, %1149), scope: __module.discriminator_predictions/__module.discriminator_predictions.dense_prediction # torch/nn/functional.py:1678:0
  %1163 : Float(17:13, 13:1) = aten::squeeze(%1162), scope: __module.discriminator_predictions # transformers/modeling_electra.py:507:0
  %7 : (Float(17:13, 13:1)) = prim::TupleConstruct(%1163)
  return (%7)
