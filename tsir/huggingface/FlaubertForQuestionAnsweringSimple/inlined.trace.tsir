graph(%self.1 : __torch__.transformers.modeling_flaubert.FlaubertForQuestionAnsweringSimple,
      %input_ids : Long(17:13, 13:1),
      %padding_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_387.Linear = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_flaubert.___torch_mangle_386.FlaubertModel = prim::GetAttr[name="transformer"](%self.1)
  %17 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %18 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %19 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %20 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %21 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %22 : None = prim::Constant(), scope: __module.transformer
  %23 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %24 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
  %25 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %26 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %27 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %28 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %30 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %31 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %32 : int = prim::Constant[value=4](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %33 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %34 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %35 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %36 : __torch__.torch.nn.modules.normalization.___torch_mangle_384.LayerNorm = prim::GetAttr[name="11"](%35)
  %37 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %38 : __torch__.transformers.modeling_xlm.___torch_mangle_371.TransformerFFN = prim::GetAttr[name="11"](%37)
  %39 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %40 : __torch__.torch.nn.modules.normalization.___torch_mangle_334.LayerNorm = prim::GetAttr[name="11"](%39)
  %41 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %42 : __torch__.transformers.modeling_xlm.___torch_mangle_321.MultiHeadAttention = prim::GetAttr[name="11"](%41)
  %43 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %44 : __torch__.torch.nn.modules.normalization.___torch_mangle_383.LayerNorm = prim::GetAttr[name="10"](%43)
  %45 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %46 : __torch__.transformers.modeling_xlm.___torch_mangle_368.TransformerFFN = prim::GetAttr[name="10"](%45)
  %47 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %48 : __torch__.torch.nn.modules.normalization.___torch_mangle_333.LayerNorm = prim::GetAttr[name="10"](%47)
  %49 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %50 : __torch__.transformers.modeling_xlm.___torch_mangle_316.MultiHeadAttention = prim::GetAttr[name="10"](%49)
  %51 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %52 : __torch__.torch.nn.modules.normalization.___torch_mangle_382.LayerNorm = prim::GetAttr[name="9"](%51)
  %53 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %54 : __torch__.transformers.modeling_xlm.___torch_mangle_365.TransformerFFN = prim::GetAttr[name="9"](%53)
  %55 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %56 : __torch__.torch.nn.modules.normalization.___torch_mangle_332.LayerNorm = prim::GetAttr[name="9"](%55)
  %57 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %58 : __torch__.transformers.modeling_xlm.___torch_mangle_311.MultiHeadAttention = prim::GetAttr[name="9"](%57)
  %59 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %60 : __torch__.torch.nn.modules.normalization.___torch_mangle_381.LayerNorm = prim::GetAttr[name="8"](%59)
  %61 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %62 : __torch__.transformers.modeling_xlm.___torch_mangle_362.TransformerFFN = prim::GetAttr[name="8"](%61)
  %63 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %64 : __torch__.torch.nn.modules.normalization.___torch_mangle_331.LayerNorm = prim::GetAttr[name="8"](%63)
  %65 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %66 : __torch__.transformers.modeling_xlm.___torch_mangle_306.MultiHeadAttention = prim::GetAttr[name="8"](%65)
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %68 : __torch__.torch.nn.modules.normalization.___torch_mangle_380.LayerNorm = prim::GetAttr[name="7"](%67)
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %70 : __torch__.transformers.modeling_xlm.___torch_mangle_359.TransformerFFN = prim::GetAttr[name="7"](%69)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %72 : __torch__.torch.nn.modules.normalization.___torch_mangle_330.LayerNorm = prim::GetAttr[name="7"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %74 : __torch__.transformers.modeling_xlm.___torch_mangle_301.MultiHeadAttention = prim::GetAttr[name="7"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %76 : __torch__.torch.nn.modules.normalization.___torch_mangle_379.LayerNorm = prim::GetAttr[name="6"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %78 : __torch__.transformers.modeling_xlm.___torch_mangle_356.TransformerFFN = prim::GetAttr[name="6"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %80 : __torch__.torch.nn.modules.normalization.___torch_mangle_329.LayerNorm = prim::GetAttr[name="6"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %82 : __torch__.transformers.modeling_xlm.___torch_mangle_296.MultiHeadAttention = prim::GetAttr[name="6"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %84 : __torch__.torch.nn.modules.normalization.___torch_mangle_378.LayerNorm = prim::GetAttr[name="5"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %86 : __torch__.transformers.modeling_xlm.___torch_mangle_353.TransformerFFN = prim::GetAttr[name="5"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %88 : __torch__.torch.nn.modules.normalization.___torch_mangle_328.LayerNorm = prim::GetAttr[name="5"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %90 : __torch__.transformers.modeling_xlm.___torch_mangle_291.MultiHeadAttention = prim::GetAttr[name="5"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %92 : __torch__.torch.nn.modules.normalization.___torch_mangle_377.LayerNorm = prim::GetAttr[name="4"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %94 : __torch__.transformers.modeling_xlm.___torch_mangle_350.TransformerFFN = prim::GetAttr[name="4"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %96 : __torch__.torch.nn.modules.normalization.___torch_mangle_327.LayerNorm = prim::GetAttr[name="4"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %98 : __torch__.transformers.modeling_xlm.___torch_mangle_286.MultiHeadAttention = prim::GetAttr[name="4"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %100 : __torch__.torch.nn.modules.normalization.___torch_mangle_376.LayerNorm = prim::GetAttr[name="3"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %102 : __torch__.transformers.modeling_xlm.___torch_mangle_347.TransformerFFN = prim::GetAttr[name="3"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %104 : __torch__.torch.nn.modules.normalization.___torch_mangle_326.LayerNorm = prim::GetAttr[name="3"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %106 : __torch__.transformers.modeling_xlm.___torch_mangle_281.MultiHeadAttention = prim::GetAttr[name="3"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %108 : __torch__.torch.nn.modules.normalization.___torch_mangle_375.LayerNorm = prim::GetAttr[name="2"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %110 : __torch__.transformers.modeling_xlm.___torch_mangle_344.TransformerFFN = prim::GetAttr[name="2"](%109)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %112 : __torch__.torch.nn.modules.normalization.___torch_mangle_325.LayerNorm = prim::GetAttr[name="2"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %114 : __torch__.transformers.modeling_xlm.___torch_mangle_276.MultiHeadAttention = prim::GetAttr[name="2"](%113)
  %115 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %116 : __torch__.torch.nn.modules.normalization.___torch_mangle_374.LayerNorm = prim::GetAttr[name="1"](%115)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %118 : __torch__.transformers.modeling_xlm.___torch_mangle_341.TransformerFFN = prim::GetAttr[name="1"](%117)
  %119 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %120 : __torch__.torch.nn.modules.normalization.___torch_mangle_324.LayerNorm = prim::GetAttr[name="1"](%119)
  %121 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %122 : __torch__.transformers.modeling_xlm.___torch_mangle_271.MultiHeadAttention = prim::GetAttr[name="1"](%121)
  %123 : __torch__.torch.nn.modules.container.___torch_mangle_385.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %124 : __torch__.torch.nn.modules.normalization.___torch_mangle_373.LayerNorm = prim::GetAttr[name="0"](%123)
  %125 : __torch__.torch.nn.modules.container.___torch_mangle_372.ModuleList = prim::GetAttr[name="ffns"](%4)
  %126 : __torch__.transformers.modeling_xlm.___torch_mangle_338.TransformerFFN = prim::GetAttr[name="0"](%125)
  %127 : __torch__.torch.nn.modules.container.___torch_mangle_335.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %128 : __torch__.torch.nn.modules.normalization.___torch_mangle_323.LayerNorm = prim::GetAttr[name="0"](%127)
  %129 : __torch__.torch.nn.modules.container.___torch_mangle_322.ModuleList = prim::GetAttr[name="attentions"](%4)
  %130 : __torch__.transformers.modeling_xlm.___torch_mangle_266.MultiHeadAttention = prim::GetAttr[name="0"](%129)
  %131 : __torch__.torch.nn.modules.normalization.___torch_mangle_261.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%4)
  %132 : __torch__.torch.nn.modules.sparse.___torch_mangle_259.Embedding = prim::GetAttr[name="position_embeddings"](%4)
  %133 : __torch__.torch.nn.modules.sparse.___torch_mangle_260.Embedding = prim::GetAttr[name="embeddings"](%4)
  %134 : int = aten::size(%input_ids, %34), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %135 : int = aten::size(%input_ids, %33), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %position_ids : Long(13:1) = aten::arange(%135, %32, %34, %31, %30), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %137 : Long(1:13, 13:1) = aten::unsqueeze(%position_ids, %34), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
  %138 : int[] = prim::ListConstruct(%134, %135), scope: __module.transformer
  %input.1 : Long(17:0, 13:1) = aten::expand(%137, %138, %30), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
  %140 : Tensor = prim::GetAttr[name="weight"](%133)
  %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%140, %input_ids, %29, %30, %30), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %142 : Tensor = prim::GetAttr[name="weight"](%132)
  %143 : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%142, %input.1, %28, %30, %30), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %144 : Float(17:26624, 13:2048, 2048:1) = aten::expand_as(%143, %inputs_embeds), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
  %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%inputs_embeds, %144, %33), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
  %146 : Tensor = prim::GetAttr[name="bias"](%131)
  %147 : Tensor = prim::GetAttr[name="weight"](%131)
  %148 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm_emb
  %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %148, %147, %146, %26, %27), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.3, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %151 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %152 : Float(17:13, 13:1, 1:1) = aten::to(%151, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %152), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %154 : __torch__.torch.nn.modules.linear.___torch_mangle_265.Linear = prim::GetAttr[name="out_lin"](%130)
  %155 : __torch__.torch.nn.modules.linear.___torch_mangle_264.Linear = prim::GetAttr[name="v_lin"](%130)
  %156 : __torch__.torch.nn.modules.linear.___torch_mangle_263.Linear = prim::GetAttr[name="k_lin"](%130)
  %157 : __torch__.torch.nn.modules.linear.___torch_mangle_262.Linear = prim::GetAttr[name="q_lin"](%130)
  %158 : int = aten::size(%input.4, %34), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %159 : int = aten::size(%input.4, %33), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %160 : Tensor = prim::GetAttr[name="bias"](%157)
  %161 : Tensor = prim::GetAttr[name="weight"](%157)
  %162 : Float(2048:1, 2048:2048) = aten::t(%161), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %162), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %160, %33), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1678:0
  %165 : int[] = prim::ListConstruct(%158, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.0
  %166 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.1, %165), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%166, %33, %29), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %168 : Tensor = prim::GetAttr[name="bias"](%156)
  %169 : Tensor = prim::GetAttr[name="weight"](%156)
  %170 : Float(2048:1, 2048:2048) = aten::t(%169), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %170), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %168, %33), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1678:0
  %173 : int[] = prim::ListConstruct(%158, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.0
  %174 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.2, %173), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%174, %33, %29), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %176 : Tensor = prim::GetAttr[name="bias"](%155)
  %177 : Tensor = prim::GetAttr[name="weight"](%155)
  %178 : Float(2048:1, 2048:2048) = aten::t(%177), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %178), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %176, %33), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1678:0
  %181 : int[] = prim::ListConstruct(%158, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.0
  %182 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.3, %181), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%182, %33, %29), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %19), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %185 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %29, %20), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %185), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %187 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.0 # torch/tensor.py:22:0
  %188 : int[] = prim::ListConstruct(%158, %33, %33, %159), scope: __module.transformer/__module.transformer.attentions.0
  %189 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%187, %188), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%189, %scores.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %21), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
  %193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %28, %22), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:1498:0
  %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%193, %24, %30), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
  %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:200:0
  %196 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %33, %29), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %197 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%196, %34), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %198 : int[] = prim::ListConstruct(%158, %28, %25), scope: __module.transformer/__module.transformer.attentions.0
  %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%197, %198), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %200 : Tensor = prim::GetAttr[name="bias"](%154)
  %201 : Tensor = prim::GetAttr[name="weight"](%154)
  %202 : Float(2048:1, 2048:2048) = aten::t(%201), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %202), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %200, %33), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1678:0
  %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.8, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %207 : Tensor = prim::GetAttr[name="bias"](%128)
  %208 : Tensor = prim::GetAttr[name="weight"](%128)
  %209 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.0
  %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %209, %208, %207, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
  %211 : __torch__.torch.nn.modules.linear.___torch_mangle_337.Linear = prim::GetAttr[name="lin2"](%126)
  %212 : __torch__.torch.nn.modules.linear.___torch_mangle_336.Linear = prim::GetAttr[name="lin1"](%126)
  %213 : Tensor = prim::GetAttr[name="bias"](%212)
  %214 : Tensor = prim::GetAttr[name="weight"](%212)
  %215 : Float(2048:1, 8192:2048) = aten::t(%214), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.1, %215), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %213, %33), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1678:0
  %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.10), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:1369:0
  %219 : Tensor = prim::GetAttr[name="bias"](%211)
  %220 : Tensor = prim::GetAttr[name="weight"](%211)
  %221 : Float(8192:1, 2048:8192) = aten::t(%220), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %221), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %219, %33), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1678:0
  %224 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.12, %24, %30), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
  %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.1, %224, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %226 : Tensor = prim::GetAttr[name="bias"](%124)
  %227 : Tensor = prim::GetAttr[name="weight"](%124)
  %228 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.0
  %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %228, %227, %226, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
  %230 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %231 : Float(17:13, 13:1, 1:1) = aten::to(%230, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.2, %231), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %233 : __torch__.torch.nn.modules.linear.___torch_mangle_270.Linear = prim::GetAttr[name="out_lin"](%122)
  %234 : __torch__.torch.nn.modules.linear.___torch_mangle_269.Linear = prim::GetAttr[name="v_lin"](%122)
  %235 : __torch__.torch.nn.modules.linear.___torch_mangle_268.Linear = prim::GetAttr[name="k_lin"](%122)
  %236 : __torch__.torch.nn.modules.linear.___torch_mangle_267.Linear = prim::GetAttr[name="q_lin"](%122)
  %237 : int = aten::size(%input.14, %34), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %238 : int = aten::size(%input.14, %33), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %239 : Tensor = prim::GetAttr[name="bias"](%236)
  %240 : Tensor = prim::GetAttr[name="weight"](%236)
  %241 : Float(2048:1, 2048:2048) = aten::t(%240), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %241), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %239, %33), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1678:0
  %244 : int[] = prim::ListConstruct(%237, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.1
  %245 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.5, %244), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%245, %33, %29), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %247 : Tensor = prim::GetAttr[name="bias"](%235)
  %248 : Tensor = prim::GetAttr[name="weight"](%235)
  %249 : Float(2048:1, 2048:2048) = aten::t(%248), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %249), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %247, %33), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1678:0
  %252 : int[] = prim::ListConstruct(%237, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.1
  %253 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.6, %252), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%253, %33, %29), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %255 : Tensor = prim::GetAttr[name="bias"](%234)
  %256 : Tensor = prim::GetAttr[name="weight"](%234)
  %257 : Float(2048:1, 2048:2048) = aten::t(%256), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %257), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %255, %33), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1678:0
  %260 : int[] = prim::ListConstruct(%237, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.1
  %261 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.7, %260), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%261, %33, %29), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %19), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:188:0
  %264 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %29, %20), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %264), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %266 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.1 # torch/tensor.py:22:0
  %267 : int[] = prim::ListConstruct(%237, %33, %33, %238), scope: __module.transformer/__module.transformer.attentions.1
  %268 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%266, %267), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%268, %scores.3), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %21), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:191:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
  %272 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %28, %22), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:1498:0
  %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%272, %24, %30), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
  %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:200:0
  %275 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %33, %29), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %276 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%275, %34), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %277 : int[] = prim::ListConstruct(%237, %28, %25), scope: __module.transformer/__module.transformer.attentions.1
  %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%276, %277), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %279 : Tensor = prim::GetAttr[name="bias"](%233)
  %280 : Tensor = prim::GetAttr[name="weight"](%233)
  %281 : Float(2048:1, 2048:2048) = aten::t(%280), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %281), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %279, %33), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1678:0
  %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.18, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %286 : Tensor = prim::GetAttr[name="bias"](%120)
  %287 : Tensor = prim::GetAttr[name="weight"](%120)
  %288 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.1
  %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %288, %287, %286, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
  %290 : __torch__.torch.nn.modules.linear.___torch_mangle_340.Linear = prim::GetAttr[name="lin2"](%118)
  %291 : __torch__.torch.nn.modules.linear.___torch_mangle_339.Linear = prim::GetAttr[name="lin1"](%118)
  %292 : Tensor = prim::GetAttr[name="bias"](%291)
  %293 : Tensor = prim::GetAttr[name="weight"](%291)
  %294 : Float(2048:1, 8192:2048) = aten::t(%293), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.2, %294), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %292, %33), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1678:0
  %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.20), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:1369:0
  %298 : Tensor = prim::GetAttr[name="bias"](%290)
  %299 : Tensor = prim::GetAttr[name="weight"](%290)
  %300 : Float(8192:1, 2048:8192) = aten::t(%299), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %300), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %298, %33), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1678:0
  %303 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.22, %24, %30), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
  %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.2, %303, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %305 : Tensor = prim::GetAttr[name="bias"](%116)
  %306 : Tensor = prim::GetAttr[name="weight"](%116)
  %307 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.1
  %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %307, %306, %305, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
  %309 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %310 : Float(17:13, 13:1, 1:1) = aten::to(%309, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.3, %310), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %312 : __torch__.torch.nn.modules.linear.___torch_mangle_275.Linear = prim::GetAttr[name="out_lin"](%114)
  %313 : __torch__.torch.nn.modules.linear.___torch_mangle_274.Linear = prim::GetAttr[name="v_lin"](%114)
  %314 : __torch__.torch.nn.modules.linear.___torch_mangle_273.Linear = prim::GetAttr[name="k_lin"](%114)
  %315 : __torch__.torch.nn.modules.linear.___torch_mangle_272.Linear = prim::GetAttr[name="q_lin"](%114)
  %316 : int = aten::size(%input.24, %34), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %317 : int = aten::size(%input.24, %33), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %318 : Tensor = prim::GetAttr[name="bias"](%315)
  %319 : Tensor = prim::GetAttr[name="weight"](%315)
  %320 : Float(2048:1, 2048:2048) = aten::t(%319), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %320), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %318, %33), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1678:0
  %323 : int[] = prim::ListConstruct(%316, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.2
  %324 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.9, %323), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%324, %33, %29), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %326 : Tensor = prim::GetAttr[name="bias"](%314)
  %327 : Tensor = prim::GetAttr[name="weight"](%314)
  %328 : Float(2048:1, 2048:2048) = aten::t(%327), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %328), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %326, %33), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1678:0
  %331 : int[] = prim::ListConstruct(%316, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.2
  %332 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.10, %331), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%332, %33, %29), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %334 : Tensor = prim::GetAttr[name="bias"](%313)
  %335 : Tensor = prim::GetAttr[name="weight"](%313)
  %336 : Float(2048:1, 2048:2048) = aten::t(%335), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %336), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %334, %33), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1678:0
  %339 : int[] = prim::ListConstruct(%316, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.2
  %340 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.11, %339), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%340, %33, %29), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %19), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:188:0
  %343 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %29, %20), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %343), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %345 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.2 # torch/tensor.py:22:0
  %346 : int[] = prim::ListConstruct(%316, %33, %33, %317), scope: __module.transformer/__module.transformer.attentions.2
  %347 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%345, %346), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%347, %scores.5), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %21), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:191:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
  %351 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %28, %22), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:1498:0
  %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%351, %24, %30), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
  %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:200:0
  %354 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %33, %29), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %355 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%354, %34), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %356 : int[] = prim::ListConstruct(%316, %28, %25), scope: __module.transformer/__module.transformer.attentions.2
  %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%355, %356), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %358 : Tensor = prim::GetAttr[name="bias"](%312)
  %359 : Tensor = prim::GetAttr[name="weight"](%312)
  %360 : Float(2048:1, 2048:2048) = aten::t(%359), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %360), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %358, %33), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1678:0
  %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.28, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %365 : Tensor = prim::GetAttr[name="bias"](%112)
  %366 : Tensor = prim::GetAttr[name="weight"](%112)
  %367 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.2
  %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %367, %366, %365, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
  %369 : __torch__.torch.nn.modules.linear.___torch_mangle_343.Linear = prim::GetAttr[name="lin2"](%110)
  %370 : __torch__.torch.nn.modules.linear.___torch_mangle_342.Linear = prim::GetAttr[name="lin1"](%110)
  %371 : Tensor = prim::GetAttr[name="bias"](%370)
  %372 : Tensor = prim::GetAttr[name="weight"](%370)
  %373 : Float(2048:1, 8192:2048) = aten::t(%372), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.3, %373), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %371, %33), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1678:0
  %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.30), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:1369:0
  %377 : Tensor = prim::GetAttr[name="bias"](%369)
  %378 : Tensor = prim::GetAttr[name="weight"](%369)
  %379 : Float(8192:1, 2048:8192) = aten::t(%378), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %379), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %377, %33), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1678:0
  %382 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.32, %24, %30), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
  %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.3, %382, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %384 : Tensor = prim::GetAttr[name="bias"](%108)
  %385 : Tensor = prim::GetAttr[name="weight"](%108)
  %386 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.2
  %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %386, %385, %384, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
  %388 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %389 : Float(17:13, 13:1, 1:1) = aten::to(%388, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.4, %389), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %391 : __torch__.torch.nn.modules.linear.___torch_mangle_280.Linear = prim::GetAttr[name="out_lin"](%106)
  %392 : __torch__.torch.nn.modules.linear.___torch_mangle_279.Linear = prim::GetAttr[name="v_lin"](%106)
  %393 : __torch__.torch.nn.modules.linear.___torch_mangle_278.Linear = prim::GetAttr[name="k_lin"](%106)
  %394 : __torch__.torch.nn.modules.linear.___torch_mangle_277.Linear = prim::GetAttr[name="q_lin"](%106)
  %395 : int = aten::size(%input.34, %34), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %396 : int = aten::size(%input.34, %33), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %397 : Tensor = prim::GetAttr[name="bias"](%394)
  %398 : Tensor = prim::GetAttr[name="weight"](%394)
  %399 : Float(2048:1, 2048:2048) = aten::t(%398), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %399), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %397, %33), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1678:0
  %402 : int[] = prim::ListConstruct(%395, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.3
  %403 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.13, %402), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%403, %33, %29), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %405 : Tensor = prim::GetAttr[name="bias"](%393)
  %406 : Tensor = prim::GetAttr[name="weight"](%393)
  %407 : Float(2048:1, 2048:2048) = aten::t(%406), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %407), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %405, %33), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1678:0
  %410 : int[] = prim::ListConstruct(%395, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.3
  %411 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.14, %410), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%411, %33, %29), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %413 : Tensor = prim::GetAttr[name="bias"](%392)
  %414 : Tensor = prim::GetAttr[name="weight"](%392)
  %415 : Float(2048:1, 2048:2048) = aten::t(%414), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %415), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %413, %33), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1678:0
  %418 : int[] = prim::ListConstruct(%395, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.3
  %419 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.15, %418), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%419, %33, %29), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %19), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:188:0
  %422 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %29, %20), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %422), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %424 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.3 # torch/tensor.py:22:0
  %425 : int[] = prim::ListConstruct(%395, %33, %33, %396), scope: __module.transformer/__module.transformer.attentions.3
  %426 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%424, %425), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%426, %scores.7), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %21), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:191:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
  %430 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %28, %22), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:1498:0
  %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%430, %24, %30), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
  %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:200:0
  %433 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %33, %29), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %434 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%433, %34), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %435 : int[] = prim::ListConstruct(%395, %28, %25), scope: __module.transformer/__module.transformer.attentions.3
  %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%434, %435), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %437 : Tensor = prim::GetAttr[name="bias"](%391)
  %438 : Tensor = prim::GetAttr[name="weight"](%391)
  %439 : Float(2048:1, 2048:2048) = aten::t(%438), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %439), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %437, %33), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1678:0
  %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.38, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %444 : Tensor = prim::GetAttr[name="bias"](%104)
  %445 : Tensor = prim::GetAttr[name="weight"](%104)
  %446 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.3
  %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %446, %445, %444, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
  %448 : __torch__.torch.nn.modules.linear.___torch_mangle_346.Linear = prim::GetAttr[name="lin2"](%102)
  %449 : __torch__.torch.nn.modules.linear.___torch_mangle_345.Linear = prim::GetAttr[name="lin1"](%102)
  %450 : Tensor = prim::GetAttr[name="bias"](%449)
  %451 : Tensor = prim::GetAttr[name="weight"](%449)
  %452 : Float(2048:1, 8192:2048) = aten::t(%451), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.4, %452), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %450, %33), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1678:0
  %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.40), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:1369:0
  %456 : Tensor = prim::GetAttr[name="bias"](%448)
  %457 : Tensor = prim::GetAttr[name="weight"](%448)
  %458 : Float(8192:1, 2048:8192) = aten::t(%457), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %458), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %456, %33), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1678:0
  %461 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.42, %24, %30), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
  %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.4, %461, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %463 : Tensor = prim::GetAttr[name="bias"](%100)
  %464 : Tensor = prim::GetAttr[name="weight"](%100)
  %465 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.3
  %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %465, %464, %463, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
  %467 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %468 : Float(17:13, 13:1, 1:1) = aten::to(%467, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.5, %468), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %470 : __torch__.torch.nn.modules.linear.___torch_mangle_285.Linear = prim::GetAttr[name="out_lin"](%98)
  %471 : __torch__.torch.nn.modules.linear.___torch_mangle_284.Linear = prim::GetAttr[name="v_lin"](%98)
  %472 : __torch__.torch.nn.modules.linear.___torch_mangle_283.Linear = prim::GetAttr[name="k_lin"](%98)
  %473 : __torch__.torch.nn.modules.linear.___torch_mangle_282.Linear = prim::GetAttr[name="q_lin"](%98)
  %474 : int = aten::size(%input.44, %34), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %475 : int = aten::size(%input.44, %33), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %476 : Tensor = prim::GetAttr[name="bias"](%473)
  %477 : Tensor = prim::GetAttr[name="weight"](%473)
  %478 : Float(2048:1, 2048:2048) = aten::t(%477), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %478), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %476, %33), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1678:0
  %481 : int[] = prim::ListConstruct(%474, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.4
  %482 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.17, %481), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%482, %33, %29), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %484 : Tensor = prim::GetAttr[name="bias"](%472)
  %485 : Tensor = prim::GetAttr[name="weight"](%472)
  %486 : Float(2048:1, 2048:2048) = aten::t(%485), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %486), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %484, %33), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1678:0
  %489 : int[] = prim::ListConstruct(%474, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.4
  %490 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.18, %489), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%490, %33, %29), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %492 : Tensor = prim::GetAttr[name="bias"](%471)
  %493 : Tensor = prim::GetAttr[name="weight"](%471)
  %494 : Float(2048:1, 2048:2048) = aten::t(%493), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %494), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %492, %33), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1678:0
  %497 : int[] = prim::ListConstruct(%474, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.4
  %498 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.19, %497), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%498, %33, %29), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %19), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:188:0
  %501 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %29, %20), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %501), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %503 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.4 # torch/tensor.py:22:0
  %504 : int[] = prim::ListConstruct(%474, %33, %33, %475), scope: __module.transformer/__module.transformer.attentions.4
  %505 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%503, %504), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%505, %scores.9), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %21), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:191:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
  %509 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %28, %22), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:1498:0
  %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%509, %24, %30), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
  %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:200:0
  %512 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %33, %29), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %513 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%512, %34), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %514 : int[] = prim::ListConstruct(%474, %28, %25), scope: __module.transformer/__module.transformer.attentions.4
  %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%513, %514), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %516 : Tensor = prim::GetAttr[name="bias"](%470)
  %517 : Tensor = prim::GetAttr[name="weight"](%470)
  %518 : Float(2048:1, 2048:2048) = aten::t(%517), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %518), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %516, %33), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1678:0
  %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.48, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %523 : Tensor = prim::GetAttr[name="bias"](%96)
  %524 : Tensor = prim::GetAttr[name="weight"](%96)
  %525 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.4
  %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %525, %524, %523, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
  %527 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="lin2"](%94)
  %528 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="lin1"](%94)
  %529 : Tensor = prim::GetAttr[name="bias"](%528)
  %530 : Tensor = prim::GetAttr[name="weight"](%528)
  %531 : Float(2048:1, 8192:2048) = aten::t(%530), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.5, %531), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %529, %33), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1678:0
  %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.50), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:1369:0
  %535 : Tensor = prim::GetAttr[name="bias"](%527)
  %536 : Tensor = prim::GetAttr[name="weight"](%527)
  %537 : Float(8192:1, 2048:8192) = aten::t(%536), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %537), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %535, %33), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1678:0
  %540 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.52, %24, %30), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
  %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.5, %540, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %542 : Tensor = prim::GetAttr[name="bias"](%92)
  %543 : Tensor = prim::GetAttr[name="weight"](%92)
  %544 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.4
  %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %544, %543, %542, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
  %546 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %547 : Float(17:13, 13:1, 1:1) = aten::to(%546, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.6, %547), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %549 : __torch__.torch.nn.modules.linear.___torch_mangle_290.Linear = prim::GetAttr[name="out_lin"](%90)
  %550 : __torch__.torch.nn.modules.linear.___torch_mangle_289.Linear = prim::GetAttr[name="v_lin"](%90)
  %551 : __torch__.torch.nn.modules.linear.___torch_mangle_288.Linear = prim::GetAttr[name="k_lin"](%90)
  %552 : __torch__.torch.nn.modules.linear.___torch_mangle_287.Linear = prim::GetAttr[name="q_lin"](%90)
  %553 : int = aten::size(%input.54, %34), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %554 : int = aten::size(%input.54, %33), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %555 : Tensor = prim::GetAttr[name="bias"](%552)
  %556 : Tensor = prim::GetAttr[name="weight"](%552)
  %557 : Float(2048:1, 2048:2048) = aten::t(%556), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %557), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %555, %33), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1678:0
  %560 : int[] = prim::ListConstruct(%553, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.5
  %561 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.21, %560), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%561, %33, %29), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %563 : Tensor = prim::GetAttr[name="bias"](%551)
  %564 : Tensor = prim::GetAttr[name="weight"](%551)
  %565 : Float(2048:1, 2048:2048) = aten::t(%564), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %565), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %563, %33), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1678:0
  %568 : int[] = prim::ListConstruct(%553, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.5
  %569 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.22, %568), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%569, %33, %29), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %571 : Tensor = prim::GetAttr[name="bias"](%550)
  %572 : Tensor = prim::GetAttr[name="weight"](%550)
  %573 : Float(2048:1, 2048:2048) = aten::t(%572), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %573), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %571, %33), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1678:0
  %576 : int[] = prim::ListConstruct(%553, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.5
  %577 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.23, %576), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%577, %33, %29), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %19), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:188:0
  %580 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %29, %20), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %580), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %582 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.5 # torch/tensor.py:22:0
  %583 : int[] = prim::ListConstruct(%553, %33, %33, %554), scope: __module.transformer/__module.transformer.attentions.5
  %584 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%582, %583), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%584, %scores.11), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %21), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:191:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
  %588 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %28, %22), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:1498:0
  %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%588, %24, %30), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
  %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:200:0
  %591 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %33, %29), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %592 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%591, %34), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %593 : int[] = prim::ListConstruct(%553, %28, %25), scope: __module.transformer/__module.transformer.attentions.5
  %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%592, %593), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %595 : Tensor = prim::GetAttr[name="bias"](%549)
  %596 : Tensor = prim::GetAttr[name="weight"](%549)
  %597 : Float(2048:1, 2048:2048) = aten::t(%596), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %597), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %595, %33), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1678:0
  %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.58, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %602 : Tensor = prim::GetAttr[name="bias"](%88)
  %603 : Tensor = prim::GetAttr[name="weight"](%88)
  %604 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.5
  %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %604, %603, %602, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
  %606 : __torch__.torch.nn.modules.linear.___torch_mangle_352.Linear = prim::GetAttr[name="lin2"](%86)
  %607 : __torch__.torch.nn.modules.linear.___torch_mangle_351.Linear = prim::GetAttr[name="lin1"](%86)
  %608 : Tensor = prim::GetAttr[name="bias"](%607)
  %609 : Tensor = prim::GetAttr[name="weight"](%607)
  %610 : Float(2048:1, 8192:2048) = aten::t(%609), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.6, %610), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %608, %33), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1678:0
  %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.60), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:1369:0
  %614 : Tensor = prim::GetAttr[name="bias"](%606)
  %615 : Tensor = prim::GetAttr[name="weight"](%606)
  %616 : Float(8192:1, 2048:8192) = aten::t(%615), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %616), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %614, %33), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1678:0
  %619 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.62, %24, %30), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
  %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.6, %619, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %621 : Tensor = prim::GetAttr[name="bias"](%84)
  %622 : Tensor = prim::GetAttr[name="weight"](%84)
  %623 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.5
  %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %623, %622, %621, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
  %625 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %626 : Float(17:13, 13:1, 1:1) = aten::to(%625, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.7, %626), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %628 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="out_lin"](%82)
  %629 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="v_lin"](%82)
  %630 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="k_lin"](%82)
  %631 : __torch__.torch.nn.modules.linear.___torch_mangle_292.Linear = prim::GetAttr[name="q_lin"](%82)
  %632 : int = aten::size(%input.64, %34), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %633 : int = aten::size(%input.64, %33), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %634 : Tensor = prim::GetAttr[name="bias"](%631)
  %635 : Tensor = prim::GetAttr[name="weight"](%631)
  %636 : Float(2048:1, 2048:2048) = aten::t(%635), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %636), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %634, %33), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1678:0
  %639 : int[] = prim::ListConstruct(%632, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.6
  %640 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.25, %639), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%640, %33, %29), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %642 : Tensor = prim::GetAttr[name="bias"](%630)
  %643 : Tensor = prim::GetAttr[name="weight"](%630)
  %644 : Float(2048:1, 2048:2048) = aten::t(%643), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %644), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %642, %33), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1678:0
  %647 : int[] = prim::ListConstruct(%632, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.6
  %648 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.26, %647), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%648, %33, %29), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %650 : Tensor = prim::GetAttr[name="bias"](%629)
  %651 : Tensor = prim::GetAttr[name="weight"](%629)
  %652 : Float(2048:1, 2048:2048) = aten::t(%651), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %652), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %650, %33), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1678:0
  %655 : int[] = prim::ListConstruct(%632, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.6
  %656 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.27, %655), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%656, %33, %29), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %19), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:188:0
  %659 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %29, %20), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %659), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %661 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.6 # torch/tensor.py:22:0
  %662 : int[] = prim::ListConstruct(%632, %33, %33, %633), scope: __module.transformer/__module.transformer.attentions.6
  %663 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%661, %662), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%663, %scores.13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %21), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:191:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
  %667 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %28, %22), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:1498:0
  %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%667, %24, %30), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
  %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:200:0
  %670 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %33, %29), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %671 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%670, %34), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %672 : int[] = prim::ListConstruct(%632, %28, %25), scope: __module.transformer/__module.transformer.attentions.6
  %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%671, %672), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %674 : Tensor = prim::GetAttr[name="bias"](%628)
  %675 : Tensor = prim::GetAttr[name="weight"](%628)
  %676 : Float(2048:1, 2048:2048) = aten::t(%675), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %676), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %674, %33), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1678:0
  %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.68, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %681 : Tensor = prim::GetAttr[name="bias"](%80)
  %682 : Tensor = prim::GetAttr[name="weight"](%80)
  %683 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.6
  %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %683, %682, %681, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
  %685 : __torch__.torch.nn.modules.linear.___torch_mangle_355.Linear = prim::GetAttr[name="lin2"](%78)
  %686 : __torch__.torch.nn.modules.linear.___torch_mangle_354.Linear = prim::GetAttr[name="lin1"](%78)
  %687 : Tensor = prim::GetAttr[name="bias"](%686)
  %688 : Tensor = prim::GetAttr[name="weight"](%686)
  %689 : Float(2048:1, 8192:2048) = aten::t(%688), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.7, %689), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %687, %33), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1678:0
  %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:1369:0
  %693 : Tensor = prim::GetAttr[name="bias"](%685)
  %694 : Tensor = prim::GetAttr[name="weight"](%685)
  %695 : Float(8192:1, 2048:8192) = aten::t(%694), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %695), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %693, %33), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1678:0
  %698 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.72, %24, %30), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
  %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.7, %698, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %700 : Tensor = prim::GetAttr[name="bias"](%76)
  %701 : Tensor = prim::GetAttr[name="weight"](%76)
  %702 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.6
  %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %702, %701, %700, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
  %704 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %705 : Float(17:13, 13:1, 1:1) = aten::to(%704, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.8, %705), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %707 : __torch__.torch.nn.modules.linear.___torch_mangle_300.Linear = prim::GetAttr[name="out_lin"](%74)
  %708 : __torch__.torch.nn.modules.linear.___torch_mangle_299.Linear = prim::GetAttr[name="v_lin"](%74)
  %709 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="k_lin"](%74)
  %710 : __torch__.torch.nn.modules.linear.___torch_mangle_297.Linear = prim::GetAttr[name="q_lin"](%74)
  %711 : int = aten::size(%input.74, %34), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %712 : int = aten::size(%input.74, %33), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %713 : Tensor = prim::GetAttr[name="bias"](%710)
  %714 : Tensor = prim::GetAttr[name="weight"](%710)
  %715 : Float(2048:1, 2048:2048) = aten::t(%714), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %715), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %713, %33), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1678:0
  %718 : int[] = prim::ListConstruct(%711, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.7
  %719 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.29, %718), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%719, %33, %29), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %721 : Tensor = prim::GetAttr[name="bias"](%709)
  %722 : Tensor = prim::GetAttr[name="weight"](%709)
  %723 : Float(2048:1, 2048:2048) = aten::t(%722), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %723), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %721, %33), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1678:0
  %726 : int[] = prim::ListConstruct(%711, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.7
  %727 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.30, %726), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%727, %33, %29), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %729 : Tensor = prim::GetAttr[name="bias"](%708)
  %730 : Tensor = prim::GetAttr[name="weight"](%708)
  %731 : Float(2048:1, 2048:2048) = aten::t(%730), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %731), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %729, %33), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1678:0
  %734 : int[] = prim::ListConstruct(%711, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.7
  %735 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.31, %734), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%735, %33, %29), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %19), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:188:0
  %738 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %29, %20), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %738), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %740 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.7 # torch/tensor.py:22:0
  %741 : int[] = prim::ListConstruct(%711, %33, %33, %712), scope: __module.transformer/__module.transformer.attentions.7
  %742 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%740, %741), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%742, %scores.15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %21), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:191:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
  %746 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %28, %22), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:1498:0
  %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%746, %24, %30), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
  %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:200:0
  %749 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %33, %29), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %750 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%749, %34), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %751 : int[] = prim::ListConstruct(%711, %28, %25), scope: __module.transformer/__module.transformer.attentions.7
  %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%750, %751), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %753 : Tensor = prim::GetAttr[name="bias"](%707)
  %754 : Tensor = prim::GetAttr[name="weight"](%707)
  %755 : Float(2048:1, 2048:2048) = aten::t(%754), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %755), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %753, %33), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1678:0
  %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.78, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %760 : Tensor = prim::GetAttr[name="bias"](%72)
  %761 : Tensor = prim::GetAttr[name="weight"](%72)
  %762 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.7
  %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %762, %761, %760, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
  %764 : __torch__.torch.nn.modules.linear.___torch_mangle_358.Linear = prim::GetAttr[name="lin2"](%70)
  %765 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="lin1"](%70)
  %766 : Tensor = prim::GetAttr[name="bias"](%765)
  %767 : Tensor = prim::GetAttr[name="weight"](%765)
  %768 : Float(2048:1, 8192:2048) = aten::t(%767), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.8, %768), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %766, %33), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1678:0
  %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.80), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:1369:0
  %772 : Tensor = prim::GetAttr[name="bias"](%764)
  %773 : Tensor = prim::GetAttr[name="weight"](%764)
  %774 : Float(8192:1, 2048:8192) = aten::t(%773), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %774), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %772, %33), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1678:0
  %777 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.82, %24, %30), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
  %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.8, %777, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %779 : Tensor = prim::GetAttr[name="bias"](%68)
  %780 : Tensor = prim::GetAttr[name="weight"](%68)
  %781 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.7
  %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %781, %780, %779, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
  %783 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %784 : Float(17:13, 13:1, 1:1) = aten::to(%783, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.9, %784), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %786 : __torch__.torch.nn.modules.linear.___torch_mangle_305.Linear = prim::GetAttr[name="out_lin"](%66)
  %787 : __torch__.torch.nn.modules.linear.___torch_mangle_304.Linear = prim::GetAttr[name="v_lin"](%66)
  %788 : __torch__.torch.nn.modules.linear.___torch_mangle_303.Linear = prim::GetAttr[name="k_lin"](%66)
  %789 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="q_lin"](%66)
  %790 : int = aten::size(%input.84, %34), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %791 : int = aten::size(%input.84, %33), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %792 : Tensor = prim::GetAttr[name="bias"](%789)
  %793 : Tensor = prim::GetAttr[name="weight"](%789)
  %794 : Float(2048:1, 2048:2048) = aten::t(%793), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %794), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %792, %33), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1678:0
  %797 : int[] = prim::ListConstruct(%790, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.8
  %798 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.33, %797), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%798, %33, %29), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %800 : Tensor = prim::GetAttr[name="bias"](%788)
  %801 : Tensor = prim::GetAttr[name="weight"](%788)
  %802 : Float(2048:1, 2048:2048) = aten::t(%801), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %802), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %800, %33), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1678:0
  %805 : int[] = prim::ListConstruct(%790, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.8
  %806 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.34, %805), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%806, %33, %29), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %808 : Tensor = prim::GetAttr[name="bias"](%787)
  %809 : Tensor = prim::GetAttr[name="weight"](%787)
  %810 : Float(2048:1, 2048:2048) = aten::t(%809), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %810), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %808, %33), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1678:0
  %813 : int[] = prim::ListConstruct(%790, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.8
  %814 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.35, %813), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%814, %33, %29), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %19), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:188:0
  %817 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %29, %20), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %817), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %819 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.8 # torch/tensor.py:22:0
  %820 : int[] = prim::ListConstruct(%790, %33, %33, %791), scope: __module.transformer/__module.transformer.attentions.8
  %821 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%819, %820), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%821, %scores.17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %21), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:191:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
  %825 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %28, %22), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:1498:0
  %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%825, %24, %30), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
  %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:200:0
  %828 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %33, %29), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %829 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%828, %34), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %830 : int[] = prim::ListConstruct(%790, %28, %25), scope: __module.transformer/__module.transformer.attentions.8
  %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%829, %830), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %832 : Tensor = prim::GetAttr[name="bias"](%786)
  %833 : Tensor = prim::GetAttr[name="weight"](%786)
  %834 : Float(2048:1, 2048:2048) = aten::t(%833), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %834), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %832, %33), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1678:0
  %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.88, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %839 : Tensor = prim::GetAttr[name="bias"](%64)
  %840 : Tensor = prim::GetAttr[name="weight"](%64)
  %841 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.8
  %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %841, %840, %839, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
  %843 : __torch__.torch.nn.modules.linear.___torch_mangle_361.Linear = prim::GetAttr[name="lin2"](%62)
  %844 : __torch__.torch.nn.modules.linear.___torch_mangle_360.Linear = prim::GetAttr[name="lin1"](%62)
  %845 : Tensor = prim::GetAttr[name="bias"](%844)
  %846 : Tensor = prim::GetAttr[name="weight"](%844)
  %847 : Float(2048:1, 8192:2048) = aten::t(%846), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.9, %847), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %845, %33), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1678:0
  %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.90), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:1369:0
  %851 : Tensor = prim::GetAttr[name="bias"](%843)
  %852 : Tensor = prim::GetAttr[name="weight"](%843)
  %853 : Float(8192:1, 2048:8192) = aten::t(%852), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %853), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %851, %33), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1678:0
  %856 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.92, %24, %30), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
  %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.9, %856, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %858 : Tensor = prim::GetAttr[name="bias"](%60)
  %859 : Tensor = prim::GetAttr[name="weight"](%60)
  %860 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.8
  %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %860, %859, %858, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
  %862 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %863 : Float(17:13, 13:1, 1:1) = aten::to(%862, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.10, %863), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %865 : __torch__.torch.nn.modules.linear.___torch_mangle_310.Linear = prim::GetAttr[name="out_lin"](%58)
  %866 : __torch__.torch.nn.modules.linear.___torch_mangle_309.Linear = prim::GetAttr[name="v_lin"](%58)
  %867 : __torch__.torch.nn.modules.linear.___torch_mangle_308.Linear = prim::GetAttr[name="k_lin"](%58)
  %868 : __torch__.torch.nn.modules.linear.___torch_mangle_307.Linear = prim::GetAttr[name="q_lin"](%58)
  %869 : int = aten::size(%input.94, %34), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %870 : int = aten::size(%input.94, %33), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %871 : Tensor = prim::GetAttr[name="bias"](%868)
  %872 : Tensor = prim::GetAttr[name="weight"](%868)
  %873 : Float(2048:1, 2048:2048) = aten::t(%872), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %873), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %871, %33), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1678:0
  %876 : int[] = prim::ListConstruct(%869, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.9
  %877 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.37, %876), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%877, %33, %29), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %879 : Tensor = prim::GetAttr[name="bias"](%867)
  %880 : Tensor = prim::GetAttr[name="weight"](%867)
  %881 : Float(2048:1, 2048:2048) = aten::t(%880), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %881), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %879, %33), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1678:0
  %884 : int[] = prim::ListConstruct(%869, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.9
  %885 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.38, %884), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%885, %33, %29), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %887 : Tensor = prim::GetAttr[name="bias"](%866)
  %888 : Tensor = prim::GetAttr[name="weight"](%866)
  %889 : Float(2048:1, 2048:2048) = aten::t(%888), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %889), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %887, %33), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1678:0
  %892 : int[] = prim::ListConstruct(%869, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.9
  %893 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.39, %892), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%893, %33, %29), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %19), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:188:0
  %896 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %29, %20), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %896), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %898 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.9 # torch/tensor.py:22:0
  %899 : int[] = prim::ListConstruct(%869, %33, %33, %870), scope: __module.transformer/__module.transformer.attentions.9
  %900 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%898, %899), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%900, %scores.19), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %21), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:191:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
  %904 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %28, %22), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:1498:0
  %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%904, %24, %30), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
  %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:200:0
  %907 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %33, %29), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %908 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%907, %34), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %909 : int[] = prim::ListConstruct(%869, %28, %25), scope: __module.transformer/__module.transformer.attentions.9
  %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%908, %909), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %911 : Tensor = prim::GetAttr[name="bias"](%865)
  %912 : Tensor = prim::GetAttr[name="weight"](%865)
  %913 : Float(2048:1, 2048:2048) = aten::t(%912), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %913), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %911, %33), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1678:0
  %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.98, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %918 : Tensor = prim::GetAttr[name="bias"](%56)
  %919 : Tensor = prim::GetAttr[name="weight"](%56)
  %920 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.9
  %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %920, %919, %918, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
  %922 : __torch__.torch.nn.modules.linear.___torch_mangle_364.Linear = prim::GetAttr[name="lin2"](%54)
  %923 : __torch__.torch.nn.modules.linear.___torch_mangle_363.Linear = prim::GetAttr[name="lin1"](%54)
  %924 : Tensor = prim::GetAttr[name="bias"](%923)
  %925 : Tensor = prim::GetAttr[name="weight"](%923)
  %926 : Float(2048:1, 8192:2048) = aten::t(%925), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.10, %926), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %924, %33), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1678:0
  %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.100), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:1369:0
  %930 : Tensor = prim::GetAttr[name="bias"](%922)
  %931 : Tensor = prim::GetAttr[name="weight"](%922)
  %932 : Float(8192:1, 2048:8192) = aten::t(%931), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %932), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %930, %33), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1678:0
  %935 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.102, %24, %30), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
  %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.10, %935, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %937 : Tensor = prim::GetAttr[name="bias"](%52)
  %938 : Tensor = prim::GetAttr[name="weight"](%52)
  %939 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.9
  %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %939, %938, %937, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
  %941 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %942 : Float(17:13, 13:1, 1:1) = aten::to(%941, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.11, %942), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %944 : __torch__.torch.nn.modules.linear.___torch_mangle_315.Linear = prim::GetAttr[name="out_lin"](%50)
  %945 : __torch__.torch.nn.modules.linear.___torch_mangle_314.Linear = prim::GetAttr[name="v_lin"](%50)
  %946 : __torch__.torch.nn.modules.linear.___torch_mangle_313.Linear = prim::GetAttr[name="k_lin"](%50)
  %947 : __torch__.torch.nn.modules.linear.___torch_mangle_312.Linear = prim::GetAttr[name="q_lin"](%50)
  %948 : int = aten::size(%input.104, %34), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %949 : int = aten::size(%input.104, %33), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %950 : Tensor = prim::GetAttr[name="bias"](%947)
  %951 : Tensor = prim::GetAttr[name="weight"](%947)
  %952 : Float(2048:1, 2048:2048) = aten::t(%951), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %952), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %950, %33), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1678:0
  %955 : int[] = prim::ListConstruct(%948, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.10
  %956 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.41, %955), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%956, %33, %29), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %958 : Tensor = prim::GetAttr[name="bias"](%946)
  %959 : Tensor = prim::GetAttr[name="weight"](%946)
  %960 : Float(2048:1, 2048:2048) = aten::t(%959), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %960), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %958, %33), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1678:0
  %963 : int[] = prim::ListConstruct(%948, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.10
  %964 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.42, %963), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%964, %33, %29), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %966 : Tensor = prim::GetAttr[name="bias"](%945)
  %967 : Tensor = prim::GetAttr[name="weight"](%945)
  %968 : Float(2048:1, 2048:2048) = aten::t(%967), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %968), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %966, %33), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1678:0
  %971 : int[] = prim::ListConstruct(%948, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.10
  %972 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.43, %971), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%972, %33, %29), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %19), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:188:0
  %975 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %29, %20), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %975), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %977 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.10 # torch/tensor.py:22:0
  %978 : int[] = prim::ListConstruct(%948, %33, %33, %949), scope: __module.transformer/__module.transformer.attentions.10
  %979 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%977, %978), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%979, %scores.21), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %21), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:191:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
  %983 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %28, %22), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:1498:0
  %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%983, %24, %30), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
  %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:200:0
  %986 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %33, %29), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %987 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%986, %34), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %988 : int[] = prim::ListConstruct(%948, %28, %25), scope: __module.transformer/__module.transformer.attentions.10
  %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%987, %988), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %990 : Tensor = prim::GetAttr[name="bias"](%944)
  %991 : Tensor = prim::GetAttr[name="weight"](%944)
  %992 : Float(2048:1, 2048:2048) = aten::t(%991), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %992), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %990, %33), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1678:0
  %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.108, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %997 : Tensor = prim::GetAttr[name="bias"](%48)
  %998 : Tensor = prim::GetAttr[name="weight"](%48)
  %999 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.10
  %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %999, %998, %997, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
  %1001 : __torch__.torch.nn.modules.linear.___torch_mangle_367.Linear = prim::GetAttr[name="lin2"](%46)
  %1002 : __torch__.torch.nn.modules.linear.___torch_mangle_366.Linear = prim::GetAttr[name="lin1"](%46)
  %1003 : Tensor = prim::GetAttr[name="bias"](%1002)
  %1004 : Tensor = prim::GetAttr[name="weight"](%1002)
  %1005 : Float(2048:1, 8192:2048) = aten::t(%1004), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.11, %1005), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %1003, %33), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1678:0
  %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.110), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:1369:0
  %1009 : Tensor = prim::GetAttr[name="bias"](%1001)
  %1010 : Tensor = prim::GetAttr[name="weight"](%1001)
  %1011 : Float(8192:1, 2048:8192) = aten::t(%1010), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %1011), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %1009, %33), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1678:0
  %1014 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.112, %24, %30), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
  %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.11, %1014, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %1016 : Tensor = prim::GetAttr[name="bias"](%44)
  %1017 : Tensor = prim::GetAttr[name="weight"](%44)
  %1018 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.10
  %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %1018, %1017, %1016, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1020 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1021 : Float(17:13, 13:1, 1:1) = aten::to(%1020, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.12, %1021), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1023 : __torch__.torch.nn.modules.linear.___torch_mangle_320.Linear = prim::GetAttr[name="out_lin"](%42)
  %1024 : __torch__.torch.nn.modules.linear.___torch_mangle_319.Linear = prim::GetAttr[name="v_lin"](%42)
  %1025 : __torch__.torch.nn.modules.linear.___torch_mangle_318.Linear = prim::GetAttr[name="k_lin"](%42)
  %1026 : __torch__.torch.nn.modules.linear.___torch_mangle_317.Linear = prim::GetAttr[name="q_lin"](%42)
  %1027 : int = aten::size(%input.114, %34), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1028 : int = aten::size(%input.114, %33), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1029 : Tensor = prim::GetAttr[name="bias"](%1026)
  %1030 : Tensor = prim::GetAttr[name="weight"](%1026)
  %1031 : Float(2048:1, 2048:2048) = aten::t(%1030), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1031), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %1029, %33), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1678:0
  %1034 : int[] = prim::ListConstruct(%1027, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.11
  %1035 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.45, %1034), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1035, %33, %29), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1037 : Tensor = prim::GetAttr[name="bias"](%1025)
  %1038 : Tensor = prim::GetAttr[name="weight"](%1025)
  %1039 : Float(2048:1, 2048:2048) = aten::t(%1038), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1039), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %1037, %33), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1678:0
  %1042 : int[] = prim::ListConstruct(%1027, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.11
  %1043 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.46, %1042), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1043, %33, %29), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1045 : Tensor = prim::GetAttr[name="bias"](%1024)
  %1046 : Tensor = prim::GetAttr[name="weight"](%1024)
  %1047 : Float(2048:1, 2048:2048) = aten::t(%1046), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1047), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %1045, %33), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1678:0
  %1050 : int[] = prim::ListConstruct(%1027, %28, %17, %18), scope: __module.transformer/__module.transformer.attentions.11
  %1051 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.47, %1050), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1051, %33, %29), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %19), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:188:0
  %1054 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %29, %20), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %1054), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %1056 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %34), scope: __module.transformer/__module.transformer.attentions.11 # torch/tensor.py:22:0
  %1057 : int[] = prim::ListConstruct(%1027, %33, %33, %1028), scope: __module.transformer/__module.transformer.attentions.11
  %1058 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1056, %1057), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1058, %scores.23), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %21), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:191:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %23, %30, %30, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
  %1062 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %28, %22), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:1498:0
  %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1062, %24, %30), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
  %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:200:0
  %1065 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %33, %29), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1066 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1065, %34), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1067 : int[] = prim::ListConstruct(%1027, %28, %25), scope: __module.transformer/__module.transformer.attentions.11
  %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1066, %1067), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1069 : Tensor = prim::GetAttr[name="bias"](%1023)
  %1070 : Tensor = prim::GetAttr[name="weight"](%1023)
  %1071 : Float(2048:1, 2048:2048) = aten::t(%1070), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %1071), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %1069, %33), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1678:0
  %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.118, %24, %30), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %33), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %1076 : Tensor = prim::GetAttr[name="bias"](%40)
  %1077 : Tensor = prim::GetAttr[name="weight"](%40)
  %1078 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm1.11
  %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %1078, %1077, %1076, %26, %27), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1080 : __torch__.torch.nn.modules.linear.___torch_mangle_370.Linear = prim::GetAttr[name="lin2"](%38)
  %1081 : __torch__.torch.nn.modules.linear.___torch_mangle_369.Linear = prim::GetAttr[name="lin1"](%38)
  %1082 : Tensor = prim::GetAttr[name="bias"](%1081)
  %1083 : Tensor = prim::GetAttr[name="weight"](%1081)
  %1084 : Float(2048:1, 8192:2048) = aten::t(%1083), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor, %1084), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %1082, %33), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1678:0
  %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.120), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:1369:0
  %1088 : Tensor = prim::GetAttr[name="bias"](%1080)
  %1089 : Tensor = prim::GetAttr[name="weight"](%1080)
  %1090 : Float(8192:1, 2048:8192) = aten::t(%1089), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %output.72 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %1090), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.72, %1088, %33), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1678:0
  %1093 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.122, %24, %30), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
  %input.123 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor, %1093, %33), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %1095 : Tensor = prim::GetAttr[name="bias"](%36)
  %1096 : Tensor = prim::GetAttr[name="weight"](%36)
  %1097 : int[] = prim::ListConstruct(%25), scope: __module.transformer/__module.transformer.layer_norm2.11
  %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.123, %1097, %1096, %1095, %26, %27), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1099 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %28), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1100 : Float(17:13, 13:1, 1:1) = aten::to(%1099, %23, %30, %30, %22), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor, %1100), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1102 : int = prim::Constant[value=1](), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %1103 : Tensor = prim::GetAttr[name="bias"](%3)
  %1104 : Tensor = prim::GetAttr[name="weight"](%3)
  %1105 : Float(2048:1, 2:2048) = aten::t(%1104), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1105), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %1107 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1103, %1102), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %7 : int = prim::Constant[value=1]() # torch/tensor.py:371:0
  %8 : int = prim::Constant[value=-1]() # torch/tensor.py:371:0
  %9 : Tensor[] = aten::split(%1107, %7, %8) # torch/tensor.py:371:0
  %start_logits : Float(17:26, 13:2, 1:1), %end_logits : Float(17:26, 13:2, 1:1) = prim::ListUnpack(%9)
  %12 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:921:0
  %13 : Float(17:26, 13:2) = aten::squeeze(%start_logits, %12) # transformers/modeling_xlm.py:921:0
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_xlm.py:922:0
  %15 : Float(17:26, 13:2) = aten::squeeze(%end_logits, %14) # transformers/modeling_xlm.py:922:0
  %16 : (Float(17:26, 13:2), Float(17:26, 13:2)) = prim::TupleConstruct(%13, %15)
  return (%16)
