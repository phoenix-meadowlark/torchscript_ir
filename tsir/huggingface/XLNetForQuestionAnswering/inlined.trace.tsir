graph(%self.1 : __torch__.transformers.modeling_xlnet.XLNetForQuestionAnswering,
      %input_ids.1 : Long(17:13, 13:1),
      %attention_mask : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_utils.___torch_mangle_40984.PoolerAnswerClass = prim::GetAttr[name="answer_class"](%self.1)
  %4 : __torch__.transformers.modeling_utils.___torch_mangle_40980.PoolerEndLogits = prim::GetAttr[name="end_logits"](%self.1)
  %5 : __torch__.transformers.modeling_utils.___torch_mangle_40975.PoolerStartLogits = prim::GetAttr[name="start_logits"](%self.1)
  %6 : __torch__.transformers.modeling_xlnet.___torch_mangle_40973.XLNetModel = prim::GetAttr[name="transformer"](%self.1)
  %96 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %97 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %98 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %99 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %100 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %101 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %102 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %103 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %104 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %105 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %106 : str = prim::Constant[value="i,d->id"](), scope: __module.transformer # torch/functional.py:327:0
  %107 : float = prim::Constant[value=-1.](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %108 : Long() = prim::Constant[value={1}](), scope: __module.transformer # torch/tensor.py:400:0
  %109 : int = prim::Constant[value=10000](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %110 : Long() = prim::Constant[value={1024}](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %111 : float = prim::Constant[value=2.](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %112 : int = prim::Constant[value=1024](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %113 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %114 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %115 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %116 : None = prim::Constant(), scope: __module.transformer
  %117 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %118 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %119 : int = prim::Constant[value=3](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %120 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %121 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %122 : float = prim::Constant[value=1.](), scope: __module.transformer # torch/tensor.py:396:0
  %123 : Long() = prim::Constant[value={0}](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %124 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %125 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %126 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %127 : __torch__.transformers.modeling_xlnet.___torch_mangle_40970.XLNetLayer = prim::GetAttr[name="23"](%126)
  %128 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %129 : __torch__.transformers.modeling_xlnet.___torch_mangle_40960.XLNetLayer = prim::GetAttr[name="22"](%128)
  %130 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %131 : __torch__.transformers.modeling_xlnet.___torch_mangle_40950.XLNetLayer = prim::GetAttr[name="21"](%130)
  %132 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %133 : __torch__.transformers.modeling_xlnet.___torch_mangle_40940.XLNetLayer = prim::GetAttr[name="20"](%132)
  %134 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %135 : __torch__.transformers.modeling_xlnet.___torch_mangle_40930.XLNetLayer = prim::GetAttr[name="19"](%134)
  %136 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %137 : __torch__.transformers.modeling_xlnet.___torch_mangle_40920.XLNetLayer = prim::GetAttr[name="18"](%136)
  %138 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %139 : __torch__.transformers.modeling_xlnet.___torch_mangle_40910.XLNetLayer = prim::GetAttr[name="17"](%138)
  %140 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %141 : __torch__.transformers.modeling_xlnet.___torch_mangle_40900.XLNetLayer = prim::GetAttr[name="16"](%140)
  %142 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %143 : __torch__.transformers.modeling_xlnet.___torch_mangle_40890.XLNetLayer = prim::GetAttr[name="15"](%142)
  %144 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %145 : __torch__.transformers.modeling_xlnet.___torch_mangle_40880.XLNetLayer = prim::GetAttr[name="14"](%144)
  %146 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %147 : __torch__.transformers.modeling_xlnet.___torch_mangle_40870.XLNetLayer = prim::GetAttr[name="13"](%146)
  %148 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %149 : __torch__.transformers.modeling_xlnet.___torch_mangle_40860.XLNetLayer = prim::GetAttr[name="12"](%148)
  %150 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %151 : __torch__.transformers.modeling_xlnet.___torch_mangle_40850.XLNetLayer = prim::GetAttr[name="11"](%150)
  %152 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %153 : __torch__.transformers.modeling_xlnet.___torch_mangle_40840.XLNetLayer = prim::GetAttr[name="10"](%152)
  %154 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %155 : __torch__.transformers.modeling_xlnet.___torch_mangle_40830.XLNetLayer = prim::GetAttr[name="9"](%154)
  %156 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %157 : __torch__.transformers.modeling_xlnet.___torch_mangle_40820.XLNetLayer = prim::GetAttr[name="8"](%156)
  %158 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %159 : __torch__.transformers.modeling_xlnet.___torch_mangle_40810.XLNetLayer = prim::GetAttr[name="7"](%158)
  %160 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %161 : __torch__.transformers.modeling_xlnet.___torch_mangle_40800.XLNetLayer = prim::GetAttr[name="6"](%160)
  %162 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %163 : __torch__.transformers.modeling_xlnet.___torch_mangle_40790.XLNetLayer = prim::GetAttr[name="5"](%162)
  %164 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %165 : __torch__.transformers.modeling_xlnet.___torch_mangle_40780.XLNetLayer = prim::GetAttr[name="4"](%164)
  %166 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %167 : __torch__.transformers.modeling_xlnet.___torch_mangle_40770.XLNetLayer = prim::GetAttr[name="3"](%166)
  %168 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %169 : __torch__.transformers.modeling_xlnet.___torch_mangle_40760.XLNetLayer = prim::GetAttr[name="2"](%168)
  %170 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %171 : __torch__.transformers.modeling_xlnet.___torch_mangle_40750.XLNetLayer = prim::GetAttr[name="1"](%170)
  %172 : __torch__.torch.nn.modules.container.___torch_mangle_40971.ModuleList = prim::GetAttr[name="layer"](%6)
  %173 : __torch__.transformers.modeling_xlnet.___torch_mangle_40740.XLNetLayer = prim::GetAttr[name="0"](%172)
  %174 : __torch__.torch.nn.modules.sparse.___torch_mangle_40730.Embedding = prim::GetAttr[name="word_embedding"](%6)
  %175 : Long(13:1, 17:13) = aten::transpose(%input_ids.1, %125, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %input_ids : Long(13:17, 17:1) = aten::contiguous(%175, %125), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %177 : int = aten::size(%input_ids, %125), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %qlen : Long() = prim::NumToTensor(%177), scope: __module.transformer
  %179 : int = aten::size(%input_ids, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %180 : Long(13:1, 17:13) = aten::transpose(%attention_mask, %125, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
  %181 : Long(13:17, 17:1) = aten::contiguous(%180, %125), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
  %klen.1 : Long() = aten::add(%qlen, %123, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %183 : Scalar = aten::ScalarImplicit(%klen.1), scope: __module.transformer
  %input_mask : Float(13:17, 17:1) = aten::rsub(%181, %122, %124), scope: __module.transformer # torch/tensor.py:396:0
  %data_mask : Float(1:221, 13:17, 17:1) = aten::unsqueeze(%input_mask, %125), scope: __module.transformer # transformers/modeling_xlnet.py:1139:0
  %186 : Float(1:221, 13:17, 17:1) = aten::slice(%data_mask, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %187 : Float(1:221, 13:17, 17:1) = aten::slice(%186, %124, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %188 : Float(1:221, 13:17, 17:1) = aten::slice(%187, %120, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %189 : Float(1:221, 13:17, 17:1, 1:1) = aten::unsqueeze(%188, %119), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %190 : Bool(1:221, 13:17, 17:1, 1:1) = aten::gt(%189, %125), scope: __module.transformer # torch/tensor.py:22:0
  %attn_mask : Float(1:221, 13:17, 17:1, 1:1) = aten::to(%190, %118, %117, %117, %116), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %192 : Float(13:13, 13:1) = aten::eye(%177, %118, %125, %115, %117), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %193 : Float(13:13, 13:1) = aten::to(%192, %115, %118, %117, %117, %116), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %non_tgt_mask : Float(13:13, 13:1) = aten::neg(%193), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %195 : Float(13:13, 13:1) = aten::slice(%non_tgt_mask, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %196 : Float(13:13, 13:1) = aten::slice(%195, %124, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %197 : Float(13:13, 13:1, 1:1) = aten::unsqueeze(%196, %120), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %198 : Float(13:13, 13:1, 1:1, 1:1) = aten::unsqueeze(%197, %119), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %199 : Float(13:221, 13:17, 17:1, 1:1) = aten::add(%attn_mask, %198, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %200 : Bool(13:221, 13:17, 17:1, 1:1) = aten::gt(%199, %125), scope: __module.transformer # torch/tensor.py:22:0
  %201 : Float(13:221, 13:17, 17:1, 1:1) = aten::to(%200, %115, %118, %117, %117, %116), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %202 : Tensor = prim::GetAttr[name="weight"](%174)
  %input.1 : Float(13:17408, 17:1024, 1024:1) = aten::embedding(%202, %input_ids, %114, %117, %117), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %curr_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.1, %113, %117), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %freq_seq : Float(512:1) = aten::arange(%125, %112, %111, %118, %125, %115, %117), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %206 : Float(512:1) = aten::div(%freq_seq, %110), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %207 : Float(512:1) = aten::pow(%109, %206), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %208 : Float(512:1) = aten::reciprocal(%207), scope: __module.transformer # torch/tensor.py:400:0
  %209 : Float(512:1) = aten::mul(%208, %108), scope: __module.transformer # torch/tensor.py:400:0
  %end : Long() = aten::neg(%qlen), scope: __module.transformer # transformers/modeling_xlnet.py:1033:0
  %211 : Scalar = aten::ScalarImplicit(%end), scope: __module.transformer
  %212 : Float(26:1) = aten::arange(%183, %211, %107, %116, %125, %115, %117), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %213 : Tensor[] = prim::ListConstruct(%212, %209), scope: __module.transformer
  %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%106, %213), scope: __module.transformer # torch/functional.py:327:0
  %215 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %216 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %217 : Tensor[] = prim::ListConstruct(%215, %216), scope: __module.transformer
  %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%217, %114), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %219 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %220 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%219, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%220, %120, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %222 : int[] = prim::ListConstruct(%114, %179, %114), scope: __module.transformer
  %pos_emb : Float(26:1024, 17:0, 1024:1) = aten::expand(%pos_emb.2, %222, %117), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
  %input.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%pos_emb, %118, %125, %115, %117, %117, %117, %116), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
  %r.1 : Float(26:1024, 17:0, 1024:1) = aten::dropout(%input.2, %113, %117), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %new_mem.1 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%curr_out.1, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %227 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.1), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %228 : __torch__.transformers.modeling_xlnet.___torch_mangle_40738.XLNetFeedForward = prim::GetAttr[name="ff"](%173)
  %229 : __torch__.transformers.modeling_xlnet.___torch_mangle_40733.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%173)
  %230 : __torch__.torch.nn.modules.normalization.___torch_mangle_40731.LayerNorm = prim::GetAttr[name="layer_norm"](%229)
  %231 : Tensor = prim::GetAttr[name="o"](%229)
  %232 : Tensor = prim::GetAttr[name="r_r_bias"](%229)
  %233 : Tensor = prim::GetAttr[name="r_w_bias"](%229)
  %234 : Tensor = prim::GetAttr[name="r"](%229)
  %235 : Tensor = prim::GetAttr[name="v"](%229)
  %236 : Tensor = prim::GetAttr[name="k"](%229)
  %237 : Tensor = prim::GetAttr[name="q"](%229)
  %238 : Tensor[] = prim::ListConstruct(%curr_out.1, %237), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %q_head.1 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %238), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %240 : Tensor[] = prim::ListConstruct(%curr_out.1, %236), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %241 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %240), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %242 : Tensor[] = prim::ListConstruct(%curr_out.1, %235), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %243 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %242), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %r.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%r.1, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %245 : Tensor[] = prim::ListConstruct(%r.2, %234), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %246 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %245), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %247 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %233, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
  %248 : Tensor[] = prim::ListConstruct(%247, %241), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %ac.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %248), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %250 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %232, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
  %251 : Tensor[] = prim::ListConstruct(%250, %246), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.1 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %251), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %253 : int = aten::size(%ac.1, %119), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
  %254 : int = aten::size(%x.1, %125), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %255 : int = aten::size(%x.1, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %256 : int = aten::size(%x.1, %120), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %257 : int = aten::size(%x.1, %119), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %258 : Long() = prim::NumToTensor(%257), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %259 : int[] = prim::ListConstruct(%254, %255, %257, %256), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.2 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %259), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
  %261 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %262 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%261, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %263 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%262, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.3 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%263, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %265 : Long() = aten::sub(%258, %108, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %266 : int = aten::Int(%265), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %267 : int[] = prim::ListConstruct(%254, %255, %256, %266), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.4 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %267), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %269 : Long(13:1) = aten::arange(%253, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %119, %269), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %271 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %272 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%271, %123, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%272, %102), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %274 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %275 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %274), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %276 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%275, %100), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.1, %276, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %119, %116), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
  %279 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %113, %117), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %280 : Tensor[] = prim::ListConstruct(%279, %243), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %281 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %280), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %282 : Tensor[] = prim::ListConstruct(%281, %231), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %input.5 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %282), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %attn_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.5, %113, %117), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.1, %curr_out.1, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
  %286 : Tensor = prim::GetAttr[name="bias"](%230)
  %287 : Tensor = prim::GetAttr[name="weight"](%230)
  %288 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm
  %input_tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.6, %288, %287, %286, %96, %97), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %290 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.1, %r.2)
  %291 : Float(13:17408, 17:1024, 1024:1), %292 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%290)
  %293 : __torch__.torch.nn.modules.normalization.___torch_mangle_40734.LayerNorm = prim::GetAttr[name="layer_norm"](%228)
  %294 : __torch__.torch.nn.modules.linear.___torch_mangle_40736.Linear = prim::GetAttr[name="layer_2"](%228)
  %295 : __torch__.torch.nn.modules.linear.___torch_mangle_40735.Linear = prim::GetAttr[name="layer_1"](%228)
  %296 : Tensor = prim::GetAttr[name="bias"](%295)
  %297 : Tensor = prim::GetAttr[name="weight"](%295)
  %298 : Float(1024:1, 4096:1024) = aten::t(%297), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.1 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%291, %298), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.7 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.1, %296, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.8 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # torch/nn/functional.py:1369:0
  %input.9 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.8, %113, %117), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %303 : Tensor = prim::GetAttr[name="bias"](%294)
  %304 : Tensor = prim::GetAttr[name="weight"](%294)
  %305 : Float(4096:1, 1024:4096) = aten::t(%304), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.9, %305), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %303, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.10, %113, %117), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.3, %291, %124), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
  %310 : Tensor = prim::GetAttr[name="bias"](%293)
  %311 : Tensor = prim::GetAttr[name="weight"](%293)
  %312 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm
  %curr_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.11, %312, %311, %310, %96, %97), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
  %314 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.2, %292)
  %315 : Float(13:17408, 17:1024, 1024:1), %316 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%314)
  %new_mem.2 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%315, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %318 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.2), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %319 : __torch__.transformers.modeling_xlnet.___torch_mangle_40748.XLNetFeedForward = prim::GetAttr[name="ff"](%171)
  %320 : __torch__.transformers.modeling_xlnet.___torch_mangle_40743.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%171)
  %321 : __torch__.torch.nn.modules.normalization.___torch_mangle_40741.LayerNorm = prim::GetAttr[name="layer_norm"](%320)
  %322 : Tensor = prim::GetAttr[name="o"](%320)
  %323 : Tensor = prim::GetAttr[name="r_r_bias"](%320)
  %324 : Tensor = prim::GetAttr[name="r_w_bias"](%320)
  %325 : Tensor = prim::GetAttr[name="r"](%320)
  %326 : Tensor = prim::GetAttr[name="v"](%320)
  %327 : Tensor = prim::GetAttr[name="k"](%320)
  %328 : Tensor = prim::GetAttr[name="q"](%320)
  %329 : Tensor[] = prim::ListConstruct(%315, %328), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %q_head.2 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %329), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %331 : Tensor[] = prim::ListConstruct(%315, %327), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %332 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %331), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %333 : Tensor[] = prim::ListConstruct(%315, %326), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %334 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %333), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %r.3 : Float(26:1024, 17:0, 1024:1) = aten::to(%316, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %336 : Tensor[] = prim::ListConstruct(%r.3, %325), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %337 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %336), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %338 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %324, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
  %339 : Tensor[] = prim::ListConstruct(%338, %332), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %ac.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %339), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %341 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %323, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
  %342 : Tensor[] = prim::ListConstruct(%341, %337), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.5 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %342), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %344 : int = aten::size(%ac.2, %119), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
  %345 : int = aten::size(%x.5, %125), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %346 : int = aten::size(%x.5, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %347 : int = aten::size(%x.5, %120), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %348 : int = aten::size(%x.5, %119), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %349 : Long() = prim::NumToTensor(%348), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %350 : int[] = prim::ListConstruct(%345, %346, %348, %347), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.6 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %350), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
  %352 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %353 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%352, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %354 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%353, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.7 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%354, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %356 : Long() = aten::sub(%349, %108, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %357 : int = aten::Int(%356), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %358 : int[] = prim::ListConstruct(%345, %346, %347, %357), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.8 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %358), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %360 : Long(13:1) = aten::arange(%344, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %119, %360), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %362 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %363 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%362, %123, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%363, %102), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %365 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %366 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %365), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %367 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%366, %100), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.2, %367, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %119, %116), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
  %370 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %113, %117), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %371 : Tensor[] = prim::ListConstruct(%370, %334), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %372 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %371), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %373 : Tensor[] = prim::ListConstruct(%372, %322), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %373), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %attn_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.14, %113, %117), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.2, %315, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
  %377 : Tensor = prim::GetAttr[name="bias"](%321)
  %378 : Tensor = prim::GetAttr[name="weight"](%321)
  %379 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm
  %input_tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.15, %379, %378, %377, %96, %97), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %381 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.2, %r.3)
  %382 : Float(13:17408, 17:1024, 1024:1), %383 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%381)
  %384 : __torch__.torch.nn.modules.normalization.___torch_mangle_40744.LayerNorm = prim::GetAttr[name="layer_norm"](%319)
  %385 : __torch__.torch.nn.modules.linear.___torch_mangle_40746.Linear = prim::GetAttr[name="layer_2"](%319)
  %386 : __torch__.torch.nn.modules.linear.___torch_mangle_40745.Linear = prim::GetAttr[name="layer_1"](%319)
  %387 : Tensor = prim::GetAttr[name="bias"](%386)
  %388 : Tensor = prim::GetAttr[name="weight"](%386)
  %389 : Float(1024:1, 4096:1024) = aten::t(%388), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%382, %389), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.16 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.4, %387, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.17 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.16), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # torch/nn/functional.py:1369:0
  %input.18 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.17, %113, %117), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %394 : Tensor = prim::GetAttr[name="bias"](%385)
  %395 : Tensor = prim::GetAttr[name="weight"](%385)
  %396 : Float(4096:1, 1024:4096) = aten::t(%395), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.18, %396), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.5, %394, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.19, %113, %117), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.6, %382, %124), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
  %401 : Tensor = prim::GetAttr[name="bias"](%384)
  %402 : Tensor = prim::GetAttr[name="weight"](%384)
  %403 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm
  %curr_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.20, %403, %402, %401, %96, %97), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
  %405 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.3, %383)
  %406 : Float(13:17408, 17:1024, 1024:1), %407 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%405)
  %new_mem.3 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%406, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %409 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.3), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %410 : __torch__.transformers.modeling_xlnet.___torch_mangle_40758.XLNetFeedForward = prim::GetAttr[name="ff"](%169)
  %411 : __torch__.transformers.modeling_xlnet.___torch_mangle_40753.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%169)
  %412 : __torch__.torch.nn.modules.normalization.___torch_mangle_40751.LayerNorm = prim::GetAttr[name="layer_norm"](%411)
  %413 : Tensor = prim::GetAttr[name="o"](%411)
  %414 : Tensor = prim::GetAttr[name="r_r_bias"](%411)
  %415 : Tensor = prim::GetAttr[name="r_w_bias"](%411)
  %416 : Tensor = prim::GetAttr[name="r"](%411)
  %417 : Tensor = prim::GetAttr[name="v"](%411)
  %418 : Tensor = prim::GetAttr[name="k"](%411)
  %419 : Tensor = prim::GetAttr[name="q"](%411)
  %420 : Tensor[] = prim::ListConstruct(%406, %419), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %q_head.3 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %420), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %422 : Tensor[] = prim::ListConstruct(%406, %418), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %423 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %422), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %424 : Tensor[] = prim::ListConstruct(%406, %417), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %425 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %424), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %r.4 : Float(26:1024, 17:0, 1024:1) = aten::to(%407, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %427 : Tensor[] = prim::ListConstruct(%r.4, %416), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %428 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %427), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %429 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %415, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
  %430 : Tensor[] = prim::ListConstruct(%429, %423), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %ac.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %430), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %432 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %414, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
  %433 : Tensor[] = prim::ListConstruct(%432, %428), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.9 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %433), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %435 : int = aten::size(%ac.3, %119), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
  %436 : int = aten::size(%x.9, %125), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %437 : int = aten::size(%x.9, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %438 : int = aten::size(%x.9, %120), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %439 : int = aten::size(%x.9, %119), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %440 : Long() = prim::NumToTensor(%439), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %441 : int[] = prim::ListConstruct(%436, %437, %439, %438), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.10 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %441), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
  %443 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %444 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%443, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %445 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%444, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.11 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%445, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %447 : Long() = aten::sub(%440, %108, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %448 : int = aten::Int(%447), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %449 : int[] = prim::ListConstruct(%436, %437, %438, %448), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.12 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %449), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %451 : Long(13:1) = aten::arange(%435, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %119, %451), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %453 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %454 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%453, %123, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%454, %102), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %456 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %457 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %456), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %458 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%457, %100), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.3, %458, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %119, %116), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
  %461 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %113, %117), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %462 : Tensor[] = prim::ListConstruct(%461, %425), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %463 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %462), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %464 : Tensor[] = prim::ListConstruct(%463, %413), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %input.23 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %464), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %attn_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.23, %113, %117), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.3, %406, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
  %468 : Tensor = prim::GetAttr[name="bias"](%412)
  %469 : Tensor = prim::GetAttr[name="weight"](%412)
  %470 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm
  %input_tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.24, %470, %469, %468, %96, %97), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %472 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.3, %r.4)
  %473 : Float(13:17408, 17:1024, 1024:1), %474 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%472)
  %475 : __torch__.torch.nn.modules.normalization.___torch_mangle_40754.LayerNorm = prim::GetAttr[name="layer_norm"](%410)
  %476 : __torch__.torch.nn.modules.linear.___torch_mangle_40756.Linear = prim::GetAttr[name="layer_2"](%410)
  %477 : __torch__.torch.nn.modules.linear.___torch_mangle_40755.Linear = prim::GetAttr[name="layer_1"](%410)
  %478 : Tensor = prim::GetAttr[name="bias"](%477)
  %479 : Tensor = prim::GetAttr[name="weight"](%477)
  %480 : Float(1024:1, 4096:1024) = aten::t(%479), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.7 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%473, %480), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.25 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.7, %478, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.26 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.25), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # torch/nn/functional.py:1369:0
  %input.27 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.26, %113, %117), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %485 : Tensor = prim::GetAttr[name="bias"](%476)
  %486 : Tensor = prim::GetAttr[name="weight"](%476)
  %487 : Float(4096:1, 1024:4096) = aten::t(%486), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.27, %487), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %485, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.28, %113, %117), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.9, %473, %124), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
  %492 : Tensor = prim::GetAttr[name="bias"](%475)
  %493 : Tensor = prim::GetAttr[name="weight"](%475)
  %494 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm
  %curr_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.29, %494, %493, %492, %96, %97), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
  %496 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.4, %474)
  %497 : Float(13:17408, 17:1024, 1024:1), %498 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%496)
  %new_mem.4 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%497, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %500 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.4), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %501 : __torch__.transformers.modeling_xlnet.___torch_mangle_40768.XLNetFeedForward = prim::GetAttr[name="ff"](%167)
  %502 : __torch__.transformers.modeling_xlnet.___torch_mangle_40763.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%167)
  %503 : __torch__.torch.nn.modules.normalization.___torch_mangle_40761.LayerNorm = prim::GetAttr[name="layer_norm"](%502)
  %504 : Tensor = prim::GetAttr[name="o"](%502)
  %505 : Tensor = prim::GetAttr[name="r_r_bias"](%502)
  %506 : Tensor = prim::GetAttr[name="r_w_bias"](%502)
  %507 : Tensor = prim::GetAttr[name="r"](%502)
  %508 : Tensor = prim::GetAttr[name="v"](%502)
  %509 : Tensor = prim::GetAttr[name="k"](%502)
  %510 : Tensor = prim::GetAttr[name="q"](%502)
  %511 : Tensor[] = prim::ListConstruct(%497, %510), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %q_head.4 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %511), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %513 : Tensor[] = prim::ListConstruct(%497, %509), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %514 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %513), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %515 : Tensor[] = prim::ListConstruct(%497, %508), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %516 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %515), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %r.5 : Float(26:1024, 17:0, 1024:1) = aten::to(%498, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %518 : Tensor[] = prim::ListConstruct(%r.5, %507), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %519 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %518), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %520 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %506, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
  %521 : Tensor[] = prim::ListConstruct(%520, %514), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %ac.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %521), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %523 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %505, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
  %524 : Tensor[] = prim::ListConstruct(%523, %519), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.13 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %524), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %526 : int = aten::size(%ac.4, %119), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
  %527 : int = aten::size(%x.13, %125), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %528 : int = aten::size(%x.13, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %529 : int = aten::size(%x.13, %120), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %530 : int = aten::size(%x.13, %119), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %531 : Long() = prim::NumToTensor(%530), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %532 : int[] = prim::ListConstruct(%527, %528, %530, %529), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.14 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %532), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
  %534 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %535 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%534, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %536 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%535, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.15 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%536, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %538 : Long() = aten::sub(%531, %108, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %539 : int = aten::Int(%538), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %540 : int[] = prim::ListConstruct(%527, %528, %529, %539), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.16 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %540), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %542 : Long(13:1) = aten::arange(%526, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %119, %542), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %544 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %545 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%544, %123, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%545, %102), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %547 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %548 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %547), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %549 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%548, %100), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.4, %549, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %119, %116), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
  %552 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %113, %117), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %553 : Tensor[] = prim::ListConstruct(%552, %516), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %554 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %553), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %555 : Tensor[] = prim::ListConstruct(%554, %504), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %input.32 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %555), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %attn_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.32, %113, %117), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.4, %497, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
  %559 : Tensor = prim::GetAttr[name="bias"](%503)
  %560 : Tensor = prim::GetAttr[name="weight"](%503)
  %561 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm
  %input_tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.33, %561, %560, %559, %96, %97), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %563 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.4, %r.5)
  %564 : Float(13:17408, 17:1024, 1024:1), %565 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%563)
  %566 : __torch__.torch.nn.modules.normalization.___torch_mangle_40764.LayerNorm = prim::GetAttr[name="layer_norm"](%501)
  %567 : __torch__.torch.nn.modules.linear.___torch_mangle_40766.Linear = prim::GetAttr[name="layer_2"](%501)
  %568 : __torch__.torch.nn.modules.linear.___torch_mangle_40765.Linear = prim::GetAttr[name="layer_1"](%501)
  %569 : Tensor = prim::GetAttr[name="bias"](%568)
  %570 : Tensor = prim::GetAttr[name="weight"](%568)
  %571 : Float(1024:1, 4096:1024) = aten::t(%570), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.10 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%564, %571), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.10, %569, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.35 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.34), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # torch/nn/functional.py:1369:0
  %input.36 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.35, %113, %117), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %576 : Tensor = prim::GetAttr[name="bias"](%567)
  %577 : Tensor = prim::GetAttr[name="weight"](%567)
  %578 : Float(4096:1, 1024:4096) = aten::t(%577), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.36, %578), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.37 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.11, %576, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.37, %113, %117), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.12, %564, %124), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
  %583 : Tensor = prim::GetAttr[name="bias"](%566)
  %584 : Tensor = prim::GetAttr[name="weight"](%566)
  %585 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm
  %curr_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.38, %585, %584, %583, %96, %97), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
  %587 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.5, %565)
  %588 : Float(13:17408, 17:1024, 1024:1), %589 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%587)
  %new_mem.5 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%588, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %591 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.5), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %592 : __torch__.transformers.modeling_xlnet.___torch_mangle_40778.XLNetFeedForward = prim::GetAttr[name="ff"](%165)
  %593 : __torch__.transformers.modeling_xlnet.___torch_mangle_40773.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%165)
  %594 : __torch__.torch.nn.modules.normalization.___torch_mangle_40771.LayerNorm = prim::GetAttr[name="layer_norm"](%593)
  %595 : Tensor = prim::GetAttr[name="o"](%593)
  %596 : Tensor = prim::GetAttr[name="r_r_bias"](%593)
  %597 : Tensor = prim::GetAttr[name="r_w_bias"](%593)
  %598 : Tensor = prim::GetAttr[name="r"](%593)
  %599 : Tensor = prim::GetAttr[name="v"](%593)
  %600 : Tensor = prim::GetAttr[name="k"](%593)
  %601 : Tensor = prim::GetAttr[name="q"](%593)
  %602 : Tensor[] = prim::ListConstruct(%588, %601), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %q_head.5 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %602), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %604 : Tensor[] = prim::ListConstruct(%588, %600), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %605 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %604), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %606 : Tensor[] = prim::ListConstruct(%588, %599), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %607 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %606), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %r.6 : Float(26:1024, 17:0, 1024:1) = aten::to(%589, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %609 : Tensor[] = prim::ListConstruct(%r.6, %598), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %610 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %609), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %611 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %597, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
  %612 : Tensor[] = prim::ListConstruct(%611, %605), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %ac.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %612), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %614 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %596, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
  %615 : Tensor[] = prim::ListConstruct(%614, %610), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.17 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %615), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %617 : int = aten::size(%ac.5, %119), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
  %618 : int = aten::size(%x.17, %125), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %619 : int = aten::size(%x.17, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %620 : int = aten::size(%x.17, %120), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %621 : int = aten::size(%x.17, %119), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %622 : Long() = prim::NumToTensor(%621), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %623 : int[] = prim::ListConstruct(%618, %619, %621, %620), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.18 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %623), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
  %625 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %626 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%625, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %627 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%626, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.19 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%627, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %629 : Long() = aten::sub(%622, %108, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %630 : int = aten::Int(%629), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %631 : int[] = prim::ListConstruct(%618, %619, %620, %630), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.20 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %631), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %633 : Long(13:1) = aten::arange(%617, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %119, %633), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %635 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %636 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%635, %123, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%636, %102), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %638 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %639 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %638), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %640 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%639, %100), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.5, %640, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %119, %116), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
  %643 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %113, %117), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %644 : Tensor[] = prim::ListConstruct(%643, %607), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %645 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %644), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %646 : Tensor[] = prim::ListConstruct(%645, %595), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %646), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %attn_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.41, %113, %117), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.5, %588, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
  %650 : Tensor = prim::GetAttr[name="bias"](%594)
  %651 : Tensor = prim::GetAttr[name="weight"](%594)
  %652 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm
  %input_tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.42, %652, %651, %650, %96, %97), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %654 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.5, %r.6)
  %655 : Float(13:17408, 17:1024, 1024:1), %656 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%654)
  %657 : __torch__.torch.nn.modules.normalization.___torch_mangle_40774.LayerNorm = prim::GetAttr[name="layer_norm"](%592)
  %658 : __torch__.torch.nn.modules.linear.___torch_mangle_40776.Linear = prim::GetAttr[name="layer_2"](%592)
  %659 : __torch__.torch.nn.modules.linear.___torch_mangle_40775.Linear = prim::GetAttr[name="layer_1"](%592)
  %660 : Tensor = prim::GetAttr[name="bias"](%659)
  %661 : Tensor = prim::GetAttr[name="weight"](%659)
  %662 : Float(1024:1, 4096:1024) = aten::t(%661), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.13 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%655, %662), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.13, %660, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.43), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # torch/nn/functional.py:1369:0
  %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %113, %117), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %667 : Tensor = prim::GetAttr[name="bias"](%658)
  %668 : Tensor = prim::GetAttr[name="weight"](%658)
  %669 : Float(4096:1, 1024:4096) = aten::t(%668), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %669), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %667, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.46, %113, %117), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.15, %655, %124), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
  %674 : Tensor = prim::GetAttr[name="bias"](%657)
  %675 : Tensor = prim::GetAttr[name="weight"](%657)
  %676 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm
  %curr_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %676, %675, %674, %96, %97), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
  %678 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.6, %656)
  %679 : Float(13:17408, 17:1024, 1024:1), %680 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%678)
  %new_mem.6 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%679, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %682 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.6), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %683 : __torch__.transformers.modeling_xlnet.___torch_mangle_40788.XLNetFeedForward = prim::GetAttr[name="ff"](%163)
  %684 : __torch__.transformers.modeling_xlnet.___torch_mangle_40783.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%163)
  %685 : __torch__.torch.nn.modules.normalization.___torch_mangle_40781.LayerNorm = prim::GetAttr[name="layer_norm"](%684)
  %686 : Tensor = prim::GetAttr[name="o"](%684)
  %687 : Tensor = prim::GetAttr[name="r_r_bias"](%684)
  %688 : Tensor = prim::GetAttr[name="r_w_bias"](%684)
  %689 : Tensor = prim::GetAttr[name="r"](%684)
  %690 : Tensor = prim::GetAttr[name="v"](%684)
  %691 : Tensor = prim::GetAttr[name="k"](%684)
  %692 : Tensor = prim::GetAttr[name="q"](%684)
  %693 : Tensor[] = prim::ListConstruct(%679, %692), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %q_head.6 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %693), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %695 : Tensor[] = prim::ListConstruct(%679, %691), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %696 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %695), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %697 : Tensor[] = prim::ListConstruct(%679, %690), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %698 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %697), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %r.7 : Float(26:1024, 17:0, 1024:1) = aten::to(%680, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %700 : Tensor[] = prim::ListConstruct(%r.7, %689), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %701 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %700), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %702 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %688, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
  %703 : Tensor[] = prim::ListConstruct(%702, %696), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %ac.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %703), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %705 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %687, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
  %706 : Tensor[] = prim::ListConstruct(%705, %701), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.21 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %706), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %708 : int = aten::size(%ac.6, %119), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
  %709 : int = aten::size(%x.21, %125), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %710 : int = aten::size(%x.21, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %711 : int = aten::size(%x.21, %120), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %712 : int = aten::size(%x.21, %119), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %713 : Long() = prim::NumToTensor(%712), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %714 : int[] = prim::ListConstruct(%709, %710, %712, %711), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.22 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %714), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
  %716 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %717 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%716, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %718 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%717, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.23 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%718, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %720 : Long() = aten::sub(%713, %108, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %721 : int = aten::Int(%720), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %722 : int[] = prim::ListConstruct(%709, %710, %711, %721), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.24 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %722), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %724 : Long(13:1) = aten::arange(%708, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %119, %724), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %726 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %727 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%726, %123, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%727, %102), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %729 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %730 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %729), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %731 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%730, %100), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.48 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.6, %731, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.49 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %119, %116), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
  %734 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %113, %117), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %735 : Tensor[] = prim::ListConstruct(%734, %698), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %736 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %735), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %737 : Tensor[] = prim::ListConstruct(%736, %686), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %737), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %attn_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.50, %113, %117), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.6, %679, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
  %741 : Tensor = prim::GetAttr[name="bias"](%685)
  %742 : Tensor = prim::GetAttr[name="weight"](%685)
  %743 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm
  %input_tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.51, %743, %742, %741, %96, %97), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %745 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.6, %r.7)
  %746 : Float(13:17408, 17:1024, 1024:1), %747 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%745)
  %748 : __torch__.torch.nn.modules.normalization.___torch_mangle_40784.LayerNorm = prim::GetAttr[name="layer_norm"](%683)
  %749 : __torch__.torch.nn.modules.linear.___torch_mangle_40786.Linear = prim::GetAttr[name="layer_2"](%683)
  %750 : __torch__.torch.nn.modules.linear.___torch_mangle_40785.Linear = prim::GetAttr[name="layer_1"](%683)
  %751 : Tensor = prim::GetAttr[name="bias"](%750)
  %752 : Tensor = prim::GetAttr[name="weight"](%750)
  %753 : Float(1024:1, 4096:1024) = aten::t(%752), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.16 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%746, %753), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.52 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.16, %751, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.53 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.52), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # torch/nn/functional.py:1369:0
  %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.53, %113, %117), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %758 : Tensor = prim::GetAttr[name="bias"](%749)
  %759 : Tensor = prim::GetAttr[name="weight"](%749)
  %760 : Float(4096:1, 1024:4096) = aten::t(%759), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.54, %760), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.55 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.17, %758, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.55, %113, %117), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %input.56 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.18, %746, %124), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
  %765 : Tensor = prim::GetAttr[name="bias"](%748)
  %766 : Tensor = prim::GetAttr[name="weight"](%748)
  %767 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm
  %curr_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.56, %767, %766, %765, %96, %97), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
  %769 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.7, %747)
  %770 : Float(13:17408, 17:1024, 1024:1), %771 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%769)
  %new_mem.7 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%770, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %773 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.7), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %774 : __torch__.transformers.modeling_xlnet.___torch_mangle_40798.XLNetFeedForward = prim::GetAttr[name="ff"](%161)
  %775 : __torch__.transformers.modeling_xlnet.___torch_mangle_40793.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%161)
  %776 : __torch__.torch.nn.modules.normalization.___torch_mangle_40791.LayerNorm = prim::GetAttr[name="layer_norm"](%775)
  %777 : Tensor = prim::GetAttr[name="o"](%775)
  %778 : Tensor = prim::GetAttr[name="r_r_bias"](%775)
  %779 : Tensor = prim::GetAttr[name="r_w_bias"](%775)
  %780 : Tensor = prim::GetAttr[name="r"](%775)
  %781 : Tensor = prim::GetAttr[name="v"](%775)
  %782 : Tensor = prim::GetAttr[name="k"](%775)
  %783 : Tensor = prim::GetAttr[name="q"](%775)
  %784 : Tensor[] = prim::ListConstruct(%770, %783), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %q_head.7 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %784), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %786 : Tensor[] = prim::ListConstruct(%770, %782), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %787 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %786), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %788 : Tensor[] = prim::ListConstruct(%770, %781), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %789 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %788), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %r.8 : Float(26:1024, 17:0, 1024:1) = aten::to(%771, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %791 : Tensor[] = prim::ListConstruct(%r.8, %780), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %792 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %791), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %793 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %779, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
  %794 : Tensor[] = prim::ListConstruct(%793, %787), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %ac.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %794), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %796 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %778, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
  %797 : Tensor[] = prim::ListConstruct(%796, %792), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.25 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %797), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %799 : int = aten::size(%ac.7, %119), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
  %800 : int = aten::size(%x.25, %125), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %801 : int = aten::size(%x.25, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %802 : int = aten::size(%x.25, %120), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %803 : int = aten::size(%x.25, %119), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %804 : Long() = prim::NumToTensor(%803), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %805 : int[] = prim::ListConstruct(%800, %801, %803, %802), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.26 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %805), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
  %807 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %808 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%807, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %809 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%808, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.27 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%809, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %811 : Long() = aten::sub(%804, %108, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %812 : int = aten::Int(%811), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %813 : int[] = prim::ListConstruct(%800, %801, %802, %812), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.28 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %813), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %815 : Long(13:1) = aten::arange(%799, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %119, %815), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %817 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %818 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%817, %123, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%818, %102), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %820 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %821 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %820), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %822 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%821, %100), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.7, %822, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.58 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %119, %116), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
  %825 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %113, %117), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %826 : Tensor[] = prim::ListConstruct(%825, %789), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %827 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %826), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %828 : Tensor[] = prim::ListConstruct(%827, %777), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %input.59 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %828), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %attn_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.59, %113, %117), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.7, %770, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
  %832 : Tensor = prim::GetAttr[name="bias"](%776)
  %833 : Tensor = prim::GetAttr[name="weight"](%776)
  %834 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm
  %input_tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.60, %834, %833, %832, %96, %97), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %836 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.7, %r.8)
  %837 : Float(13:17408, 17:1024, 1024:1), %838 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%836)
  %839 : __torch__.torch.nn.modules.normalization.___torch_mangle_40794.LayerNorm = prim::GetAttr[name="layer_norm"](%774)
  %840 : __torch__.torch.nn.modules.linear.___torch_mangle_40796.Linear = prim::GetAttr[name="layer_2"](%774)
  %841 : __torch__.torch.nn.modules.linear.___torch_mangle_40795.Linear = prim::GetAttr[name="layer_1"](%774)
  %842 : Tensor = prim::GetAttr[name="bias"](%841)
  %843 : Tensor = prim::GetAttr[name="weight"](%841)
  %844 : Float(1024:1, 4096:1024) = aten::t(%843), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%837, %844), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.61 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.19, %842, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.62 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # torch/nn/functional.py:1369:0
  %input.63 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.62, %113, %117), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %849 : Tensor = prim::GetAttr[name="bias"](%840)
  %850 : Tensor = prim::GetAttr[name="weight"](%840)
  %851 : Float(4096:1, 1024:4096) = aten::t(%850), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.63, %851), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %849, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.64, %113, %117), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.21, %837, %124), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
  %856 : Tensor = prim::GetAttr[name="bias"](%839)
  %857 : Tensor = prim::GetAttr[name="weight"](%839)
  %858 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm
  %curr_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.65, %858, %857, %856, %96, %97), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
  %860 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.8, %838)
  %861 : Float(13:17408, 17:1024, 1024:1), %862 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%860)
  %new_mem.8 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%861, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %864 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.8), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %865 : __torch__.transformers.modeling_xlnet.___torch_mangle_40808.XLNetFeedForward = prim::GetAttr[name="ff"](%159)
  %866 : __torch__.transformers.modeling_xlnet.___torch_mangle_40803.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%159)
  %867 : __torch__.torch.nn.modules.normalization.___torch_mangle_40801.LayerNorm = prim::GetAttr[name="layer_norm"](%866)
  %868 : Tensor = prim::GetAttr[name="o"](%866)
  %869 : Tensor = prim::GetAttr[name="r_r_bias"](%866)
  %870 : Tensor = prim::GetAttr[name="r_w_bias"](%866)
  %871 : Tensor = prim::GetAttr[name="r"](%866)
  %872 : Tensor = prim::GetAttr[name="v"](%866)
  %873 : Tensor = prim::GetAttr[name="k"](%866)
  %874 : Tensor = prim::GetAttr[name="q"](%866)
  %875 : Tensor[] = prim::ListConstruct(%861, %874), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %q_head.8 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %875), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %877 : Tensor[] = prim::ListConstruct(%861, %873), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %878 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %877), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %879 : Tensor[] = prim::ListConstruct(%861, %872), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %880 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %879), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %r.9 : Float(26:1024, 17:0, 1024:1) = aten::to(%862, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %882 : Tensor[] = prim::ListConstruct(%r.9, %871), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %883 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %882), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %884 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %870, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
  %885 : Tensor[] = prim::ListConstruct(%884, %878), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %ac.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %885), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %887 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %869, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
  %888 : Tensor[] = prim::ListConstruct(%887, %883), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.29 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %888), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %890 : int = aten::size(%ac.8, %119), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
  %891 : int = aten::size(%x.29, %125), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %892 : int = aten::size(%x.29, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %893 : int = aten::size(%x.29, %120), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %894 : int = aten::size(%x.29, %119), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %895 : Long() = prim::NumToTensor(%894), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %896 : int[] = prim::ListConstruct(%891, %892, %894, %893), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.30 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %896), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
  %898 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %899 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%898, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %900 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%899, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.31 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%900, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %902 : Long() = aten::sub(%895, %108, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %903 : int = aten::Int(%902), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %904 : int[] = prim::ListConstruct(%891, %892, %893, %903), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.32 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %904), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %906 : Long(13:1) = aten::arange(%890, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %119, %906), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %908 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %909 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%908, %123, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%909, %102), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %911 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %912 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %911), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %913 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%912, %100), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.8, %913, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %119, %116), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
  %916 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %113, %117), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %917 : Tensor[] = prim::ListConstruct(%916, %880), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %918 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %917), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %919 : Tensor[] = prim::ListConstruct(%918, %868), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %919), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %attn_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.68, %113, %117), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.8, %861, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
  %923 : Tensor = prim::GetAttr[name="bias"](%867)
  %924 : Tensor = prim::GetAttr[name="weight"](%867)
  %925 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm
  %input_tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %925, %924, %923, %96, %97), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %927 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.8, %r.9)
  %928 : Float(13:17408, 17:1024, 1024:1), %929 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%927)
  %930 : __torch__.torch.nn.modules.normalization.___torch_mangle_40804.LayerNorm = prim::GetAttr[name="layer_norm"](%865)
  %931 : __torch__.torch.nn.modules.linear.___torch_mangle_40806.Linear = prim::GetAttr[name="layer_2"](%865)
  %932 : __torch__.torch.nn.modules.linear.___torch_mangle_40805.Linear = prim::GetAttr[name="layer_1"](%865)
  %933 : Tensor = prim::GetAttr[name="bias"](%932)
  %934 : Tensor = prim::GetAttr[name="weight"](%932)
  %935 : Float(1024:1, 4096:1024) = aten::t(%934), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.22 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%928, %935), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.70 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.22, %933, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.71 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # torch/nn/functional.py:1369:0
  %input.72 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.71, %113, %117), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %940 : Tensor = prim::GetAttr[name="bias"](%931)
  %941 : Tensor = prim::GetAttr[name="weight"](%931)
  %942 : Float(4096:1, 1024:4096) = aten::t(%941), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %942), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.23, %940, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.73, %113, %117), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.24, %928, %124), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
  %947 : Tensor = prim::GetAttr[name="bias"](%930)
  %948 : Tensor = prim::GetAttr[name="weight"](%930)
  %949 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm
  %curr_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %949, %948, %947, %96, %97), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
  %951 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.9, %929)
  %952 : Float(13:17408, 17:1024, 1024:1), %953 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%951)
  %new_mem.9 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%952, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %955 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.9), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %956 : __torch__.transformers.modeling_xlnet.___torch_mangle_40818.XLNetFeedForward = prim::GetAttr[name="ff"](%157)
  %957 : __torch__.transformers.modeling_xlnet.___torch_mangle_40813.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%157)
  %958 : __torch__.torch.nn.modules.normalization.___torch_mangle_40811.LayerNorm = prim::GetAttr[name="layer_norm"](%957)
  %959 : Tensor = prim::GetAttr[name="o"](%957)
  %960 : Tensor = prim::GetAttr[name="r_r_bias"](%957)
  %961 : Tensor = prim::GetAttr[name="r_w_bias"](%957)
  %962 : Tensor = prim::GetAttr[name="r"](%957)
  %963 : Tensor = prim::GetAttr[name="v"](%957)
  %964 : Tensor = prim::GetAttr[name="k"](%957)
  %965 : Tensor = prim::GetAttr[name="q"](%957)
  %966 : Tensor[] = prim::ListConstruct(%952, %965), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %q_head.9 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %966), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %968 : Tensor[] = prim::ListConstruct(%952, %964), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %969 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %968), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %970 : Tensor[] = prim::ListConstruct(%952, %963), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %971 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %970), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %r.10 : Float(26:1024, 17:0, 1024:1) = aten::to(%953, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %973 : Tensor[] = prim::ListConstruct(%r.10, %962), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %974 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %973), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %975 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %961, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
  %976 : Tensor[] = prim::ListConstruct(%975, %969), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %ac.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %976), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %978 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %960, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
  %979 : Tensor[] = prim::ListConstruct(%978, %974), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.33 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %979), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %981 : int = aten::size(%ac.9, %119), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
  %982 : int = aten::size(%x.33, %125), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %983 : int = aten::size(%x.33, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %984 : int = aten::size(%x.33, %120), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %985 : int = aten::size(%x.33, %119), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %986 : Long() = prim::NumToTensor(%985), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %987 : int[] = prim::ListConstruct(%982, %983, %985, %984), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.34 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %987), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
  %989 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %990 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%989, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %991 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%990, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.35 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%991, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %993 : Long() = aten::sub(%986, %108, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %994 : int = aten::Int(%993), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %995 : int[] = prim::ListConstruct(%982, %983, %984, %994), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.36 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %995), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %997 : Long(13:1) = aten::arange(%981, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %119, %997), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %999 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %1000 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%999, %123, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1000, %102), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %1002 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %1003 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1002), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %1004 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1003, %100), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.9, %1004, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %119, %116), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
  %1007 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %113, %117), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %1008 : Tensor[] = prim::ListConstruct(%1007, %971), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %1009 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1008), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %1010 : Tensor[] = prim::ListConstruct(%1009, %959), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %input.77 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1010), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %attn_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.77, %113, %117), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.9, %952, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
  %1014 : Tensor = prim::GetAttr[name="bias"](%958)
  %1015 : Tensor = prim::GetAttr[name="weight"](%958)
  %1016 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm
  %input_tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.78, %1016, %1015, %1014, %96, %97), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1018 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.9, %r.10)
  %1019 : Float(13:17408, 17:1024, 1024:1), %1020 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1018)
  %1021 : __torch__.torch.nn.modules.normalization.___torch_mangle_40814.LayerNorm = prim::GetAttr[name="layer_norm"](%956)
  %1022 : __torch__.torch.nn.modules.linear.___torch_mangle_40816.Linear = prim::GetAttr[name="layer_2"](%956)
  %1023 : __torch__.torch.nn.modules.linear.___torch_mangle_40815.Linear = prim::GetAttr[name="layer_1"](%956)
  %1024 : Tensor = prim::GetAttr[name="bias"](%1023)
  %1025 : Tensor = prim::GetAttr[name="weight"](%1023)
  %1026 : Float(1024:1, 4096:1024) = aten::t(%1025), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.25 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1019, %1026), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.25, %1024, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.80 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.79), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # torch/nn/functional.py:1369:0
  %input.81 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.80, %113, %117), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %1031 : Tensor = prim::GetAttr[name="bias"](%1022)
  %1032 : Tensor = prim::GetAttr[name="weight"](%1022)
  %1033 : Float(4096:1, 1024:4096) = aten::t(%1032), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.81, %1033), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.82 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %1031, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.82, %113, %117), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.27, %1019, %124), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
  %1038 : Tensor = prim::GetAttr[name="bias"](%1021)
  %1039 : Tensor = prim::GetAttr[name="weight"](%1021)
  %1040 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm
  %curr_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.83, %1040, %1039, %1038, %96, %97), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
  %1042 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.10, %1020)
  %1043 : Float(13:17408, 17:1024, 1024:1), %1044 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1042)
  %new_mem.10 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1043, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1046 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.10), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1047 : __torch__.transformers.modeling_xlnet.___torch_mangle_40828.XLNetFeedForward = prim::GetAttr[name="ff"](%155)
  %1048 : __torch__.transformers.modeling_xlnet.___torch_mangle_40823.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%155)
  %1049 : __torch__.torch.nn.modules.normalization.___torch_mangle_40821.LayerNorm = prim::GetAttr[name="layer_norm"](%1048)
  %1050 : Tensor = prim::GetAttr[name="o"](%1048)
  %1051 : Tensor = prim::GetAttr[name="r_r_bias"](%1048)
  %1052 : Tensor = prim::GetAttr[name="r_w_bias"](%1048)
  %1053 : Tensor = prim::GetAttr[name="r"](%1048)
  %1054 : Tensor = prim::GetAttr[name="v"](%1048)
  %1055 : Tensor = prim::GetAttr[name="k"](%1048)
  %1056 : Tensor = prim::GetAttr[name="q"](%1048)
  %1057 : Tensor[] = prim::ListConstruct(%1043, %1056), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %q_head.10 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1057), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1059 : Tensor[] = prim::ListConstruct(%1043, %1055), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1060 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1059), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1061 : Tensor[] = prim::ListConstruct(%1043, %1054), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1062 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1061), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %r.11 : Float(26:1024, 17:0, 1024:1) = aten::to(%1044, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1064 : Tensor[] = prim::ListConstruct(%r.11, %1053), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1065 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1064), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1066 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %1052, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
  %1067 : Tensor[] = prim::ListConstruct(%1066, %1060), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %ac.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1067), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1069 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %1051, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
  %1070 : Tensor[] = prim::ListConstruct(%1069, %1065), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.37 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1070), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1072 : int = aten::size(%ac.10, %119), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
  %1073 : int = aten::size(%x.37, %125), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1074 : int = aten::size(%x.37, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1075 : int = aten::size(%x.37, %120), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1076 : int = aten::size(%x.37, %119), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1077 : Long() = prim::NumToTensor(%1076), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1078 : int[] = prim::ListConstruct(%1073, %1074, %1076, %1075), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.38 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %1078), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
  %1080 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1081 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1080, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1082 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1081, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.39 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1082, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1084 : Long() = aten::sub(%1077, %108, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1085 : int = aten::Int(%1084), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1086 : int[] = prim::ListConstruct(%1073, %1074, %1075, %1085), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.40 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %1086), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1088 : Long(13:1) = aten::arange(%1072, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %119, %1088), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %1090 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1091 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1090, %123, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1091, %102), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1093 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1094 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1093), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1095 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1094, %100), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.84 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.10, %1095, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %119, %116), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
  %1098 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %113, %117), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %1099 : Tensor[] = prim::ListConstruct(%1098, %1062), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1100 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1099), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1101 : Tensor[] = prim::ListConstruct(%1100, %1050), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1101), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %attn_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.86, %113, %117), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.10, %1043, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
  %1105 : Tensor = prim::GetAttr[name="bias"](%1049)
  %1106 : Tensor = prim::GetAttr[name="weight"](%1049)
  %1107 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm
  %input_tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.87, %1107, %1106, %1105, %96, %97), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1109 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.10, %r.11)
  %1110 : Float(13:17408, 17:1024, 1024:1), %1111 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1109)
  %1112 : __torch__.torch.nn.modules.normalization.___torch_mangle_40824.LayerNorm = prim::GetAttr[name="layer_norm"](%1047)
  %1113 : __torch__.torch.nn.modules.linear.___torch_mangle_40826.Linear = prim::GetAttr[name="layer_2"](%1047)
  %1114 : __torch__.torch.nn.modules.linear.___torch_mangle_40825.Linear = prim::GetAttr[name="layer_1"](%1047)
  %1115 : Tensor = prim::GetAttr[name="bias"](%1114)
  %1116 : Tensor = prim::GetAttr[name="weight"](%1114)
  %1117 : Float(1024:1, 4096:1024) = aten::t(%1116), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.28 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1110, %1117), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.28, %1115, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.88), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # torch/nn/functional.py:1369:0
  %input.90 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.89, %113, %117), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %1122 : Tensor = prim::GetAttr[name="bias"](%1113)
  %1123 : Tensor = prim::GetAttr[name="weight"](%1113)
  %1124 : Float(4096:1, 1024:4096) = aten::t(%1123), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.90, %1124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.29, %1122, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.91, %113, %117), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %input.92 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.30, %1110, %124), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
  %1129 : Tensor = prim::GetAttr[name="bias"](%1112)
  %1130 : Tensor = prim::GetAttr[name="weight"](%1112)
  %1131 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm
  %curr_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.92, %1131, %1130, %1129, %96, %97), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
  %1133 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.11, %1111)
  %1134 : Float(13:17408, 17:1024, 1024:1), %1135 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1133)
  %new_mem.11 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1134, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1137 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.11), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1138 : __torch__.transformers.modeling_xlnet.___torch_mangle_40838.XLNetFeedForward = prim::GetAttr[name="ff"](%153)
  %1139 : __torch__.transformers.modeling_xlnet.___torch_mangle_40833.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%153)
  %1140 : __torch__.torch.nn.modules.normalization.___torch_mangle_40831.LayerNorm = prim::GetAttr[name="layer_norm"](%1139)
  %1141 : Tensor = prim::GetAttr[name="o"](%1139)
  %1142 : Tensor = prim::GetAttr[name="r_r_bias"](%1139)
  %1143 : Tensor = prim::GetAttr[name="r_w_bias"](%1139)
  %1144 : Tensor = prim::GetAttr[name="r"](%1139)
  %1145 : Tensor = prim::GetAttr[name="v"](%1139)
  %1146 : Tensor = prim::GetAttr[name="k"](%1139)
  %1147 : Tensor = prim::GetAttr[name="q"](%1139)
  %1148 : Tensor[] = prim::ListConstruct(%1134, %1147), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %q_head.11 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1148), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1150 : Tensor[] = prim::ListConstruct(%1134, %1146), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1151 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1150), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1152 : Tensor[] = prim::ListConstruct(%1134, %1145), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1153 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1152), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %r.12 : Float(26:1024, 17:0, 1024:1) = aten::to(%1135, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1155 : Tensor[] = prim::ListConstruct(%r.12, %1144), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1156 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1155), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1157 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1143, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
  %1158 : Tensor[] = prim::ListConstruct(%1157, %1151), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %ac.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1158), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1160 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1142, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
  %1161 : Tensor[] = prim::ListConstruct(%1160, %1156), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.41 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1161), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1163 : int = aten::size(%ac.11, %119), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
  %1164 : int = aten::size(%x.41, %125), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1165 : int = aten::size(%x.41, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1166 : int = aten::size(%x.41, %120), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1167 : int = aten::size(%x.41, %119), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1168 : Long() = prim::NumToTensor(%1167), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1169 : int[] = prim::ListConstruct(%1164, %1165, %1167, %1166), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.42 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %1169), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
  %1171 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1172 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1171, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1173 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1172, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.43 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1173, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1175 : Long() = aten::sub(%1168, %108, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1176 : int = aten::Int(%1175), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1177 : int[] = prim::ListConstruct(%1164, %1165, %1166, %1176), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.44 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %1177), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1179 : Long(13:1) = aten::arange(%1163, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %119, %1179), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %1181 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1182 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1181, %123, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1182, %102), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1184 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1185 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1184), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1186 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1185, %100), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.93 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.11, %1186, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.94 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %119, %116), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
  %1189 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %113, %117), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %1190 : Tensor[] = prim::ListConstruct(%1189, %1153), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1191 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1190), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1192 : Tensor[] = prim::ListConstruct(%1191, %1141), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1192), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %attn_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.95, %113, %117), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.11, %1134, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
  %1196 : Tensor = prim::GetAttr[name="bias"](%1140)
  %1197 : Tensor = prim::GetAttr[name="weight"](%1140)
  %1198 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm
  %input_tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %1198, %1197, %1196, %96, %97), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1200 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.11, %r.12)
  %1201 : Float(13:17408, 17:1024, 1024:1), %1202 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1200)
  %1203 : __torch__.torch.nn.modules.normalization.___torch_mangle_40834.LayerNorm = prim::GetAttr[name="layer_norm"](%1138)
  %1204 : __torch__.torch.nn.modules.linear.___torch_mangle_40836.Linear = prim::GetAttr[name="layer_2"](%1138)
  %1205 : __torch__.torch.nn.modules.linear.___torch_mangle_40835.Linear = prim::GetAttr[name="layer_1"](%1138)
  %1206 : Tensor = prim::GetAttr[name="bias"](%1205)
  %1207 : Tensor = prim::GetAttr[name="weight"](%1205)
  %1208 : Float(1024:1, 4096:1024) = aten::t(%1207), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.31 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1201, %1208), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.97 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.31, %1206, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.97), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # torch/nn/functional.py:1369:0
  %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.98, %113, %117), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %1213 : Tensor = prim::GetAttr[name="bias"](%1204)
  %1214 : Tensor = prim::GetAttr[name="weight"](%1204)
  %1215 : Float(4096:1, 1024:4096) = aten::t(%1214), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.99, %1215), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.100 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %1213, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.100, %113, %117), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.33, %1201, %124), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
  %1220 : Tensor = prim::GetAttr[name="bias"](%1203)
  %1221 : Tensor = prim::GetAttr[name="weight"](%1203)
  %1222 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm
  %curr_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.101, %1222, %1221, %1220, %96, %97), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
  %1224 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.12, %1202)
  %1225 : Float(13:17408, 17:1024, 1024:1), %1226 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1224)
  %new_mem.12 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1225, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1228 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.12), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1229 : __torch__.transformers.modeling_xlnet.___torch_mangle_40848.XLNetFeedForward = prim::GetAttr[name="ff"](%151)
  %1230 : __torch__.transformers.modeling_xlnet.___torch_mangle_40843.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%151)
  %1231 : __torch__.torch.nn.modules.normalization.___torch_mangle_40841.LayerNorm = prim::GetAttr[name="layer_norm"](%1230)
  %1232 : Tensor = prim::GetAttr[name="o"](%1230)
  %1233 : Tensor = prim::GetAttr[name="r_r_bias"](%1230)
  %1234 : Tensor = prim::GetAttr[name="r_w_bias"](%1230)
  %1235 : Tensor = prim::GetAttr[name="r"](%1230)
  %1236 : Tensor = prim::GetAttr[name="v"](%1230)
  %1237 : Tensor = prim::GetAttr[name="k"](%1230)
  %1238 : Tensor = prim::GetAttr[name="q"](%1230)
  %1239 : Tensor[] = prim::ListConstruct(%1225, %1238), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %q_head.12 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1239), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1241 : Tensor[] = prim::ListConstruct(%1225, %1237), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1242 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1241), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1243 : Tensor[] = prim::ListConstruct(%1225, %1236), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1244 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1243), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %r.13 : Float(26:1024, 17:0, 1024:1) = aten::to(%1226, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1246 : Tensor[] = prim::ListConstruct(%r.13, %1235), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1247 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1246), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1248 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1234, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
  %1249 : Tensor[] = prim::ListConstruct(%1248, %1242), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %ac.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1249), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1251 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1233, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
  %1252 : Tensor[] = prim::ListConstruct(%1251, %1247), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.45 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1252), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1254 : int = aten::size(%ac.12, %119), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
  %1255 : int = aten::size(%x.45, %125), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1256 : int = aten::size(%x.45, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1257 : int = aten::size(%x.45, %120), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1258 : int = aten::size(%x.45, %119), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1259 : Long() = prim::NumToTensor(%1258), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1260 : int[] = prim::ListConstruct(%1255, %1256, %1258, %1257), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.46 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %1260), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
  %1262 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1263 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1262, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1264 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1263, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.47 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1264, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1266 : Long() = aten::sub(%1259, %108, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1267 : int = aten::Int(%1266), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1268 : int[] = prim::ListConstruct(%1255, %1256, %1257, %1267), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.48 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %1268), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1270 : Long(13:1) = aten::arange(%1254, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %119, %1270), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %1272 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1273 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1272, %123, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1273, %102), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1275 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1276 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1275), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1277 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1276, %100), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.12, %1277, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %119, %116), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
  %1280 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %113, %117), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %1281 : Tensor[] = prim::ListConstruct(%1280, %1244), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1282 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1281), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1283 : Tensor[] = prim::ListConstruct(%1282, %1232), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %input.104 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1283), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %attn_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.104, %113, %117), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.12, %1225, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
  %1287 : Tensor = prim::GetAttr[name="bias"](%1231)
  %1288 : Tensor = prim::GetAttr[name="weight"](%1231)
  %1289 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm
  %input_tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.105, %1289, %1288, %1287, %96, %97), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1291 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.12, %r.13)
  %1292 : Float(13:17408, 17:1024, 1024:1), %1293 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1291)
  %1294 : __torch__.torch.nn.modules.normalization.___torch_mangle_40844.LayerNorm = prim::GetAttr[name="layer_norm"](%1229)
  %1295 : __torch__.torch.nn.modules.linear.___torch_mangle_40846.Linear = prim::GetAttr[name="layer_2"](%1229)
  %1296 : __torch__.torch.nn.modules.linear.___torch_mangle_40845.Linear = prim::GetAttr[name="layer_1"](%1229)
  %1297 : Tensor = prim::GetAttr[name="bias"](%1296)
  %1298 : Tensor = prim::GetAttr[name="weight"](%1296)
  %1299 : Float(1024:1, 4096:1024) = aten::t(%1298), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1292, %1299), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.106 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.34, %1297, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.107 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.106), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # torch/nn/functional.py:1369:0
  %input.108 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.107, %113, %117), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %1304 : Tensor = prim::GetAttr[name="bias"](%1295)
  %1305 : Tensor = prim::GetAttr[name="weight"](%1295)
  %1306 : Float(4096:1, 1024:4096) = aten::t(%1305), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.108, %1306), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.35, %1304, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.109, %113, %117), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.36, %1292, %124), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
  %1311 : Tensor = prim::GetAttr[name="bias"](%1294)
  %1312 : Tensor = prim::GetAttr[name="weight"](%1294)
  %1313 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm
  %curr_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.110, %1313, %1312, %1311, %96, %97), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
  %1315 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.13, %1293)
  %1316 : Float(13:17408, 17:1024, 1024:1), %1317 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1315)
  %new_mem.13 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1316, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1319 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.13), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1320 : __torch__.transformers.modeling_xlnet.___torch_mangle_40858.XLNetFeedForward = prim::GetAttr[name="ff"](%149)
  %1321 : __torch__.transformers.modeling_xlnet.___torch_mangle_40853.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%149)
  %1322 : __torch__.torch.nn.modules.normalization.___torch_mangle_40851.LayerNorm = prim::GetAttr[name="layer_norm"](%1321)
  %1323 : Tensor = prim::GetAttr[name="o"](%1321)
  %1324 : Tensor = prim::GetAttr[name="r_r_bias"](%1321)
  %1325 : Tensor = prim::GetAttr[name="r_w_bias"](%1321)
  %1326 : Tensor = prim::GetAttr[name="r"](%1321)
  %1327 : Tensor = prim::GetAttr[name="v"](%1321)
  %1328 : Tensor = prim::GetAttr[name="k"](%1321)
  %1329 : Tensor = prim::GetAttr[name="q"](%1321)
  %1330 : Tensor[] = prim::ListConstruct(%1316, %1329), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %q_head.13 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1330), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1332 : Tensor[] = prim::ListConstruct(%1316, %1328), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1333 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1332), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1334 : Tensor[] = prim::ListConstruct(%1316, %1327), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1335 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1334), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %r.14 : Float(26:1024, 17:0, 1024:1) = aten::to(%1317, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1337 : Tensor[] = prim::ListConstruct(%r.14, %1326), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1338 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1337), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1339 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1325, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
  %1340 : Tensor[] = prim::ListConstruct(%1339, %1333), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %ac.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1340), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1342 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1324, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
  %1343 : Tensor[] = prim::ListConstruct(%1342, %1338), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.49 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1343), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1345 : int = aten::size(%ac.13, %119), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
  %1346 : int = aten::size(%x.49, %125), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1347 : int = aten::size(%x.49, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1348 : int = aten::size(%x.49, %120), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1349 : int = aten::size(%x.49, %119), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1350 : Long() = prim::NumToTensor(%1349), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1351 : int[] = prim::ListConstruct(%1346, %1347, %1349, %1348), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.50 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %1351), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
  %1353 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1354 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1353, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1355 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1354, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.51 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1355, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1357 : Long() = aten::sub(%1350, %108, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1358 : int = aten::Int(%1357), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1359 : int[] = prim::ListConstruct(%1346, %1347, %1348, %1358), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.52 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %1359), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1361 : Long(13:1) = aten::arange(%1345, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %119, %1361), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %1363 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1364 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1363, %123, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1364, %102), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1366 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1367 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1366), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1368 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1367, %100), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.111 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.13, %1368, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %119, %116), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
  %1371 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %113, %117), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %1372 : Tensor[] = prim::ListConstruct(%1371, %1335), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1373 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1372), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1374 : Tensor[] = prim::ListConstruct(%1373, %1323), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1374), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %attn_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.113, %113, %117), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.13, %1316, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
  %1378 : Tensor = prim::GetAttr[name="bias"](%1322)
  %1379 : Tensor = prim::GetAttr[name="weight"](%1322)
  %1380 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm
  %input_tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.114, %1380, %1379, %1378, %96, %97), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1382 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.13, %r.14)
  %1383 : Float(13:17408, 17:1024, 1024:1), %1384 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1382)
  %1385 : __torch__.torch.nn.modules.normalization.___torch_mangle_40854.LayerNorm = prim::GetAttr[name="layer_norm"](%1320)
  %1386 : __torch__.torch.nn.modules.linear.___torch_mangle_40856.Linear = prim::GetAttr[name="layer_2"](%1320)
  %1387 : __torch__.torch.nn.modules.linear.___torch_mangle_40855.Linear = prim::GetAttr[name="layer_1"](%1320)
  %1388 : Tensor = prim::GetAttr[name="bias"](%1387)
  %1389 : Tensor = prim::GetAttr[name="weight"](%1387)
  %1390 : Float(1024:1, 4096:1024) = aten::t(%1389), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.37 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1383, %1390), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.115 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.37, %1388, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.116 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.115), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # torch/nn/functional.py:1369:0
  %input.117 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.116, %113, %117), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %1395 : Tensor = prim::GetAttr[name="bias"](%1386)
  %1396 : Tensor = prim::GetAttr[name="weight"](%1386)
  %1397 : Float(4096:1, 1024:4096) = aten::t(%1396), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.117, %1397), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %1395, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.118, %113, %117), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.39, %1383, %124), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
  %1402 : Tensor = prim::GetAttr[name="bias"](%1385)
  %1403 : Tensor = prim::GetAttr[name="weight"](%1385)
  %1404 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm
  %curr_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.119, %1404, %1403, %1402, %96, %97), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
  %1406 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.14, %1384)
  %1407 : Float(13:17408, 17:1024, 1024:1), %1408 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1406)
  %new_mem.14 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1407, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1410 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.14), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1411 : __torch__.transformers.modeling_xlnet.___torch_mangle_40868.XLNetFeedForward = prim::GetAttr[name="ff"](%147)
  %1412 : __torch__.transformers.modeling_xlnet.___torch_mangle_40863.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%147)
  %1413 : __torch__.torch.nn.modules.normalization.___torch_mangle_40861.LayerNorm = prim::GetAttr[name="layer_norm"](%1412)
  %1414 : Tensor = prim::GetAttr[name="o"](%1412)
  %1415 : Tensor = prim::GetAttr[name="r_r_bias"](%1412)
  %1416 : Tensor = prim::GetAttr[name="r_w_bias"](%1412)
  %1417 : Tensor = prim::GetAttr[name="r"](%1412)
  %1418 : Tensor = prim::GetAttr[name="v"](%1412)
  %1419 : Tensor = prim::GetAttr[name="k"](%1412)
  %1420 : Tensor = prim::GetAttr[name="q"](%1412)
  %1421 : Tensor[] = prim::ListConstruct(%1407, %1420), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %q_head.14 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1421), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1423 : Tensor[] = prim::ListConstruct(%1407, %1419), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1424 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1423), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1425 : Tensor[] = prim::ListConstruct(%1407, %1418), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1426 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1425), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %r.15 : Float(26:1024, 17:0, 1024:1) = aten::to(%1408, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1428 : Tensor[] = prim::ListConstruct(%r.15, %1417), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1429 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1428), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1430 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1416, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
  %1431 : Tensor[] = prim::ListConstruct(%1430, %1424), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %ac.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1431), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1433 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1415, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
  %1434 : Tensor[] = prim::ListConstruct(%1433, %1429), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.53 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1434), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1436 : int = aten::size(%ac.14, %119), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
  %1437 : int = aten::size(%x.53, %125), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1438 : int = aten::size(%x.53, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1439 : int = aten::size(%x.53, %120), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1440 : int = aten::size(%x.53, %119), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1441 : Long() = prim::NumToTensor(%1440), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1442 : int[] = prim::ListConstruct(%1437, %1438, %1440, %1439), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.54 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %1442), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
  %1444 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1445 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1444, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1446 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1445, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.55 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1446, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1448 : Long() = aten::sub(%1441, %108, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1449 : int = aten::Int(%1448), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1450 : int[] = prim::ListConstruct(%1437, %1438, %1439, %1449), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.56 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %1450), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1452 : Long(13:1) = aten::arange(%1436, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %119, %1452), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %1454 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1455 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1454, %123, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1455, %102), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1457 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1458 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1457), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1459 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1458, %100), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.14, %1459, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %119, %116), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
  %1462 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %113, %117), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %1463 : Tensor[] = prim::ListConstruct(%1462, %1426), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1464 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1463), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1465 : Tensor[] = prim::ListConstruct(%1464, %1414), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %input.122 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1465), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %attn_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.122, %113, %117), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.14, %1407, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
  %1469 : Tensor = prim::GetAttr[name="bias"](%1413)
  %1470 : Tensor = prim::GetAttr[name="weight"](%1413)
  %1471 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm
  %input_tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.123, %1471, %1470, %1469, %96, %97), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1473 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.14, %r.15)
  %1474 : Float(13:17408, 17:1024, 1024:1), %1475 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1473)
  %1476 : __torch__.torch.nn.modules.normalization.___torch_mangle_40864.LayerNorm = prim::GetAttr[name="layer_norm"](%1411)
  %1477 : __torch__.torch.nn.modules.linear.___torch_mangle_40866.Linear = prim::GetAttr[name="layer_2"](%1411)
  %1478 : __torch__.torch.nn.modules.linear.___torch_mangle_40865.Linear = prim::GetAttr[name="layer_1"](%1411)
  %1479 : Tensor = prim::GetAttr[name="bias"](%1478)
  %1480 : Tensor = prim::GetAttr[name="weight"](%1478)
  %1481 : Float(1024:1, 4096:1024) = aten::t(%1480), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.40 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1474, %1481), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.124 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.40, %1479, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.125 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # torch/nn/functional.py:1369:0
  %input.126 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.125, %113, %117), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %1486 : Tensor = prim::GetAttr[name="bias"](%1477)
  %1487 : Tensor = prim::GetAttr[name="weight"](%1477)
  %1488 : Float(4096:1, 1024:4096) = aten::t(%1487), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.126, %1488), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.41, %1486, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.127, %113, %117), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.42, %1474, %124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
  %1493 : Tensor = prim::GetAttr[name="bias"](%1476)
  %1494 : Tensor = prim::GetAttr[name="weight"](%1476)
  %1495 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm
  %curr_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.128, %1495, %1494, %1493, %96, %97), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
  %1497 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.15, %1475)
  %1498 : Float(13:17408, 17:1024, 1024:1), %1499 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1497)
  %new_mem.15 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1498, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1501 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.15), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1502 : __torch__.transformers.modeling_xlnet.___torch_mangle_40878.XLNetFeedForward = prim::GetAttr[name="ff"](%145)
  %1503 : __torch__.transformers.modeling_xlnet.___torch_mangle_40873.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%145)
  %1504 : __torch__.torch.nn.modules.normalization.___torch_mangle_40871.LayerNorm = prim::GetAttr[name="layer_norm"](%1503)
  %1505 : Tensor = prim::GetAttr[name="o"](%1503)
  %1506 : Tensor = prim::GetAttr[name="r_r_bias"](%1503)
  %1507 : Tensor = prim::GetAttr[name="r_w_bias"](%1503)
  %1508 : Tensor = prim::GetAttr[name="r"](%1503)
  %1509 : Tensor = prim::GetAttr[name="v"](%1503)
  %1510 : Tensor = prim::GetAttr[name="k"](%1503)
  %1511 : Tensor = prim::GetAttr[name="q"](%1503)
  %1512 : Tensor[] = prim::ListConstruct(%1498, %1511), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %q_head.15 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1512), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1514 : Tensor[] = prim::ListConstruct(%1498, %1510), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1515 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1514), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1516 : Tensor[] = prim::ListConstruct(%1498, %1509), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1517 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1516), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %r.16 : Float(26:1024, 17:0, 1024:1) = aten::to(%1499, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %1519 : Tensor[] = prim::ListConstruct(%r.16, %1508), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1520 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1519), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1521 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %1507, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
  %1522 : Tensor[] = prim::ListConstruct(%1521, %1515), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %ac.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1522), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1524 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %1506, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
  %1525 : Tensor[] = prim::ListConstruct(%1524, %1520), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.57 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1525), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1527 : int = aten::size(%ac.15, %119), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
  %1528 : int = aten::size(%x.57, %125), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1529 : int = aten::size(%x.57, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1530 : int = aten::size(%x.57, %120), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1531 : int = aten::size(%x.57, %119), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1532 : Long() = prim::NumToTensor(%1531), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1533 : int[] = prim::ListConstruct(%1528, %1529, %1531, %1530), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.58 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %1533), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
  %1535 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1536 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1535, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1537 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1536, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.59 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1537, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1539 : Long() = aten::sub(%1532, %108, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1540 : int = aten::Int(%1539), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1541 : int[] = prim::ListConstruct(%1528, %1529, %1530, %1540), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.60 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %1541), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1543 : Long(13:1) = aten::arange(%1527, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %119, %1543), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %1545 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1546 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1545, %123, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1546, %102), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1548 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1549 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1548), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1550 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1549, %100), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.15, %1550, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %119, %116), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
  %1553 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %113, %117), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %1554 : Tensor[] = prim::ListConstruct(%1553, %1517), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1555 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1554), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1556 : Tensor[] = prim::ListConstruct(%1555, %1505), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %input.131 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1556), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %attn_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.131, %113, %117), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.132 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.15, %1498, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
  %1560 : Tensor = prim::GetAttr[name="bias"](%1504)
  %1561 : Tensor = prim::GetAttr[name="weight"](%1504)
  %1562 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm
  %input_tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.132, %1562, %1561, %1560, %96, %97), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1564 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.15, %r.16)
  %1565 : Float(13:17408, 17:1024, 1024:1), %1566 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1564)
  %1567 : __torch__.torch.nn.modules.normalization.___torch_mangle_40874.LayerNorm = prim::GetAttr[name="layer_norm"](%1502)
  %1568 : __torch__.torch.nn.modules.linear.___torch_mangle_40876.Linear = prim::GetAttr[name="layer_2"](%1502)
  %1569 : __torch__.torch.nn.modules.linear.___torch_mangle_40875.Linear = prim::GetAttr[name="layer_1"](%1502)
  %1570 : Tensor = prim::GetAttr[name="bias"](%1569)
  %1571 : Tensor = prim::GetAttr[name="weight"](%1569)
  %1572 : Float(1024:1, 4096:1024) = aten::t(%1571), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.43 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1565, %1572), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.43, %1570, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.134 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.133), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # torch/nn/functional.py:1369:0
  %input.135 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.134, %113, %117), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %1577 : Tensor = prim::GetAttr[name="bias"](%1568)
  %1578 : Tensor = prim::GetAttr[name="weight"](%1568)
  %1579 : Float(4096:1, 1024:4096) = aten::t(%1578), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.135, %1579), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %1577, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.136, %113, %117), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.45, %1565, %124), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
  %1584 : Tensor = prim::GetAttr[name="bias"](%1567)
  %1585 : Tensor = prim::GetAttr[name="weight"](%1567)
  %1586 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm
  %curr_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.137, %1586, %1585, %1584, %96, %97), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
  %1588 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.16, %1566)
  %1589 : Float(13:17408, 17:1024, 1024:1), %1590 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1588)
  %new_mem.16 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1589, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1592 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.16), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1593 : __torch__.transformers.modeling_xlnet.___torch_mangle_40888.XLNetFeedForward = prim::GetAttr[name="ff"](%143)
  %1594 : __torch__.transformers.modeling_xlnet.___torch_mangle_40883.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%143)
  %1595 : __torch__.torch.nn.modules.normalization.___torch_mangle_40881.LayerNorm = prim::GetAttr[name="layer_norm"](%1594)
  %1596 : Tensor = prim::GetAttr[name="o"](%1594)
  %1597 : Tensor = prim::GetAttr[name="r_r_bias"](%1594)
  %1598 : Tensor = prim::GetAttr[name="r_w_bias"](%1594)
  %1599 : Tensor = prim::GetAttr[name="r"](%1594)
  %1600 : Tensor = prim::GetAttr[name="v"](%1594)
  %1601 : Tensor = prim::GetAttr[name="k"](%1594)
  %1602 : Tensor = prim::GetAttr[name="q"](%1594)
  %1603 : Tensor[] = prim::ListConstruct(%1589, %1602), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %q_head.16 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1603), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1605 : Tensor[] = prim::ListConstruct(%1589, %1601), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1606 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1605), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1607 : Tensor[] = prim::ListConstruct(%1589, %1600), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1608 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1607), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %r.17 : Float(26:1024, 17:0, 1024:1) = aten::to(%1590, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %1610 : Tensor[] = prim::ListConstruct(%r.17, %1599), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1611 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1610), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1612 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %1598, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
  %1613 : Tensor[] = prim::ListConstruct(%1612, %1606), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %ac.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1613), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1615 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %1597, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
  %1616 : Tensor[] = prim::ListConstruct(%1615, %1611), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.61 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1616), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1618 : int = aten::size(%ac.16, %119), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
  %1619 : int = aten::size(%x.61, %125), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1620 : int = aten::size(%x.61, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1621 : int = aten::size(%x.61, %120), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1622 : int = aten::size(%x.61, %119), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1623 : Long() = prim::NumToTensor(%1622), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1624 : int[] = prim::ListConstruct(%1619, %1620, %1622, %1621), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.62 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %1624), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
  %1626 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1627 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1626, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1628 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1627, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.63 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1628, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1630 : Long() = aten::sub(%1623, %108, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1631 : int = aten::Int(%1630), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1632 : int[] = prim::ListConstruct(%1619, %1620, %1621, %1631), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.64 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %1632), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1634 : Long(13:1) = aten::arange(%1618, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %119, %1634), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %1636 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1637 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1636, %123, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1637, %102), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1639 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1640 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1639), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1641 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1640, %100), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.138 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.16, %1641, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.139 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %119, %116), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
  %1644 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %113, %117), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %1645 : Tensor[] = prim::ListConstruct(%1644, %1608), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1646 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1645), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1647 : Tensor[] = prim::ListConstruct(%1646, %1596), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %input.140 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1647), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %attn_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.140, %113, %117), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.16, %1589, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
  %1651 : Tensor = prim::GetAttr[name="bias"](%1595)
  %1652 : Tensor = prim::GetAttr[name="weight"](%1595)
  %1653 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm
  %input_tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.141, %1653, %1652, %1651, %96, %97), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1655 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.16, %r.17)
  %1656 : Float(13:17408, 17:1024, 1024:1), %1657 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1655)
  %1658 : __torch__.torch.nn.modules.normalization.___torch_mangle_40884.LayerNorm = prim::GetAttr[name="layer_norm"](%1593)
  %1659 : __torch__.torch.nn.modules.linear.___torch_mangle_40886.Linear = prim::GetAttr[name="layer_2"](%1593)
  %1660 : __torch__.torch.nn.modules.linear.___torch_mangle_40885.Linear = prim::GetAttr[name="layer_1"](%1593)
  %1661 : Tensor = prim::GetAttr[name="bias"](%1660)
  %1662 : Tensor = prim::GetAttr[name="weight"](%1660)
  %1663 : Float(1024:1, 4096:1024) = aten::t(%1662), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.46 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1656, %1663), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.142 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.46, %1661, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.143 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.142), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # torch/nn/functional.py:1369:0
  %input.144 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.143, %113, %117), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %1668 : Tensor = prim::GetAttr[name="bias"](%1659)
  %1669 : Tensor = prim::GetAttr[name="weight"](%1659)
  %1670 : Float(4096:1, 1024:4096) = aten::t(%1669), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1670), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.145 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.47, %1668, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.145, %113, %117), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.48, %1656, %124), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
  %1675 : Tensor = prim::GetAttr[name="bias"](%1658)
  %1676 : Tensor = prim::GetAttr[name="weight"](%1658)
  %1677 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm
  %curr_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.146, %1677, %1676, %1675, %96, %97), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
  %1679 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.17, %1657)
  %1680 : Float(13:17408, 17:1024, 1024:1), %1681 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1679)
  %new_mem.17 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1680, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1683 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.17), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1684 : __torch__.transformers.modeling_xlnet.___torch_mangle_40898.XLNetFeedForward = prim::GetAttr[name="ff"](%141)
  %1685 : __torch__.transformers.modeling_xlnet.___torch_mangle_40893.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%141)
  %1686 : __torch__.torch.nn.modules.normalization.___torch_mangle_40891.LayerNorm = prim::GetAttr[name="layer_norm"](%1685)
  %1687 : Tensor = prim::GetAttr[name="o"](%1685)
  %1688 : Tensor = prim::GetAttr[name="r_r_bias"](%1685)
  %1689 : Tensor = prim::GetAttr[name="r_w_bias"](%1685)
  %1690 : Tensor = prim::GetAttr[name="r"](%1685)
  %1691 : Tensor = prim::GetAttr[name="v"](%1685)
  %1692 : Tensor = prim::GetAttr[name="k"](%1685)
  %1693 : Tensor = prim::GetAttr[name="q"](%1685)
  %1694 : Tensor[] = prim::ListConstruct(%1680, %1693), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %q_head.17 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1694), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1696 : Tensor[] = prim::ListConstruct(%1680, %1692), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1697 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1696), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1698 : Tensor[] = prim::ListConstruct(%1680, %1691), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1699 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1698), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %r.18 : Float(26:1024, 17:0, 1024:1) = aten::to(%1681, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %1701 : Tensor[] = prim::ListConstruct(%r.18, %1690), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1702 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1701), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1703 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %1689, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
  %1704 : Tensor[] = prim::ListConstruct(%1703, %1697), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %ac.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1704), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1706 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %1688, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
  %1707 : Tensor[] = prim::ListConstruct(%1706, %1702), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.65 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1707), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1709 : int = aten::size(%ac.17, %119), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
  %1710 : int = aten::size(%x.65, %125), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1711 : int = aten::size(%x.65, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1712 : int = aten::size(%x.65, %120), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1713 : int = aten::size(%x.65, %119), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1714 : Long() = prim::NumToTensor(%1713), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1715 : int[] = prim::ListConstruct(%1710, %1711, %1713, %1712), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.66 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %1715), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
  %1717 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1718 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1717, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1719 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1718, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.67 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1719, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1721 : Long() = aten::sub(%1714, %108, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1722 : int = aten::Int(%1721), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1723 : int[] = prim::ListConstruct(%1710, %1711, %1712, %1722), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.68 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %1723), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1725 : Long(13:1) = aten::arange(%1709, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %119, %1725), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %1727 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1728 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1727, %123, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1728, %102), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1730 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1731 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1730), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1732 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1731, %100), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.147 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.17, %1732, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.148 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %119, %116), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
  %1735 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %113, %117), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %1736 : Tensor[] = prim::ListConstruct(%1735, %1699), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1737 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1736), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1738 : Tensor[] = prim::ListConstruct(%1737, %1687), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1738), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %attn_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.149, %113, %117), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.17, %1680, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
  %1742 : Tensor = prim::GetAttr[name="bias"](%1686)
  %1743 : Tensor = prim::GetAttr[name="weight"](%1686)
  %1744 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm
  %input_tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.150, %1744, %1743, %1742, %96, %97), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1746 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.17, %r.18)
  %1747 : Float(13:17408, 17:1024, 1024:1), %1748 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1746)
  %1749 : __torch__.torch.nn.modules.normalization.___torch_mangle_40894.LayerNorm = prim::GetAttr[name="layer_norm"](%1684)
  %1750 : __torch__.torch.nn.modules.linear.___torch_mangle_40896.Linear = prim::GetAttr[name="layer_2"](%1684)
  %1751 : __torch__.torch.nn.modules.linear.___torch_mangle_40895.Linear = prim::GetAttr[name="layer_1"](%1684)
  %1752 : Tensor = prim::GetAttr[name="bias"](%1751)
  %1753 : Tensor = prim::GetAttr[name="weight"](%1751)
  %1754 : Float(1024:1, 4096:1024) = aten::t(%1753), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1747, %1754), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.49, %1752, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.151), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # torch/nn/functional.py:1369:0
  %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %113, %117), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %1759 : Tensor = prim::GetAttr[name="bias"](%1750)
  %1760 : Tensor = prim::GetAttr[name="weight"](%1750)
  %1761 : Float(4096:1, 1024:4096) = aten::t(%1760), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %1761), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %1759, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.154, %113, %117), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.51, %1747, %124), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
  %1766 : Tensor = prim::GetAttr[name="bias"](%1749)
  %1767 : Tensor = prim::GetAttr[name="weight"](%1749)
  %1768 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm
  %curr_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %1768, %1767, %1766, %96, %97), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
  %1770 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.18, %1748)
  %1771 : Float(13:17408, 17:1024, 1024:1), %1772 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1770)
  %new_mem.18 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1771, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1774 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.18), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1775 : __torch__.transformers.modeling_xlnet.___torch_mangle_40908.XLNetFeedForward = prim::GetAttr[name="ff"](%139)
  %1776 : __torch__.transformers.modeling_xlnet.___torch_mangle_40903.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%139)
  %1777 : __torch__.torch.nn.modules.normalization.___torch_mangle_40901.LayerNorm = prim::GetAttr[name="layer_norm"](%1776)
  %1778 : Tensor = prim::GetAttr[name="o"](%1776)
  %1779 : Tensor = prim::GetAttr[name="r_r_bias"](%1776)
  %1780 : Tensor = prim::GetAttr[name="r_w_bias"](%1776)
  %1781 : Tensor = prim::GetAttr[name="r"](%1776)
  %1782 : Tensor = prim::GetAttr[name="v"](%1776)
  %1783 : Tensor = prim::GetAttr[name="k"](%1776)
  %1784 : Tensor = prim::GetAttr[name="q"](%1776)
  %1785 : Tensor[] = prim::ListConstruct(%1771, %1784), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %q_head.18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1785), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1787 : Tensor[] = prim::ListConstruct(%1771, %1783), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1788 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1787), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1789 : Tensor[] = prim::ListConstruct(%1771, %1782), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1790 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1789), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %r.19 : Float(26:1024, 17:0, 1024:1) = aten::to(%1772, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %1792 : Tensor[] = prim::ListConstruct(%r.19, %1781), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1793 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1792), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1794 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %1780, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
  %1795 : Tensor[] = prim::ListConstruct(%1794, %1788), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %ac.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1795), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1797 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %1779, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
  %1798 : Tensor[] = prim::ListConstruct(%1797, %1793), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.69 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1798), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1800 : int = aten::size(%ac.18, %119), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
  %1801 : int = aten::size(%x.69, %125), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1802 : int = aten::size(%x.69, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1803 : int = aten::size(%x.69, %120), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1804 : int = aten::size(%x.69, %119), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1805 : Long() = prim::NumToTensor(%1804), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1806 : int[] = prim::ListConstruct(%1801, %1802, %1804, %1803), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.70 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %1806), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
  %1808 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1809 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1808, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1810 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1809, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.71 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1810, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1812 : Long() = aten::sub(%1805, %108, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1813 : int = aten::Int(%1812), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1814 : int[] = prim::ListConstruct(%1801, %1802, %1803, %1813), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.72 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %1814), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1816 : Long(13:1) = aten::arange(%1800, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %119, %1816), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %1818 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1819 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1818, %123, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1819, %102), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1821 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1822 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1821), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1823 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1822, %100), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.18, %1823, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.157 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %119, %116), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
  %1826 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %113, %117), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %1827 : Tensor[] = prim::ListConstruct(%1826, %1790), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1828 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1827), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1829 : Tensor[] = prim::ListConstruct(%1828, %1778), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1829), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %attn_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.158, %113, %117), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.18, %1771, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
  %1833 : Tensor = prim::GetAttr[name="bias"](%1777)
  %1834 : Tensor = prim::GetAttr[name="weight"](%1777)
  %1835 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm
  %input_tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.159, %1835, %1834, %1833, %96, %97), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1837 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.18, %r.19)
  %1838 : Float(13:17408, 17:1024, 1024:1), %1839 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1837)
  %1840 : __torch__.torch.nn.modules.normalization.___torch_mangle_40904.LayerNorm = prim::GetAttr[name="layer_norm"](%1775)
  %1841 : __torch__.torch.nn.modules.linear.___torch_mangle_40906.Linear = prim::GetAttr[name="layer_2"](%1775)
  %1842 : __torch__.torch.nn.modules.linear.___torch_mangle_40905.Linear = prim::GetAttr[name="layer_1"](%1775)
  %1843 : Tensor = prim::GetAttr[name="bias"](%1842)
  %1844 : Tensor = prim::GetAttr[name="weight"](%1842)
  %1845 : Float(1024:1, 4096:1024) = aten::t(%1844), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.52 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1838, %1845), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.160 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.52, %1843, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.161 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.160), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # torch/nn/functional.py:1369:0
  %input.162 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.161, %113, %117), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %1850 : Tensor = prim::GetAttr[name="bias"](%1841)
  %1851 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1852 : Float(4096:1, 1024:4096) = aten::t(%1851), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.162, %1852), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.53, %1850, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.163, %113, %117), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.54, %1838, %124), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
  %1857 : Tensor = prim::GetAttr[name="bias"](%1840)
  %1858 : Tensor = prim::GetAttr[name="weight"](%1840)
  %1859 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm
  %curr_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.164, %1859, %1858, %1857, %96, %97), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
  %1861 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.19, %1839)
  %1862 : Float(13:17408, 17:1024, 1024:1), %1863 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1861)
  %new_mem.19 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1862, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1865 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.19), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1866 : __torch__.transformers.modeling_xlnet.___torch_mangle_40918.XLNetFeedForward = prim::GetAttr[name="ff"](%137)
  %1867 : __torch__.transformers.modeling_xlnet.___torch_mangle_40913.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%137)
  %1868 : __torch__.torch.nn.modules.normalization.___torch_mangle_40911.LayerNorm = prim::GetAttr[name="layer_norm"](%1867)
  %1869 : Tensor = prim::GetAttr[name="o"](%1867)
  %1870 : Tensor = prim::GetAttr[name="r_r_bias"](%1867)
  %1871 : Tensor = prim::GetAttr[name="r_w_bias"](%1867)
  %1872 : Tensor = prim::GetAttr[name="r"](%1867)
  %1873 : Tensor = prim::GetAttr[name="v"](%1867)
  %1874 : Tensor = prim::GetAttr[name="k"](%1867)
  %1875 : Tensor = prim::GetAttr[name="q"](%1867)
  %1876 : Tensor[] = prim::ListConstruct(%1862, %1875), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %q_head.19 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1876), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1878 : Tensor[] = prim::ListConstruct(%1862, %1874), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1879 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1878), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1880 : Tensor[] = prim::ListConstruct(%1862, %1873), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1881 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1880), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %r.20 : Float(26:1024, 17:0, 1024:1) = aten::to(%1863, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %1883 : Tensor[] = prim::ListConstruct(%r.20, %1872), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1884 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1883), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1885 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %1871, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
  %1886 : Tensor[] = prim::ListConstruct(%1885, %1879), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %ac.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1886), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1888 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %1870, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
  %1889 : Tensor[] = prim::ListConstruct(%1888, %1884), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.73 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1889), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1891 : int = aten::size(%ac.19, %119), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
  %1892 : int = aten::size(%x.73, %125), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1893 : int = aten::size(%x.73, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1894 : int = aten::size(%x.73, %120), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1895 : int = aten::size(%x.73, %119), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1896 : Long() = prim::NumToTensor(%1895), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1897 : int[] = prim::ListConstruct(%1892, %1893, %1895, %1894), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.74 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %1897), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
  %1899 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1900 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1899, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1901 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1900, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.75 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1901, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1903 : Long() = aten::sub(%1896, %108, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1904 : int = aten::Int(%1903), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1905 : int[] = prim::ListConstruct(%1892, %1893, %1894, %1904), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.76 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %1905), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1907 : Long(13:1) = aten::arange(%1891, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %119, %1907), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %1909 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1910 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1909, %123, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1910, %102), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1912 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1913 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %1912), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1914 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1913, %100), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.19, %1914, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %119, %116), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
  %1917 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %113, %117), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %1918 : Tensor[] = prim::ListConstruct(%1917, %1881), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1919 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %1918), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1920 : Tensor[] = prim::ListConstruct(%1919, %1869), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %input.167 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %1920), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %attn_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.167, %113, %117), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.168 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.19, %1862, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
  %1924 : Tensor = prim::GetAttr[name="bias"](%1868)
  %1925 : Tensor = prim::GetAttr[name="weight"](%1868)
  %1926 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm
  %input_tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.168, %1926, %1925, %1924, %96, %97), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1928 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.19, %r.20)
  %1929 : Float(13:17408, 17:1024, 1024:1), %1930 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1928)
  %1931 : __torch__.torch.nn.modules.normalization.___torch_mangle_40914.LayerNorm = prim::GetAttr[name="layer_norm"](%1866)
  %1932 : __torch__.torch.nn.modules.linear.___torch_mangle_40916.Linear = prim::GetAttr[name="layer_2"](%1866)
  %1933 : __torch__.torch.nn.modules.linear.___torch_mangle_40915.Linear = prim::GetAttr[name="layer_1"](%1866)
  %1934 : Tensor = prim::GetAttr[name="bias"](%1933)
  %1935 : Tensor = prim::GetAttr[name="weight"](%1933)
  %1936 : Float(1024:1, 4096:1024) = aten::t(%1935), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.55 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1929, %1936), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.55, %1934, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.170 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.169), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # torch/nn/functional.py:1369:0
  %input.171 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.170, %113, %117), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %1941 : Tensor = prim::GetAttr[name="bias"](%1932)
  %1942 : Tensor = prim::GetAttr[name="weight"](%1932)
  %1943 : Float(4096:1, 1024:4096) = aten::t(%1942), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.171, %1943), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.172 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %1941, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.172, %113, %117), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %input.173 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.57, %1929, %124), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
  %1948 : Tensor = prim::GetAttr[name="bias"](%1931)
  %1949 : Tensor = prim::GetAttr[name="weight"](%1931)
  %1950 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm
  %curr_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.173, %1950, %1949, %1948, %96, %97), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
  %1952 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.20, %1930)
  %1953 : Float(13:17408, 17:1024, 1024:1), %1954 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1952)
  %new_mem.20 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1953, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1956 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.20), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1957 : __torch__.transformers.modeling_xlnet.___torch_mangle_40928.XLNetFeedForward = prim::GetAttr[name="ff"](%135)
  %1958 : __torch__.transformers.modeling_xlnet.___torch_mangle_40923.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%135)
  %1959 : __torch__.torch.nn.modules.normalization.___torch_mangle_40921.LayerNorm = prim::GetAttr[name="layer_norm"](%1958)
  %1960 : Tensor = prim::GetAttr[name="o"](%1958)
  %1961 : Tensor = prim::GetAttr[name="r_r_bias"](%1958)
  %1962 : Tensor = prim::GetAttr[name="r_w_bias"](%1958)
  %1963 : Tensor = prim::GetAttr[name="r"](%1958)
  %1964 : Tensor = prim::GetAttr[name="v"](%1958)
  %1965 : Tensor = prim::GetAttr[name="k"](%1958)
  %1966 : Tensor = prim::GetAttr[name="q"](%1958)
  %1967 : Tensor[] = prim::ListConstruct(%1953, %1966), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %q_head.20 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1967), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1969 : Tensor[] = prim::ListConstruct(%1953, %1965), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1970 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1969), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1971 : Tensor[] = prim::ListConstruct(%1953, %1964), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1972 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1971), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %r.21 : Float(26:1024, 17:0, 1024:1) = aten::to(%1954, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %1974 : Tensor[] = prim::ListConstruct(%r.21, %1963), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1975 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %1974), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1976 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %1962, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
  %1977 : Tensor[] = prim::ListConstruct(%1976, %1970), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %ac.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %1977), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1979 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %1961, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
  %1980 : Tensor[] = prim::ListConstruct(%1979, %1975), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.77 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %1980), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1982 : int = aten::size(%ac.20, %119), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
  %1983 : int = aten::size(%x.77, %125), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1984 : int = aten::size(%x.77, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1985 : int = aten::size(%x.77, %120), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1986 : int = aten::size(%x.77, %119), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1987 : Long() = prim::NumToTensor(%1986), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1988 : int[] = prim::ListConstruct(%1983, %1984, %1986, %1985), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.78 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %1988), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
  %1990 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1991 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1990, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1992 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1991, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.79 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1992, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1994 : Long() = aten::sub(%1987, %108, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %1995 : int = aten::Int(%1994), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1996 : int[] = prim::ListConstruct(%1983, %1984, %1985, %1995), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.80 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %1996), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %1998 : Long(13:1) = aten::arange(%1982, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %119, %1998), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %2000 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %2001 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2000, %123, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2001, %102), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %2003 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %2004 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %2003), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %2005 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2004, %100), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.174 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.20, %2005, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %119, %116), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
  %2008 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %113, %117), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %2009 : Tensor[] = prim::ListConstruct(%2008, %1972), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %2010 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %2009), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %2011 : Tensor[] = prim::ListConstruct(%2010, %1960), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %2011), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %attn_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.176, %113, %117), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.177 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.20, %1953, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
  %2015 : Tensor = prim::GetAttr[name="bias"](%1959)
  %2016 : Tensor = prim::GetAttr[name="weight"](%1959)
  %2017 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm
  %input_tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.177, %2017, %2016, %2015, %96, %97), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2019 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.20, %r.21)
  %2020 : Float(13:17408, 17:1024, 1024:1), %2021 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2019)
  %2022 : __torch__.torch.nn.modules.normalization.___torch_mangle_40924.LayerNorm = prim::GetAttr[name="layer_norm"](%1957)
  %2023 : __torch__.torch.nn.modules.linear.___torch_mangle_40926.Linear = prim::GetAttr[name="layer_2"](%1957)
  %2024 : __torch__.torch.nn.modules.linear.___torch_mangle_40925.Linear = prim::GetAttr[name="layer_1"](%1957)
  %2025 : Tensor = prim::GetAttr[name="bias"](%2024)
  %2026 : Tensor = prim::GetAttr[name="weight"](%2024)
  %2027 : Float(1024:1, 4096:1024) = aten::t(%2026), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.58 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2020, %2027), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.178 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.58, %2025, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.179 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.178), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # torch/nn/functional.py:1369:0
  %input.180 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.179, %113, %117), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %2032 : Tensor = prim::GetAttr[name="bias"](%2023)
  %2033 : Tensor = prim::GetAttr[name="weight"](%2023)
  %2034 : Float(4096:1, 1024:4096) = aten::t(%2033), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.180, %2034), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.59, %2032, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.181, %113, %117), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.60, %2020, %124), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
  %2039 : Tensor = prim::GetAttr[name="bias"](%2022)
  %2040 : Tensor = prim::GetAttr[name="weight"](%2022)
  %2041 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm
  %curr_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.182, %2041, %2040, %2039, %96, %97), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
  %2043 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.21, %2021)
  %2044 : Float(13:17408, 17:1024, 1024:1), %2045 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2043)
  %new_mem.21 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2044, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2047 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.21), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2048 : __torch__.transformers.modeling_xlnet.___torch_mangle_40938.XLNetFeedForward = prim::GetAttr[name="ff"](%133)
  %2049 : __torch__.transformers.modeling_xlnet.___torch_mangle_40933.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%133)
  %2050 : __torch__.torch.nn.modules.normalization.___torch_mangle_40931.LayerNorm = prim::GetAttr[name="layer_norm"](%2049)
  %2051 : Tensor = prim::GetAttr[name="o"](%2049)
  %2052 : Tensor = prim::GetAttr[name="r_r_bias"](%2049)
  %2053 : Tensor = prim::GetAttr[name="r_w_bias"](%2049)
  %2054 : Tensor = prim::GetAttr[name="r"](%2049)
  %2055 : Tensor = prim::GetAttr[name="v"](%2049)
  %2056 : Tensor = prim::GetAttr[name="k"](%2049)
  %2057 : Tensor = prim::GetAttr[name="q"](%2049)
  %2058 : Tensor[] = prim::ListConstruct(%2044, %2057), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %q_head.21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2058), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2060 : Tensor[] = prim::ListConstruct(%2044, %2056), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2061 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2060), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2062 : Tensor[] = prim::ListConstruct(%2044, %2055), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2063 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2062), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %r.22 : Float(26:1024, 17:0, 1024:1) = aten::to(%2045, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2065 : Tensor[] = prim::ListConstruct(%r.22, %2054), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2066 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2065), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2067 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %2053, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
  %2068 : Tensor[] = prim::ListConstruct(%2067, %2061), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %ac.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %2068), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2070 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %2052, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
  %2071 : Tensor[] = prim::ListConstruct(%2070, %2066), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.81 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %2071), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2073 : int = aten::size(%ac.21, %119), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
  %2074 : int = aten::size(%x.81, %125), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2075 : int = aten::size(%x.81, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2076 : int = aten::size(%x.81, %120), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2077 : int = aten::size(%x.81, %119), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2078 : Long() = prim::NumToTensor(%2077), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2079 : int[] = prim::ListConstruct(%2074, %2075, %2077, %2076), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.82 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %2079), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
  %2081 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2082 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2081, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2083 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2082, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.83 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2083, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2085 : Long() = aten::sub(%2078, %108, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2086 : int = aten::Int(%2085), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2087 : int[] = prim::ListConstruct(%2074, %2075, %2076, %2086), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.84 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %2087), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2089 : Long(13:1) = aten::arange(%2073, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %119, %2089), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %2091 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2092 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2091, %123, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2092, %102), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2094 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2095 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %2094), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2096 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2095, %100), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.21, %2096, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.184 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %119, %116), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
  %2099 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %113, %117), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %2100 : Tensor[] = prim::ListConstruct(%2099, %2063), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2101 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %2100), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2102 : Tensor[] = prim::ListConstruct(%2101, %2051), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %input.185 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %2102), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %attn_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.185, %113, %117), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.21, %2044, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
  %2106 : Tensor = prim::GetAttr[name="bias"](%2050)
  %2107 : Tensor = prim::GetAttr[name="weight"](%2050)
  %2108 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm
  %input_tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.186, %2108, %2107, %2106, %96, %97), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2110 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.21, %r.22)
  %2111 : Float(13:17408, 17:1024, 1024:1), %2112 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2110)
  %2113 : __torch__.torch.nn.modules.normalization.___torch_mangle_40934.LayerNorm = prim::GetAttr[name="layer_norm"](%2048)
  %2114 : __torch__.torch.nn.modules.linear.___torch_mangle_40936.Linear = prim::GetAttr[name="layer_2"](%2048)
  %2115 : __torch__.torch.nn.modules.linear.___torch_mangle_40935.Linear = prim::GetAttr[name="layer_1"](%2048)
  %2116 : Tensor = prim::GetAttr[name="bias"](%2115)
  %2117 : Tensor = prim::GetAttr[name="weight"](%2115)
  %2118 : Float(1024:1, 4096:1024) = aten::t(%2117), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.61 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2111, %2118), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.187 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.61, %2116, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.188 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.187), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # torch/nn/functional.py:1369:0
  %input.189 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.188, %113, %117), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %2123 : Tensor = prim::GetAttr[name="bias"](%2114)
  %2124 : Tensor = prim::GetAttr[name="weight"](%2114)
  %2125 : Float(4096:1, 1024:4096) = aten::t(%2124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.189, %2125), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %2123, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.190, %113, %117), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.63, %2111, %124), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
  %2130 : Tensor = prim::GetAttr[name="bias"](%2113)
  %2131 : Tensor = prim::GetAttr[name="weight"](%2113)
  %2132 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm
  %curr_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.191, %2132, %2131, %2130, %96, %97), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
  %2134 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.22, %2112)
  %2135 : Float(13:17408, 17:1024, 1024:1), %2136 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2134)
  %new_mem.22 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2135, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2138 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.22), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2139 : __torch__.transformers.modeling_xlnet.___torch_mangle_40948.XLNetFeedForward = prim::GetAttr[name="ff"](%131)
  %2140 : __torch__.transformers.modeling_xlnet.___torch_mangle_40943.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%131)
  %2141 : __torch__.torch.nn.modules.normalization.___torch_mangle_40941.LayerNorm = prim::GetAttr[name="layer_norm"](%2140)
  %2142 : Tensor = prim::GetAttr[name="o"](%2140)
  %2143 : Tensor = prim::GetAttr[name="r_r_bias"](%2140)
  %2144 : Tensor = prim::GetAttr[name="r_w_bias"](%2140)
  %2145 : Tensor = prim::GetAttr[name="r"](%2140)
  %2146 : Tensor = prim::GetAttr[name="v"](%2140)
  %2147 : Tensor = prim::GetAttr[name="k"](%2140)
  %2148 : Tensor = prim::GetAttr[name="q"](%2140)
  %2149 : Tensor[] = prim::ListConstruct(%2135, %2148), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %q_head.22 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2149), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2151 : Tensor[] = prim::ListConstruct(%2135, %2147), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2152 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2151), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2153 : Tensor[] = prim::ListConstruct(%2135, %2146), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2154 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2153), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %r.23 : Float(26:1024, 17:0, 1024:1) = aten::to(%2136, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2156 : Tensor[] = prim::ListConstruct(%r.23, %2145), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2157 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2156), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2158 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2144, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
  %2159 : Tensor[] = prim::ListConstruct(%2158, %2152), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %ac.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %2159), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2161 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2143, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
  %2162 : Tensor[] = prim::ListConstruct(%2161, %2157), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.85 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %2162), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2164 : int = aten::size(%ac.22, %119), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
  %2165 : int = aten::size(%x.85, %125), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2166 : int = aten::size(%x.85, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2167 : int = aten::size(%x.85, %120), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2168 : int = aten::size(%x.85, %119), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2169 : Long() = prim::NumToTensor(%2168), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2170 : int[] = prim::ListConstruct(%2165, %2166, %2168, %2167), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.86 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %2170), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
  %2172 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2173 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2172, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2174 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2173, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.87 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2174, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2176 : Long() = aten::sub(%2169, %108, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2177 : int = aten::Int(%2176), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2178 : int[] = prim::ListConstruct(%2165, %2166, %2167, %2177), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.88 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %2178), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2180 : Long(13:1) = aten::arange(%2164, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %119, %2180), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %2182 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2182, %123, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2183, %102), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2185 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2186 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %2185), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2187 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2186, %100), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.192 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.22, %2187, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %119, %116), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
  %2190 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %113, %117), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %2191 : Tensor[] = prim::ListConstruct(%2190, %2154), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2192 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %2191), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2193 : Tensor[] = prim::ListConstruct(%2192, %2142), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %input.194 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %2193), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %attn_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.194, %113, %117), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.22, %2135, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
  %2197 : Tensor = prim::GetAttr[name="bias"](%2141)
  %2198 : Tensor = prim::GetAttr[name="weight"](%2141)
  %2199 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm
  %input_tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.195, %2199, %2198, %2197, %96, %97), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2201 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.22, %r.23)
  %2202 : Float(13:17408, 17:1024, 1024:1), %2203 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2201)
  %2204 : __torch__.torch.nn.modules.normalization.___torch_mangle_40944.LayerNorm = prim::GetAttr[name="layer_norm"](%2139)
  %2205 : __torch__.torch.nn.modules.linear.___torch_mangle_40946.Linear = prim::GetAttr[name="layer_2"](%2139)
  %2206 : __torch__.torch.nn.modules.linear.___torch_mangle_40945.Linear = prim::GetAttr[name="layer_1"](%2139)
  %2207 : Tensor = prim::GetAttr[name="bias"](%2206)
  %2208 : Tensor = prim::GetAttr[name="weight"](%2206)
  %2209 : Float(1024:1, 4096:1024) = aten::t(%2208), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.64 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2202, %2209), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.196 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.64, %2207, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.197 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.196), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # torch/nn/functional.py:1369:0
  %input.198 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.197, %113, %117), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %2214 : Tensor = prim::GetAttr[name="bias"](%2205)
  %2215 : Tensor = prim::GetAttr[name="weight"](%2205)
  %2216 : Float(4096:1, 1024:4096) = aten::t(%2215), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.198, %2216), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.65, %2214, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.199, %113, %117), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %input.200 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.66, %2202, %124), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
  %2221 : Tensor = prim::GetAttr[name="bias"](%2204)
  %2222 : Tensor = prim::GetAttr[name="weight"](%2204)
  %2223 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm
  %curr_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.200, %2223, %2222, %2221, %96, %97), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
  %2225 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.23, %2203)
  %2226 : Float(13:17408, 17:1024, 1024:1), %2227 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2225)
  %new_mem.23 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2226, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2229 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.23), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2230 : __torch__.transformers.modeling_xlnet.___torch_mangle_40958.XLNetFeedForward = prim::GetAttr[name="ff"](%129)
  %2231 : __torch__.transformers.modeling_xlnet.___torch_mangle_40953.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%129)
  %2232 : __torch__.torch.nn.modules.normalization.___torch_mangle_40951.LayerNorm = prim::GetAttr[name="layer_norm"](%2231)
  %2233 : Tensor = prim::GetAttr[name="o"](%2231)
  %2234 : Tensor = prim::GetAttr[name="r_r_bias"](%2231)
  %2235 : Tensor = prim::GetAttr[name="r_w_bias"](%2231)
  %2236 : Tensor = prim::GetAttr[name="r"](%2231)
  %2237 : Tensor = prim::GetAttr[name="v"](%2231)
  %2238 : Tensor = prim::GetAttr[name="k"](%2231)
  %2239 : Tensor = prim::GetAttr[name="q"](%2231)
  %2240 : Tensor[] = prim::ListConstruct(%2226, %2239), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %q_head.23 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2240), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2242 : Tensor[] = prim::ListConstruct(%2226, %2238), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2243 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2242), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2244 : Tensor[] = prim::ListConstruct(%2226, %2237), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2245 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2244), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %r : Float(26:1024, 17:0, 1024:1) = aten::to(%2227, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2247 : Tensor[] = prim::ListConstruct(%r, %2236), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2248 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2247), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2249 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2235, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
  %2250 : Tensor[] = prim::ListConstruct(%2249, %2243), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %ac.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %2250), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2252 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2234, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
  %2253 : Tensor[] = prim::ListConstruct(%2252, %2248), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.89 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %2253), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2255 : int = aten::size(%ac.23, %119), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
  %2256 : int = aten::size(%x.89, %125), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2257 : int = aten::size(%x.89, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2258 : int = aten::size(%x.89, %120), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2259 : int = aten::size(%x.89, %119), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2260 : Long() = prim::NumToTensor(%2259), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2261 : int[] = prim::ListConstruct(%2256, %2257, %2259, %2258), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.90 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %2261), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
  %2263 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2264 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2263, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2265 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2264, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.91 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2265, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2267 : Long() = aten::sub(%2260, %108, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2268 : int = aten::Int(%2267), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2269 : int[] = prim::ListConstruct(%2256, %2257, %2258, %2268), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.92 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %2269), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2271 : Long(13:1) = aten::arange(%2255, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %119, %2271), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %2273 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2274 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2273, %123, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2274, %102), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2276 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2277 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %2276), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2278 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2277, %100), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.201 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.23, %2278, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.202 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %119, %116), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
  %2281 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %113, %117), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %2282 : Tensor[] = prim::ListConstruct(%2281, %2245), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2283 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %2282), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2284 : Tensor[] = prim::ListConstruct(%2283, %2233), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %2284), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %attn_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.203, %113, %117), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.204 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.23, %2226, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
  %2288 : Tensor = prim::GetAttr[name="bias"](%2232)
  %2289 : Tensor = prim::GetAttr[name="weight"](%2232)
  %2290 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm
  %input_tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.204, %2290, %2289, %2288, %96, %97), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2292 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.23, %r)
  %2293 : Float(13:17408, 17:1024, 1024:1), %2294 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2292)
  %2295 : __torch__.torch.nn.modules.normalization.___torch_mangle_40954.LayerNorm = prim::GetAttr[name="layer_norm"](%2230)
  %2296 : __torch__.torch.nn.modules.linear.___torch_mangle_40956.Linear = prim::GetAttr[name="layer_2"](%2230)
  %2297 : __torch__.torch.nn.modules.linear.___torch_mangle_40955.Linear = prim::GetAttr[name="layer_1"](%2230)
  %2298 : Tensor = prim::GetAttr[name="bias"](%2297)
  %2299 : Tensor = prim::GetAttr[name="weight"](%2297)
  %2300 : Float(1024:1, 4096:1024) = aten::t(%2299), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.67 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2293, %2300), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.205 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.67, %2298, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.206 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.205), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # torch/nn/functional.py:1369:0
  %input.207 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.206, %113, %117), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %2305 : Tensor = prim::GetAttr[name="bias"](%2296)
  %2306 : Tensor = prim::GetAttr[name="weight"](%2296)
  %2307 : Float(4096:1, 1024:4096) = aten::t(%2306), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.207, %2307), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %2305, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.208, %113, %117), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %input.209 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.69, %2293, %124), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
  %2312 : Tensor = prim::GetAttr[name="bias"](%2295)
  %2313 : Tensor = prim::GetAttr[name="weight"](%2295)
  %2314 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm
  %curr_out : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.209, %2314, %2313, %2312, %96, %97), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
  %2316 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out, %2294)
  %2317 : Float(13:17408, 17:1024, 1024:1), %2318 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2316)
  %new_mem : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2317, %125, %125, %121, %124), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2320 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2321 : __torch__.transformers.modeling_xlnet.___torch_mangle_40968.XLNetFeedForward = prim::GetAttr[name="ff"](%127)
  %2322 : __torch__.transformers.modeling_xlnet.___torch_mangle_40963.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%127)
  %2323 : __torch__.torch.nn.modules.normalization.___torch_mangle_40961.LayerNorm = prim::GetAttr[name="layer_norm"](%2322)
  %2324 : Tensor = prim::GetAttr[name="o"](%2322)
  %2325 : Tensor = prim::GetAttr[name="r_r_bias"](%2322)
  %2326 : Tensor = prim::GetAttr[name="r_w_bias"](%2322)
  %2327 : Tensor = prim::GetAttr[name="r"](%2322)
  %2328 : Tensor = prim::GetAttr[name="v"](%2322)
  %2329 : Tensor = prim::GetAttr[name="k"](%2322)
  %2330 : Tensor = prim::GetAttr[name="q"](%2322)
  %2331 : Tensor[] = prim::ListConstruct(%2317, %2330), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %q_head : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2331), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2333 : Tensor[] = prim::ListConstruct(%2317, %2329), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2334 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2333), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2335 : Tensor[] = prim::ListConstruct(%2317, %2328), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2336 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2335), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2337 : Float(26:1024, 17:0, 1024:1) = aten::to(%2318, %115, %118, %117, %117, %116), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %2338 : Tensor[] = prim::ListConstruct(%2337, %2327), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2339 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%105, %2338), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2340 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2326, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
  %2341 : Tensor[] = prim::ListConstruct(%2340, %2334), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %ac : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%104, %2341), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2343 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2325, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
  %2344 : Tensor[] = prim::ListConstruct(%2343, %2339), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.93 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%104, %2344), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2346 : int = aten::size(%ac, %119), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
  %2347 : int = aten::size(%x.93, %125), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2348 : int = aten::size(%x.93, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2349 : int = aten::size(%x.93, %120), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2350 : int = aten::size(%x.93, %119), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2351 : Long() = prim::NumToTensor(%2350), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2352 : int[] = prim::ListConstruct(%2347, %2348, %2350, %2349), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.94 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %2352), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
  %2354 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %125, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2355 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2354, %124, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2356 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2355, %120, %124, %121, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.95 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2356, %119, %125, %121, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2358 : Long() = aten::sub(%2351, %108, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2359 : int = aten::Int(%2358), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2360 : int[] = prim::ListConstruct(%2347, %2348, %2349, %2359), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %2360), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2362 : Long(13:1) = aten::arange(%2346, %103, %125, %115, %117), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %119, %2362), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %2364 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2365 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2364, %123, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2365, %102), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2367 : Tensor[] = prim::ListConstruct(%201), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2368 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%101, %2367), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2369 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2368, %100), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score, %2369, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %119, %116), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
  %2372 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %113, %117), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %2373 : Tensor[] = prim::ListConstruct(%2372, %2336), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2374 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%99, %2373), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2375 : Tensor[] = prim::ListConstruct(%2374, %2324), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%98, %2375), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %attn_out : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.212, %113, %117), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out, %2317, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
  %2379 : Tensor = prim::GetAttr[name="bias"](%2323)
  %2380 : Tensor = prim::GetAttr[name="weight"](%2323)
  %2381 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm
  %input_tensor : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %2381, %2380, %2379, %96, %97), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2383 : __torch__.torch.nn.modules.normalization.___torch_mangle_40964.LayerNorm = prim::GetAttr[name="layer_norm"](%2321)
  %2384 : __torch__.torch.nn.modules.linear.___torch_mangle_40966.Linear = prim::GetAttr[name="layer_2"](%2321)
  %2385 : __torch__.torch.nn.modules.linear.___torch_mangle_40965.Linear = prim::GetAttr[name="layer_1"](%2321)
  %2386 : Tensor = prim::GetAttr[name="bias"](%2385)
  %2387 : Tensor = prim::GetAttr[name="weight"](%2385)
  %2388 : Float(1024:1, 4096:1024) = aten::t(%2387), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.70 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input_tensor, %2388), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.214 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.70, %2386, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.214), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # torch/nn/functional.py:1369:0
  %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.215, %113, %117), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %2393 : Tensor = prim::GetAttr[name="bias"](%2384)
  %2394 : Tensor = prim::GetAttr[name="weight"](%2384)
  %2395 : Float(4096:1, 1024:4096) = aten::t(%2394), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.216, %2395), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.217 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.71, %2393, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.72 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.217, %113, %117), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.72, %input_tensor, %124), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
  %2400 : Tensor = prim::GetAttr[name="bias"](%2383)
  %2401 : Tensor = prim::GetAttr[name="weight"](%2383)
  %2402 : int[] = prim::ListConstruct(%112), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm
  %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.218, %2402, %2401, %2400, %96, %97), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
  %output_h : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.219, %113, %117), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %2405 : int[] = prim::ListConstruct(%124, %125, %120), scope: __module.transformer
  %2406 : Float(17:1024, 13:17408, 1024:1) = aten::permute(%output_h, %2405), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %input.220 : Float(17:13312, 13:1024, 1024:1) = aten::contiguous(%2406, %125), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %2408 : (Float(17:13312, 13:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%input.220, %227, %318, %409, %500, %591, %682, %773, %864, %955, %1046, %1137, %1228, %1319, %1410, %1501, %1592, %1683, %1774, %1865, %1956, %2047, %2138, %2229, %2320)
  %8 : Float(17:13312, 13:1024, 1024:1), %9 : Float(13:17408, 17:1024, 1024:1), %10 : Float(13:17408, 17:1024, 1024:1), %11 : Float(13:17408, 17:1024, 1024:1), %12 : Float(13:17408, 17:1024, 1024:1), %13 : Float(13:17408, 17:1024, 1024:1), %14 : Float(13:17408, 17:1024, 1024:1), %15 : Float(13:17408, 17:1024, 1024:1), %16 : Float(13:17408, 17:1024, 1024:1), %17 : Float(13:17408, 17:1024, 1024:1), %18 : Float(13:17408, 17:1024, 1024:1), %19 : Float(13:17408, 17:1024, 1024:1), %20 : Float(13:17408, 17:1024, 1024:1), %21 : Float(13:17408, 17:1024, 1024:1), %22 : Float(13:17408, 17:1024, 1024:1), %23 : Float(13:17408, 17:1024, 1024:1), %24 : Float(13:17408, 17:1024, 1024:1), %25 : Float(13:17408, 17:1024, 1024:1), %26 : Float(13:17408, 17:1024, 1024:1), %27 : Float(13:17408, 17:1024, 1024:1), %28 : Float(13:17408, 17:1024, 1024:1), %29 : Float(13:17408, 17:1024, 1024:1), %30 : Float(13:17408, 17:1024, 1024:1), %31 : Float(13:17408, 17:1024, 1024:1), %32 : Float(13:17408, 17:1024, 1024:1) = prim::TupleUnpack(%2408)
  %2409 : int = prim::Constant[value=-1](), scope: __module.start_logits # transformers/modeling_utils.py:1126:0
  %2410 : int = prim::Constant[value=1](), scope: __module.start_logits/__module.start_logits.dense # torch/nn/functional.py:1678:0
  %2411 : __torch__.torch.nn.modules.linear.___torch_mangle_40974.Linear = prim::GetAttr[name="dense"](%5)
  %2412 : Tensor = prim::GetAttr[name="bias"](%2411)
  %2413 : Tensor = prim::GetAttr[name="weight"](%2411)
  %2414 : Float(1024:1, 1:1024) = aten::t(%2413), scope: __module.start_logits/__module.start_logits.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:13, 13:1, 1:1) = aten::matmul(%8, %2414), scope: __module.start_logits/__module.start_logits.dense # torch/nn/functional.py:1676:0
  %2416 : Float(17:13, 13:1, 1:1) = aten::add_(%output.73, %2412, %2410), scope: __module.start_logits/__module.start_logits.dense # torch/nn/functional.py:1678:0
  %input.221 : Float(17:13, 13:1) = aten::squeeze(%2416, %2409), scope: __module.start_logits # transformers/modeling_utils.py:1126:0
  %34 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1976:0
  %35 : int = aten::size(%8, %34) # transformers/modeling_xlnet.py:1976:0
  %slen : Long() = prim::NumToTensor(%35)
  %37 : int = aten::Int(%slen)
  %38 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1976:0
  %39 : int = aten::size(%8, %38) # transformers/modeling_xlnet.py:1976:0
  %hsz : Long() = prim::NumToTensor(%39)
  %41 : int = aten::Int(%hsz)
  %42 : int = prim::Constant[value=-1]() # torch/nn/functional.py:1498:0
  %43 : None = prim::Constant()
  %start_log_probs : Float(17:13, 13:1) = aten::softmax(%input.221, %42, %43) # torch/nn/functional.py:1498:0
  %45 : int = prim::Constant[value=5]() # transformers/modeling_xlnet.py:1979:0
  %46 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1979:0
  %47 : bool = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1979:0
  %48 : bool = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1979:0
  %49 : Float(17:5, 5:1), %start_top_index : Long(17:5, 5:1) = aten::topk(%start_log_probs, %45, %46, %47, %48) # transformers/modeling_xlnet.py:1979:0
  %51 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1982:0
  %52 : Long(17:5, 5:1, 1:1) = aten::unsqueeze(%start_top_index, %51) # transformers/modeling_xlnet.py:1982:0
  %53 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1982:0
  %54 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1982:0
  %55 : int[] = prim::ListConstruct(%53, %54, %41)
  %56 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1982:0
  %start_top_index_exp : Long(17:5, 5:1, 1024:0) = aten::expand(%52, %55, %56) # transformers/modeling_xlnet.py:1982:0
  %58 : int = prim::Constant[value=-2]() # transformers/modeling_xlnet.py:1983:0
  %59 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1983:0
  %start_states.1 : Float(17:5120, 5:1024, 1024:1) = aten::gather(%8, %58, %start_top_index_exp, %59) # transformers/modeling_xlnet.py:1983:0
  %61 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1984:0
  %62 : Float(17:5120, 1:5120, 5:1024, 1024:1) = aten::unsqueeze(%start_states.1, %61) # transformers/modeling_xlnet.py:1984:0
  %63 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1984:0
  %64 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1984:0
  %65 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1984:0
  %66 : int[] = prim::ListConstruct(%63, %37, %64, %65)
  %67 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1984:0
  %start_states.2 : Float(17:5120, 13:0, 5:1024, 1024:1) = aten::expand(%62, %66, %67) # transformers/modeling_xlnet.py:1984:0
  %69 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1986:0
  %70 : Float(17:13312, 13:1024, 1:1024, 1024:1) = aten::unsqueeze(%8, %69) # transformers/modeling_xlnet.py:1986:0
  %hidden_states : Float(17:13312, 13:1024, 5:0, 1024:1) = aten::expand_as(%70, %start_states.2) # transformers/modeling_xlnet.py:1986:0
  %2418 : int = prim::Constant[value=1024](), scope: __module.end_logits/__module.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %2419 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.end_logits/__module.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %2420 : bool = prim::Constant[value=1](), scope: __module.end_logits/__module.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %2421 : int = prim::Constant[value=1](), scope: __module.end_logits/__module.end_logits.dense_0 # torch/nn/functional.py:1678:0
  %2422 : int = prim::Constant[value=-1](), scope: __module.end_logits # transformers/modeling_utils.py:1190:0
  %2423 : __torch__.torch.nn.modules.linear.___torch_mangle_40979.Linear = prim::GetAttr[name="dense_1"](%4)
  %2424 : __torch__.torch.nn.modules.normalization.___torch_mangle_40978.LayerNorm = prim::GetAttr[name="LayerNorm"](%4)
  %2425 : __torch__.torch.nn.modules.linear.___torch_mangle_40976.Linear = prim::GetAttr[name="dense_0"](%4)
  %2426 : Tensor[] = prim::ListConstruct(%hidden_states, %start_states.2), scope: __module.end_logits
  %input.222 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::cat(%2426, %2422), scope: __module.end_logits # transformers/modeling_utils.py:1190:0
  %2428 : Tensor = prim::GetAttr[name="bias"](%2425)
  %2429 : Tensor = prim::GetAttr[name="weight"](%2425)
  %2430 : Float(2048:1, 1024:2048) = aten::t(%2429), scope: __module.end_logits/__module.end_logits.dense_0 # torch/nn/functional.py:1676:0
  %output.74 : Float(17:66560, 13:5120, 5:1024, 1024:1) = aten::matmul(%input.222, %2430), scope: __module.end_logits/__module.end_logits.dense_0 # torch/nn/functional.py:1676:0
  %input.223 : Float(17:66560, 13:5120, 5:1024, 1024:1) = aten::add_(%output.74, %2428, %2421), scope: __module.end_logits/__module.end_logits.dense_0 # torch/nn/functional.py:1678:0
  %input.224 : Float(17:66560, 13:5120, 5:1024, 1024:1) = aten::tanh(%input.223), scope: __module.end_logits/__module.end_logits.activation # torch/nn/modules/activation.py:350:0
  %2434 : Tensor = prim::GetAttr[name="bias"](%2424)
  %2435 : Tensor = prim::GetAttr[name="weight"](%2424)
  %2436 : int[] = prim::ListConstruct(%2418), scope: __module.end_logits/__module.end_logits.LayerNorm
  %input.225 : Float(17:66560, 13:5120, 5:1024, 1024:1) = aten::layer_norm(%input.224, %2436, %2435, %2434, %2419, %2420), scope: __module.end_logits/__module.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %2438 : Tensor = prim::GetAttr[name="bias"](%2423)
  %2439 : Tensor = prim::GetAttr[name="weight"](%2423)
  %2440 : Float(1024:1, 1:1024) = aten::t(%2439), scope: __module.end_logits/__module.end_logits.dense_1 # torch/nn/functional.py:1676:0
  %output : Float(17:65, 13:5, 5:1, 1:1) = aten::matmul(%input.225, %2440), scope: __module.end_logits/__module.end_logits.dense_1 # torch/nn/functional.py:1676:0
  %2442 : Float(17:65, 13:5, 5:1, 1:1) = aten::add_(%output, %2438, %2421), scope: __module.end_logits/__module.end_logits.dense_1 # torch/nn/functional.py:1678:0
  %input.226 : Float(17:65, 13:5, 5:1) = aten::squeeze(%2442, %2422), scope: __module.end_logits # transformers/modeling_utils.py:1193:0
  %73 : int = prim::Constant[value=1]() # torch/nn/functional.py:1498:0
  %74 : None = prim::Constant()
  %end_log_probs : Float(17:65, 13:5, 5:1) = aten::softmax(%input.226, %73, %74) # torch/nn/functional.py:1498:0
  %76 : int = prim::Constant[value=5]() # transformers/modeling_xlnet.py:1993:0
  %77 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1993:0
  %78 : bool = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1993:0
  %79 : bool = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1993:0
  %end_top_log_probs : Float(17:25, 5:5, 5:1), %end_top_index : Long(17:25, 5:5, 5:1) = aten::topk(%end_log_probs, %76, %77, %78, %79) # transformers/modeling_xlnet.py:1993:0
  %82 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1996:0
  %83 : int = prim::Constant[value=25]() # transformers/modeling_xlnet.py:1996:0
  %84 : int[] = prim::ListConstruct(%82, %83)
  %85 : Float(17:25, 25:1) = aten::view(%end_top_log_probs, %84) # transformers/modeling_xlnet.py:1996:0
  %86 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1997:0
  %87 : int = prim::Constant[value=25]() # transformers/modeling_xlnet.py:1997:0
  %88 : int[] = prim::ListConstruct(%86, %87)
  %89 : Long(17:25, 25:1) = aten::view(%end_top_index, %88) # transformers/modeling_xlnet.py:1997:0
  %90 : str = prim::Constant[value="blh,bl->bh"]() # torch/functional.py:327:0
  %91 : Tensor[] = prim::ListConstruct(%8, %start_log_probs)
  %start_states : Float(17:1024, 1024:1) = aten::einsum(%90, %91) # torch/functional.py:327:0
  %2444 : int = prim::Constant[value=-1](), scope: __module.answer_class # transformers/modeling_utils.py:1258:0
  %2445 : int = prim::Constant[value=1](), scope: __module.answer_class # transformers/modeling_utils.py:1258:0
  %2446 : int = prim::Constant[value=9223372036854775807](), scope: __module.answer_class # transformers/modeling_utils.py:1258:0
  %2447 : int = prim::Constant[value=0](), scope: __module.answer_class # transformers/modeling_utils.py:1258:0
  %2448 : __torch__.torch.nn.modules.linear.___torch_mangle_40983.Linear = prim::GetAttr[name="dense_1"](%3)
  %2449 : __torch__.torch.nn.modules.linear.___torch_mangle_40981.Linear = prim::GetAttr[name="dense_0"](%3)
  %2450 : Float(17:13312, 13:1024, 1024:1) = aten::slice(%8, %2447, %2447, %2446, %2445), scope: __module.answer_class # transformers/modeling_utils.py:1258:0
  %2451 : Float(17:13312, 1024:1) = aten::select(%2450, %2445, %2444), scope: __module.answer_class # transformers/modeling_utils.py:1258:0
  %cls_token_state : Float(17:13312, 1024:1) = aten::slice(%2451, %2445, %2447, %2446, %2445), scope: __module.answer_class # transformers/modeling_utils.py:1258:0
  %2453 : Tensor[] = prim::ListConstruct(%start_states, %cls_token_state), scope: __module.answer_class
  %input.227 : Float(17:2048, 2048:1) = aten::cat(%2453, %2444), scope: __module.answer_class # transformers/modeling_utils.py:1260:0
  %2455 : Tensor = prim::GetAttr[name="bias"](%2449)
  %2456 : Tensor = prim::GetAttr[name="weight"](%2449)
  %2457 : Float(2048:1, 1024:2048) = aten::t(%2456), scope: __module.answer_class/__module.answer_class.dense_0 # torch/nn/functional.py:1674:0
  %input.228 : Float(17:1024, 1024:1) = aten::addmm(%2455, %input.227, %2457, %2445, %2445), scope: __module.answer_class/__module.answer_class.dense_0 # torch/nn/functional.py:1674:0
  %input : Float(17:1024, 1024:1) = aten::tanh(%input.228), scope: __module.answer_class/__module.answer_class.activation # torch/nn/modules/activation.py:350:0
  %2460 : Tensor = prim::GetAttr[name="weight"](%2448)
  %2461 : Float(1024:1, 1:1024) = aten::t(%2460), scope: __module.answer_class/__module.answer_class.dense_1 # torch/nn/functional.py:1676:0
  %2462 : Float(17:1, 1:1) = aten::matmul(%input, %2461), scope: __module.answer_class/__module.answer_class.dense_1 # torch/nn/functional.py:1676:0
  %2463 : Float(17:1) = aten::squeeze(%2462, %2444), scope: __module.answer_class # transformers/modeling_utils.py:1262:0
  %94 : (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, %28, %29, %30, %31, %32)
  %95 : (Float(17:5, 5:1), Long(17:5, 5:1), Float(17:25, 25:1), Long(17:25, 25:1), Float(17:1), (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1))) = prim::TupleConstruct(%49, %start_top_index, %85, %89, %2463, %94)
  return (%95)
