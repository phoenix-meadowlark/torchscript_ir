graph(%self.1 : __torch__.transformers.modeling_xlnet.XLNetLMHeadModel,
      %input_ids.1 : Long(17:13, 13:1),
      %attention_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_42966.Linear = prim::GetAttr[name="lm_loss"](%self.1)
  %4 : __torch__.transformers.modeling_xlnet.___torch_mangle_42965.XLNetModel = prim::GetAttr[name="transformer"](%self.1)
  %34 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %35 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %36 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %37 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %38 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %39 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %40 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %41 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %42 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %43 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %44 : str = prim::Constant[value="i,d->id"](), scope: __module.transformer # torch/functional.py:327:0
  %45 : float = prim::Constant[value=-1.](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %46 : Long() = prim::Constant[value={1}](), scope: __module.transformer # torch/tensor.py:400:0
  %47 : int = prim::Constant[value=10000](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %48 : Long() = prim::Constant[value={1024}](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %49 : float = prim::Constant[value=2.](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %50 : int = prim::Constant[value=1024](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %51 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %52 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %53 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %54 : None = prim::Constant(), scope: __module.transformer
  %55 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %56 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %57 : int = prim::Constant[value=3](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %58 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %59 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %60 : float = prim::Constant[value=1.](), scope: __module.transformer # torch/tensor.py:396:0
  %61 : Long() = prim::Constant[value={0}](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %62 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %63 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %64 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %65 : __torch__.transformers.modeling_xlnet.___torch_mangle_42962.XLNetLayer = prim::GetAttr[name="23"](%64)
  %66 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %67 : __torch__.transformers.modeling_xlnet.___torch_mangle_42952.XLNetLayer = prim::GetAttr[name="22"](%66)
  %68 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %69 : __torch__.transformers.modeling_xlnet.___torch_mangle_42942.XLNetLayer = prim::GetAttr[name="21"](%68)
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %71 : __torch__.transformers.modeling_xlnet.___torch_mangle_42932.XLNetLayer = prim::GetAttr[name="20"](%70)
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %73 : __torch__.transformers.modeling_xlnet.___torch_mangle_42922.XLNetLayer = prim::GetAttr[name="19"](%72)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %75 : __torch__.transformers.modeling_xlnet.___torch_mangle_42912.XLNetLayer = prim::GetAttr[name="18"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %77 : __torch__.transformers.modeling_xlnet.___torch_mangle_42902.XLNetLayer = prim::GetAttr[name="17"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %79 : __torch__.transformers.modeling_xlnet.___torch_mangle_42892.XLNetLayer = prim::GetAttr[name="16"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %81 : __torch__.transformers.modeling_xlnet.___torch_mangle_42882.XLNetLayer = prim::GetAttr[name="15"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %83 : __torch__.transformers.modeling_xlnet.___torch_mangle_42872.XLNetLayer = prim::GetAttr[name="14"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %85 : __torch__.transformers.modeling_xlnet.___torch_mangle_42862.XLNetLayer = prim::GetAttr[name="13"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %87 : __torch__.transformers.modeling_xlnet.___torch_mangle_42852.XLNetLayer = prim::GetAttr[name="12"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %89 : __torch__.transformers.modeling_xlnet.___torch_mangle_42842.XLNetLayer = prim::GetAttr[name="11"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %91 : __torch__.transformers.modeling_xlnet.___torch_mangle_42832.XLNetLayer = prim::GetAttr[name="10"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %93 : __torch__.transformers.modeling_xlnet.___torch_mangle_42822.XLNetLayer = prim::GetAttr[name="9"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %95 : __torch__.transformers.modeling_xlnet.___torch_mangle_42812.XLNetLayer = prim::GetAttr[name="8"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %97 : __torch__.transformers.modeling_xlnet.___torch_mangle_42802.XLNetLayer = prim::GetAttr[name="7"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %99 : __torch__.transformers.modeling_xlnet.___torch_mangle_42792.XLNetLayer = prim::GetAttr[name="6"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %101 : __torch__.transformers.modeling_xlnet.___torch_mangle_42782.XLNetLayer = prim::GetAttr[name="5"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %103 : __torch__.transformers.modeling_xlnet.___torch_mangle_42772.XLNetLayer = prim::GetAttr[name="4"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %105 : __torch__.transformers.modeling_xlnet.___torch_mangle_42762.XLNetLayer = prim::GetAttr[name="3"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %107 : __torch__.transformers.modeling_xlnet.___torch_mangle_42752.XLNetLayer = prim::GetAttr[name="2"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %109 : __torch__.transformers.modeling_xlnet.___torch_mangle_42742.XLNetLayer = prim::GetAttr[name="1"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_42963.ModuleList = prim::GetAttr[name="layer"](%4)
  %111 : __torch__.transformers.modeling_xlnet.___torch_mangle_42732.XLNetLayer = prim::GetAttr[name="0"](%110)
  %112 : __torch__.torch.nn.modules.sparse.___torch_mangle_42722.Embedding = prim::GetAttr[name="word_embedding"](%4)
  %113 : Long(13:1, 17:13) = aten::transpose(%input_ids.1, %63, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %input_ids : Long(13:17, 17:1) = aten::contiguous(%113, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
  %115 : int = aten::size(%input_ids, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %qlen : Long() = prim::NumToTensor(%115), scope: __module.transformer
  %117 : int = aten::size(%input_ids, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
  %118 : Long(13:1, 17:13) = aten::transpose(%attention_mask, %63, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
  %119 : Long(13:17, 17:1) = aten::contiguous(%118, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
  %klen.1 : Long() = aten::add(%qlen, %61, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
  %121 : Scalar = aten::ScalarImplicit(%klen.1), scope: __module.transformer
  %input_mask : Float(13:17, 17:1) = aten::rsub(%119, %60, %62), scope: __module.transformer # torch/tensor.py:396:0
  %data_mask : Float(1:221, 13:17, 17:1) = aten::unsqueeze(%input_mask, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1139:0
  %124 : Float(1:221, 13:17, 17:1) = aten::slice(%data_mask, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %125 : Float(1:221, 13:17, 17:1) = aten::slice(%124, %62, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %126 : Float(1:221, 13:17, 17:1) = aten::slice(%125, %58, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %127 : Float(1:221, 13:17, 17:1, 1:1) = aten::unsqueeze(%126, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
  %128 : Bool(1:221, 13:17, 17:1, 1:1) = aten::gt(%127, %63), scope: __module.transformer # torch/tensor.py:22:0
  %attn_mask : Float(1:221, 13:17, 17:1, 1:1) = aten::to(%128, %56, %55, %55, %54), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
  %130 : Float(13:13, 13:1) = aten::eye(%115, %56, %63, %53, %55), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %131 : Float(13:13, 13:1) = aten::to(%130, %53, %56, %55, %55, %54), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %non_tgt_mask : Float(13:13, 13:1) = aten::neg(%131), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
  %133 : Float(13:13, 13:1) = aten::slice(%non_tgt_mask, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %134 : Float(13:13, 13:1) = aten::slice(%133, %62, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %135 : Float(13:13, 13:1, 1:1) = aten::unsqueeze(%134, %58), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %136 : Float(13:13, 13:1, 1:1, 1:1) = aten::unsqueeze(%135, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %137 : Float(13:221, 13:17, 17:1, 1:1) = aten::add(%attn_mask, %136, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %138 : Bool(13:221, 13:17, 17:1, 1:1) = aten::gt(%137, %63), scope: __module.transformer # torch/tensor.py:22:0
  %139 : Float(13:221, 13:17, 17:1, 1:1) = aten::to(%138, %53, %56, %55, %55, %54), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
  %140 : Tensor = prim::GetAttr[name="weight"](%112)
  %input.1 : Float(13:17408, 17:1024, 1024:1) = aten::embedding(%140, %input_ids, %52, %55, %55), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
  %curr_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.1, %51, %55), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %freq_seq : Float(512:1) = aten::arange(%63, %50, %49, %56, %63, %53, %55), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
  %144 : Float(512:1) = aten::div(%freq_seq, %48), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %145 : Float(512:1) = aten::pow(%47, %144), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
  %146 : Float(512:1) = aten::reciprocal(%145), scope: __module.transformer # torch/tensor.py:400:0
  %147 : Float(512:1) = aten::mul(%146, %46), scope: __module.transformer # torch/tensor.py:400:0
  %end : Long() = aten::neg(%qlen), scope: __module.transformer # transformers/modeling_xlnet.py:1033:0
  %149 : Scalar = aten::ScalarImplicit(%end), scope: __module.transformer
  %150 : Float(26:1) = aten::arange(%121, %149, %45, %54, %63, %53, %55), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
  %151 : Tensor[] = prim::ListConstruct(%150, %147), scope: __module.transformer
  %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%44, %151), scope: __module.transformer # torch/functional.py:327:0
  %153 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %154 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %155 : Tensor[] = prim::ListConstruct(%153, %154), scope: __module.transformer
  %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%155, %52), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
  %157 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %158 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%157, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%158, %58, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
  %160 : int[] = prim::ListConstruct(%52, %117, %52), scope: __module.transformer
  %pos_emb : Float(26:1024, 17:0, 1024:1) = aten::expand(%pos_emb.2, %160, %55), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
  %input.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%pos_emb, %56, %63, %53, %55, %55, %55, %54), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
  %r.1 : Float(26:1024, 17:0, 1024:1) = aten::dropout(%input.2, %51, %55), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %new_mem.1 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%curr_out.1, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %165 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.1), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %166 : __torch__.transformers.modeling_xlnet.___torch_mangle_42730.XLNetFeedForward = prim::GetAttr[name="ff"](%111)
  %167 : __torch__.transformers.modeling_xlnet.___torch_mangle_42725.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%111)
  %168 : __torch__.torch.nn.modules.normalization.___torch_mangle_42723.LayerNorm = prim::GetAttr[name="layer_norm"](%167)
  %169 : Tensor = prim::GetAttr[name="o"](%167)
  %170 : Tensor = prim::GetAttr[name="r_r_bias"](%167)
  %171 : Tensor = prim::GetAttr[name="r_w_bias"](%167)
  %172 : Tensor = prim::GetAttr[name="r"](%167)
  %173 : Tensor = prim::GetAttr[name="v"](%167)
  %174 : Tensor = prim::GetAttr[name="k"](%167)
  %175 : Tensor = prim::GetAttr[name="q"](%167)
  %176 : Tensor[] = prim::ListConstruct(%curr_out.1, %175), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %q_head.1 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %176), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %178 : Tensor[] = prim::ListConstruct(%curr_out.1, %174), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %179 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %178), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %180 : Tensor[] = prim::ListConstruct(%curr_out.1, %173), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %181 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %180), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %r.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%r.1, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %183 : Tensor[] = prim::ListConstruct(%r.2, %172), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %184 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %183), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %185 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %171, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
  %186 : Tensor[] = prim::ListConstruct(%185, %179), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %ac.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %186), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %188 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %170, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
  %189 : Tensor[] = prim::ListConstruct(%188, %184), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.1 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %189), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %191 : int = aten::size(%ac.1, %57), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
  %192 : int = aten::size(%x.1, %63), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %193 : int = aten::size(%x.1, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %194 : int = aten::size(%x.1, %58), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %195 : int = aten::size(%x.1, %57), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %196 : Long() = prim::NumToTensor(%195), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %197 : int[] = prim::ListConstruct(%192, %193, %195, %194), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.2 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %197), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
  %199 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %200 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%199, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %201 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%200, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.3 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%201, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %203 : Long() = aten::sub(%196, %46, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %204 : int = aten::Int(%203), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %205 : int[] = prim::ListConstruct(%192, %193, %194, %204), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %x.4 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %205), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %207 : Long(13:1) = aten::arange(%191, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %57, %207), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %209 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%209, %61, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%210, %40), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %212 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %213 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %212), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %214 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%213, %38), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.1, %214, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %57, %54), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
  %217 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %51, %55), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %218 : Tensor[] = prim::ListConstruct(%217, %181), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %219 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %218), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %220 : Tensor[] = prim::ListConstruct(%219, %169), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
  %input.5 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %220), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
  %attn_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.5, %51, %55), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.1, %curr_out.1, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
  %224 : Tensor = prim::GetAttr[name="bias"](%168)
  %225 : Tensor = prim::GetAttr[name="weight"](%168)
  %226 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm
  %input_tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.6, %226, %225, %224, %34, %35), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %228 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.1, %r.2)
  %229 : Float(13:17408, 17:1024, 1024:1), %230 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%228)
  %231 : __torch__.torch.nn.modules.normalization.___torch_mangle_42726.LayerNorm = prim::GetAttr[name="layer_norm"](%166)
  %232 : __torch__.torch.nn.modules.linear.___torch_mangle_42728.Linear = prim::GetAttr[name="layer_2"](%166)
  %233 : __torch__.torch.nn.modules.linear.___torch_mangle_42727.Linear = prim::GetAttr[name="layer_1"](%166)
  %234 : Tensor = prim::GetAttr[name="bias"](%233)
  %235 : Tensor = prim::GetAttr[name="weight"](%233)
  %236 : Float(1024:1, 4096:1024) = aten::t(%235), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.1 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%229, %236), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.7 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.1, %234, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.8 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # torch/nn/functional.py:1369:0
  %input.9 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.8, %51, %55), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %241 : Tensor = prim::GetAttr[name="bias"](%232)
  %242 : Tensor = prim::GetAttr[name="weight"](%232)
  %243 : Float(4096:1, 1024:4096) = aten::t(%242), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.9, %243), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %241, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.10, %51, %55), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.3, %229, %62), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
  %248 : Tensor = prim::GetAttr[name="bias"](%231)
  %249 : Tensor = prim::GetAttr[name="weight"](%231)
  %250 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm
  %curr_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.11, %250, %249, %248, %34, %35), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
  %252 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.2, %230)
  %253 : Float(13:17408, 17:1024, 1024:1), %254 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%252)
  %new_mem.2 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%253, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %256 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.2), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %257 : __torch__.transformers.modeling_xlnet.___torch_mangle_42740.XLNetFeedForward = prim::GetAttr[name="ff"](%109)
  %258 : __torch__.transformers.modeling_xlnet.___torch_mangle_42735.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%109)
  %259 : __torch__.torch.nn.modules.normalization.___torch_mangle_42733.LayerNorm = prim::GetAttr[name="layer_norm"](%258)
  %260 : Tensor = prim::GetAttr[name="o"](%258)
  %261 : Tensor = prim::GetAttr[name="r_r_bias"](%258)
  %262 : Tensor = prim::GetAttr[name="r_w_bias"](%258)
  %263 : Tensor = prim::GetAttr[name="r"](%258)
  %264 : Tensor = prim::GetAttr[name="v"](%258)
  %265 : Tensor = prim::GetAttr[name="k"](%258)
  %266 : Tensor = prim::GetAttr[name="q"](%258)
  %267 : Tensor[] = prim::ListConstruct(%253, %266), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %q_head.2 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %267), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %269 : Tensor[] = prim::ListConstruct(%253, %265), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %270 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %269), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %271 : Tensor[] = prim::ListConstruct(%253, %264), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %272 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %271), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %r.3 : Float(26:1024, 17:0, 1024:1) = aten::to(%254, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %274 : Tensor[] = prim::ListConstruct(%r.3, %263), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %275 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %274), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %276 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %262, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
  %277 : Tensor[] = prim::ListConstruct(%276, %270), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %ac.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %277), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %279 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %261, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
  %280 : Tensor[] = prim::ListConstruct(%279, %275), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.5 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %280), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %282 : int = aten::size(%ac.2, %57), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
  %283 : int = aten::size(%x.5, %63), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %284 : int = aten::size(%x.5, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %285 : int = aten::size(%x.5, %58), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %286 : int = aten::size(%x.5, %57), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %287 : Long() = prim::NumToTensor(%286), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %288 : int[] = prim::ListConstruct(%283, %284, %286, %285), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.6 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %288), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
  %290 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %291 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%290, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %292 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%291, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.7 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%292, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %294 : Long() = aten::sub(%287, %46, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %295 : int = aten::Int(%294), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %296 : int[] = prim::ListConstruct(%283, %284, %285, %295), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %x.8 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %296), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %298 : Long(13:1) = aten::arange(%282, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %57, %298), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %300 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %301 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%300, %61, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%301, %40), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %303 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %304 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %303), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %305 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%304, %38), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.2, %305, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %57, %54), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
  %308 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %51, %55), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %309 : Tensor[] = prim::ListConstruct(%308, %272), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %310 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %309), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %311 : Tensor[] = prim::ListConstruct(%310, %260), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
  %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %311), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
  %attn_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.14, %51, %55), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.2, %253, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
  %315 : Tensor = prim::GetAttr[name="bias"](%259)
  %316 : Tensor = prim::GetAttr[name="weight"](%259)
  %317 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm
  %input_tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.15, %317, %316, %315, %34, %35), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %319 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.2, %r.3)
  %320 : Float(13:17408, 17:1024, 1024:1), %321 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%319)
  %322 : __torch__.torch.nn.modules.normalization.___torch_mangle_42736.LayerNorm = prim::GetAttr[name="layer_norm"](%257)
  %323 : __torch__.torch.nn.modules.linear.___torch_mangle_42738.Linear = prim::GetAttr[name="layer_2"](%257)
  %324 : __torch__.torch.nn.modules.linear.___torch_mangle_42737.Linear = prim::GetAttr[name="layer_1"](%257)
  %325 : Tensor = prim::GetAttr[name="bias"](%324)
  %326 : Tensor = prim::GetAttr[name="weight"](%324)
  %327 : Float(1024:1, 4096:1024) = aten::t(%326), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%320, %327), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.16 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.4, %325, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.17 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.16), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # torch/nn/functional.py:1369:0
  %input.18 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.17, %51, %55), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %332 : Tensor = prim::GetAttr[name="bias"](%323)
  %333 : Tensor = prim::GetAttr[name="weight"](%323)
  %334 : Float(4096:1, 1024:4096) = aten::t(%333), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.18, %334), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.5, %332, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.19, %51, %55), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.6, %320, %62), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
  %339 : Tensor = prim::GetAttr[name="bias"](%322)
  %340 : Tensor = prim::GetAttr[name="weight"](%322)
  %341 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm
  %curr_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.20, %341, %340, %339, %34, %35), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
  %343 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.3, %321)
  %344 : Float(13:17408, 17:1024, 1024:1), %345 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%343)
  %new_mem.3 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%344, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %347 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.3), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %348 : __torch__.transformers.modeling_xlnet.___torch_mangle_42750.XLNetFeedForward = prim::GetAttr[name="ff"](%107)
  %349 : __torch__.transformers.modeling_xlnet.___torch_mangle_42745.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%107)
  %350 : __torch__.torch.nn.modules.normalization.___torch_mangle_42743.LayerNorm = prim::GetAttr[name="layer_norm"](%349)
  %351 : Tensor = prim::GetAttr[name="o"](%349)
  %352 : Tensor = prim::GetAttr[name="r_r_bias"](%349)
  %353 : Tensor = prim::GetAttr[name="r_w_bias"](%349)
  %354 : Tensor = prim::GetAttr[name="r"](%349)
  %355 : Tensor = prim::GetAttr[name="v"](%349)
  %356 : Tensor = prim::GetAttr[name="k"](%349)
  %357 : Tensor = prim::GetAttr[name="q"](%349)
  %358 : Tensor[] = prim::ListConstruct(%344, %357), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %q_head.3 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %358), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %360 : Tensor[] = prim::ListConstruct(%344, %356), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %361 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %360), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %362 : Tensor[] = prim::ListConstruct(%344, %355), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %363 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %362), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %r.4 : Float(26:1024, 17:0, 1024:1) = aten::to(%345, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %365 : Tensor[] = prim::ListConstruct(%r.4, %354), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %366 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %365), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %367 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %353, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
  %368 : Tensor[] = prim::ListConstruct(%367, %361), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %ac.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %368), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %370 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %352, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
  %371 : Tensor[] = prim::ListConstruct(%370, %366), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.9 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %371), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %373 : int = aten::size(%ac.3, %57), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
  %374 : int = aten::size(%x.9, %63), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %375 : int = aten::size(%x.9, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %376 : int = aten::size(%x.9, %58), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %377 : int = aten::size(%x.9, %57), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %378 : Long() = prim::NumToTensor(%377), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %379 : int[] = prim::ListConstruct(%374, %375, %377, %376), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.10 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %379), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
  %381 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %382 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%381, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %383 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%382, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.11 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%383, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %385 : Long() = aten::sub(%378, %46, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %386 : int = aten::Int(%385), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %387 : int[] = prim::ListConstruct(%374, %375, %376, %386), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %x.12 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %387), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %389 : Long(13:1) = aten::arange(%373, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %57, %389), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %391 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %392 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%391, %61, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%392, %40), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %394 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %395 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %394), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %396 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%395, %38), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.3, %396, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %57, %54), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
  %399 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %51, %55), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %400 : Tensor[] = prim::ListConstruct(%399, %363), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %401 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %400), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %402 : Tensor[] = prim::ListConstruct(%401, %351), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
  %input.23 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %402), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
  %attn_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.23, %51, %55), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.3, %344, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
  %406 : Tensor = prim::GetAttr[name="bias"](%350)
  %407 : Tensor = prim::GetAttr[name="weight"](%350)
  %408 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm
  %input_tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.24, %408, %407, %406, %34, %35), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %410 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.3, %r.4)
  %411 : Float(13:17408, 17:1024, 1024:1), %412 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%410)
  %413 : __torch__.torch.nn.modules.normalization.___torch_mangle_42746.LayerNorm = prim::GetAttr[name="layer_norm"](%348)
  %414 : __torch__.torch.nn.modules.linear.___torch_mangle_42748.Linear = prim::GetAttr[name="layer_2"](%348)
  %415 : __torch__.torch.nn.modules.linear.___torch_mangle_42747.Linear = prim::GetAttr[name="layer_1"](%348)
  %416 : Tensor = prim::GetAttr[name="bias"](%415)
  %417 : Tensor = prim::GetAttr[name="weight"](%415)
  %418 : Float(1024:1, 4096:1024) = aten::t(%417), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.7 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%411, %418), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.25 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.7, %416, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.26 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.25), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # torch/nn/functional.py:1369:0
  %input.27 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.26, %51, %55), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %423 : Tensor = prim::GetAttr[name="bias"](%414)
  %424 : Tensor = prim::GetAttr[name="weight"](%414)
  %425 : Float(4096:1, 1024:4096) = aten::t(%424), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.27, %425), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %423, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.28, %51, %55), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.9, %411, %62), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
  %430 : Tensor = prim::GetAttr[name="bias"](%413)
  %431 : Tensor = prim::GetAttr[name="weight"](%413)
  %432 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm
  %curr_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.29, %432, %431, %430, %34, %35), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
  %434 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.4, %412)
  %435 : Float(13:17408, 17:1024, 1024:1), %436 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%434)
  %new_mem.4 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%435, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %438 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.4), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %439 : __torch__.transformers.modeling_xlnet.___torch_mangle_42760.XLNetFeedForward = prim::GetAttr[name="ff"](%105)
  %440 : __torch__.transformers.modeling_xlnet.___torch_mangle_42755.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%105)
  %441 : __torch__.torch.nn.modules.normalization.___torch_mangle_42753.LayerNorm = prim::GetAttr[name="layer_norm"](%440)
  %442 : Tensor = prim::GetAttr[name="o"](%440)
  %443 : Tensor = prim::GetAttr[name="r_r_bias"](%440)
  %444 : Tensor = prim::GetAttr[name="r_w_bias"](%440)
  %445 : Tensor = prim::GetAttr[name="r"](%440)
  %446 : Tensor = prim::GetAttr[name="v"](%440)
  %447 : Tensor = prim::GetAttr[name="k"](%440)
  %448 : Tensor = prim::GetAttr[name="q"](%440)
  %449 : Tensor[] = prim::ListConstruct(%435, %448), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %q_head.4 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %449), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %451 : Tensor[] = prim::ListConstruct(%435, %447), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %452 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %451), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %453 : Tensor[] = prim::ListConstruct(%435, %446), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %454 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %453), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %r.5 : Float(26:1024, 17:0, 1024:1) = aten::to(%436, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %456 : Tensor[] = prim::ListConstruct(%r.5, %445), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %457 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %456), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %458 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %444, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
  %459 : Tensor[] = prim::ListConstruct(%458, %452), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %ac.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %459), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %461 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %443, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
  %462 : Tensor[] = prim::ListConstruct(%461, %457), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.13 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %462), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %464 : int = aten::size(%ac.4, %57), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
  %465 : int = aten::size(%x.13, %63), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %466 : int = aten::size(%x.13, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %467 : int = aten::size(%x.13, %58), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %468 : int = aten::size(%x.13, %57), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %469 : Long() = prim::NumToTensor(%468), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %470 : int[] = prim::ListConstruct(%465, %466, %468, %467), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.14 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %470), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
  %472 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %473 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%472, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %474 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%473, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.15 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%474, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %476 : Long() = aten::sub(%469, %46, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %477 : int = aten::Int(%476), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %478 : int[] = prim::ListConstruct(%465, %466, %467, %477), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %x.16 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %478), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %480 : Long(13:1) = aten::arange(%464, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %57, %480), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %482 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %483 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%482, %61, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%483, %40), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %485 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %486 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %485), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %487 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%486, %38), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.4, %487, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %57, %54), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
  %490 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %51, %55), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %491 : Tensor[] = prim::ListConstruct(%490, %454), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %492 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %491), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %493 : Tensor[] = prim::ListConstruct(%492, %442), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
  %input.32 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %493), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
  %attn_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.32, %51, %55), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.4, %435, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
  %497 : Tensor = prim::GetAttr[name="bias"](%441)
  %498 : Tensor = prim::GetAttr[name="weight"](%441)
  %499 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm
  %input_tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.33, %499, %498, %497, %34, %35), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %501 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.4, %r.5)
  %502 : Float(13:17408, 17:1024, 1024:1), %503 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%501)
  %504 : __torch__.torch.nn.modules.normalization.___torch_mangle_42756.LayerNorm = prim::GetAttr[name="layer_norm"](%439)
  %505 : __torch__.torch.nn.modules.linear.___torch_mangle_42758.Linear = prim::GetAttr[name="layer_2"](%439)
  %506 : __torch__.torch.nn.modules.linear.___torch_mangle_42757.Linear = prim::GetAttr[name="layer_1"](%439)
  %507 : Tensor = prim::GetAttr[name="bias"](%506)
  %508 : Tensor = prim::GetAttr[name="weight"](%506)
  %509 : Float(1024:1, 4096:1024) = aten::t(%508), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.10 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%502, %509), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.10, %507, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.35 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.34), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # torch/nn/functional.py:1369:0
  %input.36 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.35, %51, %55), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %514 : Tensor = prim::GetAttr[name="bias"](%505)
  %515 : Tensor = prim::GetAttr[name="weight"](%505)
  %516 : Float(4096:1, 1024:4096) = aten::t(%515), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.36, %516), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.37 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.11, %514, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.37, %51, %55), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.12, %502, %62), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
  %521 : Tensor = prim::GetAttr[name="bias"](%504)
  %522 : Tensor = prim::GetAttr[name="weight"](%504)
  %523 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm
  %curr_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.38, %523, %522, %521, %34, %35), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
  %525 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.5, %503)
  %526 : Float(13:17408, 17:1024, 1024:1), %527 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%525)
  %new_mem.5 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%526, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %529 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.5), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %530 : __torch__.transformers.modeling_xlnet.___torch_mangle_42770.XLNetFeedForward = prim::GetAttr[name="ff"](%103)
  %531 : __torch__.transformers.modeling_xlnet.___torch_mangle_42765.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%103)
  %532 : __torch__.torch.nn.modules.normalization.___torch_mangle_42763.LayerNorm = prim::GetAttr[name="layer_norm"](%531)
  %533 : Tensor = prim::GetAttr[name="o"](%531)
  %534 : Tensor = prim::GetAttr[name="r_r_bias"](%531)
  %535 : Tensor = prim::GetAttr[name="r_w_bias"](%531)
  %536 : Tensor = prim::GetAttr[name="r"](%531)
  %537 : Tensor = prim::GetAttr[name="v"](%531)
  %538 : Tensor = prim::GetAttr[name="k"](%531)
  %539 : Tensor = prim::GetAttr[name="q"](%531)
  %540 : Tensor[] = prim::ListConstruct(%526, %539), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %q_head.5 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %540), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %542 : Tensor[] = prim::ListConstruct(%526, %538), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %543 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %542), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %544 : Tensor[] = prim::ListConstruct(%526, %537), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %545 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %544), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %r.6 : Float(26:1024, 17:0, 1024:1) = aten::to(%527, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %547 : Tensor[] = prim::ListConstruct(%r.6, %536), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %548 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %547), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %549 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %535, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
  %550 : Tensor[] = prim::ListConstruct(%549, %543), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %ac.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %550), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %552 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %534, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
  %553 : Tensor[] = prim::ListConstruct(%552, %548), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.17 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %553), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %555 : int = aten::size(%ac.5, %57), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
  %556 : int = aten::size(%x.17, %63), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %557 : int = aten::size(%x.17, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %558 : int = aten::size(%x.17, %58), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %559 : int = aten::size(%x.17, %57), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %560 : Long() = prim::NumToTensor(%559), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %561 : int[] = prim::ListConstruct(%556, %557, %559, %558), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.18 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %561), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
  %563 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %564 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%563, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %565 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%564, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.19 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%565, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %567 : Long() = aten::sub(%560, %46, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %568 : int = aten::Int(%567), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %569 : int[] = prim::ListConstruct(%556, %557, %558, %568), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %x.20 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %569), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %571 : Long(13:1) = aten::arange(%555, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %57, %571), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %573 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %574 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%573, %61, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%574, %40), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %576 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %577 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %576), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %578 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%577, %38), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.5, %578, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %57, %54), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
  %581 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %51, %55), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %582 : Tensor[] = prim::ListConstruct(%581, %545), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %583 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %582), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %584 : Tensor[] = prim::ListConstruct(%583, %533), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
  %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %584), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
  %attn_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.41, %51, %55), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.5, %526, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
  %588 : Tensor = prim::GetAttr[name="bias"](%532)
  %589 : Tensor = prim::GetAttr[name="weight"](%532)
  %590 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm
  %input_tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.42, %590, %589, %588, %34, %35), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %592 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.5, %r.6)
  %593 : Float(13:17408, 17:1024, 1024:1), %594 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%592)
  %595 : __torch__.torch.nn.modules.normalization.___torch_mangle_42766.LayerNorm = prim::GetAttr[name="layer_norm"](%530)
  %596 : __torch__.torch.nn.modules.linear.___torch_mangle_42768.Linear = prim::GetAttr[name="layer_2"](%530)
  %597 : __torch__.torch.nn.modules.linear.___torch_mangle_42767.Linear = prim::GetAttr[name="layer_1"](%530)
  %598 : Tensor = prim::GetAttr[name="bias"](%597)
  %599 : Tensor = prim::GetAttr[name="weight"](%597)
  %600 : Float(1024:1, 4096:1024) = aten::t(%599), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.13 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%593, %600), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.13, %598, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.43), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # torch/nn/functional.py:1369:0
  %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %51, %55), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %605 : Tensor = prim::GetAttr[name="bias"](%596)
  %606 : Tensor = prim::GetAttr[name="weight"](%596)
  %607 : Float(4096:1, 1024:4096) = aten::t(%606), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %607), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %605, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.46, %51, %55), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.15, %593, %62), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
  %612 : Tensor = prim::GetAttr[name="bias"](%595)
  %613 : Tensor = prim::GetAttr[name="weight"](%595)
  %614 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm
  %curr_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %614, %613, %612, %34, %35), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
  %616 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.6, %594)
  %617 : Float(13:17408, 17:1024, 1024:1), %618 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%616)
  %new_mem.6 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%617, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %620 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.6), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %621 : __torch__.transformers.modeling_xlnet.___torch_mangle_42780.XLNetFeedForward = prim::GetAttr[name="ff"](%101)
  %622 : __torch__.transformers.modeling_xlnet.___torch_mangle_42775.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%101)
  %623 : __torch__.torch.nn.modules.normalization.___torch_mangle_42773.LayerNorm = prim::GetAttr[name="layer_norm"](%622)
  %624 : Tensor = prim::GetAttr[name="o"](%622)
  %625 : Tensor = prim::GetAttr[name="r_r_bias"](%622)
  %626 : Tensor = prim::GetAttr[name="r_w_bias"](%622)
  %627 : Tensor = prim::GetAttr[name="r"](%622)
  %628 : Tensor = prim::GetAttr[name="v"](%622)
  %629 : Tensor = prim::GetAttr[name="k"](%622)
  %630 : Tensor = prim::GetAttr[name="q"](%622)
  %631 : Tensor[] = prim::ListConstruct(%617, %630), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %q_head.6 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %631), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %633 : Tensor[] = prim::ListConstruct(%617, %629), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %634 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %633), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %635 : Tensor[] = prim::ListConstruct(%617, %628), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %636 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %635), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %r.7 : Float(26:1024, 17:0, 1024:1) = aten::to(%618, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %638 : Tensor[] = prim::ListConstruct(%r.7, %627), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %639 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %638), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %640 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %626, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
  %641 : Tensor[] = prim::ListConstruct(%640, %634), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %ac.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %641), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %643 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %625, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
  %644 : Tensor[] = prim::ListConstruct(%643, %639), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.21 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %644), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %646 : int = aten::size(%ac.6, %57), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
  %647 : int = aten::size(%x.21, %63), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %648 : int = aten::size(%x.21, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %649 : int = aten::size(%x.21, %58), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %650 : int = aten::size(%x.21, %57), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %651 : Long() = prim::NumToTensor(%650), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %652 : int[] = prim::ListConstruct(%647, %648, %650, %649), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.22 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %652), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
  %654 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %655 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%654, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %656 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%655, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.23 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%656, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %658 : Long() = aten::sub(%651, %46, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %659 : int = aten::Int(%658), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %660 : int[] = prim::ListConstruct(%647, %648, %649, %659), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %x.24 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %660), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %662 : Long(13:1) = aten::arange(%646, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %57, %662), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %664 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %665 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%664, %61, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%665, %40), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %667 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %668 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %667), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %669 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%668, %38), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.48 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.6, %669, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.49 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %57, %54), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
  %672 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %51, %55), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %673 : Tensor[] = prim::ListConstruct(%672, %636), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %674 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %673), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %675 : Tensor[] = prim::ListConstruct(%674, %624), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
  %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %675), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
  %attn_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.50, %51, %55), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.6, %617, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
  %679 : Tensor = prim::GetAttr[name="bias"](%623)
  %680 : Tensor = prim::GetAttr[name="weight"](%623)
  %681 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm
  %input_tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.51, %681, %680, %679, %34, %35), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %683 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.6, %r.7)
  %684 : Float(13:17408, 17:1024, 1024:1), %685 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%683)
  %686 : __torch__.torch.nn.modules.normalization.___torch_mangle_42776.LayerNorm = prim::GetAttr[name="layer_norm"](%621)
  %687 : __torch__.torch.nn.modules.linear.___torch_mangle_42778.Linear = prim::GetAttr[name="layer_2"](%621)
  %688 : __torch__.torch.nn.modules.linear.___torch_mangle_42777.Linear = prim::GetAttr[name="layer_1"](%621)
  %689 : Tensor = prim::GetAttr[name="bias"](%688)
  %690 : Tensor = prim::GetAttr[name="weight"](%688)
  %691 : Float(1024:1, 4096:1024) = aten::t(%690), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.16 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%684, %691), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.52 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.16, %689, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.53 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.52), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # torch/nn/functional.py:1369:0
  %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.53, %51, %55), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %696 : Tensor = prim::GetAttr[name="bias"](%687)
  %697 : Tensor = prim::GetAttr[name="weight"](%687)
  %698 : Float(4096:1, 1024:4096) = aten::t(%697), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.54, %698), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.55 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.17, %696, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.55, %51, %55), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %input.56 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.18, %684, %62), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
  %703 : Tensor = prim::GetAttr[name="bias"](%686)
  %704 : Tensor = prim::GetAttr[name="weight"](%686)
  %705 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm
  %curr_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.56, %705, %704, %703, %34, %35), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
  %707 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.7, %685)
  %708 : Float(13:17408, 17:1024, 1024:1), %709 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%707)
  %new_mem.7 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%708, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %711 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.7), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %712 : __torch__.transformers.modeling_xlnet.___torch_mangle_42790.XLNetFeedForward = prim::GetAttr[name="ff"](%99)
  %713 : __torch__.transformers.modeling_xlnet.___torch_mangle_42785.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%99)
  %714 : __torch__.torch.nn.modules.normalization.___torch_mangle_42783.LayerNorm = prim::GetAttr[name="layer_norm"](%713)
  %715 : Tensor = prim::GetAttr[name="o"](%713)
  %716 : Tensor = prim::GetAttr[name="r_r_bias"](%713)
  %717 : Tensor = prim::GetAttr[name="r_w_bias"](%713)
  %718 : Tensor = prim::GetAttr[name="r"](%713)
  %719 : Tensor = prim::GetAttr[name="v"](%713)
  %720 : Tensor = prim::GetAttr[name="k"](%713)
  %721 : Tensor = prim::GetAttr[name="q"](%713)
  %722 : Tensor[] = prim::ListConstruct(%708, %721), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %q_head.7 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %722), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %724 : Tensor[] = prim::ListConstruct(%708, %720), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %725 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %724), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %726 : Tensor[] = prim::ListConstruct(%708, %719), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %727 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %726), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %r.8 : Float(26:1024, 17:0, 1024:1) = aten::to(%709, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %729 : Tensor[] = prim::ListConstruct(%r.8, %718), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %730 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %729), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %731 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %717, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
  %732 : Tensor[] = prim::ListConstruct(%731, %725), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %ac.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %732), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %734 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %716, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
  %735 : Tensor[] = prim::ListConstruct(%734, %730), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.25 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %735), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %737 : int = aten::size(%ac.7, %57), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
  %738 : int = aten::size(%x.25, %63), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %739 : int = aten::size(%x.25, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %740 : int = aten::size(%x.25, %58), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %741 : int = aten::size(%x.25, %57), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %742 : Long() = prim::NumToTensor(%741), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %743 : int[] = prim::ListConstruct(%738, %739, %741, %740), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.26 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %743), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
  %745 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %746 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%745, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %747 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%746, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.27 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%747, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %749 : Long() = aten::sub(%742, %46, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %750 : int = aten::Int(%749), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %751 : int[] = prim::ListConstruct(%738, %739, %740, %750), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %x.28 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %751), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %753 : Long(13:1) = aten::arange(%737, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %57, %753), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %755 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %756 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%755, %61, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%756, %40), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %758 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %759 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %758), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %760 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%759, %38), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.7, %760, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.58 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %57, %54), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
  %763 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %51, %55), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %764 : Tensor[] = prim::ListConstruct(%763, %727), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %765 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %764), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %766 : Tensor[] = prim::ListConstruct(%765, %715), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
  %input.59 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %766), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
  %attn_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.59, %51, %55), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.7, %708, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
  %770 : Tensor = prim::GetAttr[name="bias"](%714)
  %771 : Tensor = prim::GetAttr[name="weight"](%714)
  %772 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm
  %input_tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.60, %772, %771, %770, %34, %35), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %774 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.7, %r.8)
  %775 : Float(13:17408, 17:1024, 1024:1), %776 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%774)
  %777 : __torch__.torch.nn.modules.normalization.___torch_mangle_42786.LayerNorm = prim::GetAttr[name="layer_norm"](%712)
  %778 : __torch__.torch.nn.modules.linear.___torch_mangle_42788.Linear = prim::GetAttr[name="layer_2"](%712)
  %779 : __torch__.torch.nn.modules.linear.___torch_mangle_42787.Linear = prim::GetAttr[name="layer_1"](%712)
  %780 : Tensor = prim::GetAttr[name="bias"](%779)
  %781 : Tensor = prim::GetAttr[name="weight"](%779)
  %782 : Float(1024:1, 4096:1024) = aten::t(%781), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%775, %782), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.61 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.19, %780, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.62 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # torch/nn/functional.py:1369:0
  %input.63 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.62, %51, %55), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %787 : Tensor = prim::GetAttr[name="bias"](%778)
  %788 : Tensor = prim::GetAttr[name="weight"](%778)
  %789 : Float(4096:1, 1024:4096) = aten::t(%788), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.63, %789), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %787, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.64, %51, %55), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.21, %775, %62), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
  %794 : Tensor = prim::GetAttr[name="bias"](%777)
  %795 : Tensor = prim::GetAttr[name="weight"](%777)
  %796 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm
  %curr_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.65, %796, %795, %794, %34, %35), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
  %798 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.8, %776)
  %799 : Float(13:17408, 17:1024, 1024:1), %800 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%798)
  %new_mem.8 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%799, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %802 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.8), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %803 : __torch__.transformers.modeling_xlnet.___torch_mangle_42800.XLNetFeedForward = prim::GetAttr[name="ff"](%97)
  %804 : __torch__.transformers.modeling_xlnet.___torch_mangle_42795.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%97)
  %805 : __torch__.torch.nn.modules.normalization.___torch_mangle_42793.LayerNorm = prim::GetAttr[name="layer_norm"](%804)
  %806 : Tensor = prim::GetAttr[name="o"](%804)
  %807 : Tensor = prim::GetAttr[name="r_r_bias"](%804)
  %808 : Tensor = prim::GetAttr[name="r_w_bias"](%804)
  %809 : Tensor = prim::GetAttr[name="r"](%804)
  %810 : Tensor = prim::GetAttr[name="v"](%804)
  %811 : Tensor = prim::GetAttr[name="k"](%804)
  %812 : Tensor = prim::GetAttr[name="q"](%804)
  %813 : Tensor[] = prim::ListConstruct(%799, %812), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %q_head.8 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %813), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %815 : Tensor[] = prim::ListConstruct(%799, %811), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %816 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %815), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %817 : Tensor[] = prim::ListConstruct(%799, %810), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %818 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %817), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %r.9 : Float(26:1024, 17:0, 1024:1) = aten::to(%800, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %820 : Tensor[] = prim::ListConstruct(%r.9, %809), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %821 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %820), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %822 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %808, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
  %823 : Tensor[] = prim::ListConstruct(%822, %816), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %ac.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %823), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %825 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %807, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
  %826 : Tensor[] = prim::ListConstruct(%825, %821), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.29 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %826), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %828 : int = aten::size(%ac.8, %57), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
  %829 : int = aten::size(%x.29, %63), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %830 : int = aten::size(%x.29, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %831 : int = aten::size(%x.29, %58), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %832 : int = aten::size(%x.29, %57), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %833 : Long() = prim::NumToTensor(%832), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %834 : int[] = prim::ListConstruct(%829, %830, %832, %831), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.30 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %834), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
  %836 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %837 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%836, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %838 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%837, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.31 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%838, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %840 : Long() = aten::sub(%833, %46, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %841 : int = aten::Int(%840), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %842 : int[] = prim::ListConstruct(%829, %830, %831, %841), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %x.32 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %842), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %844 : Long(13:1) = aten::arange(%828, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %57, %844), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %846 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %847 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%846, %61, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%847, %40), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %849 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %850 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %849), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %851 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%850, %38), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.8, %851, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %57, %54), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
  %854 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %51, %55), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %855 : Tensor[] = prim::ListConstruct(%854, %818), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %856 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %855), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %857 : Tensor[] = prim::ListConstruct(%856, %806), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
  %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %857), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
  %attn_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.68, %51, %55), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.8, %799, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
  %861 : Tensor = prim::GetAttr[name="bias"](%805)
  %862 : Tensor = prim::GetAttr[name="weight"](%805)
  %863 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm
  %input_tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %863, %862, %861, %34, %35), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %865 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.8, %r.9)
  %866 : Float(13:17408, 17:1024, 1024:1), %867 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%865)
  %868 : __torch__.torch.nn.modules.normalization.___torch_mangle_42796.LayerNorm = prim::GetAttr[name="layer_norm"](%803)
  %869 : __torch__.torch.nn.modules.linear.___torch_mangle_42798.Linear = prim::GetAttr[name="layer_2"](%803)
  %870 : __torch__.torch.nn.modules.linear.___torch_mangle_42797.Linear = prim::GetAttr[name="layer_1"](%803)
  %871 : Tensor = prim::GetAttr[name="bias"](%870)
  %872 : Tensor = prim::GetAttr[name="weight"](%870)
  %873 : Float(1024:1, 4096:1024) = aten::t(%872), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.22 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%866, %873), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.70 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.22, %871, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.71 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # torch/nn/functional.py:1369:0
  %input.72 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.71, %51, %55), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %878 : Tensor = prim::GetAttr[name="bias"](%869)
  %879 : Tensor = prim::GetAttr[name="weight"](%869)
  %880 : Float(4096:1, 1024:4096) = aten::t(%879), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %880), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.23, %878, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.73, %51, %55), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.24, %866, %62), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
  %885 : Tensor = prim::GetAttr[name="bias"](%868)
  %886 : Tensor = prim::GetAttr[name="weight"](%868)
  %887 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm
  %curr_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %887, %886, %885, %34, %35), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
  %889 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.9, %867)
  %890 : Float(13:17408, 17:1024, 1024:1), %891 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%889)
  %new_mem.9 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%890, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %893 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.9), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %894 : __torch__.transformers.modeling_xlnet.___torch_mangle_42810.XLNetFeedForward = prim::GetAttr[name="ff"](%95)
  %895 : __torch__.transformers.modeling_xlnet.___torch_mangle_42805.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%95)
  %896 : __torch__.torch.nn.modules.normalization.___torch_mangle_42803.LayerNorm = prim::GetAttr[name="layer_norm"](%895)
  %897 : Tensor = prim::GetAttr[name="o"](%895)
  %898 : Tensor = prim::GetAttr[name="r_r_bias"](%895)
  %899 : Tensor = prim::GetAttr[name="r_w_bias"](%895)
  %900 : Tensor = prim::GetAttr[name="r"](%895)
  %901 : Tensor = prim::GetAttr[name="v"](%895)
  %902 : Tensor = prim::GetAttr[name="k"](%895)
  %903 : Tensor = prim::GetAttr[name="q"](%895)
  %904 : Tensor[] = prim::ListConstruct(%890, %903), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %q_head.9 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %904), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %906 : Tensor[] = prim::ListConstruct(%890, %902), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %907 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %906), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %908 : Tensor[] = prim::ListConstruct(%890, %901), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %909 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %908), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %r.10 : Float(26:1024, 17:0, 1024:1) = aten::to(%891, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %911 : Tensor[] = prim::ListConstruct(%r.10, %900), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %912 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %911), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %913 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %899, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
  %914 : Tensor[] = prim::ListConstruct(%913, %907), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %ac.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %914), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %916 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %898, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
  %917 : Tensor[] = prim::ListConstruct(%916, %912), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.33 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %917), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %919 : int = aten::size(%ac.9, %57), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
  %920 : int = aten::size(%x.33, %63), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %921 : int = aten::size(%x.33, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %922 : int = aten::size(%x.33, %58), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %923 : int = aten::size(%x.33, %57), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %924 : Long() = prim::NumToTensor(%923), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %925 : int[] = prim::ListConstruct(%920, %921, %923, %922), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.34 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %925), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
  %927 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %928 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%927, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %929 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%928, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.35 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%929, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %931 : Long() = aten::sub(%924, %46, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %932 : int = aten::Int(%931), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %933 : int[] = prim::ListConstruct(%920, %921, %922, %932), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %x.36 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %933), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %935 : Long(13:1) = aten::arange(%919, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %57, %935), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %937 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %938 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%937, %61, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%938, %40), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %940 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %941 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %940), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %942 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%941, %38), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.9, %942, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %57, %54), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
  %945 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %51, %55), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %946 : Tensor[] = prim::ListConstruct(%945, %909), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %947 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %946), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %948 : Tensor[] = prim::ListConstruct(%947, %897), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
  %input.77 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %948), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
  %attn_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.77, %51, %55), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.9, %890, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
  %952 : Tensor = prim::GetAttr[name="bias"](%896)
  %953 : Tensor = prim::GetAttr[name="weight"](%896)
  %954 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm
  %input_tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.78, %954, %953, %952, %34, %35), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %956 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.9, %r.10)
  %957 : Float(13:17408, 17:1024, 1024:1), %958 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%956)
  %959 : __torch__.torch.nn.modules.normalization.___torch_mangle_42806.LayerNorm = prim::GetAttr[name="layer_norm"](%894)
  %960 : __torch__.torch.nn.modules.linear.___torch_mangle_42808.Linear = prim::GetAttr[name="layer_2"](%894)
  %961 : __torch__.torch.nn.modules.linear.___torch_mangle_42807.Linear = prim::GetAttr[name="layer_1"](%894)
  %962 : Tensor = prim::GetAttr[name="bias"](%961)
  %963 : Tensor = prim::GetAttr[name="weight"](%961)
  %964 : Float(1024:1, 4096:1024) = aten::t(%963), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.25 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%957, %964), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.25, %962, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.80 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.79), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # torch/nn/functional.py:1369:0
  %input.81 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.80, %51, %55), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %969 : Tensor = prim::GetAttr[name="bias"](%960)
  %970 : Tensor = prim::GetAttr[name="weight"](%960)
  %971 : Float(4096:1, 1024:4096) = aten::t(%970), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.81, %971), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.82 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %969, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.82, %51, %55), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.27, %957, %62), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
  %976 : Tensor = prim::GetAttr[name="bias"](%959)
  %977 : Tensor = prim::GetAttr[name="weight"](%959)
  %978 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm
  %curr_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.83, %978, %977, %976, %34, %35), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
  %980 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.10, %958)
  %981 : Float(13:17408, 17:1024, 1024:1), %982 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%980)
  %new_mem.10 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%981, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %984 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.10), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %985 : __torch__.transformers.modeling_xlnet.___torch_mangle_42820.XLNetFeedForward = prim::GetAttr[name="ff"](%93)
  %986 : __torch__.transformers.modeling_xlnet.___torch_mangle_42815.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%93)
  %987 : __torch__.torch.nn.modules.normalization.___torch_mangle_42813.LayerNorm = prim::GetAttr[name="layer_norm"](%986)
  %988 : Tensor = prim::GetAttr[name="o"](%986)
  %989 : Tensor = prim::GetAttr[name="r_r_bias"](%986)
  %990 : Tensor = prim::GetAttr[name="r_w_bias"](%986)
  %991 : Tensor = prim::GetAttr[name="r"](%986)
  %992 : Tensor = prim::GetAttr[name="v"](%986)
  %993 : Tensor = prim::GetAttr[name="k"](%986)
  %994 : Tensor = prim::GetAttr[name="q"](%986)
  %995 : Tensor[] = prim::ListConstruct(%981, %994), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %q_head.10 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %995), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %997 : Tensor[] = prim::ListConstruct(%981, %993), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %998 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %997), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %999 : Tensor[] = prim::ListConstruct(%981, %992), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1000 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %999), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %r.11 : Float(26:1024, 17:0, 1024:1) = aten::to(%982, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1002 : Tensor[] = prim::ListConstruct(%r.11, %991), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1003 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1002), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1004 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %990, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
  %1005 : Tensor[] = prim::ListConstruct(%1004, %998), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %ac.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1005), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1007 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %989, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
  %1008 : Tensor[] = prim::ListConstruct(%1007, %1003), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.37 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1008), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1010 : int = aten::size(%ac.10, %57), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
  %1011 : int = aten::size(%x.37, %63), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1012 : int = aten::size(%x.37, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1013 : int = aten::size(%x.37, %58), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1014 : int = aten::size(%x.37, %57), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1015 : Long() = prim::NumToTensor(%1014), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1016 : int[] = prim::ListConstruct(%1011, %1012, %1014, %1013), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.38 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %1016), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
  %1018 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1019 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1018, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1020 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1019, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.39 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1020, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1022 : Long() = aten::sub(%1015, %46, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1023 : int = aten::Int(%1022), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1024 : int[] = prim::ListConstruct(%1011, %1012, %1013, %1023), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %x.40 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %1024), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1026 : Long(13:1) = aten::arange(%1010, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %57, %1026), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %1028 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1029 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1028, %61, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1029, %40), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1031 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1032 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1031), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1033 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1032, %38), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.84 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.10, %1033, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %57, %54), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
  %1036 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %51, %55), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %1037 : Tensor[] = prim::ListConstruct(%1036, %1000), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %1038 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1037), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %1039 : Tensor[] = prim::ListConstruct(%1038, %988), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
  %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1039), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
  %attn_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.86, %51, %55), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.10, %981, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
  %1043 : Tensor = prim::GetAttr[name="bias"](%987)
  %1044 : Tensor = prim::GetAttr[name="weight"](%987)
  %1045 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm
  %input_tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.87, %1045, %1044, %1043, %34, %35), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1047 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.10, %r.11)
  %1048 : Float(13:17408, 17:1024, 1024:1), %1049 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1047)
  %1050 : __torch__.torch.nn.modules.normalization.___torch_mangle_42816.LayerNorm = prim::GetAttr[name="layer_norm"](%985)
  %1051 : __torch__.torch.nn.modules.linear.___torch_mangle_42818.Linear = prim::GetAttr[name="layer_2"](%985)
  %1052 : __torch__.torch.nn.modules.linear.___torch_mangle_42817.Linear = prim::GetAttr[name="layer_1"](%985)
  %1053 : Tensor = prim::GetAttr[name="bias"](%1052)
  %1054 : Tensor = prim::GetAttr[name="weight"](%1052)
  %1055 : Float(1024:1, 4096:1024) = aten::t(%1054), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.28 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1048, %1055), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.28, %1053, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.88), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # torch/nn/functional.py:1369:0
  %input.90 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.89, %51, %55), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %1060 : Tensor = prim::GetAttr[name="bias"](%1051)
  %1061 : Tensor = prim::GetAttr[name="weight"](%1051)
  %1062 : Float(4096:1, 1024:4096) = aten::t(%1061), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.90, %1062), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.29, %1060, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.91, %51, %55), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %input.92 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.30, %1048, %62), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
  %1067 : Tensor = prim::GetAttr[name="bias"](%1050)
  %1068 : Tensor = prim::GetAttr[name="weight"](%1050)
  %1069 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm
  %curr_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.92, %1069, %1068, %1067, %34, %35), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
  %1071 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.11, %1049)
  %1072 : Float(13:17408, 17:1024, 1024:1), %1073 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1071)
  %new_mem.11 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1072, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1075 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.11), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1076 : __torch__.transformers.modeling_xlnet.___torch_mangle_42830.XLNetFeedForward = prim::GetAttr[name="ff"](%91)
  %1077 : __torch__.transformers.modeling_xlnet.___torch_mangle_42825.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%91)
  %1078 : __torch__.torch.nn.modules.normalization.___torch_mangle_42823.LayerNorm = prim::GetAttr[name="layer_norm"](%1077)
  %1079 : Tensor = prim::GetAttr[name="o"](%1077)
  %1080 : Tensor = prim::GetAttr[name="r_r_bias"](%1077)
  %1081 : Tensor = prim::GetAttr[name="r_w_bias"](%1077)
  %1082 : Tensor = prim::GetAttr[name="r"](%1077)
  %1083 : Tensor = prim::GetAttr[name="v"](%1077)
  %1084 : Tensor = prim::GetAttr[name="k"](%1077)
  %1085 : Tensor = prim::GetAttr[name="q"](%1077)
  %1086 : Tensor[] = prim::ListConstruct(%1072, %1085), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %q_head.11 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1086), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1088 : Tensor[] = prim::ListConstruct(%1072, %1084), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1089 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1088), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1090 : Tensor[] = prim::ListConstruct(%1072, %1083), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1091 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1090), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %r.12 : Float(26:1024, 17:0, 1024:1) = aten::to(%1073, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1093 : Tensor[] = prim::ListConstruct(%r.12, %1082), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1094 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1093), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1095 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1081, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
  %1096 : Tensor[] = prim::ListConstruct(%1095, %1089), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %ac.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1096), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1098 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1080, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
  %1099 : Tensor[] = prim::ListConstruct(%1098, %1094), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.41 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1099), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1101 : int = aten::size(%ac.11, %57), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
  %1102 : int = aten::size(%x.41, %63), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1103 : int = aten::size(%x.41, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1104 : int = aten::size(%x.41, %58), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1105 : int = aten::size(%x.41, %57), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1106 : Long() = prim::NumToTensor(%1105), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1107 : int[] = prim::ListConstruct(%1102, %1103, %1105, %1104), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.42 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %1107), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
  %1109 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1110 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1109, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1111 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1110, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.43 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1111, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1113 : Long() = aten::sub(%1106, %46, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1114 : int = aten::Int(%1113), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1115 : int[] = prim::ListConstruct(%1102, %1103, %1104, %1114), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %x.44 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %1115), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1117 : Long(13:1) = aten::arange(%1101, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %57, %1117), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %1119 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1119, %61, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1120, %40), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1122 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1123 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1122), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1124 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1123, %38), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.93 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.11, %1124, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.94 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %57, %54), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
  %1127 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %51, %55), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %1128 : Tensor[] = prim::ListConstruct(%1127, %1091), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %1129 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1128), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %1130 : Tensor[] = prim::ListConstruct(%1129, %1079), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
  %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1130), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
  %attn_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.95, %51, %55), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.11, %1072, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
  %1134 : Tensor = prim::GetAttr[name="bias"](%1078)
  %1135 : Tensor = prim::GetAttr[name="weight"](%1078)
  %1136 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm
  %input_tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %1136, %1135, %1134, %34, %35), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1138 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.11, %r.12)
  %1139 : Float(13:17408, 17:1024, 1024:1), %1140 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1138)
  %1141 : __torch__.torch.nn.modules.normalization.___torch_mangle_42826.LayerNorm = prim::GetAttr[name="layer_norm"](%1076)
  %1142 : __torch__.torch.nn.modules.linear.___torch_mangle_42828.Linear = prim::GetAttr[name="layer_2"](%1076)
  %1143 : __torch__.torch.nn.modules.linear.___torch_mangle_42827.Linear = prim::GetAttr[name="layer_1"](%1076)
  %1144 : Tensor = prim::GetAttr[name="bias"](%1143)
  %1145 : Tensor = prim::GetAttr[name="weight"](%1143)
  %1146 : Float(1024:1, 4096:1024) = aten::t(%1145), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.31 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1139, %1146), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.97 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.31, %1144, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.97), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # torch/nn/functional.py:1369:0
  %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.98, %51, %55), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %1151 : Tensor = prim::GetAttr[name="bias"](%1142)
  %1152 : Tensor = prim::GetAttr[name="weight"](%1142)
  %1153 : Float(4096:1, 1024:4096) = aten::t(%1152), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.99, %1153), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.100 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %1151, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.100, %51, %55), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.33, %1139, %62), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
  %1158 : Tensor = prim::GetAttr[name="bias"](%1141)
  %1159 : Tensor = prim::GetAttr[name="weight"](%1141)
  %1160 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm
  %curr_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.101, %1160, %1159, %1158, %34, %35), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
  %1162 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.12, %1140)
  %1163 : Float(13:17408, 17:1024, 1024:1), %1164 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1162)
  %new_mem.12 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1163, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1166 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.12), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1167 : __torch__.transformers.modeling_xlnet.___torch_mangle_42840.XLNetFeedForward = prim::GetAttr[name="ff"](%89)
  %1168 : __torch__.transformers.modeling_xlnet.___torch_mangle_42835.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%89)
  %1169 : __torch__.torch.nn.modules.normalization.___torch_mangle_42833.LayerNorm = prim::GetAttr[name="layer_norm"](%1168)
  %1170 : Tensor = prim::GetAttr[name="o"](%1168)
  %1171 : Tensor = prim::GetAttr[name="r_r_bias"](%1168)
  %1172 : Tensor = prim::GetAttr[name="r_w_bias"](%1168)
  %1173 : Tensor = prim::GetAttr[name="r"](%1168)
  %1174 : Tensor = prim::GetAttr[name="v"](%1168)
  %1175 : Tensor = prim::GetAttr[name="k"](%1168)
  %1176 : Tensor = prim::GetAttr[name="q"](%1168)
  %1177 : Tensor[] = prim::ListConstruct(%1163, %1176), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %q_head.12 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1177), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1179 : Tensor[] = prim::ListConstruct(%1163, %1175), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1180 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1179), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1181 : Tensor[] = prim::ListConstruct(%1163, %1174), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1182 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1181), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %r.13 : Float(26:1024, 17:0, 1024:1) = aten::to(%1164, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1184 : Tensor[] = prim::ListConstruct(%r.13, %1173), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1185 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1184), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1186 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1172, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
  %1187 : Tensor[] = prim::ListConstruct(%1186, %1180), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %ac.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1187), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1189 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1171, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
  %1190 : Tensor[] = prim::ListConstruct(%1189, %1185), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.45 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1190), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1192 : int = aten::size(%ac.12, %57), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
  %1193 : int = aten::size(%x.45, %63), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1194 : int = aten::size(%x.45, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1195 : int = aten::size(%x.45, %58), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1196 : int = aten::size(%x.45, %57), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1197 : Long() = prim::NumToTensor(%1196), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1198 : int[] = prim::ListConstruct(%1193, %1194, %1196, %1195), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.46 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %1198), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
  %1200 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1201 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1200, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1202 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1201, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.47 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1202, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1204 : Long() = aten::sub(%1197, %46, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1205 : int = aten::Int(%1204), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1206 : int[] = prim::ListConstruct(%1193, %1194, %1195, %1205), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %x.48 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %1206), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1208 : Long(13:1) = aten::arange(%1192, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %57, %1208), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %1210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1210, %61, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1211, %40), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1213 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1214 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1213), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1215 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1214, %38), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.12, %1215, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %57, %54), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
  %1218 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %51, %55), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %1219 : Tensor[] = prim::ListConstruct(%1218, %1182), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %1220 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1219), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %1221 : Tensor[] = prim::ListConstruct(%1220, %1170), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
  %input.104 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1221), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
  %attn_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.104, %51, %55), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.12, %1163, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
  %1225 : Tensor = prim::GetAttr[name="bias"](%1169)
  %1226 : Tensor = prim::GetAttr[name="weight"](%1169)
  %1227 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm
  %input_tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.105, %1227, %1226, %1225, %34, %35), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1229 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.12, %r.13)
  %1230 : Float(13:17408, 17:1024, 1024:1), %1231 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1229)
  %1232 : __torch__.torch.nn.modules.normalization.___torch_mangle_42836.LayerNorm = prim::GetAttr[name="layer_norm"](%1167)
  %1233 : __torch__.torch.nn.modules.linear.___torch_mangle_42838.Linear = prim::GetAttr[name="layer_2"](%1167)
  %1234 : __torch__.torch.nn.modules.linear.___torch_mangle_42837.Linear = prim::GetAttr[name="layer_1"](%1167)
  %1235 : Tensor = prim::GetAttr[name="bias"](%1234)
  %1236 : Tensor = prim::GetAttr[name="weight"](%1234)
  %1237 : Float(1024:1, 4096:1024) = aten::t(%1236), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1230, %1237), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.106 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.34, %1235, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.107 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.106), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # torch/nn/functional.py:1369:0
  %input.108 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.107, %51, %55), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %1242 : Tensor = prim::GetAttr[name="bias"](%1233)
  %1243 : Tensor = prim::GetAttr[name="weight"](%1233)
  %1244 : Float(4096:1, 1024:4096) = aten::t(%1243), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.108, %1244), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.35, %1242, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.109, %51, %55), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.36, %1230, %62), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
  %1249 : Tensor = prim::GetAttr[name="bias"](%1232)
  %1250 : Tensor = prim::GetAttr[name="weight"](%1232)
  %1251 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm
  %curr_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.110, %1251, %1250, %1249, %34, %35), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
  %1253 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.13, %1231)
  %1254 : Float(13:17408, 17:1024, 1024:1), %1255 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1253)
  %new_mem.13 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1254, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1257 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.13), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1258 : __torch__.transformers.modeling_xlnet.___torch_mangle_42850.XLNetFeedForward = prim::GetAttr[name="ff"](%87)
  %1259 : __torch__.transformers.modeling_xlnet.___torch_mangle_42845.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%87)
  %1260 : __torch__.torch.nn.modules.normalization.___torch_mangle_42843.LayerNorm = prim::GetAttr[name="layer_norm"](%1259)
  %1261 : Tensor = prim::GetAttr[name="o"](%1259)
  %1262 : Tensor = prim::GetAttr[name="r_r_bias"](%1259)
  %1263 : Tensor = prim::GetAttr[name="r_w_bias"](%1259)
  %1264 : Tensor = prim::GetAttr[name="r"](%1259)
  %1265 : Tensor = prim::GetAttr[name="v"](%1259)
  %1266 : Tensor = prim::GetAttr[name="k"](%1259)
  %1267 : Tensor = prim::GetAttr[name="q"](%1259)
  %1268 : Tensor[] = prim::ListConstruct(%1254, %1267), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %q_head.13 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1268), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1270 : Tensor[] = prim::ListConstruct(%1254, %1266), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1271 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1270), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1272 : Tensor[] = prim::ListConstruct(%1254, %1265), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1273 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1272), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %r.14 : Float(26:1024, 17:0, 1024:1) = aten::to(%1255, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1275 : Tensor[] = prim::ListConstruct(%r.14, %1264), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1276 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1275), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1277 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1263, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
  %1278 : Tensor[] = prim::ListConstruct(%1277, %1271), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %ac.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1278), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1280 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1262, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
  %1281 : Tensor[] = prim::ListConstruct(%1280, %1276), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.49 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1281), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1283 : int = aten::size(%ac.13, %57), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
  %1284 : int = aten::size(%x.49, %63), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1285 : int = aten::size(%x.49, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1286 : int = aten::size(%x.49, %58), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1287 : int = aten::size(%x.49, %57), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1288 : Long() = prim::NumToTensor(%1287), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1289 : int[] = prim::ListConstruct(%1284, %1285, %1287, %1286), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.50 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %1289), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
  %1291 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1292 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1291, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1293 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1292, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.51 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1293, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1295 : Long() = aten::sub(%1288, %46, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1296 : int = aten::Int(%1295), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1297 : int[] = prim::ListConstruct(%1284, %1285, %1286, %1296), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %x.52 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %1297), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1299 : Long(13:1) = aten::arange(%1283, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %57, %1299), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %1301 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1302 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1301, %61, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1302, %40), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1304 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1305 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1304), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1306 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1305, %38), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.111 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.13, %1306, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %57, %54), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
  %1309 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %51, %55), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %1310 : Tensor[] = prim::ListConstruct(%1309, %1273), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %1311 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1310), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %1312 : Tensor[] = prim::ListConstruct(%1311, %1261), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
  %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1312), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
  %attn_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.113, %51, %55), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.13, %1254, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
  %1316 : Tensor = prim::GetAttr[name="bias"](%1260)
  %1317 : Tensor = prim::GetAttr[name="weight"](%1260)
  %1318 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm
  %input_tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.114, %1318, %1317, %1316, %34, %35), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1320 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.13, %r.14)
  %1321 : Float(13:17408, 17:1024, 1024:1), %1322 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1320)
  %1323 : __torch__.torch.nn.modules.normalization.___torch_mangle_42846.LayerNorm = prim::GetAttr[name="layer_norm"](%1258)
  %1324 : __torch__.torch.nn.modules.linear.___torch_mangle_42848.Linear = prim::GetAttr[name="layer_2"](%1258)
  %1325 : __torch__.torch.nn.modules.linear.___torch_mangle_42847.Linear = prim::GetAttr[name="layer_1"](%1258)
  %1326 : Tensor = prim::GetAttr[name="bias"](%1325)
  %1327 : Tensor = prim::GetAttr[name="weight"](%1325)
  %1328 : Float(1024:1, 4096:1024) = aten::t(%1327), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.37 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1321, %1328), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.115 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.37, %1326, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.116 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.115), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # torch/nn/functional.py:1369:0
  %input.117 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.116, %51, %55), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %1333 : Tensor = prim::GetAttr[name="bias"](%1324)
  %1334 : Tensor = prim::GetAttr[name="weight"](%1324)
  %1335 : Float(4096:1, 1024:4096) = aten::t(%1334), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.117, %1335), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %1333, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.118, %51, %55), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.39, %1321, %62), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
  %1340 : Tensor = prim::GetAttr[name="bias"](%1323)
  %1341 : Tensor = prim::GetAttr[name="weight"](%1323)
  %1342 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm
  %curr_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.119, %1342, %1341, %1340, %34, %35), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
  %1344 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.14, %1322)
  %1345 : Float(13:17408, 17:1024, 1024:1), %1346 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1344)
  %new_mem.14 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1345, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1348 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.14), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1349 : __torch__.transformers.modeling_xlnet.___torch_mangle_42860.XLNetFeedForward = prim::GetAttr[name="ff"](%85)
  %1350 : __torch__.transformers.modeling_xlnet.___torch_mangle_42855.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%85)
  %1351 : __torch__.torch.nn.modules.normalization.___torch_mangle_42853.LayerNorm = prim::GetAttr[name="layer_norm"](%1350)
  %1352 : Tensor = prim::GetAttr[name="o"](%1350)
  %1353 : Tensor = prim::GetAttr[name="r_r_bias"](%1350)
  %1354 : Tensor = prim::GetAttr[name="r_w_bias"](%1350)
  %1355 : Tensor = prim::GetAttr[name="r"](%1350)
  %1356 : Tensor = prim::GetAttr[name="v"](%1350)
  %1357 : Tensor = prim::GetAttr[name="k"](%1350)
  %1358 : Tensor = prim::GetAttr[name="q"](%1350)
  %1359 : Tensor[] = prim::ListConstruct(%1345, %1358), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %q_head.14 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1359), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1361 : Tensor[] = prim::ListConstruct(%1345, %1357), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1362 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1361), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1363 : Tensor[] = prim::ListConstruct(%1345, %1356), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1364 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1363), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %r.15 : Float(26:1024, 17:0, 1024:1) = aten::to(%1346, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1366 : Tensor[] = prim::ListConstruct(%r.15, %1355), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1367 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1366), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1368 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1354, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
  %1369 : Tensor[] = prim::ListConstruct(%1368, %1362), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %ac.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1369), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1371 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1353, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
  %1372 : Tensor[] = prim::ListConstruct(%1371, %1367), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.53 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1372), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1374 : int = aten::size(%ac.14, %57), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
  %1375 : int = aten::size(%x.53, %63), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1376 : int = aten::size(%x.53, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1377 : int = aten::size(%x.53, %58), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1378 : int = aten::size(%x.53, %57), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1379 : Long() = prim::NumToTensor(%1378), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1380 : int[] = prim::ListConstruct(%1375, %1376, %1378, %1377), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.54 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %1380), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
  %1382 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1383 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1382, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1384 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1383, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.55 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1384, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1386 : Long() = aten::sub(%1379, %46, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1387 : int = aten::Int(%1386), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1388 : int[] = prim::ListConstruct(%1375, %1376, %1377, %1387), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %x.56 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %1388), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1390 : Long(13:1) = aten::arange(%1374, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %57, %1390), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %1392 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1393 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1392, %61, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1393, %40), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1395 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1396 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1395), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1397 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1396, %38), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.14, %1397, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %57, %54), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
  %1400 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %51, %55), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %1401 : Tensor[] = prim::ListConstruct(%1400, %1364), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %1402 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1401), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %1403 : Tensor[] = prim::ListConstruct(%1402, %1352), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
  %input.122 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1403), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
  %attn_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.122, %51, %55), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.14, %1345, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
  %1407 : Tensor = prim::GetAttr[name="bias"](%1351)
  %1408 : Tensor = prim::GetAttr[name="weight"](%1351)
  %1409 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm
  %input_tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.123, %1409, %1408, %1407, %34, %35), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1411 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.14, %r.15)
  %1412 : Float(13:17408, 17:1024, 1024:1), %1413 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1411)
  %1414 : __torch__.torch.nn.modules.normalization.___torch_mangle_42856.LayerNorm = prim::GetAttr[name="layer_norm"](%1349)
  %1415 : __torch__.torch.nn.modules.linear.___torch_mangle_42858.Linear = prim::GetAttr[name="layer_2"](%1349)
  %1416 : __torch__.torch.nn.modules.linear.___torch_mangle_42857.Linear = prim::GetAttr[name="layer_1"](%1349)
  %1417 : Tensor = prim::GetAttr[name="bias"](%1416)
  %1418 : Tensor = prim::GetAttr[name="weight"](%1416)
  %1419 : Float(1024:1, 4096:1024) = aten::t(%1418), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.40 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1412, %1419), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.124 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.40, %1417, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.125 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.124), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # torch/nn/functional.py:1369:0
  %input.126 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.125, %51, %55), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %1424 : Tensor = prim::GetAttr[name="bias"](%1415)
  %1425 : Tensor = prim::GetAttr[name="weight"](%1415)
  %1426 : Float(4096:1, 1024:4096) = aten::t(%1425), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.126, %1426), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.41, %1424, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.127, %51, %55), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.42, %1412, %62), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
  %1431 : Tensor = prim::GetAttr[name="bias"](%1414)
  %1432 : Tensor = prim::GetAttr[name="weight"](%1414)
  %1433 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm
  %curr_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.128, %1433, %1432, %1431, %34, %35), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
  %1435 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.15, %1413)
  %1436 : Float(13:17408, 17:1024, 1024:1), %1437 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1435)
  %new_mem.15 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1436, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1439 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.15), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1440 : __torch__.transformers.modeling_xlnet.___torch_mangle_42870.XLNetFeedForward = prim::GetAttr[name="ff"](%83)
  %1441 : __torch__.transformers.modeling_xlnet.___torch_mangle_42865.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%83)
  %1442 : __torch__.torch.nn.modules.normalization.___torch_mangle_42863.LayerNorm = prim::GetAttr[name="layer_norm"](%1441)
  %1443 : Tensor = prim::GetAttr[name="o"](%1441)
  %1444 : Tensor = prim::GetAttr[name="r_r_bias"](%1441)
  %1445 : Tensor = prim::GetAttr[name="r_w_bias"](%1441)
  %1446 : Tensor = prim::GetAttr[name="r"](%1441)
  %1447 : Tensor = prim::GetAttr[name="v"](%1441)
  %1448 : Tensor = prim::GetAttr[name="k"](%1441)
  %1449 : Tensor = prim::GetAttr[name="q"](%1441)
  %1450 : Tensor[] = prim::ListConstruct(%1436, %1449), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %q_head.15 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1450), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1452 : Tensor[] = prim::ListConstruct(%1436, %1448), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1453 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1452), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1454 : Tensor[] = prim::ListConstruct(%1436, %1447), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1455 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1454), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %r.16 : Float(26:1024, 17:0, 1024:1) = aten::to(%1437, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %1457 : Tensor[] = prim::ListConstruct(%r.16, %1446), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1458 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1457), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1459 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %1445, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
  %1460 : Tensor[] = prim::ListConstruct(%1459, %1453), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %ac.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1460), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1462 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %1444, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
  %1463 : Tensor[] = prim::ListConstruct(%1462, %1458), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.57 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1463), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1465 : int = aten::size(%ac.15, %57), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
  %1466 : int = aten::size(%x.57, %63), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1467 : int = aten::size(%x.57, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1468 : int = aten::size(%x.57, %58), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1469 : int = aten::size(%x.57, %57), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1470 : Long() = prim::NumToTensor(%1469), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1471 : int[] = prim::ListConstruct(%1466, %1467, %1469, %1468), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.58 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %1471), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
  %1473 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1474 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1473, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1475 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1474, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.59 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1475, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1477 : Long() = aten::sub(%1470, %46, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1478 : int = aten::Int(%1477), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1479 : int[] = prim::ListConstruct(%1466, %1467, %1468, %1478), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %x.60 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %1479), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1481 : Long(13:1) = aten::arange(%1465, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %57, %1481), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %1483 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1484 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1483, %61, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1484, %40), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1486 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1487 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1486), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1488 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1487, %38), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.15, %1488, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %57, %54), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
  %1491 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %51, %55), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %1492 : Tensor[] = prim::ListConstruct(%1491, %1455), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %1493 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1492), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %1494 : Tensor[] = prim::ListConstruct(%1493, %1443), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
  %input.131 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1494), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
  %attn_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.131, %51, %55), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.132 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.15, %1436, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
  %1498 : Tensor = prim::GetAttr[name="bias"](%1442)
  %1499 : Tensor = prim::GetAttr[name="weight"](%1442)
  %1500 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm
  %input_tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.132, %1500, %1499, %1498, %34, %35), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1502 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.15, %r.16)
  %1503 : Float(13:17408, 17:1024, 1024:1), %1504 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1502)
  %1505 : __torch__.torch.nn.modules.normalization.___torch_mangle_42866.LayerNorm = prim::GetAttr[name="layer_norm"](%1440)
  %1506 : __torch__.torch.nn.modules.linear.___torch_mangle_42868.Linear = prim::GetAttr[name="layer_2"](%1440)
  %1507 : __torch__.torch.nn.modules.linear.___torch_mangle_42867.Linear = prim::GetAttr[name="layer_1"](%1440)
  %1508 : Tensor = prim::GetAttr[name="bias"](%1507)
  %1509 : Tensor = prim::GetAttr[name="weight"](%1507)
  %1510 : Float(1024:1, 4096:1024) = aten::t(%1509), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.43 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1503, %1510), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.43, %1508, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.134 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.133), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # torch/nn/functional.py:1369:0
  %input.135 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.134, %51, %55), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %1515 : Tensor = prim::GetAttr[name="bias"](%1506)
  %1516 : Tensor = prim::GetAttr[name="weight"](%1506)
  %1517 : Float(4096:1, 1024:4096) = aten::t(%1516), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.135, %1517), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %1515, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.136, %51, %55), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.45, %1503, %62), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
  %1522 : Tensor = prim::GetAttr[name="bias"](%1505)
  %1523 : Tensor = prim::GetAttr[name="weight"](%1505)
  %1524 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm
  %curr_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.137, %1524, %1523, %1522, %34, %35), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
  %1526 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.16, %1504)
  %1527 : Float(13:17408, 17:1024, 1024:1), %1528 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1526)
  %new_mem.16 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1527, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1530 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.16), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1531 : __torch__.transformers.modeling_xlnet.___torch_mangle_42880.XLNetFeedForward = prim::GetAttr[name="ff"](%81)
  %1532 : __torch__.transformers.modeling_xlnet.___torch_mangle_42875.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%81)
  %1533 : __torch__.torch.nn.modules.normalization.___torch_mangle_42873.LayerNorm = prim::GetAttr[name="layer_norm"](%1532)
  %1534 : Tensor = prim::GetAttr[name="o"](%1532)
  %1535 : Tensor = prim::GetAttr[name="r_r_bias"](%1532)
  %1536 : Tensor = prim::GetAttr[name="r_w_bias"](%1532)
  %1537 : Tensor = prim::GetAttr[name="r"](%1532)
  %1538 : Tensor = prim::GetAttr[name="v"](%1532)
  %1539 : Tensor = prim::GetAttr[name="k"](%1532)
  %1540 : Tensor = prim::GetAttr[name="q"](%1532)
  %1541 : Tensor[] = prim::ListConstruct(%1527, %1540), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %q_head.16 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1541), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1543 : Tensor[] = prim::ListConstruct(%1527, %1539), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1544 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1543), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1545 : Tensor[] = prim::ListConstruct(%1527, %1538), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1546 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1545), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %r.17 : Float(26:1024, 17:0, 1024:1) = aten::to(%1528, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %1548 : Tensor[] = prim::ListConstruct(%r.17, %1537), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1549 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1548), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1550 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %1536, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
  %1551 : Tensor[] = prim::ListConstruct(%1550, %1544), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %ac.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1551), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1553 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %1535, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
  %1554 : Tensor[] = prim::ListConstruct(%1553, %1549), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.61 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1554), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1556 : int = aten::size(%ac.16, %57), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
  %1557 : int = aten::size(%x.61, %63), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1558 : int = aten::size(%x.61, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1559 : int = aten::size(%x.61, %58), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1560 : int = aten::size(%x.61, %57), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %1561 : Long() = prim::NumToTensor(%1560), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1562 : int[] = prim::ListConstruct(%1557, %1558, %1560, %1559), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.62 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %1562), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
  %1564 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1565 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1564, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1566 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1565, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.63 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1566, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %1568 : Long() = aten::sub(%1561, %46, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1569 : int = aten::Int(%1568), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1570 : int[] = prim::ListConstruct(%1557, %1558, %1559, %1569), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %x.64 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %1570), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %1572 : Long(13:1) = aten::arange(%1556, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %57, %1572), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %1574 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1575 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1574, %61, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1575, %40), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %1577 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1578 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1577), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1579 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1578, %38), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.138 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.16, %1579, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.139 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %57, %54), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
  %1582 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %51, %55), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %1583 : Tensor[] = prim::ListConstruct(%1582, %1546), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %1584 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1583), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %1585 : Tensor[] = prim::ListConstruct(%1584, %1534), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
  %input.140 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1585), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
  %attn_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.140, %51, %55), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.16, %1527, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
  %1589 : Tensor = prim::GetAttr[name="bias"](%1533)
  %1590 : Tensor = prim::GetAttr[name="weight"](%1533)
  %1591 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm
  %input_tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.141, %1591, %1590, %1589, %34, %35), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1593 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.16, %r.17)
  %1594 : Float(13:17408, 17:1024, 1024:1), %1595 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1593)
  %1596 : __torch__.torch.nn.modules.normalization.___torch_mangle_42876.LayerNorm = prim::GetAttr[name="layer_norm"](%1531)
  %1597 : __torch__.torch.nn.modules.linear.___torch_mangle_42878.Linear = prim::GetAttr[name="layer_2"](%1531)
  %1598 : __torch__.torch.nn.modules.linear.___torch_mangle_42877.Linear = prim::GetAttr[name="layer_1"](%1531)
  %1599 : Tensor = prim::GetAttr[name="bias"](%1598)
  %1600 : Tensor = prim::GetAttr[name="weight"](%1598)
  %1601 : Float(1024:1, 4096:1024) = aten::t(%1600), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.46 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1594, %1601), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.142 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.46, %1599, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.143 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.142), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # torch/nn/functional.py:1369:0
  %input.144 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.143, %51, %55), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %1606 : Tensor = prim::GetAttr[name="bias"](%1597)
  %1607 : Tensor = prim::GetAttr[name="weight"](%1597)
  %1608 : Float(4096:1, 1024:4096) = aten::t(%1607), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1608), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.145 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.47, %1606, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.145, %51, %55), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.48, %1594, %62), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
  %1613 : Tensor = prim::GetAttr[name="bias"](%1596)
  %1614 : Tensor = prim::GetAttr[name="weight"](%1596)
  %1615 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm
  %curr_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.146, %1615, %1614, %1613, %34, %35), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
  %1617 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.17, %1595)
  %1618 : Float(13:17408, 17:1024, 1024:1), %1619 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1617)
  %new_mem.17 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1618, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1621 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.17), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1622 : __torch__.transformers.modeling_xlnet.___torch_mangle_42890.XLNetFeedForward = prim::GetAttr[name="ff"](%79)
  %1623 : __torch__.transformers.modeling_xlnet.___torch_mangle_42885.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%79)
  %1624 : __torch__.torch.nn.modules.normalization.___torch_mangle_42883.LayerNorm = prim::GetAttr[name="layer_norm"](%1623)
  %1625 : Tensor = prim::GetAttr[name="o"](%1623)
  %1626 : Tensor = prim::GetAttr[name="r_r_bias"](%1623)
  %1627 : Tensor = prim::GetAttr[name="r_w_bias"](%1623)
  %1628 : Tensor = prim::GetAttr[name="r"](%1623)
  %1629 : Tensor = prim::GetAttr[name="v"](%1623)
  %1630 : Tensor = prim::GetAttr[name="k"](%1623)
  %1631 : Tensor = prim::GetAttr[name="q"](%1623)
  %1632 : Tensor[] = prim::ListConstruct(%1618, %1631), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %q_head.17 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1632), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1634 : Tensor[] = prim::ListConstruct(%1618, %1630), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1635 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1634), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1636 : Tensor[] = prim::ListConstruct(%1618, %1629), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1637 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1636), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %r.18 : Float(26:1024, 17:0, 1024:1) = aten::to(%1619, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %1639 : Tensor[] = prim::ListConstruct(%r.18, %1628), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1640 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1639), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1641 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %1627, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
  %1642 : Tensor[] = prim::ListConstruct(%1641, %1635), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %ac.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1642), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1644 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %1626, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
  %1645 : Tensor[] = prim::ListConstruct(%1644, %1640), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.65 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1645), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1647 : int = aten::size(%ac.17, %57), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
  %1648 : int = aten::size(%x.65, %63), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1649 : int = aten::size(%x.65, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1650 : int = aten::size(%x.65, %58), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1651 : int = aten::size(%x.65, %57), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %1652 : Long() = prim::NumToTensor(%1651), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1653 : int[] = prim::ListConstruct(%1648, %1649, %1651, %1650), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.66 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %1653), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
  %1655 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1656 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1655, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1657 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1656, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.67 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1657, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %1659 : Long() = aten::sub(%1652, %46, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1660 : int = aten::Int(%1659), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1661 : int[] = prim::ListConstruct(%1648, %1649, %1650, %1660), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %x.68 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %1661), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %1663 : Long(13:1) = aten::arange(%1647, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %57, %1663), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %1665 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1666 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1665, %61, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1666, %40), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %1668 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1669 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1668), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1670 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1669, %38), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.147 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.17, %1670, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.148 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %57, %54), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
  %1673 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %51, %55), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %1674 : Tensor[] = prim::ListConstruct(%1673, %1637), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %1675 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1674), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %1676 : Tensor[] = prim::ListConstruct(%1675, %1625), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
  %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1676), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
  %attn_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.149, %51, %55), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.17, %1618, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
  %1680 : Tensor = prim::GetAttr[name="bias"](%1624)
  %1681 : Tensor = prim::GetAttr[name="weight"](%1624)
  %1682 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm
  %input_tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.150, %1682, %1681, %1680, %34, %35), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1684 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.17, %r.18)
  %1685 : Float(13:17408, 17:1024, 1024:1), %1686 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1684)
  %1687 : __torch__.torch.nn.modules.normalization.___torch_mangle_42886.LayerNorm = prim::GetAttr[name="layer_norm"](%1622)
  %1688 : __torch__.torch.nn.modules.linear.___torch_mangle_42888.Linear = prim::GetAttr[name="layer_2"](%1622)
  %1689 : __torch__.torch.nn.modules.linear.___torch_mangle_42887.Linear = prim::GetAttr[name="layer_1"](%1622)
  %1690 : Tensor = prim::GetAttr[name="bias"](%1689)
  %1691 : Tensor = prim::GetAttr[name="weight"](%1689)
  %1692 : Float(1024:1, 4096:1024) = aten::t(%1691), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1685, %1692), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.49, %1690, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.151), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # torch/nn/functional.py:1369:0
  %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %51, %55), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %1697 : Tensor = prim::GetAttr[name="bias"](%1688)
  %1698 : Tensor = prim::GetAttr[name="weight"](%1688)
  %1699 : Float(4096:1, 1024:4096) = aten::t(%1698), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %1699), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %1697, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.154, %51, %55), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.51, %1685, %62), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
  %1704 : Tensor = prim::GetAttr[name="bias"](%1687)
  %1705 : Tensor = prim::GetAttr[name="weight"](%1687)
  %1706 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm
  %curr_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %1706, %1705, %1704, %34, %35), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
  %1708 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.18, %1686)
  %1709 : Float(13:17408, 17:1024, 1024:1), %1710 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1708)
  %new_mem.18 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1709, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1712 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.18), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1713 : __torch__.transformers.modeling_xlnet.___torch_mangle_42900.XLNetFeedForward = prim::GetAttr[name="ff"](%77)
  %1714 : __torch__.transformers.modeling_xlnet.___torch_mangle_42895.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%77)
  %1715 : __torch__.torch.nn.modules.normalization.___torch_mangle_42893.LayerNorm = prim::GetAttr[name="layer_norm"](%1714)
  %1716 : Tensor = prim::GetAttr[name="o"](%1714)
  %1717 : Tensor = prim::GetAttr[name="r_r_bias"](%1714)
  %1718 : Tensor = prim::GetAttr[name="r_w_bias"](%1714)
  %1719 : Tensor = prim::GetAttr[name="r"](%1714)
  %1720 : Tensor = prim::GetAttr[name="v"](%1714)
  %1721 : Tensor = prim::GetAttr[name="k"](%1714)
  %1722 : Tensor = prim::GetAttr[name="q"](%1714)
  %1723 : Tensor[] = prim::ListConstruct(%1709, %1722), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %q_head.18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1723), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1725 : Tensor[] = prim::ListConstruct(%1709, %1721), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1726 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1725), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1727 : Tensor[] = prim::ListConstruct(%1709, %1720), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1728 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1727), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %r.19 : Float(26:1024, 17:0, 1024:1) = aten::to(%1710, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %1730 : Tensor[] = prim::ListConstruct(%r.19, %1719), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1731 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1730), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1732 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %1718, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
  %1733 : Tensor[] = prim::ListConstruct(%1732, %1726), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %ac.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1733), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1735 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %1717, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
  %1736 : Tensor[] = prim::ListConstruct(%1735, %1731), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.69 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1736), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1738 : int = aten::size(%ac.18, %57), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
  %1739 : int = aten::size(%x.69, %63), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1740 : int = aten::size(%x.69, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1741 : int = aten::size(%x.69, %58), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1742 : int = aten::size(%x.69, %57), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %1743 : Long() = prim::NumToTensor(%1742), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1744 : int[] = prim::ListConstruct(%1739, %1740, %1742, %1741), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.70 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %1744), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
  %1746 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1747 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1746, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1748 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1747, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.71 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1748, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %1750 : Long() = aten::sub(%1743, %46, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1751 : int = aten::Int(%1750), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1752 : int[] = prim::ListConstruct(%1739, %1740, %1741, %1751), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %x.72 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %1752), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %1754 : Long(13:1) = aten::arange(%1738, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %57, %1754), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %1756 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1757 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1756, %61, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1757, %40), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %1759 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1760 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1759), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1761 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1760, %38), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.18, %1761, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.157 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %57, %54), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
  %1764 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %51, %55), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %1765 : Tensor[] = prim::ListConstruct(%1764, %1728), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %1766 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1765), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %1767 : Tensor[] = prim::ListConstruct(%1766, %1716), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
  %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1767), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
  %attn_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.158, %51, %55), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.18, %1709, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
  %1771 : Tensor = prim::GetAttr[name="bias"](%1715)
  %1772 : Tensor = prim::GetAttr[name="weight"](%1715)
  %1773 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm
  %input_tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.159, %1773, %1772, %1771, %34, %35), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1775 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.18, %r.19)
  %1776 : Float(13:17408, 17:1024, 1024:1), %1777 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1775)
  %1778 : __torch__.torch.nn.modules.normalization.___torch_mangle_42896.LayerNorm = prim::GetAttr[name="layer_norm"](%1713)
  %1779 : __torch__.torch.nn.modules.linear.___torch_mangle_42898.Linear = prim::GetAttr[name="layer_2"](%1713)
  %1780 : __torch__.torch.nn.modules.linear.___torch_mangle_42897.Linear = prim::GetAttr[name="layer_1"](%1713)
  %1781 : Tensor = prim::GetAttr[name="bias"](%1780)
  %1782 : Tensor = prim::GetAttr[name="weight"](%1780)
  %1783 : Float(1024:1, 4096:1024) = aten::t(%1782), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.52 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1776, %1783), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.160 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.52, %1781, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.161 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.160), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # torch/nn/functional.py:1369:0
  %input.162 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.161, %51, %55), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %1788 : Tensor = prim::GetAttr[name="bias"](%1779)
  %1789 : Tensor = prim::GetAttr[name="weight"](%1779)
  %1790 : Float(4096:1, 1024:4096) = aten::t(%1789), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.162, %1790), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.53, %1788, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.163, %51, %55), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.54, %1776, %62), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
  %1795 : Tensor = prim::GetAttr[name="bias"](%1778)
  %1796 : Tensor = prim::GetAttr[name="weight"](%1778)
  %1797 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm
  %curr_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.164, %1797, %1796, %1795, %34, %35), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
  %1799 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.19, %1777)
  %1800 : Float(13:17408, 17:1024, 1024:1), %1801 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1799)
  %new_mem.19 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1800, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1803 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.19), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1804 : __torch__.transformers.modeling_xlnet.___torch_mangle_42910.XLNetFeedForward = prim::GetAttr[name="ff"](%75)
  %1805 : __torch__.transformers.modeling_xlnet.___torch_mangle_42905.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%75)
  %1806 : __torch__.torch.nn.modules.normalization.___torch_mangle_42903.LayerNorm = prim::GetAttr[name="layer_norm"](%1805)
  %1807 : Tensor = prim::GetAttr[name="o"](%1805)
  %1808 : Tensor = prim::GetAttr[name="r_r_bias"](%1805)
  %1809 : Tensor = prim::GetAttr[name="r_w_bias"](%1805)
  %1810 : Tensor = prim::GetAttr[name="r"](%1805)
  %1811 : Tensor = prim::GetAttr[name="v"](%1805)
  %1812 : Tensor = prim::GetAttr[name="k"](%1805)
  %1813 : Tensor = prim::GetAttr[name="q"](%1805)
  %1814 : Tensor[] = prim::ListConstruct(%1800, %1813), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %q_head.19 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1814), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1816 : Tensor[] = prim::ListConstruct(%1800, %1812), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1817 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1816), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1818 : Tensor[] = prim::ListConstruct(%1800, %1811), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1819 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1818), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %r.20 : Float(26:1024, 17:0, 1024:1) = aten::to(%1801, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %1821 : Tensor[] = prim::ListConstruct(%r.20, %1810), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1822 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1821), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1823 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %1809, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
  %1824 : Tensor[] = prim::ListConstruct(%1823, %1817), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %ac.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1824), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1826 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %1808, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
  %1827 : Tensor[] = prim::ListConstruct(%1826, %1822), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.73 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1827), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1829 : int = aten::size(%ac.19, %57), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
  %1830 : int = aten::size(%x.73, %63), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1831 : int = aten::size(%x.73, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1832 : int = aten::size(%x.73, %58), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1833 : int = aten::size(%x.73, %57), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %1834 : Long() = prim::NumToTensor(%1833), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1835 : int[] = prim::ListConstruct(%1830, %1831, %1833, %1832), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.74 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %1835), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
  %1837 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1838 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1837, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1839 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1838, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.75 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1839, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %1841 : Long() = aten::sub(%1834, %46, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1842 : int = aten::Int(%1841), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1843 : int[] = prim::ListConstruct(%1830, %1831, %1832, %1842), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %x.76 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %1843), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %1845 : Long(13:1) = aten::arange(%1829, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %57, %1845), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %1847 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1848 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1847, %61, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1848, %40), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %1850 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1851 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1850), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1852 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1851, %38), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.19, %1852, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %57, %54), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
  %1855 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %51, %55), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %1856 : Tensor[] = prim::ListConstruct(%1855, %1819), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %1857 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1856), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %1858 : Tensor[] = prim::ListConstruct(%1857, %1807), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
  %input.167 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1858), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
  %attn_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.167, %51, %55), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.168 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.19, %1800, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
  %1862 : Tensor = prim::GetAttr[name="bias"](%1806)
  %1863 : Tensor = prim::GetAttr[name="weight"](%1806)
  %1864 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm
  %input_tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.168, %1864, %1863, %1862, %34, %35), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1866 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.19, %r.20)
  %1867 : Float(13:17408, 17:1024, 1024:1), %1868 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1866)
  %1869 : __torch__.torch.nn.modules.normalization.___torch_mangle_42906.LayerNorm = prim::GetAttr[name="layer_norm"](%1804)
  %1870 : __torch__.torch.nn.modules.linear.___torch_mangle_42908.Linear = prim::GetAttr[name="layer_2"](%1804)
  %1871 : __torch__.torch.nn.modules.linear.___torch_mangle_42907.Linear = prim::GetAttr[name="layer_1"](%1804)
  %1872 : Tensor = prim::GetAttr[name="bias"](%1871)
  %1873 : Tensor = prim::GetAttr[name="weight"](%1871)
  %1874 : Float(1024:1, 4096:1024) = aten::t(%1873), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.55 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1867, %1874), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.55, %1872, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.170 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.169), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # torch/nn/functional.py:1369:0
  %input.171 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.170, %51, %55), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %1879 : Tensor = prim::GetAttr[name="bias"](%1870)
  %1880 : Tensor = prim::GetAttr[name="weight"](%1870)
  %1881 : Float(4096:1, 1024:4096) = aten::t(%1880), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.171, %1881), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.172 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %1879, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.172, %51, %55), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %input.173 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.57, %1867, %62), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
  %1886 : Tensor = prim::GetAttr[name="bias"](%1869)
  %1887 : Tensor = prim::GetAttr[name="weight"](%1869)
  %1888 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm
  %curr_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.173, %1888, %1887, %1886, %34, %35), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
  %1890 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.20, %1868)
  %1891 : Float(13:17408, 17:1024, 1024:1), %1892 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1890)
  %new_mem.20 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1891, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1894 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.20), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1895 : __torch__.transformers.modeling_xlnet.___torch_mangle_42920.XLNetFeedForward = prim::GetAttr[name="ff"](%73)
  %1896 : __torch__.transformers.modeling_xlnet.___torch_mangle_42915.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%73)
  %1897 : __torch__.torch.nn.modules.normalization.___torch_mangle_42913.LayerNorm = prim::GetAttr[name="layer_norm"](%1896)
  %1898 : Tensor = prim::GetAttr[name="o"](%1896)
  %1899 : Tensor = prim::GetAttr[name="r_r_bias"](%1896)
  %1900 : Tensor = prim::GetAttr[name="r_w_bias"](%1896)
  %1901 : Tensor = prim::GetAttr[name="r"](%1896)
  %1902 : Tensor = prim::GetAttr[name="v"](%1896)
  %1903 : Tensor = prim::GetAttr[name="k"](%1896)
  %1904 : Tensor = prim::GetAttr[name="q"](%1896)
  %1905 : Tensor[] = prim::ListConstruct(%1891, %1904), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %q_head.20 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1905), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1907 : Tensor[] = prim::ListConstruct(%1891, %1903), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1908 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1907), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1909 : Tensor[] = prim::ListConstruct(%1891, %1902), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1910 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1909), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %r.21 : Float(26:1024, 17:0, 1024:1) = aten::to(%1892, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %1912 : Tensor[] = prim::ListConstruct(%r.21, %1901), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1913 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1912), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1914 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %1900, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
  %1915 : Tensor[] = prim::ListConstruct(%1914, %1908), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %ac.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %1915), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1917 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %1899, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
  %1918 : Tensor[] = prim::ListConstruct(%1917, %1913), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.77 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %1918), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1920 : int = aten::size(%ac.20, %57), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
  %1921 : int = aten::size(%x.77, %63), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1922 : int = aten::size(%x.77, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1923 : int = aten::size(%x.77, %58), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1924 : int = aten::size(%x.77, %57), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %1925 : Long() = prim::NumToTensor(%1924), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1926 : int[] = prim::ListConstruct(%1921, %1922, %1924, %1923), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.78 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %1926), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
  %1928 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1929 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1928, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1930 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1929, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.79 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1930, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %1932 : Long() = aten::sub(%1925, %46, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %1933 : int = aten::Int(%1932), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1934 : int[] = prim::ListConstruct(%1921, %1922, %1923, %1933), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %x.80 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %1934), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %1936 : Long(13:1) = aten::arange(%1920, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %57, %1936), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %1938 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %1939 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1938, %61, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1939, %40), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %1941 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1942 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %1941), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1943 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1942, %38), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.174 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.20, %1943, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %57, %54), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
  %1946 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %51, %55), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %1947 : Tensor[] = prim::ListConstruct(%1946, %1910), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %1948 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %1947), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %1949 : Tensor[] = prim::ListConstruct(%1948, %1898), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
  %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %1949), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
  %attn_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.176, %51, %55), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.177 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.20, %1891, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
  %1953 : Tensor = prim::GetAttr[name="bias"](%1897)
  %1954 : Tensor = prim::GetAttr[name="weight"](%1897)
  %1955 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm
  %input_tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.177, %1955, %1954, %1953, %34, %35), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1957 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.20, %r.21)
  %1958 : Float(13:17408, 17:1024, 1024:1), %1959 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1957)
  %1960 : __torch__.torch.nn.modules.normalization.___torch_mangle_42916.LayerNorm = prim::GetAttr[name="layer_norm"](%1895)
  %1961 : __torch__.torch.nn.modules.linear.___torch_mangle_42918.Linear = prim::GetAttr[name="layer_2"](%1895)
  %1962 : __torch__.torch.nn.modules.linear.___torch_mangle_42917.Linear = prim::GetAttr[name="layer_1"](%1895)
  %1963 : Tensor = prim::GetAttr[name="bias"](%1962)
  %1964 : Tensor = prim::GetAttr[name="weight"](%1962)
  %1965 : Float(1024:1, 4096:1024) = aten::t(%1964), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.58 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1958, %1965), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.178 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.58, %1963, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.179 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.178), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # torch/nn/functional.py:1369:0
  %input.180 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.179, %51, %55), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %1970 : Tensor = prim::GetAttr[name="bias"](%1961)
  %1971 : Tensor = prim::GetAttr[name="weight"](%1961)
  %1972 : Float(4096:1, 1024:4096) = aten::t(%1971), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.180, %1972), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.59, %1970, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.181, %51, %55), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.60, %1958, %62), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
  %1977 : Tensor = prim::GetAttr[name="bias"](%1960)
  %1978 : Tensor = prim::GetAttr[name="weight"](%1960)
  %1979 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm
  %curr_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.182, %1979, %1978, %1977, %34, %35), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
  %1981 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.21, %1959)
  %1982 : Float(13:17408, 17:1024, 1024:1), %1983 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1981)
  %new_mem.21 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%1982, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %1985 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.21), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %1986 : __torch__.transformers.modeling_xlnet.___torch_mangle_42930.XLNetFeedForward = prim::GetAttr[name="ff"](%71)
  %1987 : __torch__.transformers.modeling_xlnet.___torch_mangle_42925.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%71)
  %1988 : __torch__.torch.nn.modules.normalization.___torch_mangle_42923.LayerNorm = prim::GetAttr[name="layer_norm"](%1987)
  %1989 : Tensor = prim::GetAttr[name="o"](%1987)
  %1990 : Tensor = prim::GetAttr[name="r_r_bias"](%1987)
  %1991 : Tensor = prim::GetAttr[name="r_w_bias"](%1987)
  %1992 : Tensor = prim::GetAttr[name="r"](%1987)
  %1993 : Tensor = prim::GetAttr[name="v"](%1987)
  %1994 : Tensor = prim::GetAttr[name="k"](%1987)
  %1995 : Tensor = prim::GetAttr[name="q"](%1987)
  %1996 : Tensor[] = prim::ListConstruct(%1982, %1995), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %q_head.21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1996), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %1998 : Tensor[] = prim::ListConstruct(%1982, %1994), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %1999 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %1998), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2000 : Tensor[] = prim::ListConstruct(%1982, %1993), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2001 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2000), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %r.22 : Float(26:1024, 17:0, 1024:1) = aten::to(%1983, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2003 : Tensor[] = prim::ListConstruct(%r.22, %1992), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2004 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2003), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2005 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %1991, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
  %2006 : Tensor[] = prim::ListConstruct(%2005, %1999), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %ac.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %2006), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2008 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %1990, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
  %2009 : Tensor[] = prim::ListConstruct(%2008, %2004), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.81 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %2009), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2011 : int = aten::size(%ac.21, %57), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
  %2012 : int = aten::size(%x.81, %63), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2013 : int = aten::size(%x.81, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2014 : int = aten::size(%x.81, %58), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2015 : int = aten::size(%x.81, %57), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2016 : Long() = prim::NumToTensor(%2015), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2017 : int[] = prim::ListConstruct(%2012, %2013, %2015, %2014), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.82 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %2017), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
  %2019 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2020 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2019, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2021 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2020, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.83 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2021, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2023 : Long() = aten::sub(%2016, %46, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2024 : int = aten::Int(%2023), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2025 : int[] = prim::ListConstruct(%2012, %2013, %2014, %2024), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %x.84 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %2025), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2027 : Long(13:1) = aten::arange(%2011, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %57, %2027), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %2029 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2030 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2029, %61, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2030, %40), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2032 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2033 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %2032), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2034 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2033, %38), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.21, %2034, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.184 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %57, %54), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
  %2037 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %51, %55), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %2038 : Tensor[] = prim::ListConstruct(%2037, %2001), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %2039 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %2038), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %2040 : Tensor[] = prim::ListConstruct(%2039, %1989), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
  %input.185 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %2040), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
  %attn_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.185, %51, %55), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.21, %1982, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
  %2044 : Tensor = prim::GetAttr[name="bias"](%1988)
  %2045 : Tensor = prim::GetAttr[name="weight"](%1988)
  %2046 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm
  %input_tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.186, %2046, %2045, %2044, %34, %35), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2048 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.21, %r.22)
  %2049 : Float(13:17408, 17:1024, 1024:1), %2050 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2048)
  %2051 : __torch__.torch.nn.modules.normalization.___torch_mangle_42926.LayerNorm = prim::GetAttr[name="layer_norm"](%1986)
  %2052 : __torch__.torch.nn.modules.linear.___torch_mangle_42928.Linear = prim::GetAttr[name="layer_2"](%1986)
  %2053 : __torch__.torch.nn.modules.linear.___torch_mangle_42927.Linear = prim::GetAttr[name="layer_1"](%1986)
  %2054 : Tensor = prim::GetAttr[name="bias"](%2053)
  %2055 : Tensor = prim::GetAttr[name="weight"](%2053)
  %2056 : Float(1024:1, 4096:1024) = aten::t(%2055), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.61 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2049, %2056), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.187 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.61, %2054, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.188 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.187), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # torch/nn/functional.py:1369:0
  %input.189 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.188, %51, %55), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %2061 : Tensor = prim::GetAttr[name="bias"](%2052)
  %2062 : Tensor = prim::GetAttr[name="weight"](%2052)
  %2063 : Float(4096:1, 1024:4096) = aten::t(%2062), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.189, %2063), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %2061, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.190, %51, %55), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.63, %2049, %62), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
  %2068 : Tensor = prim::GetAttr[name="bias"](%2051)
  %2069 : Tensor = prim::GetAttr[name="weight"](%2051)
  %2070 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm
  %curr_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.191, %2070, %2069, %2068, %34, %35), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
  %2072 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.22, %2050)
  %2073 : Float(13:17408, 17:1024, 1024:1), %2074 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2072)
  %new_mem.22 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2073, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2076 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.22), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2077 : __torch__.transformers.modeling_xlnet.___torch_mangle_42940.XLNetFeedForward = prim::GetAttr[name="ff"](%69)
  %2078 : __torch__.transformers.modeling_xlnet.___torch_mangle_42935.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%69)
  %2079 : __torch__.torch.nn.modules.normalization.___torch_mangle_42933.LayerNorm = prim::GetAttr[name="layer_norm"](%2078)
  %2080 : Tensor = prim::GetAttr[name="o"](%2078)
  %2081 : Tensor = prim::GetAttr[name="r_r_bias"](%2078)
  %2082 : Tensor = prim::GetAttr[name="r_w_bias"](%2078)
  %2083 : Tensor = prim::GetAttr[name="r"](%2078)
  %2084 : Tensor = prim::GetAttr[name="v"](%2078)
  %2085 : Tensor = prim::GetAttr[name="k"](%2078)
  %2086 : Tensor = prim::GetAttr[name="q"](%2078)
  %2087 : Tensor[] = prim::ListConstruct(%2073, %2086), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %q_head.22 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2087), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2089 : Tensor[] = prim::ListConstruct(%2073, %2085), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2090 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2089), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2091 : Tensor[] = prim::ListConstruct(%2073, %2084), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2092 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2091), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %r.23 : Float(26:1024, 17:0, 1024:1) = aten::to(%2074, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2094 : Tensor[] = prim::ListConstruct(%r.23, %2083), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2095 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2094), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2096 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2082, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
  %2097 : Tensor[] = prim::ListConstruct(%2096, %2090), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %ac.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %2097), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2099 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2081, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
  %2100 : Tensor[] = prim::ListConstruct(%2099, %2095), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.85 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %2100), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2102 : int = aten::size(%ac.22, %57), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
  %2103 : int = aten::size(%x.85, %63), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2104 : int = aten::size(%x.85, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2105 : int = aten::size(%x.85, %58), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2106 : int = aten::size(%x.85, %57), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2107 : Long() = prim::NumToTensor(%2106), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2108 : int[] = prim::ListConstruct(%2103, %2104, %2106, %2105), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.86 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %2108), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
  %2110 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2111 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2110, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2112 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2111, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.87 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2112, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2114 : Long() = aten::sub(%2107, %46, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2115 : int = aten::Int(%2114), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2116 : int[] = prim::ListConstruct(%2103, %2104, %2105, %2115), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %x.88 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %2116), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2118 : Long(13:1) = aten::arange(%2102, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %57, %2118), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %2120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2120, %61, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2121, %40), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2123 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2124 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %2123), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2125 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2124, %38), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.192 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.22, %2125, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %57, %54), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
  %2128 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %51, %55), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %2129 : Tensor[] = prim::ListConstruct(%2128, %2092), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %2130 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %2129), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %2131 : Tensor[] = prim::ListConstruct(%2130, %2080), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
  %input.194 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %2131), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
  %attn_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.194, %51, %55), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.22, %2073, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
  %2135 : Tensor = prim::GetAttr[name="bias"](%2079)
  %2136 : Tensor = prim::GetAttr[name="weight"](%2079)
  %2137 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm
  %input_tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.195, %2137, %2136, %2135, %34, %35), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2139 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.22, %r.23)
  %2140 : Float(13:17408, 17:1024, 1024:1), %2141 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2139)
  %2142 : __torch__.torch.nn.modules.normalization.___torch_mangle_42936.LayerNorm = prim::GetAttr[name="layer_norm"](%2077)
  %2143 : __torch__.torch.nn.modules.linear.___torch_mangle_42938.Linear = prim::GetAttr[name="layer_2"](%2077)
  %2144 : __torch__.torch.nn.modules.linear.___torch_mangle_42937.Linear = prim::GetAttr[name="layer_1"](%2077)
  %2145 : Tensor = prim::GetAttr[name="bias"](%2144)
  %2146 : Tensor = prim::GetAttr[name="weight"](%2144)
  %2147 : Float(1024:1, 4096:1024) = aten::t(%2146), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.64 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2140, %2147), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.196 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.64, %2145, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.197 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.196), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # torch/nn/functional.py:1369:0
  %input.198 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.197, %51, %55), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %2152 : Tensor = prim::GetAttr[name="bias"](%2143)
  %2153 : Tensor = prim::GetAttr[name="weight"](%2143)
  %2154 : Float(4096:1, 1024:4096) = aten::t(%2153), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.198, %2154), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.65, %2152, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.199, %51, %55), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %input.200 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.66, %2140, %62), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
  %2159 : Tensor = prim::GetAttr[name="bias"](%2142)
  %2160 : Tensor = prim::GetAttr[name="weight"](%2142)
  %2161 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm
  %curr_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.200, %2161, %2160, %2159, %34, %35), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
  %2163 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.23, %2141)
  %2164 : Float(13:17408, 17:1024, 1024:1), %2165 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2163)
  %new_mem.23 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2164, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2167 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.23), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2168 : __torch__.transformers.modeling_xlnet.___torch_mangle_42950.XLNetFeedForward = prim::GetAttr[name="ff"](%67)
  %2169 : __torch__.transformers.modeling_xlnet.___torch_mangle_42945.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%67)
  %2170 : __torch__.torch.nn.modules.normalization.___torch_mangle_42943.LayerNorm = prim::GetAttr[name="layer_norm"](%2169)
  %2171 : Tensor = prim::GetAttr[name="o"](%2169)
  %2172 : Tensor = prim::GetAttr[name="r_r_bias"](%2169)
  %2173 : Tensor = prim::GetAttr[name="r_w_bias"](%2169)
  %2174 : Tensor = prim::GetAttr[name="r"](%2169)
  %2175 : Tensor = prim::GetAttr[name="v"](%2169)
  %2176 : Tensor = prim::GetAttr[name="k"](%2169)
  %2177 : Tensor = prim::GetAttr[name="q"](%2169)
  %2178 : Tensor[] = prim::ListConstruct(%2164, %2177), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %q_head.23 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2178), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2180 : Tensor[] = prim::ListConstruct(%2164, %2176), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2181 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2180), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2182 : Tensor[] = prim::ListConstruct(%2164, %2175), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2183 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2182), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %r : Float(26:1024, 17:0, 1024:1) = aten::to(%2165, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2185 : Tensor[] = prim::ListConstruct(%r, %2174), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2186 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2185), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2187 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2173, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
  %2188 : Tensor[] = prim::ListConstruct(%2187, %2181), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %ac.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %2188), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2190 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2172, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
  %2191 : Tensor[] = prim::ListConstruct(%2190, %2186), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.89 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %2191), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2193 : int = aten::size(%ac.23, %57), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
  %2194 : int = aten::size(%x.89, %63), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2195 : int = aten::size(%x.89, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2196 : int = aten::size(%x.89, %58), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2197 : int = aten::size(%x.89, %57), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2198 : Long() = prim::NumToTensor(%2197), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2199 : int[] = prim::ListConstruct(%2194, %2195, %2197, %2196), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.90 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %2199), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
  %2201 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2202 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2201, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2203 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2202, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.91 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2203, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2205 : Long() = aten::sub(%2198, %46, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2206 : int = aten::Int(%2205), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2207 : int[] = prim::ListConstruct(%2194, %2195, %2196, %2206), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %x.92 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %2207), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2209 : Long(13:1) = aten::arange(%2193, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %57, %2209), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %2211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2212 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2211, %61, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2212, %40), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2214 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2215 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %2214), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2216 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2215, %38), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.201 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.23, %2216, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.202 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %57, %54), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
  %2219 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %51, %55), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %2220 : Tensor[] = prim::ListConstruct(%2219, %2183), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %2221 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %2220), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %2222 : Tensor[] = prim::ListConstruct(%2221, %2171), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
  %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %2222), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
  %attn_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.203, %51, %55), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.204 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.23, %2164, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
  %2226 : Tensor = prim::GetAttr[name="bias"](%2170)
  %2227 : Tensor = prim::GetAttr[name="weight"](%2170)
  %2228 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm
  %input_tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.204, %2228, %2227, %2226, %34, %35), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2230 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.23, %r)
  %2231 : Float(13:17408, 17:1024, 1024:1), %2232 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2230)
  %2233 : __torch__.torch.nn.modules.normalization.___torch_mangle_42946.LayerNorm = prim::GetAttr[name="layer_norm"](%2168)
  %2234 : __torch__.torch.nn.modules.linear.___torch_mangle_42948.Linear = prim::GetAttr[name="layer_2"](%2168)
  %2235 : __torch__.torch.nn.modules.linear.___torch_mangle_42947.Linear = prim::GetAttr[name="layer_1"](%2168)
  %2236 : Tensor = prim::GetAttr[name="bias"](%2235)
  %2237 : Tensor = prim::GetAttr[name="weight"](%2235)
  %2238 : Float(1024:1, 4096:1024) = aten::t(%2237), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.67 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2231, %2238), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.205 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.67, %2236, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.206 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.205), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # torch/nn/functional.py:1369:0
  %input.207 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.206, %51, %55), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %2243 : Tensor = prim::GetAttr[name="bias"](%2234)
  %2244 : Tensor = prim::GetAttr[name="weight"](%2234)
  %2245 : Float(4096:1, 1024:4096) = aten::t(%2244), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.207, %2245), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %2243, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.208, %51, %55), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %input.209 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.69, %2231, %62), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
  %2250 : Tensor = prim::GetAttr[name="bias"](%2233)
  %2251 : Tensor = prim::GetAttr[name="weight"](%2233)
  %2252 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm
  %curr_out : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.209, %2252, %2251, %2250, %34, %35), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
  %2254 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out, %2232)
  %2255 : Float(13:17408, 17:1024, 1024:1), %2256 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2254)
  %new_mem : Float(13:17408, 17:1024, 1024:1) = aten::slice(%2255, %63, %63, %59, %62), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
  %2258 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
  %2259 : __torch__.transformers.modeling_xlnet.___torch_mangle_42960.XLNetFeedForward = prim::GetAttr[name="ff"](%65)
  %2260 : __torch__.transformers.modeling_xlnet.___torch_mangle_42955.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%65)
  %2261 : __torch__.torch.nn.modules.normalization.___torch_mangle_42953.LayerNorm = prim::GetAttr[name="layer_norm"](%2260)
  %2262 : Tensor = prim::GetAttr[name="o"](%2260)
  %2263 : Tensor = prim::GetAttr[name="r_r_bias"](%2260)
  %2264 : Tensor = prim::GetAttr[name="r_w_bias"](%2260)
  %2265 : Tensor = prim::GetAttr[name="r"](%2260)
  %2266 : Tensor = prim::GetAttr[name="v"](%2260)
  %2267 : Tensor = prim::GetAttr[name="k"](%2260)
  %2268 : Tensor = prim::GetAttr[name="q"](%2260)
  %2269 : Tensor[] = prim::ListConstruct(%2255, %2268), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %q_head : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2269), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2271 : Tensor[] = prim::ListConstruct(%2255, %2267), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2272 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2271), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2273 : Tensor[] = prim::ListConstruct(%2255, %2266), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2274 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2273), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2275 : Float(26:1024, 17:0, 1024:1) = aten::to(%2256, %53, %56, %55, %55, %54), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %2276 : Tensor[] = prim::ListConstruct(%2275, %2265), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2277 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%43, %2276), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2278 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2264, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
  %2279 : Tensor[] = prim::ListConstruct(%2278, %2272), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %ac : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%42, %2279), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2281 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2263, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
  %2282 : Tensor[] = prim::ListConstruct(%2281, %2277), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.93 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%42, %2282), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2284 : int = aten::size(%ac, %57), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
  %2285 : int = aten::size(%x.93, %63), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2286 : int = aten::size(%x.93, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2287 : int = aten::size(%x.93, %58), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2288 : int = aten::size(%x.93, %57), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2289 : Long() = prim::NumToTensor(%2288), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2290 : int[] = prim::ListConstruct(%2285, %2286, %2288, %2287), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x.94 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %2290), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
  %2292 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %63, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2293 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2292, %62, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2294 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2293, %58, %62, %59, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.95 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2294, %57, %63, %59, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2296 : Long() = aten::sub(%2289, %46, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2297 : int = aten::Int(%2296), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2298 : int[] = prim::ListConstruct(%2285, %2286, %2287, %2297), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %x : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %2298), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2300 : Long(13:1) = aten::arange(%2284, %41, %63, %53, %55), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %57, %2300), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %2302 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2303 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2302, %61, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2303, %40), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2305 : Tensor[] = prim::ListConstruct(%139), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2306 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%39, %2305), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2307 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2306, %38), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score, %2307, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %57, %54), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
  %2310 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %51, %55), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %2311 : Tensor[] = prim::ListConstruct(%2310, %2274), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %2312 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%37, %2311), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %2313 : Tensor[] = prim::ListConstruct(%2312, %2262), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
  %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%36, %2313), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
  %attn_out : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.212, %51, %55), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out, %2255, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
  %2317 : Tensor = prim::GetAttr[name="bias"](%2261)
  %2318 : Tensor = prim::GetAttr[name="weight"](%2261)
  %2319 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm
  %input_tensor : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %2319, %2318, %2317, %34, %35), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2321 : __torch__.torch.nn.modules.normalization.___torch_mangle_42956.LayerNorm = prim::GetAttr[name="layer_norm"](%2259)
  %2322 : __torch__.torch.nn.modules.linear.___torch_mangle_42958.Linear = prim::GetAttr[name="layer_2"](%2259)
  %2323 : __torch__.torch.nn.modules.linear.___torch_mangle_42957.Linear = prim::GetAttr[name="layer_1"](%2259)
  %2324 : Tensor = prim::GetAttr[name="bias"](%2323)
  %2325 : Tensor = prim::GetAttr[name="weight"](%2323)
  %2326 : Float(1024:1, 4096:1024) = aten::t(%2325), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.70 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input_tensor, %2326), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.214 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.70, %2324, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.214), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # torch/nn/functional.py:1369:0
  %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.215, %51, %55), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %2331 : Tensor = prim::GetAttr[name="bias"](%2322)
  %2332 : Tensor = prim::GetAttr[name="weight"](%2322)
  %2333 : Float(4096:1, 1024:4096) = aten::t(%2332), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.216, %2333), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.217 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.71, %2331, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.72 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.217, %51, %55), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.72, %input_tensor, %62), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
  %2338 : Tensor = prim::GetAttr[name="bias"](%2321)
  %2339 : Tensor = prim::GetAttr[name="weight"](%2321)
  %2340 : int[] = prim::ListConstruct(%50), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm
  %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.218, %2340, %2339, %2338, %34, %35), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
  %output_h : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.219, %51, %55), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
  %2343 : int[] = prim::ListConstruct(%62, %63, %58), scope: __module.transformer
  %2344 : Float(17:1024, 13:17408, 1024:1) = aten::permute(%output_h, %2343), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %input : Float(17:13312, 13:1024, 1024:1) = aten::contiguous(%2344, %63), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
  %2346 : (Float(17:13312, 13:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%input, %165, %256, %347, %438, %529, %620, %711, %802, %893, %984, %1075, %1166, %1257, %1348, %1439, %1530, %1621, %1712, %1803, %1894, %1985, %2076, %2167, %2258)
  %6 : Float(17:13312, 13:1024, 1024:1), %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(13:17408, 17:1024, 1024:1), %9 : Float(13:17408, 17:1024, 1024:1), %10 : Float(13:17408, 17:1024, 1024:1), %11 : Float(13:17408, 17:1024, 1024:1), %12 : Float(13:17408, 17:1024, 1024:1), %13 : Float(13:17408, 17:1024, 1024:1), %14 : Float(13:17408, 17:1024, 1024:1), %15 : Float(13:17408, 17:1024, 1024:1), %16 : Float(13:17408, 17:1024, 1024:1), %17 : Float(13:17408, 17:1024, 1024:1), %18 : Float(13:17408, 17:1024, 1024:1), %19 : Float(13:17408, 17:1024, 1024:1), %20 : Float(13:17408, 17:1024, 1024:1), %21 : Float(13:17408, 17:1024, 1024:1), %22 : Float(13:17408, 17:1024, 1024:1), %23 : Float(13:17408, 17:1024, 1024:1), %24 : Float(13:17408, 17:1024, 1024:1), %25 : Float(13:17408, 17:1024, 1024:1), %26 : Float(13:17408, 17:1024, 1024:1), %27 : Float(13:17408, 17:1024, 1024:1), %28 : Float(13:17408, 17:1024, 1024:1), %29 : Float(13:17408, 17:1024, 1024:1), %30 : Float(13:17408, 17:1024, 1024:1) = prim::TupleUnpack(%2346)
  %2347 : int = prim::Constant[value=1](), scope: __module.lm_loss # torch/nn/functional.py:1678:0
  %2348 : Tensor = prim::GetAttr[name="bias"](%3)
  %2349 : Tensor = prim::GetAttr[name="weight"](%3)
  %2350 : Float(1024:1, 32000:1024) = aten::t(%2349), scope: __module.lm_loss # torch/nn/functional.py:1676:0
  %output : Float(17:416000, 13:32000, 32000:1) = aten::matmul(%6, %2350), scope: __module.lm_loss # torch/nn/functional.py:1676:0
  %2352 : Float(17:416000, 13:32000, 32000:1) = aten::add_(%output, %2348, %2347), scope: __module.lm_loss # torch/nn/functional.py:1678:0
  %32 : (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, %28, %29, %30)
  %33 : (Float(17:416000, 13:32000, 32000:1), (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1))) = prim::TupleConstruct(%2352, %32)
  return (%33)
