graph(%self.1 : __torch__.transformers.modeling_longformer.LongformerForSequenceClassification,
      %input_ids.1 : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_longformer.LongformerClassificationHead = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.transformers.modeling_longformer.___torch_mangle_7319.LongformerModel = prim::GetAttr[name="longformer"](%self.1)
  %5 : int = prim::Constant[value=4]() # transformers/modeling_longformer.py:1438:0
  %6 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1438:0
  %7 : Device = prim::Constant[value="cpu"]() # transformers/modeling_longformer.py:1438:0
  %8 : bool = prim::Constant[value=0]() # transformers/modeling_longformer.py:1438:0
  %9 : None = prim::Constant()
  %global_attention_mask : Long(17:13, 13:1) = aten::zeros_like(%input_ids.1, %5, %6, %7, %8, %9) # transformers/modeling_longformer.py:1438:0
  %11 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1440:0
  %12 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1440:0
  %13 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_longformer.py:1440:0
  %14 : int = prim::Constant[value=1]() # transformers/modeling_longformer.py:1440:0
  %15 : Long(17:13, 13:1) = aten::slice(%global_attention_mask, %11, %12, %13, %14) # transformers/modeling_longformer.py:1440:0
  %16 : int = prim::Constant[value=1]() # transformers/modeling_longformer.py:1440:0
  %17 : int = prim::Constant[value=0]() # transformers/modeling_longformer.py:1440:0
  %18 : Long(17:13) = aten::select(%15, %16, %17) # transformers/modeling_longformer.py:1440:0
  %19 : Long() = prim::Constant[value={1}]() # transformers/modeling_longformer.py:1440:0
  %20 : int[] = prim::ListConstruct()
  %21 : Long() = aten::view(%19, %20) # transformers/modeling_longformer.py:1440:0
  %22 : int = prim::Constant[value=17]() # transformers/modeling_longformer.py:1440:0
  %23 : int[] = prim::ListConstruct(%22)
  %24 : bool = prim::Constant[value=1]() # transformers/modeling_longformer.py:1440:0
  %25 : Long(17:0) = aten::expand(%21, %23, %24) # transformers/modeling_longformer.py:1440:0
  %26 : bool = prim::Constant[value=0]()
  %27 : Long(17:13) = aten::copy_(%18, %25, %26) # transformers/modeling_longformer.py:1440:0
  %31 : str = prim::Constant[value="bcwd,bcdh->bcwh"](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %32 : int = prim::Constant[value=16384](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %33 : int = prim::Constant[value=65536](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %34 : float = prim::Constant[value=0.](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %35 : int = prim::Constant[value=17](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %36 : float = prim::Constant[value=-10000.](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %37 : int = prim::Constant[value=-256](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %38 : float = prim::Constant[value=-inf](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %39 : int = prim::Constant[value=-255](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %40 : int = prim::Constant[value=255](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %41 : int = prim::Constant[value=-257](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %42 : int = prim::Constant[value=204](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %43 : int = prim::Constant[value=257](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %44 : int = prim::Constant[value=513](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %45 : int = prim::Constant[value=256](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %46 : int = prim::Constant[value=-2](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %47 : str = prim::Constant[value="bcxd,bcyd->bcxy"](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %48 : int = prim::Constant[value=13056](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %49 : int = prim::Constant[value=3342336](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %50 : Long() = prim::Constant[value={2}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %51 : Long() = prim::Constant[value={512}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %52 : Long() = prim::Constant[value={256}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %53 : int = prim::Constant[value=64](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %54 : int = prim::Constant[value=12](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %55 : Double() = prim::Constant[value={8}](), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:261:0
  %56 : int = prim::Constant[value=-1](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %57 : bool = prim::Constant[value=1](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %58 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %59 : int = prim::Constant[value=768](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %60 : float = prim::Constant[value=0.10000000000000001](), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.dropout # torch/nn/functional.py:973:0
  %61 : Double() = prim::Constant[value={-10000}](), scope: __module.longformer # transformers/modeling_utils.py:258:0
  %62 : float = prim::Constant[value=1.](), scope: __module.longformer # torch/tensor.py:396:0
  %63 : None = prim::Constant(), scope: __module.longformer
  %64 : int = prim::Constant[value=6](), scope: __module.longformer # transformers/modeling_utils.py:257:0
  %65 : int = prim::Constant[value=3](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %66 : int = prim::Constant[value=2](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %67 : int = prim::Constant[value=9223372036854775807](), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %68 : int = prim::Constant[value=512](), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %69 : Long() = prim::Constant[value={1}](), scope: __module.longformer # transformers/modeling_longformer.py:1170:0
  %70 : bool = prim::Constant[value=0](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %71 : Device = prim::Constant[value="cpu"](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %72 : int = prim::Constant[value=4](), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %73 : int = prim::Constant[value=1](), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %74 : int = prim::Constant[value=0](), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %75 : __torch__.transformers.modeling_longformer.___torch_mangle_7318.LongformerEncoder = prim::GetAttr[name="encoder"](%4)
  %76 : __torch__.transformers.modeling_longformer.___torch_mangle_7088.LongformerEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %77 : int = aten::size(%input_ids.1, %74), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %78 : int = aten::size(%input_ids.1, %73), scope: __module.longformer # transformers/modeling_longformer.py:1228:0
  %79 : int[] = prim::ListConstruct(%77, %78), scope: __module.longformer
  %input.2 : Long(17:13, 13:1) = aten::zeros(%79, %72, %74, %71, %70), scope: __module.longformer # transformers/modeling_longformer.py:1239:0
  %81 : Long(17:13, 13:1) = aten::add(%global_attention_mask, %69, %73), scope: __module.longformer # transformers/modeling_longformer.py:1170:0
  %input.1 : Long(17:13, 13:1) = aten::mul(%attention_mask.1, %81), scope: __module.longformer # transformers/modeling_longformer.py:1170:0
  %83 : int = aten::size(%input_ids.1, %73), scope: __module.longformer # transformers/modeling_longformer.py:1136:0
  %seq_len.1 : Long() = prim::NumToTensor(%83), scope: __module.longformer
  %85 : Long() = aten::remainder(%seq_len.1, %68), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %86 : Long() = aten::rsub(%85, %68, %73), scope: __module.longformer # torch/tensor.py:396:0
  %padding_len : Long() = aten::remainder(%86, %68), scope: __module.longformer # transformers/modeling_longformer.py:1139:0
  %88 : int = aten::Int(%padding_len), scope: __module.longformer
  %89 : int = aten::Int(%padding_len), scope: __module.longformer
  %90 : int = aten::Int(%padding_len), scope: __module.longformer
  %91 : int[] = prim::ListConstruct(%74, %90), scope: __module.longformer
  %input_ids : Long(17:512, 512:1) = aten::constant_pad_nd(%input_ids.1, %91, %73), scope: __module.longformer # torch/nn/functional.py:3552:0
  %93 : int[] = prim::ListConstruct(%74, %89), scope: __module.longformer
  %attention_mask.2 : Long(17:512, 512:1) = aten::constant_pad_nd(%input.1, %93, %74), scope: __module.longformer # torch/nn/functional.py:3552:0
  %95 : int[] = prim::ListConstruct(%74, %88), scope: __module.longformer
  %input.4 : Long(17:512, 512:1) = aten::constant_pad_nd(%input.2, %95, %74), scope: __module.longformer # torch/nn/functional.py:3552:0
  %97 : Long(17:512, 512:1) = aten::slice(%attention_mask.2, %74, %74, %67, %73), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %98 : Long(17:512, 1:512, 512:1) = aten::unsqueeze(%97, %73), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %99 : Long(17:512, 1:512, 1:512, 512:1) = aten::unsqueeze(%98, %66), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:512, 1:512, 1:512, 512:1) = aten::slice(%99, %65, %74, %67, %73), scope: __module.longformer # transformers/modeling_utils.py:244:0
  %101 : Float(17:512, 1:512, 1:512, 512:1) = aten::to(%extended_attention_mask, %64, %70, %70, %63), scope: __module.longformer # transformers/modeling_utils.py:257:0
  %102 : Float(17:512, 1:512, 1:512, 512:1) = aten::rsub(%101, %62, %73), scope: __module.longformer # torch/tensor.py:396:0
  %attention_mask : Float(17:512, 1:512, 1:512, 512:1) = aten::mul(%102, %61), scope: __module.longformer # transformers/modeling_utils.py:258:0
  %104 : __torch__.torch.nn.modules.normalization.___torch_mangle_7086.LayerNorm = prim::GetAttr[name="LayerNorm"](%76)
  %105 : __torch__.torch.nn.modules.sparse.___torch_mangle_7085.Embedding = prim::GetAttr[name="token_type_embeddings"](%76)
  %106 : __torch__.torch.nn.modules.sparse.___torch_mangle_7084.Embedding = prim::GetAttr[name="position_embeddings"](%76)
  %107 : __torch__.torch.nn.modules.sparse.___torch_mangle_7083.Embedding = prim::GetAttr[name="word_embeddings"](%76)
  %108 : Bool(17:512, 512:1) = aten::ne(%input_ids, %73), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:112:0
  %mask : Int(17:512, 512:1) = aten::to(%108, %65, %70, %70, %63), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:112:0
  %110 : Long(17:512, 512:1) = aten::cumsum(%mask, %73, %63), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %111 : Int(17:512, 512:1) = aten::type_as(%110, %mask), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %incremental_indices : Int(17:512, 512:1) = aten::mul(%111, %mask), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:113:0
  %113 : Long(17:512, 512:1) = aten::to(%incremental_indices, %72, %70, %70, %63), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:114:0
  %114 : Long(17:512, 512:1) = aten::add(%113, %69, %73), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:114:0
  %input.3 : Long(17:512, 512:1) = aten::to(%114, %72, %74, %71, %70, %70, %70, %63), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:147:0
  %116 : Tensor = prim::GetAttr[name="weight"](%107)
  %inputs_embeds : Float(17:393216, 512:768, 768:1) = aten::embedding(%116, %input_ids, %73, %70, %70), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %118 : Tensor = prim::GetAttr[name="weight"](%106)
  %position_embeddings : Float(17:393216, 512:768, 768:1) = aten::embedding(%118, %input.3, %73, %70, %70), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %120 : Tensor = prim::GetAttr[name="weight"](%105)
  %token_type_embeddings : Float(17:393216, 512:768, 768:1) = aten::embedding(%120, %input.4, %56, %70, %70), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %122 : Float(17:393216, 512:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %73), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:170:0
  %input.5 : Float(17:393216, 512:768, 768:1) = aten::add(%122, %token_type_embeddings, %73), scope: __module.longformer/__module.longformer.embeddings # transformers/modeling_longformer.py:170:0
  %124 : Tensor = prim::GetAttr[name="bias"](%104)
  %125 : Tensor = prim::GetAttr[name="weight"](%104)
  %126 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm
  %input.6 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.5, %126, %125, %124, %58, %57), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %hidden_states.1 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.6, %60, %70), scope: __module.longformer/__module.longformer.embeddings/__module.longformer.embeddings.dropout # torch/nn/functional.py:973:0
  %129 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %130 : __torch__.transformers.modeling_longformer.___torch_mangle_7316.LongformerLayer = prim::GetAttr[name="11"](%129)
  %131 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %132 : __torch__.transformers.modeling_longformer.___torch_mangle_7297.LongformerLayer = prim::GetAttr[name="10"](%131)
  %133 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %134 : __torch__.transformers.modeling_longformer.___torch_mangle_7278.LongformerLayer = prim::GetAttr[name="9"](%133)
  %135 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %136 : __torch__.transformers.modeling_longformer.___torch_mangle_7259.LongformerLayer = prim::GetAttr[name="8"](%135)
  %137 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %138 : __torch__.transformers.modeling_longformer.___torch_mangle_7240.LongformerLayer = prim::GetAttr[name="7"](%137)
  %139 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %140 : __torch__.transformers.modeling_longformer.___torch_mangle_7221.LongformerLayer = prim::GetAttr[name="6"](%139)
  %141 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %142 : __torch__.transformers.modeling_longformer.___torch_mangle_7202.LongformerLayer = prim::GetAttr[name="5"](%141)
  %143 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %144 : __torch__.transformers.modeling_longformer.___torch_mangle_7183.LongformerLayer = prim::GetAttr[name="4"](%143)
  %145 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %146 : __torch__.transformers.modeling_longformer.___torch_mangle_7164.LongformerLayer = prim::GetAttr[name="3"](%145)
  %147 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %148 : __torch__.transformers.modeling_longformer.___torch_mangle_7145.LongformerLayer = prim::GetAttr[name="2"](%147)
  %149 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %150 : __torch__.transformers.modeling_longformer.___torch_mangle_7126.LongformerLayer = prim::GetAttr[name="1"](%149)
  %151 : __torch__.torch.nn.modules.container.___torch_mangle_7317.ModuleList = prim::GetAttr[name="layer"](%75)
  %152 : __torch__.transformers.modeling_longformer.___torch_mangle_7107.LongformerLayer = prim::GetAttr[name="0"](%151)
  %153 : __torch__.transformers.modeling_longformer.___torch_mangle_7106.LongformerOutput = prim::GetAttr[name="output"](%152)
  %154 : __torch__.transformers.modeling_longformer.___torch_mangle_7102.LongformerIntermediate = prim::GetAttr[name="intermediate"](%152)
  %155 : __torch__.transformers.modeling_longformer.___torch_mangle_7100.LongformerAttention = prim::GetAttr[name="attention"](%152)
  %156 : __torch__.transformers.modeling_longformer.___torch_mangle_7099.LongformerSelfOutput = prim::GetAttr[name="output"](%155)
  %157 : __torch__.transformers.modeling_longformer.___torch_mangle_7095.LongformerSelfAttention = prim::GetAttr[name="self"](%155)
  %158 : __torch__.torch.nn.modules.linear.___torch_mangle_7091.Linear = prim::GetAttr[name="value"](%157)
  %159 : __torch__.torch.nn.modules.linear.___torch_mangle_7090.Linear = prim::GetAttr[name="key"](%157)
  %160 : __torch__.torch.nn.modules.linear.___torch_mangle_7089.Linear = prim::GetAttr[name="query"](%157)
  %161 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
  %162 : Float(17:512, 512:1) = aten::squeeze(%161, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.1 : Bool(17:512, 512:1) = aten::lt(%162, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %input.7 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.1, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:248:0
  %165 : Tensor = prim::GetAttr[name="bias"](%160)
  %166 : Tensor = prim::GetAttr[name="weight"](%160)
  %167 : Float(768:1, 768:768) = aten::t(%166), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %167), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.1, %165, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %170 : Tensor = prim::GetAttr[name="bias"](%159)
  %171 : Tensor = prim::GetAttr[name="weight"](%159)
  %172 : Float(768:1, 768:768) = aten::t(%171), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %172), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.2, %170, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %175 : Tensor = prim::GetAttr[name="bias"](%158)
  %176 : Tensor = prim::GetAttr[name="weight"](%158)
  %177 : Float(768:1, 768:768) = aten::t(%176), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.7, %177), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.1 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.3, %175, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self/__module.longformer.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %180 : int = aten::size(%input.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %181 : int = aten::size(%input.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %182 : int = aten::size(%input.7, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.1, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:261:0
  %184 : int[] = prim::ListConstruct(%180, %181, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %185 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.2, %184), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %query.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%185, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:263:0
  %187 : int[] = prim::ListConstruct(%180, %181, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %188 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.1, %187), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
  %key.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%188, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:264:0
  %190 : int = aten::size(%query.1, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.2 : Long() = prim::NumToTensor(%190), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %192 : int = aten::size(%query.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.3 : Long() = prim::NumToTensor(%192), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %194 : int = aten::size(%query.1, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.1 : Long() = prim::NumToTensor(%194), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %196 : int = aten::size(%query.1, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %197 : Long() = aten::floor_divide(%seq_len.3, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.1 : Long() = aten::sub(%197, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
  %199 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.1, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %200 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %201 : int = aten::Int(%200), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %202 : int[] = prim::ListConstruct(%201, %192, %196), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.2 : Float(204:64, 512:13056, 64:1) = aten::reshape(%199, %202), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %204 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.1, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %205 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %206 : int = aten::Int(%205), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %207 : int[] = prim::ListConstruct(%206, %192, %196), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.4 : Float(204:64, 512:13056, 64:1) = aten::reshape(%204, %207), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %209 : int = aten::size(%hidden_states.2, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %210 : int = aten::size(%hidden_states.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %211 : Long() = prim::NumToTensor(%210), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %212 : Long() = aten::floor_divide(%211, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %213 : int = aten::Int(%212), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %214 : int = aten::size(%hidden_states.2, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %215 : int[] = prim::ListConstruct(%209, %213, %68, %214), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.3 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.2, %215), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %217 : int = aten::size(%hidden_states.3, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %218 : int = aten::size(%hidden_states.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %219 : Long() = prim::NumToTensor(%218), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %220 : int = aten::size(%hidden_states.3, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %221 : int = aten::size(%hidden_states.3, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %222 : Long() = aten::mul(%219, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %223 : Long() = aten::sub(%222, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %224 : int = aten::Int(%223), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %225 : int[] = prim::ListConstruct(%217, %224, %220, %221), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %226 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %227 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.3, %225, %226, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %228 : int = aten::size(%hidden_states.4, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %229 : int = aten::size(%hidden_states.4, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %230 : Long() = prim::NumToTensor(%229), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %231 : Long() = aten::floor_divide(%230, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %232 : int = aten::Int(%231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %233 : int = aten::size(%hidden_states.4, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %234 : int[] = prim::ListConstruct(%228, %232, %68, %233), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.5 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.4, %234), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %236 : int = aten::size(%hidden_states.5, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %237 : int = aten::size(%hidden_states.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %238 : Long() = prim::NumToTensor(%237), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %239 : int = aten::size(%hidden_states.5, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %240 : int = aten::size(%hidden_states.5, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %241 : Long() = aten::mul(%238, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %242 : Long() = aten::sub(%241, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %243 : int = aten::Int(%242), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %244 : int[] = prim::ListConstruct(%236, %243, %239, %240), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %245 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %246 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.5, %244, %245, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %247 : Tensor[] = prim::ListConstruct(%227, %246), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.8 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %247), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %249 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states_padded.1 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.8, %249, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %251 : int = aten::size(%hidden_states_padded.1, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %252 : int = aten::size(%hidden_states_padded.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %253 : int = aten::size(%hidden_states_padded.1, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %254 : int = aten::size(%hidden_states_padded.1, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %255 : int[] = prim::ListConstruct(%251, %252, %253, %254), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_chunked_attention_scores.1 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.1, %255), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:400:0
  %257 : Long() = aten::mul(%batch_size.2, %num_heads.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %258 : int = aten::Int(%257), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %259 : Long() = aten::add(%chunks_count.1, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %260 : int = aten::Int(%259), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %261 : int[] = prim::ListConstruct(%258, %260, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_attention_scores.1 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.1, %261, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %263 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %264 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%263, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %265 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%264, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %266 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%265, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %267 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %268 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%267, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %269 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%268, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %270 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%269, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %271 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %272 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%266, %271), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %273 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%270, %272, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %274 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %275 : Float(204:262656, 512:513, 513:1) = aten::select(%274, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %276 : Float(204:262656, 256:513, 513:1) = aten::slice(%275, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %277 : Float(204:262656, 256:513, 257:1) = aten::slice(%276, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %278 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %279 : Float(204:262656, 256:513, 513:1) = aten::select(%278, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %280 : Float(204:262656, 256:513, 513:1) = aten::slice(%279, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %281 : Float(204:262656, 256:513, 257:1) = aten::slice(%280, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %282 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %283 : Float(204:262656, 256:513, 257:1) = aten::view(%277, %282), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %284 : Float(204:262656, 256:513, 257:1) = aten::copy_(%281, %283, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %285 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %286 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%285, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %287 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%286, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %288 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%287, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %289 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %290 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%289, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %291 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%290, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %292 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%291, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %293 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %294 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%288, %293), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %295 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%292, %294, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %296 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %297 : Float(204:262656, 512:513, 513:1) = aten::select(%296, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %298 : Float(204:262656, 255:513, 513:1) = aten::slice(%297, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %299 : Float(204:262656, 255:513, 255:1) = aten::slice(%298, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %300 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %301 : Float(204:262656, 256:513, 513:1) = aten::select(%300, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %302 : Float(204:262656, 255:513, 513:1) = aten::slice(%301, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %303 : Float(204:262656, 255:513, 255:1) = aten::slice(%302, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %304 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %305 : Float(204:262656, 255:513, 255:1) = aten::view(%299, %304), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %306 : Float(204:262656, 255:513, 255:1) = aten::copy_(%303, %305, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %307 : int[] = prim::ListConstruct(%190, %194, %192, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %308 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.1, %307), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.1 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%308, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %310 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %311 : Float(256:257, 257:1) = aten::ones(%310, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %312 : Float(256:257, 257:1) = aten::tril(%311, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %313 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %beginning_mask_2d.1 : Float(256:257, 257:1) = aten::flip(%312, %313), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %315 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.1, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %316 : Float(1:65792, 256:257, 257:1) = aten::slice(%315, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %317 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%316, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.1 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%317, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %319 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %ending_mask.1 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.1, %319), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
  %321 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %322 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%321, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %323 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%322, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.1 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%323, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %325 : int = aten::size(%beginning_input.1, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %326 : int = aten::size(%beginning_input.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %327 : int = aten::size(%beginning_input.1, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %328 : int = aten::size(%beginning_input.1, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %329 : int[] = prim::ListConstruct(%325, %326, %327, %328), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %330 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.1, %329, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %331 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%330, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %332 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.1, %331, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %333 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %334 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%333, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %335 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%334, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.1 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%335, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %337 : int = aten::size(%ending_input.1, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %338 : int = aten::size(%ending_input.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %339 : int = aten::size(%ending_input.1, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %340 : int = aten::size(%ending_input.1, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %341 : int[] = prim::ListConstruct(%337, %338, %339, %340), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %342 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.1, %341, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %343 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%342, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %344 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.1, %343, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
  %345 : Bool(17:512, 512:1) = aten::ne(%162, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %346 : Bool(17:512, 512:1) = aten::slice(%345, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %347 : Bool(17:512, 512:1) = aten::slice(%346, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %348 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%347, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.1 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%348, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:272:0
  %350 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.1, %query.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.1 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%350, %remove_from_windowed_attention_mask.1, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:275:0
  %352 : int = aten::size(%float_mask.1, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %353 : int = aten::size(%float_mask.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %354 : int = aten::size(%float_mask.1, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %355 : int = aten::size(%float_mask.1, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %356 : int[] = prim::ListConstruct(%352, %353, %354, %355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %query.2 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%356, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:280:0
  %358 : int = aten::size(%query.2, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.3 : Long() = prim::NumToTensor(%358), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %360 : int = aten::size(%query.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.4 : Long() = prim::NumToTensor(%360), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %362 : int = aten::size(%query.2, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.2 : Long() = prim::NumToTensor(%362), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %364 : int = aten::size(%query.2, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:472:0
  %365 : Long() = aten::floor_divide(%seq_len.4, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.2 : Long() = aten::sub(%365, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:478:0
  %367 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.2, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %368 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %369 : int = aten::Int(%368), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %370 : int[] = prim::ListConstruct(%369, %360, %364), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.6 : Float(17:512, 512:1, 1:1) = aten::reshape(%367, %370), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:481:0
  %372 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.1, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %373 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %374 : int = aten::Int(%373), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %375 : int[] = prim::ListConstruct(%374, %360, %364), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.8 : Float(17:512, 512:1, 1:1) = aten::reshape(%372, %375), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:482:0
  %377 : int = aten::size(%hidden_states.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %378 : int = aten::size(%hidden_states.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %379 : Long() = prim::NumToTensor(%378), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %380 : Long() = aten::floor_divide(%379, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %381 : int = aten::Int(%380), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %382 : int = aten::size(%hidden_states.6, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %383 : int[] = prim::ListConstruct(%377, %381, %68, %382), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.7 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.6, %383), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %385 : int = aten::size(%hidden_states.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %386 : int = aten::size(%hidden_states.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %387 : Long() = prim::NumToTensor(%386), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %388 : int = aten::size(%hidden_states.7, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %389 : int = aten::size(%hidden_states.7, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %390 : Long() = aten::mul(%387, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %391 : Long() = aten::sub(%390, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %392 : int = aten::Int(%391), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %393 : int[] = prim::ListConstruct(%385, %392, %388, %389), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %394 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %395 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.7, %393, %394, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %396 : int = aten::size(%hidden_states.8, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:442:0
  %397 : int = aten::size(%hidden_states.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:443:0
  %398 : Long() = prim::NumToTensor(%397), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %399 : Long() = aten::floor_divide(%398, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %400 : int = aten::Int(%399), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %401 : int = aten::size(%hidden_states.8, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:445:0
  %402 : int[] = prim::ListConstruct(%396, %400, %68, %401), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states.9 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.8, %402), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:441:0
  %404 : int = aten::size(%hidden_states.9, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %405 : int = aten::size(%hidden_states.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %406 : Long() = prim::NumToTensor(%405), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %407 : int = aten::size(%hidden_states.9, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %408 : int = aten::size(%hidden_states.9, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:449:0
  %409 : Long() = aten::mul(%406, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %410 : Long() = aten::sub(%409, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:450:0
  %411 : int = aten::Int(%410), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %412 : int[] = prim::ListConstruct(%404, %411, %407, %408), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %413 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %414 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.9, %412, %413, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:454:0
  %415 : Tensor[] = prim::ListConstruct(%395, %414), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.9 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %415), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %417 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %hidden_states_padded.2 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.9, %417, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %419 : int = aten::size(%hidden_states_padded.2, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %420 : int = aten::size(%hidden_states_padded.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %421 : int = aten::size(%hidden_states_padded.2, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %422 : int = aten::size(%hidden_states_padded.2, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:401:0
  %423 : int[] = prim::ListConstruct(%419, %420, %421, %422), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_chunked_attention_scores.2 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.2, %423), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:400:0
  %425 : Long() = aten::mul(%batch_size.3, %num_heads.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %426 : int = aten::Int(%425), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %427 : Long() = aten::add(%chunks_count.2, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:504:0
  %428 : int = aten::Int(%427), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %429 : int[] = prim::ListConstruct(%426, %428, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %diagonal_attention_scores.2 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.2, %429, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:503:0
  %431 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %432 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%431, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %433 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%432, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %434 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%433, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %435 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %436 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%435, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %437 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%436, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %438 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%437, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %439 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %440 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%434, %439), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %441 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%438, %440, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:509:0
  %442 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %443 : Float(17:262656, 512:513, 513:1) = aten::select(%442, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %444 : Float(17:262656, 256:513, 513:1) = aten::slice(%443, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %445 : Float(17:262656, 256:513, 257:1) = aten::slice(%444, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %446 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %447 : Float(17:262656, 256:513, 513:1) = aten::select(%446, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %448 : Float(17:262656, 256:513, 513:1) = aten::slice(%447, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %449 : Float(17:262656, 256:513, 257:1) = aten::slice(%448, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %450 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %451 : Float(17:262656, 256:513, 257:1) = aten::view(%445, %450), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %452 : Float(17:262656, 256:513, 257:1) = aten::copy_(%449, %451, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:512:0
  %453 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %454 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%453, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %455 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%454, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %456 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%455, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %457 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %458 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%457, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %459 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%458, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %460 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%459, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %461 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %462 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%456, %461), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %463 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%460, %462, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:516:0
  %464 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %465 : Float(17:262656, 512:513, 513:1) = aten::select(%464, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %466 : Float(17:262656, 255:513, 513:1) = aten::slice(%465, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %467 : Float(17:262656, 255:513, 255:1) = aten::slice(%466, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %468 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %469 : Float(17:262656, 256:513, 513:1) = aten::select(%468, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %470 : Float(17:262656, 255:513, 513:1) = aten::slice(%469, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %471 : Float(17:262656, 255:513, 255:1) = aten::slice(%470, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %472 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %473 : Float(17:262656, 255:513, 255:1) = aten::view(%467, %472), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %474 : Float(17:262656, 255:513, 255:1) = aten::copy_(%471, %473, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:520:0
  %475 : int[] = prim::ListConstruct(%358, %362, %360, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %476 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.2, %475), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.2 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%476, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:525:0
  %478 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %479 : Float(256:257, 257:1) = aten::ones(%478, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %480 : Float(256:257, 257:1) = aten::tril(%479, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %481 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %beginning_mask_2d.2 : Float(256:257, 257:1) = aten::flip(%480, %481), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:458:0
  %483 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.2, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %484 : Float(1:65792, 256:257, 257:1) = aten::slice(%483, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %485 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%484, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.2 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%485, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:459:0
  %487 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %ending_mask.2 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.2, %487), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:460:0
  %489 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %490 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%489, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %491 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%490, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.2 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%491, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:461:0
  %493 : int = aten::size(%beginning_input.2, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %494 : int = aten::size(%beginning_input.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %495 : int = aten::size(%beginning_input.2, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %496 : int = aten::size(%beginning_input.2, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %497 : int[] = prim::ListConstruct(%493, %494, %495, %496), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %498 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.2, %497, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:462:0
  %499 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%498, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %500 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.2, %499, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:463:0
  %501 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %502 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%501, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %503 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%502, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.2 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%503, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:464:0
  %505 : int = aten::size(%ending_input.2, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %506 : int = aten::size(%ending_input.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %507 : int = aten::size(%ending_input.2, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %508 : int = aten::size(%ending_input.2, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %509 : int[] = prim::ListConstruct(%505, %506, %507, %508), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %510 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.2, %509, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:465:0
  %511 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%510, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:22:0
  %512 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.2, %511, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.1 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.1, %input_tensor.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.1 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.1, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:1500:0
  %515 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.1, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %516 : Bool(17:512, 512:1) = aten::slice(%515, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %517 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%516, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %518 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%517, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %input.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.1, %518, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.2 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.10, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:973:0
  %521 : int[] = prim::ListConstruct(%180, %181, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %522 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.1, %521), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
  %value.1 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%522, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:331:0
  %524 : int = aten::size(%value.1, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.4 : Long() = prim::NumToTensor(%524), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %526 : int = aten::size(%value.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.5 : Long() = prim::NumToTensor(%526), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %528 : int = aten::size(%value.1, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.3 : Long() = prim::NumToTensor(%528), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %530 : int = aten::size(%value.1, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:537:0
  %531 : Long() = aten::floor_divide(%seq_len.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %chunks_count.3 : Long() = aten::sub(%531, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:542:0
  %533 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.2, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
  %534 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:546:0
  %535 : int = aten::Int(%534), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %536 : Long() = aten::floor_divide(%seq_len.5, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/tensor.py:424:0
  %537 : int = aten::Int(%536), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %538 : int[] = prim::ListConstruct(%535, %537, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.1 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%533, %538), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:545:0
  %540 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.1, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %541 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %542 : int = aten::Int(%541), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %543 : int[] = prim::ListConstruct(%542, %526, %530), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %input.11 : Float(204:64, 512:13056, 64:1) = aten::reshape(%540, %543), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:550:0
  %545 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %padded_value.1 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.11, %545, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %547 : Long() = aten::mul(%batch_size.4, %num_heads.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
  %548 : int = aten::Int(%547), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %549 : Long() = aten::add(%chunks_count.3, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:556:0
  %550 : int = aten::Int(%549), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %551 : int[] = prim::ListConstruct(%548, %550, %59, %530), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %552 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %553 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.1, %551, %552, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:564:0
  %554 : int = aten::size(%chunked_hidden_states.1, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %555 : int = aten::size(%chunked_hidden_states.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %556 : int = aten::size(%chunked_hidden_states.1, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.1 : Long() = prim::NumToTensor(%556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %558 : int = aten::size(%chunked_hidden_states.1, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.1 : Long() = prim::NumToTensor(%558), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %560 : Long() = aten::add(%window_overlap.1, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:422:0
  %561 : int = aten::Int(%560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %562 : int[] = prim::ListConstruct(%74, %561), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.2 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.1, %562, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/nn/functional.py:3552:0
  %564 : int[] = prim::ListConstruct(%554, %555, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.3 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.2, %564), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:424:0
  %566 : Long() = aten::neg(%window_overlap.1), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:428:0
  %567 : int = aten::Int(%566), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %568 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %569 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%568, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.4 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%569, %66, %74, %567, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:427:0
  %571 : Long() = aten::add(%window_overlap.1, %hidden_dim.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:431:0
  %572 : int = aten::Int(%571), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %573 : int[] = prim::ListConstruct(%554, %555, %556, %572), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %chunked_hidden_states.5 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.4, %573), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:430:0
  %575 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %576 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%575, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %577 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%576, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %578 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%577, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:433:0
  %579 : Tensor[] = prim::ListConstruct(%578, %553), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %context.1 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # torch/functional.py:327:0
  %581 : int[] = prim::ListConstruct(%524, %528, %526, %530), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %582 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.1, %581), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.1 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%582, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:569:0
  %584 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.1, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %585 : int[] = prim::ListConstruct(%180, %181, %182), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self
  %586 : Float(512:13056, 17:768, 768:1) = aten::reshape(%584, %585), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.2 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%586, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:350:0
  %input.12 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.2, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.self # transformers/modeling_longformer.py:374:0
  %589 : __torch__.torch.nn.modules.normalization.___torch_mangle_7097.LayerNorm = prim::GetAttr[name="LayerNorm"](%156)
  %590 : __torch__.torch.nn.modules.linear.___torch_mangle_7096.Linear = prim::GetAttr[name="dense"](%156)
  %591 : Tensor = prim::GetAttr[name="bias"](%590)
  %592 : Tensor = prim::GetAttr[name="weight"](%590)
  %593 : Float(768:1, 768:768) = aten::t(%592), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.12, %593), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.4, %591, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.13, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.10, %hidden_states.1, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output # transformers/modeling_longformer.py:758:0
  %598 : Tensor = prim::GetAttr[name="bias"](%589)
  %599 : Tensor = prim::GetAttr[name="weight"](%589)
  %600 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.3 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.14, %600, %599, %598, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.attention/__module.longformer.encoder.layer.0.attention.output/__module.longformer.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %602 : __torch__.torch.nn.modules.linear.___torch_mangle_7101.Linear = prim::GetAttr[name="dense"](%154)
  %603 : Tensor = prim::GetAttr[name="bias"](%602)
  %604 : Tensor = prim::GetAttr[name="weight"](%602)
  %605 : Float(768:1, 3072:768) = aten::t(%604), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.3, %605), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.15 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.5, %603, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate/__module.longformer.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.16 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %609 : __torch__.torch.nn.modules.normalization.___torch_mangle_7104.LayerNorm = prim::GetAttr[name="LayerNorm"](%153)
  %610 : __torch__.torch.nn.modules.linear.___torch_mangle_7103.Linear = prim::GetAttr[name="dense"](%153)
  %611 : Tensor = prim::GetAttr[name="bias"](%610)
  %612 : Tensor = prim::GetAttr[name="weight"](%610)
  %613 : Float(3072:1, 768:3072) = aten::t(%612), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.16, %613), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.17 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.6, %611, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.17, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.11, %input_tensor.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output # transformers/modeling_longformer.py:830:0
  %618 : Tensor = prim::GetAttr[name="bias"](%609)
  %619 : Tensor = prim::GetAttr[name="weight"](%609)
  %620 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.LayerNorm
  %hidden_states.12 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.18, %620, %619, %618, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.0/__module.longformer.encoder.layer.0.output/__module.longformer.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %622 : __torch__.transformers.modeling_longformer.___torch_mangle_7125.LongformerOutput = prim::GetAttr[name="output"](%150)
  %623 : __torch__.transformers.modeling_longformer.___torch_mangle_7121.LongformerIntermediate = prim::GetAttr[name="intermediate"](%150)
  %624 : __torch__.transformers.modeling_longformer.___torch_mangle_7119.LongformerAttention = prim::GetAttr[name="attention"](%150)
  %625 : __torch__.transformers.modeling_longformer.___torch_mangle_7118.LongformerSelfOutput = prim::GetAttr[name="output"](%624)
  %626 : __torch__.transformers.modeling_longformer.___torch_mangle_7114.LongformerSelfAttention = prim::GetAttr[name="self"](%624)
  %627 : __torch__.torch.nn.modules.linear.___torch_mangle_7110.Linear = prim::GetAttr[name="value"](%626)
  %628 : __torch__.torch.nn.modules.linear.___torch_mangle_7109.Linear = prim::GetAttr[name="key"](%626)
  %629 : __torch__.torch.nn.modules.linear.___torch_mangle_7108.Linear = prim::GetAttr[name="query"](%626)
  %630 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
  %631 : Float(17:512, 512:1) = aten::squeeze(%630, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.2 : Bool(17:512, 512:1) = aten::lt(%631, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %input.19 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.12, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:248:0
  %634 : Tensor = prim::GetAttr[name="bias"](%629)
  %635 : Tensor = prim::GetAttr[name="weight"](%629)
  %636 : Float(768:1, 768:768) = aten::t(%635), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %636), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.7, %634, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %639 : Tensor = prim::GetAttr[name="bias"](%628)
  %640 : Tensor = prim::GetAttr[name="weight"](%628)
  %641 : Float(768:1, 768:768) = aten::t(%640), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %641), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.8, %639, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %644 : Tensor = prim::GetAttr[name="bias"](%627)
  %645 : Tensor = prim::GetAttr[name="weight"](%627)
  %646 : Float(768:1, 768:768) = aten::t(%645), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.19, %646), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.2 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.9, %644, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self/__module.longformer.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %649 : int = aten::size(%input.19, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %650 : int = aten::size(%input.19, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %651 : int = aten::size(%input.19, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.3, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:261:0
  %653 : int[] = prim::ListConstruct(%649, %650, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %654 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.4, %653), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
  %query.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%654, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:263:0
  %656 : int[] = prim::ListConstruct(%649, %650, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %657 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.2, %656), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
  %key.2 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%657, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:264:0
  %659 : int = aten::size(%query.3, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.6 : Long() = prim::NumToTensor(%659), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %661 : int = aten::size(%query.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.7 : Long() = prim::NumToTensor(%661), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %663 : int = aten::size(%query.3, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.4 : Long() = prim::NumToTensor(%663), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %665 : int = aten::size(%query.3, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %666 : Long() = aten::floor_divide(%seq_len.7, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.4 : Long() = aten::sub(%666, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
  %668 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.3, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %669 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %670 : int = aten::Int(%669), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %671 : int[] = prim::ListConstruct(%670, %661, %665), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.13 : Float(204:64, 512:13056, 64:1) = aten::reshape(%668, %671), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %673 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.2, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %674 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %675 : int = aten::Int(%674), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %676 : int[] = prim::ListConstruct(%675, %661, %665), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.15 : Float(204:64, 512:13056, 64:1) = aten::reshape(%673, %676), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %678 : int = aten::size(%hidden_states.13, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %679 : int = aten::size(%hidden_states.13, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %680 : Long() = prim::NumToTensor(%679), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %681 : Long() = aten::floor_divide(%680, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %682 : int = aten::Int(%681), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %683 : int = aten::size(%hidden_states.13, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %684 : int[] = prim::ListConstruct(%678, %682, %68, %683), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.14 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.13, %684), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %686 : int = aten::size(%hidden_states.14, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %687 : int = aten::size(%hidden_states.14, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %688 : Long() = prim::NumToTensor(%687), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %689 : int = aten::size(%hidden_states.14, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %690 : int = aten::size(%hidden_states.14, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %691 : Long() = aten::mul(%688, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %692 : Long() = aten::sub(%691, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %693 : int = aten::Int(%692), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %694 : int[] = prim::ListConstruct(%686, %693, %689, %690), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %695 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %696 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.14, %694, %695, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %697 : int = aten::size(%hidden_states.15, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %698 : int = aten::size(%hidden_states.15, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %699 : Long() = prim::NumToTensor(%698), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %700 : Long() = aten::floor_divide(%699, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %701 : int = aten::Int(%700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %702 : int = aten::size(%hidden_states.15, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %703 : int[] = prim::ListConstruct(%697, %701, %68, %702), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.16 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.15, %703), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %705 : int = aten::size(%hidden_states.16, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %706 : int = aten::size(%hidden_states.16, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %707 : Long() = prim::NumToTensor(%706), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %708 : int = aten::size(%hidden_states.16, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %709 : int = aten::size(%hidden_states.16, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %710 : Long() = aten::mul(%707, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %711 : Long() = aten::sub(%710, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %712 : int = aten::Int(%711), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %713 : int[] = prim::ListConstruct(%705, %712, %708, %709), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %714 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %715 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.16, %713, %714, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %716 : Tensor[] = prim::ListConstruct(%696, %715), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.20 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %716), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %718 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states_padded.3 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.20, %718, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %720 : int = aten::size(%hidden_states_padded.3, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %721 : int = aten::size(%hidden_states_padded.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %722 : int = aten::size(%hidden_states_padded.3, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %723 : int = aten::size(%hidden_states_padded.3, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %724 : int[] = prim::ListConstruct(%720, %721, %722, %723), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_chunked_attention_scores.3 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.3, %724), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:400:0
  %726 : Long() = aten::mul(%batch_size.6, %num_heads.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %727 : int = aten::Int(%726), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %728 : Long() = aten::add(%chunks_count.4, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %729 : int = aten::Int(%728), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %730 : int[] = prim::ListConstruct(%727, %729, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_attention_scores.3 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.3, %730, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
  %732 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %733 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%732, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %734 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%733, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %735 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%734, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %736 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %737 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%736, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %738 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%737, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %739 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%738, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %740 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %741 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%735, %740), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %742 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%739, %741, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %743 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %744 : Float(204:262656, 512:513, 513:1) = aten::select(%743, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %745 : Float(204:262656, 256:513, 513:1) = aten::slice(%744, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %746 : Float(204:262656, 256:513, 257:1) = aten::slice(%745, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %747 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %748 : Float(204:262656, 256:513, 513:1) = aten::select(%747, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %749 : Float(204:262656, 256:513, 513:1) = aten::slice(%748, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %750 : Float(204:262656, 256:513, 257:1) = aten::slice(%749, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %751 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %752 : Float(204:262656, 256:513, 257:1) = aten::view(%746, %751), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %753 : Float(204:262656, 256:513, 257:1) = aten::copy_(%750, %752, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %754 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %755 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%754, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %756 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%755, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %757 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%756, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %758 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %759 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%758, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %760 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%759, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %761 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%760, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %762 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %763 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%757, %762), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %764 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%761, %763, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %765 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %766 : Float(204:262656, 512:513, 513:1) = aten::select(%765, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %767 : Float(204:262656, 255:513, 513:1) = aten::slice(%766, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %768 : Float(204:262656, 255:513, 255:1) = aten::slice(%767, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %769 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %770 : Float(204:262656, 256:513, 513:1) = aten::select(%769, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %771 : Float(204:262656, 255:513, 513:1) = aten::slice(%770, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %772 : Float(204:262656, 255:513, 255:1) = aten::slice(%771, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %773 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %774 : Float(204:262656, 255:513, 255:1) = aten::view(%768, %773), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %775 : Float(204:262656, 255:513, 255:1) = aten::copy_(%772, %774, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %776 : int[] = prim::ListConstruct(%659, %663, %661, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %777 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.3, %776), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.4 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%777, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %779 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %780 : Float(256:257, 257:1) = aten::ones(%779, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %781 : Float(256:257, 257:1) = aten::tril(%780, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %782 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %beginning_mask_2d.3 : Float(256:257, 257:1) = aten::flip(%781, %782), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %784 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.3, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %785 : Float(1:65792, 256:257, 257:1) = aten::slice(%784, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %786 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%785, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.3 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%786, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %788 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %ending_mask.3 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.3, %788), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
  %790 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %791 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%790, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %792 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%791, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.3 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%792, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %794 : int = aten::size(%beginning_input.3, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %795 : int = aten::size(%beginning_input.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %796 : int = aten::size(%beginning_input.3, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %797 : int = aten::size(%beginning_input.3, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %798 : int[] = prim::ListConstruct(%794, %795, %796, %797), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %799 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.3, %798, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %800 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%799, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %801 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.3, %800, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
  %802 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %803 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%802, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %804 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%803, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.3 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%804, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %806 : int = aten::size(%ending_input.3, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %807 : int = aten::size(%ending_input.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %808 : int = aten::size(%ending_input.3, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %809 : int = aten::size(%ending_input.3, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %810 : int[] = prim::ListConstruct(%806, %807, %808, %809), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %811 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.3, %810, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %812 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%811, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %813 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.3, %812, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
  %814 : Bool(17:512, 512:1) = aten::ne(%631, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %815 : Bool(17:512, 512:1) = aten::slice(%814, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %816 : Bool(17:512, 512:1) = aten::slice(%815, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %817 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%816, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.2 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%817, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:272:0
  %819 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.2, %query.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.2 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%819, %remove_from_windowed_attention_mask.2, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:275:0
  %821 : int = aten::size(%float_mask.2, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %822 : int = aten::size(%float_mask.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %823 : int = aten::size(%float_mask.2, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %824 : int = aten::size(%float_mask.2, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %825 : int[] = prim::ListConstruct(%821, %822, %823, %824), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %query.4 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%825, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:280:0
  %827 : int = aten::size(%query.4, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.7 : Long() = prim::NumToTensor(%827), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %829 : int = aten::size(%query.4, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.8 : Long() = prim::NumToTensor(%829), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %831 : int = aten::size(%query.4, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.5 : Long() = prim::NumToTensor(%831), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %833 : int = aten::size(%query.4, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:472:0
  %834 : Long() = aten::floor_divide(%seq_len.8, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.5 : Long() = aten::sub(%834, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:478:0
  %836 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.4, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %837 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %838 : int = aten::Int(%837), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %839 : int[] = prim::ListConstruct(%838, %829, %833), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.17 : Float(17:512, 512:1, 1:1) = aten::reshape(%836, %839), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:481:0
  %841 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.2, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %842 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %843 : int = aten::Int(%842), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %844 : int[] = prim::ListConstruct(%843, %829, %833), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.19 : Float(17:512, 512:1, 1:1) = aten::reshape(%841, %844), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:482:0
  %846 : int = aten::size(%hidden_states.17, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %847 : int = aten::size(%hidden_states.17, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %848 : Long() = prim::NumToTensor(%847), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %849 : Long() = aten::floor_divide(%848, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %850 : int = aten::Int(%849), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %851 : int = aten::size(%hidden_states.17, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %852 : int[] = prim::ListConstruct(%846, %850, %68, %851), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.18 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.17, %852), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %854 : int = aten::size(%hidden_states.18, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %855 : int = aten::size(%hidden_states.18, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %856 : Long() = prim::NumToTensor(%855), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %857 : int = aten::size(%hidden_states.18, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %858 : int = aten::size(%hidden_states.18, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %859 : Long() = aten::mul(%856, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %860 : Long() = aten::sub(%859, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %861 : int = aten::Int(%860), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %862 : int[] = prim::ListConstruct(%854, %861, %857, %858), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %863 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %864 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.18, %862, %863, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %865 : int = aten::size(%hidden_states.19, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:442:0
  %866 : int = aten::size(%hidden_states.19, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:443:0
  %867 : Long() = prim::NumToTensor(%866), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %868 : Long() = aten::floor_divide(%867, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %869 : int = aten::Int(%868), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %870 : int = aten::size(%hidden_states.19, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:445:0
  %871 : int[] = prim::ListConstruct(%865, %869, %68, %870), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states.20 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.19, %871), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:441:0
  %873 : int = aten::size(%hidden_states.20, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %874 : int = aten::size(%hidden_states.20, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %875 : Long() = prim::NumToTensor(%874), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %876 : int = aten::size(%hidden_states.20, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %877 : int = aten::size(%hidden_states.20, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:449:0
  %878 : Long() = aten::mul(%875, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %879 : Long() = aten::sub(%878, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:450:0
  %880 : int = aten::Int(%879), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %881 : int[] = prim::ListConstruct(%873, %880, %876, %877), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %882 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %883 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.20, %881, %882, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:454:0
  %884 : Tensor[] = prim::ListConstruct(%864, %883), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.21 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %884), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %886 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %hidden_states_padded.4 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.21, %886, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %888 : int = aten::size(%hidden_states_padded.4, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %889 : int = aten::size(%hidden_states_padded.4, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %890 : int = aten::size(%hidden_states_padded.4, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %891 : int = aten::size(%hidden_states_padded.4, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:401:0
  %892 : int[] = prim::ListConstruct(%888, %889, %890, %891), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_chunked_attention_scores.4 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.4, %892), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:400:0
  %894 : Long() = aten::mul(%batch_size.7, %num_heads.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %895 : int = aten::Int(%894), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %896 : Long() = aten::add(%chunks_count.5, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:504:0
  %897 : int = aten::Int(%896), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %898 : int[] = prim::ListConstruct(%895, %897, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %diagonal_attention_scores.4 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.4, %898, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:503:0
  %900 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %901 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%900, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %902 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%901, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %903 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%902, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %904 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %905 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%904, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %906 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%905, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %907 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%906, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %908 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %909 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%903, %908), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %910 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%907, %909, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:509:0
  %911 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %912 : Float(17:262656, 512:513, 513:1) = aten::select(%911, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %913 : Float(17:262656, 256:513, 513:1) = aten::slice(%912, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %914 : Float(17:262656, 256:513, 257:1) = aten::slice(%913, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %915 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %916 : Float(17:262656, 256:513, 513:1) = aten::select(%915, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %917 : Float(17:262656, 256:513, 513:1) = aten::slice(%916, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %918 : Float(17:262656, 256:513, 257:1) = aten::slice(%917, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %919 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %920 : Float(17:262656, 256:513, 257:1) = aten::view(%914, %919), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %921 : Float(17:262656, 256:513, 257:1) = aten::copy_(%918, %920, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:512:0
  %922 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %923 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%922, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %924 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%923, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %925 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%924, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %926 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %927 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%926, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %928 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%927, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %929 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%928, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %930 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %931 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%925, %930), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %932 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%929, %931, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:516:0
  %933 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %934 : Float(17:262656, 512:513, 513:1) = aten::select(%933, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %935 : Float(17:262656, 255:513, 513:1) = aten::slice(%934, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %936 : Float(17:262656, 255:513, 255:1) = aten::slice(%935, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %937 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %938 : Float(17:262656, 256:513, 513:1) = aten::select(%937, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %939 : Float(17:262656, 255:513, 513:1) = aten::slice(%938, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %940 : Float(17:262656, 255:513, 255:1) = aten::slice(%939, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %941 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %942 : Float(17:262656, 255:513, 255:1) = aten::view(%936, %941), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %943 : Float(17:262656, 255:513, 255:1) = aten::copy_(%940, %942, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:520:0
  %944 : int[] = prim::ListConstruct(%827, %831, %829, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %945 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.4, %944), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.5 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%945, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:525:0
  %947 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %948 : Float(256:257, 257:1) = aten::ones(%947, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %949 : Float(256:257, 257:1) = aten::tril(%948, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %950 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %beginning_mask_2d.4 : Float(256:257, 257:1) = aten::flip(%949, %950), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:458:0
  %952 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.4, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %953 : Float(1:65792, 256:257, 257:1) = aten::slice(%952, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %954 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%953, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.4 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%954, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:459:0
  %956 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %ending_mask.4 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.4, %956), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:460:0
  %958 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %959 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%958, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %960 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%959, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.4 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%960, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:461:0
  %962 : int = aten::size(%beginning_input.4, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %963 : int = aten::size(%beginning_input.4, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %964 : int = aten::size(%beginning_input.4, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %965 : int = aten::size(%beginning_input.4, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %966 : int[] = prim::ListConstruct(%962, %963, %964, %965), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %967 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.4, %966, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:462:0
  %968 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%967, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %969 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.4, %968, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:463:0
  %970 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %971 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%970, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %972 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%971, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.4 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%972, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:464:0
  %974 : int = aten::size(%ending_input.4, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %975 : int = aten::size(%ending_input.4, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %976 : int = aten::size(%ending_input.4, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %977 : int = aten::size(%ending_input.4, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %978 : int[] = prim::ListConstruct(%974, %975, %976, %977), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %979 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.4, %978, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:465:0
  %980 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%979, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:22:0
  %981 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.4, %980, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.2 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.4, %input_tensor.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.2 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.2, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:1500:0
  %984 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.2, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %985 : Bool(17:512, 512:1) = aten::slice(%984, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %986 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%985, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %987 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%986, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %input.22 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.2, %987, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.4 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.22, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:973:0
  %990 : int[] = prim::ListConstruct(%649, %650, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %991 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.2, %990), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
  %value.2 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%991, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:331:0
  %993 : int = aten::size(%value.2, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.8 : Long() = prim::NumToTensor(%993), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %995 : int = aten::size(%value.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.9 : Long() = prim::NumToTensor(%995), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %997 : int = aten::size(%value.2, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.6 : Long() = prim::NumToTensor(%997), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %999 : int = aten::size(%value.2, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:537:0
  %1000 : Long() = aten::floor_divide(%seq_len.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %chunks_count.6 : Long() = aten::sub(%1000, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:542:0
  %1002 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.4, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
  %1003 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:546:0
  %1004 : int = aten::Int(%1003), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1005 : Long() = aten::floor_divide(%seq_len.9, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/tensor.py:424:0
  %1006 : int = aten::Int(%1005), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1007 : int[] = prim::ListConstruct(%1004, %1006, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.6 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1002, %1007), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:545:0
  %1009 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.2, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %1010 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %1011 : int = aten::Int(%1010), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1012 : int[] = prim::ListConstruct(%1011, %995, %999), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %input.23 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1009, %1012), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:550:0
  %1014 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %padded_value.2 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.23, %1014, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %1016 : Long() = aten::mul(%batch_size.8, %num_heads.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
  %1017 : int = aten::Int(%1016), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1018 : Long() = aten::add(%chunks_count.6, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:556:0
  %1019 : int = aten::Int(%1018), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1020 : int[] = prim::ListConstruct(%1017, %1019, %59, %999), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1021 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1022 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.2, %1020, %1021, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:564:0
  %1023 : int = aten::size(%chunked_hidden_states.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %1024 : int = aten::size(%chunked_hidden_states.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %1025 : int = aten::size(%chunked_hidden_states.6, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.2 : Long() = prim::NumToTensor(%1025), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1027 : int = aten::size(%chunked_hidden_states.6, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.2 : Long() = prim::NumToTensor(%1027), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1029 : Long() = aten::add(%window_overlap.2, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:422:0
  %1030 : int = aten::Int(%1029), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1031 : int[] = prim::ListConstruct(%74, %1030), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.7 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.6, %1031, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/nn/functional.py:3552:0
  %1033 : int[] = prim::ListConstruct(%1023, %1024, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.8 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.7, %1033), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:424:0
  %1035 : Long() = aten::neg(%window_overlap.2), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:428:0
  %1036 : int = aten::Int(%1035), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1037 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %1038 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1037, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.9 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1038, %66, %74, %1036, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:427:0
  %1040 : Long() = aten::add(%window_overlap.2, %hidden_dim.2, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:431:0
  %1041 : int = aten::Int(%1040), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1042 : int[] = prim::ListConstruct(%1023, %1024, %1025, %1041), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %chunked_hidden_states.10 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.9, %1042), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:430:0
  %1044 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1045 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1044, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1046 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1045, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1047 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1046, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:433:0
  %1048 : Tensor[] = prim::ListConstruct(%1047, %1022), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %context.2 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %1048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # torch/functional.py:327:0
  %1050 : int[] = prim::ListConstruct(%993, %997, %995, %999), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1051 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.2, %1050), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.3 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1051, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:569:0
  %1053 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.3, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %1054 : int[] = prim::ListConstruct(%649, %650, %651), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self
  %1055 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1053, %1054), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.4 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1055, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:350:0
  %input.24 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.4, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.self # transformers/modeling_longformer.py:374:0
  %1058 : __torch__.torch.nn.modules.normalization.___torch_mangle_7116.LayerNorm = prim::GetAttr[name="LayerNorm"](%625)
  %1059 : __torch__.torch.nn.modules.linear.___torch_mangle_7115.Linear = prim::GetAttr[name="dense"](%625)
  %1060 : Tensor = prim::GetAttr[name="bias"](%1059)
  %1061 : Tensor = prim::GetAttr[name="weight"](%1059)
  %1062 : Float(768:1, 768:768) = aten::t(%1061), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.24, %1062), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.25 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.10, %1060, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.25, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.26 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.21, %hidden_states.12, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output # transformers/modeling_longformer.py:758:0
  %1067 : Tensor = prim::GetAttr[name="bias"](%1058)
  %1068 : Tensor = prim::GetAttr[name="weight"](%1058)
  %1069 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.6 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.26, %1069, %1068, %1067, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.attention/__module.longformer.encoder.layer.1.attention.output/__module.longformer.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1071 : __torch__.torch.nn.modules.linear.___torch_mangle_7120.Linear = prim::GetAttr[name="dense"](%623)
  %1072 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1073 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1074 : Float(768:1, 3072:768) = aten::t(%1073), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.6, %1074), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.27 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.11, %1072, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate/__module.longformer.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.28 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %1078 : __torch__.torch.nn.modules.normalization.___torch_mangle_7123.LayerNorm = prim::GetAttr[name="LayerNorm"](%622)
  %1079 : __torch__.torch.nn.modules.linear.___torch_mangle_7122.Linear = prim::GetAttr[name="dense"](%622)
  %1080 : Tensor = prim::GetAttr[name="bias"](%1079)
  %1081 : Tensor = prim::GetAttr[name="weight"](%1079)
  %1082 : Float(3072:1, 768:3072) = aten::t(%1081), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.28, %1082), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.12, %1080, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.29, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output # transformers/modeling_longformer.py:830:0
  %1087 : Tensor = prim::GetAttr[name="bias"](%1078)
  %1088 : Tensor = prim::GetAttr[name="weight"](%1078)
  %1089 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.LayerNorm
  %hidden_states.23 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.30, %1089, %1088, %1087, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.1/__module.longformer.encoder.layer.1.output/__module.longformer.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %1091 : __torch__.transformers.modeling_longformer.___torch_mangle_7144.LongformerOutput = prim::GetAttr[name="output"](%148)
  %1092 : __torch__.transformers.modeling_longformer.___torch_mangle_7140.LongformerIntermediate = prim::GetAttr[name="intermediate"](%148)
  %1093 : __torch__.transformers.modeling_longformer.___torch_mangle_7138.LongformerAttention = prim::GetAttr[name="attention"](%148)
  %1094 : __torch__.transformers.modeling_longformer.___torch_mangle_7137.LongformerSelfOutput = prim::GetAttr[name="output"](%1093)
  %1095 : __torch__.transformers.modeling_longformer.___torch_mangle_7133.LongformerSelfAttention = prim::GetAttr[name="self"](%1093)
  %1096 : __torch__.torch.nn.modules.linear.___torch_mangle_7129.Linear = prim::GetAttr[name="value"](%1095)
  %1097 : __torch__.torch.nn.modules.linear.___torch_mangle_7128.Linear = prim::GetAttr[name="key"](%1095)
  %1098 : __torch__.torch.nn.modules.linear.___torch_mangle_7127.Linear = prim::GetAttr[name="query"](%1095)
  %1099 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
  %1100 : Float(17:512, 512:1) = aten::squeeze(%1099, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.3 : Bool(17:512, 512:1) = aten::lt(%1100, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %input.31 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.23, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:248:0
  %1103 : Tensor = prim::GetAttr[name="bias"](%1098)
  %1104 : Tensor = prim::GetAttr[name="weight"](%1098)
  %1105 : Float(768:1, 768:768) = aten::t(%1104), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1105), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.13, %1103, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %1108 : Tensor = prim::GetAttr[name="bias"](%1097)
  %1109 : Tensor = prim::GetAttr[name="weight"](%1097)
  %1110 : Float(768:1, 768:768) = aten::t(%1109), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1110), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.14, %1108, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %1113 : Tensor = prim::GetAttr[name="bias"](%1096)
  %1114 : Tensor = prim::GetAttr[name="weight"](%1096)
  %1115 : Float(768:1, 768:768) = aten::t(%1114), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.31, %1115), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.3 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.15, %1113, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self/__module.longformer.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %1118 : int = aten::size(%input.31, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %1119 : int = aten::size(%input.31, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %1120 : int = aten::size(%input.31, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.5, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:261:0
  %1122 : int[] = prim::ListConstruct(%1118, %1119, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1123 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.6, %1122), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
  %query.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1123, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:263:0
  %1125 : int[] = prim::ListConstruct(%1118, %1119, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1126 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.3, %1125), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
  %key.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1126, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:264:0
  %1128 : int = aten::size(%query.5, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.10 : Long() = prim::NumToTensor(%1128), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1130 : int = aten::size(%query.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.11 : Long() = prim::NumToTensor(%1130), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1132 : int = aten::size(%query.5, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.7 : Long() = prim::NumToTensor(%1132), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1134 : int = aten::size(%query.5, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %1135 : Long() = aten::floor_divide(%seq_len.11, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.7 : Long() = aten::sub(%1135, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
  %1137 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.5, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1138 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1139 : int = aten::Int(%1138), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1140 : int[] = prim::ListConstruct(%1139, %1130, %1134), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.24 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1137, %1140), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1142 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.3, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1143 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1144 : int = aten::Int(%1143), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1145 : int[] = prim::ListConstruct(%1144, %1130, %1134), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.26 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1142, %1145), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1147 : int = aten::size(%hidden_states.24, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1148 : int = aten::size(%hidden_states.24, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1149 : Long() = prim::NumToTensor(%1148), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1150 : Long() = aten::floor_divide(%1149, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1151 : int = aten::Int(%1150), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1152 : int = aten::size(%hidden_states.24, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1153 : int[] = prim::ListConstruct(%1147, %1151, %68, %1152), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.25 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.24, %1153), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1155 : int = aten::size(%hidden_states.25, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1156 : int = aten::size(%hidden_states.25, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1157 : Long() = prim::NumToTensor(%1156), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1158 : int = aten::size(%hidden_states.25, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1159 : int = aten::size(%hidden_states.25, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1160 : Long() = aten::mul(%1157, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1161 : Long() = aten::sub(%1160, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1162 : int = aten::Int(%1161), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1163 : int[] = prim::ListConstruct(%1155, %1162, %1158, %1159), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1164 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1165 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.25, %1163, %1164, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1166 : int = aten::size(%hidden_states.26, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1167 : int = aten::size(%hidden_states.26, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1168 : Long() = prim::NumToTensor(%1167), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1169 : Long() = aten::floor_divide(%1168, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1170 : int = aten::Int(%1169), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1171 : int = aten::size(%hidden_states.26, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1172 : int[] = prim::ListConstruct(%1166, %1170, %68, %1171), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.27 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.26, %1172), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1174 : int = aten::size(%hidden_states.27, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1175 : int = aten::size(%hidden_states.27, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1176 : Long() = prim::NumToTensor(%1175), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1177 : int = aten::size(%hidden_states.27, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1178 : int = aten::size(%hidden_states.27, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1179 : Long() = aten::mul(%1176, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1180 : Long() = aten::sub(%1179, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1181 : int = aten::Int(%1180), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1182 : int[] = prim::ListConstruct(%1174, %1181, %1177, %1178), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1183 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1184 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.27, %1182, %1183, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1185 : Tensor[] = prim::ListConstruct(%1165, %1184), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.32 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %1185), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1187 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states_padded.5 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.32, %1187, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1189 : int = aten::size(%hidden_states_padded.5, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1190 : int = aten::size(%hidden_states_padded.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1191 : int = aten::size(%hidden_states_padded.5, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1192 : int = aten::size(%hidden_states_padded.5, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1193 : int[] = prim::ListConstruct(%1189, %1190, %1191, %1192), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_chunked_attention_scores.5 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.5, %1193), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:400:0
  %1195 : Long() = aten::mul(%batch_size.10, %num_heads.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1196 : int = aten::Int(%1195), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1197 : Long() = aten::add(%chunks_count.7, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1198 : int = aten::Int(%1197), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1199 : int[] = prim::ListConstruct(%1196, %1198, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_attention_scores.5 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.5, %1199, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
  %1201 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1202 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1201, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1203 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1202, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1204 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%1203, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1205 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1206 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1205, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1207 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1206, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1208 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%1207, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1209 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1210 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%1204, %1209), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1211 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1208, %1210, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1212 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1213 : Float(204:262656, 512:513, 513:1) = aten::select(%1212, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1214 : Float(204:262656, 256:513, 513:1) = aten::slice(%1213, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1215 : Float(204:262656, 256:513, 257:1) = aten::slice(%1214, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1216 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1217 : Float(204:262656, 256:513, 513:1) = aten::select(%1216, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1218 : Float(204:262656, 256:513, 513:1) = aten::slice(%1217, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1219 : Float(204:262656, 256:513, 257:1) = aten::slice(%1218, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1220 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1221 : Float(204:262656, 256:513, 257:1) = aten::view(%1215, %1220), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1222 : Float(204:262656, 256:513, 257:1) = aten::copy_(%1219, %1221, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1223 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1224 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1223, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1225 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1224, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1226 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%1225, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1227 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1228 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1227, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1229 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1228, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1230 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%1229, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1231 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1232 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%1226, %1231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1233 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1230, %1232, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1234 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1235 : Float(204:262656, 512:513, 513:1) = aten::select(%1234, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1236 : Float(204:262656, 255:513, 513:1) = aten::slice(%1235, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1237 : Float(204:262656, 255:513, 255:1) = aten::slice(%1236, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1238 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1239 : Float(204:262656, 256:513, 513:1) = aten::select(%1238, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1240 : Float(204:262656, 255:513, 513:1) = aten::slice(%1239, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1241 : Float(204:262656, 255:513, 255:1) = aten::slice(%1240, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1242 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1243 : Float(204:262656, 255:513, 255:1) = aten::view(%1237, %1242), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1244 : Float(204:262656, 255:513, 255:1) = aten::copy_(%1241, %1243, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1245 : int[] = prim::ListConstruct(%1128, %1132, %1130, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1246 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.5, %1245), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.7 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%1246, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %1248 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1249 : Float(256:257, 257:1) = aten::ones(%1248, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1250 : Float(256:257, 257:1) = aten::tril(%1249, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1251 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %beginning_mask_2d.5 : Float(256:257, 257:1) = aten::flip(%1250, %1251), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1253 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.5, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1254 : Float(1:65792, 256:257, 257:1) = aten::slice(%1253, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1255 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1254, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.5 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1255, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1257 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %ending_mask.5 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.5, %1257), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
  %1259 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1260 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1259, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1261 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1260, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.5 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1261, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1263 : int = aten::size(%beginning_input.5, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1264 : int = aten::size(%beginning_input.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1265 : int = aten::size(%beginning_input.5, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1266 : int = aten::size(%beginning_input.5, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1267 : int[] = prim::ListConstruct(%1263, %1264, %1265, %1266), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1268 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.5, %1267, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1269 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1268, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1270 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.5, %1269, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
  %1271 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1272 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1271, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1273 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1272, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.5 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1273, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1275 : int = aten::size(%ending_input.5, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1276 : int = aten::size(%ending_input.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1277 : int = aten::size(%ending_input.5, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1278 : int = aten::size(%ending_input.5, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1279 : int[] = prim::ListConstruct(%1275, %1276, %1277, %1278), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1280 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.5, %1279, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1281 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1280, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1282 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.5, %1281, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
  %1283 : Bool(17:512, 512:1) = aten::ne(%1100, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1284 : Bool(17:512, 512:1) = aten::slice(%1283, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1285 : Bool(17:512, 512:1) = aten::slice(%1284, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1286 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1285, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.3 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1286, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:272:0
  %1288 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.3, %query.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.3 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%1288, %remove_from_windowed_attention_mask.3, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:275:0
  %1290 : int = aten::size(%float_mask.3, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1291 : int = aten::size(%float_mask.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1292 : int = aten::size(%float_mask.3, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1293 : int = aten::size(%float_mask.3, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1294 : int[] = prim::ListConstruct(%1290, %1291, %1292, %1293), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %query.6 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%1294, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:280:0
  %1296 : int = aten::size(%query.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.11 : Long() = prim::NumToTensor(%1296), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1298 : int = aten::size(%query.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.12 : Long() = prim::NumToTensor(%1298), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1300 : int = aten::size(%query.6, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.8 : Long() = prim::NumToTensor(%1300), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1302 : int = aten::size(%query.6, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:472:0
  %1303 : Long() = aten::floor_divide(%seq_len.12, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.8 : Long() = aten::sub(%1303, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:478:0
  %1305 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.6, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1306 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1307 : int = aten::Int(%1306), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1308 : int[] = prim::ListConstruct(%1307, %1298, %1302), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.28 : Float(17:512, 512:1, 1:1) = aten::reshape(%1305, %1308), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:481:0
  %1310 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.3, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1311 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1312 : int = aten::Int(%1311), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1313 : int[] = prim::ListConstruct(%1312, %1298, %1302), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.30 : Float(17:512, 512:1, 1:1) = aten::reshape(%1310, %1313), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:482:0
  %1315 : int = aten::size(%hidden_states.28, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1316 : int = aten::size(%hidden_states.28, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1317 : Long() = prim::NumToTensor(%1316), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1318 : Long() = aten::floor_divide(%1317, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1319 : int = aten::Int(%1318), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1320 : int = aten::size(%hidden_states.28, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1321 : int[] = prim::ListConstruct(%1315, %1319, %68, %1320), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.29 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.28, %1321), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1323 : int = aten::size(%hidden_states.29, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1324 : int = aten::size(%hidden_states.29, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1325 : Long() = prim::NumToTensor(%1324), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1326 : int = aten::size(%hidden_states.29, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1327 : int = aten::size(%hidden_states.29, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1328 : Long() = aten::mul(%1325, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1329 : Long() = aten::sub(%1328, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1330 : int = aten::Int(%1329), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1331 : int[] = prim::ListConstruct(%1323, %1330, %1326, %1327), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1332 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1333 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.29, %1331, %1332, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1334 : int = aten::size(%hidden_states.30, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:442:0
  %1335 : int = aten::size(%hidden_states.30, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:443:0
  %1336 : Long() = prim::NumToTensor(%1335), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1337 : Long() = aten::floor_divide(%1336, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1338 : int = aten::Int(%1337), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1339 : int = aten::size(%hidden_states.30, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:445:0
  %1340 : int[] = prim::ListConstruct(%1334, %1338, %68, %1339), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states.31 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.30, %1340), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:441:0
  %1342 : int = aten::size(%hidden_states.31, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1343 : int = aten::size(%hidden_states.31, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1344 : Long() = prim::NumToTensor(%1343), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1345 : int = aten::size(%hidden_states.31, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1346 : int = aten::size(%hidden_states.31, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:449:0
  %1347 : Long() = aten::mul(%1344, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1348 : Long() = aten::sub(%1347, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:450:0
  %1349 : int = aten::Int(%1348), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1350 : int[] = prim::ListConstruct(%1342, %1349, %1345, %1346), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1351 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1352 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.31, %1350, %1351, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:454:0
  %1353 : Tensor[] = prim::ListConstruct(%1333, %1352), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.33 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %1353), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1355 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %hidden_states_padded.6 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.33, %1355, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1357 : int = aten::size(%hidden_states_padded.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1358 : int = aten::size(%hidden_states_padded.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1359 : int = aten::size(%hidden_states_padded.6, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1360 : int = aten::size(%hidden_states_padded.6, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:401:0
  %1361 : int[] = prim::ListConstruct(%1357, %1358, %1359, %1360), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_chunked_attention_scores.6 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.6, %1361), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:400:0
  %1363 : Long() = aten::mul(%batch_size.11, %num_heads.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1364 : int = aten::Int(%1363), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1365 : Long() = aten::add(%chunks_count.8, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:504:0
  %1366 : int = aten::Int(%1365), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1367 : int[] = prim::ListConstruct(%1364, %1366, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %diagonal_attention_scores.6 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.6, %1367, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:503:0
  %1369 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1370 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1369, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1371 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1370, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1372 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%1371, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1373 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1374 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1373, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1375 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1374, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1376 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%1375, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1377 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1378 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%1372, %1377), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1379 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1376, %1378, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:509:0
  %1380 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1381 : Float(17:262656, 512:513, 513:1) = aten::select(%1380, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1382 : Float(17:262656, 256:513, 513:1) = aten::slice(%1381, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1383 : Float(17:262656, 256:513, 257:1) = aten::slice(%1382, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1384 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1385 : Float(17:262656, 256:513, 513:1) = aten::select(%1384, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1386 : Float(17:262656, 256:513, 513:1) = aten::slice(%1385, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1387 : Float(17:262656, 256:513, 257:1) = aten::slice(%1386, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1388 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1389 : Float(17:262656, 256:513, 257:1) = aten::view(%1383, %1388), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1390 : Float(17:262656, 256:513, 257:1) = aten::copy_(%1387, %1389, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:512:0
  %1391 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1392 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1391, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1393 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1392, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1394 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%1393, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1395 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1396 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1395, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1397 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1396, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1398 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%1397, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1399 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1400 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%1394, %1399), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1401 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1398, %1400, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:516:0
  %1402 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1403 : Float(17:262656, 512:513, 513:1) = aten::select(%1402, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1404 : Float(17:262656, 255:513, 513:1) = aten::slice(%1403, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1405 : Float(17:262656, 255:513, 255:1) = aten::slice(%1404, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1406 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1407 : Float(17:262656, 256:513, 513:1) = aten::select(%1406, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1408 : Float(17:262656, 255:513, 513:1) = aten::slice(%1407, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1409 : Float(17:262656, 255:513, 255:1) = aten::slice(%1408, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1410 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1411 : Float(17:262656, 255:513, 255:1) = aten::view(%1405, %1410), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1412 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1409, %1411, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:520:0
  %1413 : int[] = prim::ListConstruct(%1296, %1300, %1298, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1414 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.6, %1413), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.8 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1414, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:525:0
  %1416 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1417 : Float(256:257, 257:1) = aten::ones(%1416, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1418 : Float(256:257, 257:1) = aten::tril(%1417, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1419 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %beginning_mask_2d.6 : Float(256:257, 257:1) = aten::flip(%1418, %1419), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:458:0
  %1421 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1422 : Float(1:65792, 256:257, 257:1) = aten::slice(%1421, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1423 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1422, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.6 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1423, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:459:0
  %1425 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %ending_mask.6 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.6, %1425), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:460:0
  %1427 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1428 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1427, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1429 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1428, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.6 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1429, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:461:0
  %1431 : int = aten::size(%beginning_input.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1432 : int = aten::size(%beginning_input.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1433 : int = aten::size(%beginning_input.6, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1434 : int = aten::size(%beginning_input.6, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1435 : int[] = prim::ListConstruct(%1431, %1432, %1433, %1434), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1436 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.6, %1435, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:462:0
  %1437 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1436, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1438 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.6, %1437, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:463:0
  %1439 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1440 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1439, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1441 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1440, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.6 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1441, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:464:0
  %1443 : int = aten::size(%ending_input.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1444 : int = aten::size(%ending_input.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1445 : int = aten::size(%ending_input.6, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1446 : int = aten::size(%ending_input.6, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1447 : int[] = prim::ListConstruct(%1443, %1444, %1445, %1446), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1448 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.6, %1447, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:465:0
  %1449 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1448, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:22:0
  %1450 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.6, %1449, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.3 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.7, %input_tensor.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.3 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.3, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:1500:0
  %1453 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.3, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1454 : Bool(17:512, 512:1) = aten::slice(%1453, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1455 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1454, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %1456 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1455, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %input.34 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.3, %1456, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.6 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.34, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:973:0
  %1459 : int[] = prim::ListConstruct(%1118, %1119, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1460 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.3, %1459), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
  %value.3 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1460, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:331:0
  %1462 : int = aten::size(%value.3, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.12 : Long() = prim::NumToTensor(%1462), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1464 : int = aten::size(%value.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.13 : Long() = prim::NumToTensor(%1464), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1466 : int = aten::size(%value.3, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.9 : Long() = prim::NumToTensor(%1466), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1468 : int = aten::size(%value.3, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:537:0
  %1469 : Long() = aten::floor_divide(%seq_len.13, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %chunks_count.9 : Long() = aten::sub(%1469, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:542:0
  %1471 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.6, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
  %1472 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:546:0
  %1473 : int = aten::Int(%1472), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1474 : Long() = aten::floor_divide(%seq_len.13, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/tensor.py:424:0
  %1475 : int = aten::Int(%1474), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1476 : int[] = prim::ListConstruct(%1473, %1475, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.11 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1471, %1476), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:545:0
  %1478 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.3, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1479 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1480 : int = aten::Int(%1479), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1481 : int[] = prim::ListConstruct(%1480, %1464, %1468), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %input.35 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1478, %1481), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:550:0
  %1483 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %padded_value.3 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.35, %1483, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1485 : Long() = aten::mul(%batch_size.12, %num_heads.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
  %1486 : int = aten::Int(%1485), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1487 : Long() = aten::add(%chunks_count.9, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:556:0
  %1488 : int = aten::Int(%1487), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1489 : int[] = prim::ListConstruct(%1486, %1488, %59, %1468), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1490 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1491 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.3, %1489, %1490, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:564:0
  %1492 : int = aten::size(%chunked_hidden_states.11, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %1493 : int = aten::size(%chunked_hidden_states.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %1494 : int = aten::size(%chunked_hidden_states.11, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.3 : Long() = prim::NumToTensor(%1494), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1496 : int = aten::size(%chunked_hidden_states.11, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.3 : Long() = prim::NumToTensor(%1496), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1498 : Long() = aten::add(%window_overlap.3, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:422:0
  %1499 : int = aten::Int(%1498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1500 : int[] = prim::ListConstruct(%74, %1499), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.12 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.11, %1500, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/nn/functional.py:3552:0
  %1502 : int[] = prim::ListConstruct(%1492, %1493, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.13 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.12, %1502), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:424:0
  %1504 : Long() = aten::neg(%window_overlap.3), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:428:0
  %1505 : int = aten::Int(%1504), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1506 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %1507 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1506, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.14 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1507, %66, %74, %1505, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:427:0
  %1509 : Long() = aten::add(%window_overlap.3, %hidden_dim.3, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:431:0
  %1510 : int = aten::Int(%1509), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1511 : int[] = prim::ListConstruct(%1492, %1493, %1494, %1510), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %chunked_hidden_states.15 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.14, %1511), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:430:0
  %1513 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1514 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1513, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1515 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1514, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1516 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1515, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:433:0
  %1517 : Tensor[] = prim::ListConstruct(%1516, %1491), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %context.3 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %1517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # torch/functional.py:327:0
  %1519 : int[] = prim::ListConstruct(%1462, %1466, %1464, %1468), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1520 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.3, %1519), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.5 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1520, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:569:0
  %1522 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.5, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %1523 : int[] = prim::ListConstruct(%1118, %1119, %1120), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self
  %1524 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1522, %1523), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.6 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1524, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:350:0
  %input.36 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.6, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.self # transformers/modeling_longformer.py:374:0
  %1527 : __torch__.torch.nn.modules.normalization.___torch_mangle_7135.LayerNorm = prim::GetAttr[name="LayerNorm"](%1094)
  %1528 : __torch__.torch.nn.modules.linear.___torch_mangle_7134.Linear = prim::GetAttr[name="dense"](%1094)
  %1529 : Tensor = prim::GetAttr[name="bias"](%1528)
  %1530 : Tensor = prim::GetAttr[name="weight"](%1528)
  %1531 : Float(768:1, 768:768) = aten::t(%1530), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.36, %1531), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.37 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.16, %1529, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.32 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.37, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.32, %hidden_states.23, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output # transformers/modeling_longformer.py:758:0
  %1536 : Tensor = prim::GetAttr[name="bias"](%1527)
  %1537 : Tensor = prim::GetAttr[name="weight"](%1527)
  %1538 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.9 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.38, %1538, %1537, %1536, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.attention/__module.longformer.encoder.layer.2.attention.output/__module.longformer.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1540 : __torch__.torch.nn.modules.linear.___torch_mangle_7139.Linear = prim::GetAttr[name="dense"](%1092)
  %1541 : Tensor = prim::GetAttr[name="bias"](%1540)
  %1542 : Tensor = prim::GetAttr[name="weight"](%1540)
  %1543 : Float(768:1, 3072:768) = aten::t(%1542), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.9, %1543), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.17, %1541, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate/__module.longformer.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.40 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.39), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %1547 : __torch__.torch.nn.modules.normalization.___torch_mangle_7142.LayerNorm = prim::GetAttr[name="LayerNorm"](%1091)
  %1548 : __torch__.torch.nn.modules.linear.___torch_mangle_7141.Linear = prim::GetAttr[name="dense"](%1091)
  %1549 : Tensor = prim::GetAttr[name="bias"](%1548)
  %1550 : Tensor = prim::GetAttr[name="weight"](%1548)
  %1551 : Float(3072:1, 768:3072) = aten::t(%1550), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.40, %1551), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.18, %1549, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.33 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.41, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.33, %input_tensor.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output # transformers/modeling_longformer.py:830:0
  %1556 : Tensor = prim::GetAttr[name="bias"](%1547)
  %1557 : Tensor = prim::GetAttr[name="weight"](%1547)
  %1558 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.LayerNorm
  %hidden_states.34 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.42, %1558, %1557, %1556, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.2/__module.longformer.encoder.layer.2.output/__module.longformer.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %1560 : __torch__.transformers.modeling_longformer.___torch_mangle_7163.LongformerOutput = prim::GetAttr[name="output"](%146)
  %1561 : __torch__.transformers.modeling_longformer.___torch_mangle_7159.LongformerIntermediate = prim::GetAttr[name="intermediate"](%146)
  %1562 : __torch__.transformers.modeling_longformer.___torch_mangle_7157.LongformerAttention = prim::GetAttr[name="attention"](%146)
  %1563 : __torch__.transformers.modeling_longformer.___torch_mangle_7156.LongformerSelfOutput = prim::GetAttr[name="output"](%1562)
  %1564 : __torch__.transformers.modeling_longformer.___torch_mangle_7152.LongformerSelfAttention = prim::GetAttr[name="self"](%1562)
  %1565 : __torch__.torch.nn.modules.linear.___torch_mangle_7148.Linear = prim::GetAttr[name="value"](%1564)
  %1566 : __torch__.torch.nn.modules.linear.___torch_mangle_7147.Linear = prim::GetAttr[name="key"](%1564)
  %1567 : __torch__.torch.nn.modules.linear.___torch_mangle_7146.Linear = prim::GetAttr[name="query"](%1564)
  %1568 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
  %1569 : Float(17:512, 512:1) = aten::squeeze(%1568, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.4 : Bool(17:512, 512:1) = aten::lt(%1569, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %input.43 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.34, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:248:0
  %1572 : Tensor = prim::GetAttr[name="bias"](%1567)
  %1573 : Tensor = prim::GetAttr[name="weight"](%1567)
  %1574 : Float(768:1, 768:768) = aten::t(%1573), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1574), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.19, %1572, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %1577 : Tensor = prim::GetAttr[name="bias"](%1566)
  %1578 : Tensor = prim::GetAttr[name="weight"](%1566)
  %1579 : Float(768:1, 768:768) = aten::t(%1578), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.20, %1577, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %1582 : Tensor = prim::GetAttr[name="bias"](%1565)
  %1583 : Tensor = prim::GetAttr[name="weight"](%1565)
  %1584 : Float(768:1, 768:768) = aten::t(%1583), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.43, %1584), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.4 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.21, %1582, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self/__module.longformer.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %1587 : int = aten::size(%input.43, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %1588 : int = aten::size(%input.43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %1589 : int = aten::size(%input.43, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.7, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:261:0
  %1591 : int[] = prim::ListConstruct(%1587, %1588, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1592 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.8, %1591), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
  %query.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1592, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:263:0
  %1594 : int[] = prim::ListConstruct(%1587, %1588, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1595 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.4, %1594), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
  %key.4 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1595, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:264:0
  %1597 : int = aten::size(%query.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.14 : Long() = prim::NumToTensor(%1597), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1599 : int = aten::size(%query.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.15 : Long() = prim::NumToTensor(%1599), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1601 : int = aten::size(%query.7, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.10 : Long() = prim::NumToTensor(%1601), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1603 : int = aten::size(%query.7, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %1604 : Long() = aten::floor_divide(%seq_len.15, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.10 : Long() = aten::sub(%1604, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
  %1606 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.7, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1607 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1608 : int = aten::Int(%1607), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1609 : int[] = prim::ListConstruct(%1608, %1599, %1603), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.35 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1606, %1609), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1611 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.4, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1612 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1613 : int = aten::Int(%1612), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1614 : int[] = prim::ListConstruct(%1613, %1599, %1603), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.37 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1611, %1614), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1616 : int = aten::size(%hidden_states.35, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1617 : int = aten::size(%hidden_states.35, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1618 : Long() = prim::NumToTensor(%1617), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1619 : Long() = aten::floor_divide(%1618, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1620 : int = aten::Int(%1619), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1621 : int = aten::size(%hidden_states.35, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1622 : int[] = prim::ListConstruct(%1616, %1620, %68, %1621), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.36 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.35, %1622), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1624 : int = aten::size(%hidden_states.36, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1625 : int = aten::size(%hidden_states.36, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1626 : Long() = prim::NumToTensor(%1625), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1627 : int = aten::size(%hidden_states.36, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1628 : int = aten::size(%hidden_states.36, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1629 : Long() = aten::mul(%1626, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1630 : Long() = aten::sub(%1629, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1631 : int = aten::Int(%1630), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1632 : int[] = prim::ListConstruct(%1624, %1631, %1627, %1628), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1633 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1634 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.36, %1632, %1633, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1635 : int = aten::size(%hidden_states.37, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1636 : int = aten::size(%hidden_states.37, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1637 : Long() = prim::NumToTensor(%1636), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1638 : Long() = aten::floor_divide(%1637, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1639 : int = aten::Int(%1638), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1640 : int = aten::size(%hidden_states.37, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1641 : int[] = prim::ListConstruct(%1635, %1639, %68, %1640), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.38 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.37, %1641), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1643 : int = aten::size(%hidden_states.38, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1644 : int = aten::size(%hidden_states.38, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1645 : Long() = prim::NumToTensor(%1644), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1646 : int = aten::size(%hidden_states.38, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1647 : int = aten::size(%hidden_states.38, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1648 : Long() = aten::mul(%1645, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1649 : Long() = aten::sub(%1648, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1650 : int = aten::Int(%1649), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1651 : int[] = prim::ListConstruct(%1643, %1650, %1646, %1647), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1652 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1653 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.38, %1651, %1652, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1654 : Tensor[] = prim::ListConstruct(%1634, %1653), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.44 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %1654), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1656 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states_padded.7 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.44, %1656, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1658 : int = aten::size(%hidden_states_padded.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1659 : int = aten::size(%hidden_states_padded.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1660 : int = aten::size(%hidden_states_padded.7, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1661 : int = aten::size(%hidden_states_padded.7, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1662 : int[] = prim::ListConstruct(%1658, %1659, %1660, %1661), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_chunked_attention_scores.7 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.7, %1662), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:400:0
  %1664 : Long() = aten::mul(%batch_size.14, %num_heads.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1665 : int = aten::Int(%1664), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1666 : Long() = aten::add(%chunks_count.10, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1667 : int = aten::Int(%1666), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1668 : int[] = prim::ListConstruct(%1665, %1667, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_attention_scores.7 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.7, %1668, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
  %1670 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1671 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1670, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1672 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1671, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1673 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%1672, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1674 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1675 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1674, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1676 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1675, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1677 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%1676, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1678 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1679 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%1673, %1678), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1680 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1677, %1679, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1681 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1682 : Float(204:262656, 512:513, 513:1) = aten::select(%1681, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1683 : Float(204:262656, 256:513, 513:1) = aten::slice(%1682, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1684 : Float(204:262656, 256:513, 257:1) = aten::slice(%1683, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1685 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1686 : Float(204:262656, 256:513, 513:1) = aten::select(%1685, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1687 : Float(204:262656, 256:513, 513:1) = aten::slice(%1686, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1688 : Float(204:262656, 256:513, 257:1) = aten::slice(%1687, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1689 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1690 : Float(204:262656, 256:513, 257:1) = aten::view(%1684, %1689), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1691 : Float(204:262656, 256:513, 257:1) = aten::copy_(%1688, %1690, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1692 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1693 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%1692, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1694 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%1693, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1695 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%1694, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1696 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1697 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1696, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1698 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%1697, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1699 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%1698, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1700 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1701 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%1695, %1700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1702 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1699, %1701, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1703 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1704 : Float(204:262656, 512:513, 513:1) = aten::select(%1703, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1705 : Float(204:262656, 255:513, 513:1) = aten::slice(%1704, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1706 : Float(204:262656, 255:513, 255:1) = aten::slice(%1705, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1707 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1708 : Float(204:262656, 256:513, 513:1) = aten::select(%1707, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1709 : Float(204:262656, 255:513, 513:1) = aten::slice(%1708, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1710 : Float(204:262656, 255:513, 255:1) = aten::slice(%1709, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1711 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1712 : Float(204:262656, 255:513, 255:1) = aten::view(%1706, %1711), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1713 : Float(204:262656, 255:513, 255:1) = aten::copy_(%1710, %1712, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1714 : int[] = prim::ListConstruct(%1597, %1601, %1599, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1715 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.7, %1714), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.10 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%1715, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %1717 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1718 : Float(256:257, 257:1) = aten::ones(%1717, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1719 : Float(256:257, 257:1) = aten::tril(%1718, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1720 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %beginning_mask_2d.7 : Float(256:257, 257:1) = aten::flip(%1719, %1720), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1722 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1723 : Float(1:65792, 256:257, 257:1) = aten::slice(%1722, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1724 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1723, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.7 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1724, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1726 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %ending_mask.7 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.7, %1726), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
  %1728 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1729 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1728, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1730 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1729, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.7 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1730, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1732 : int = aten::size(%beginning_input.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1733 : int = aten::size(%beginning_input.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1734 : int = aten::size(%beginning_input.7, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1735 : int = aten::size(%beginning_input.7, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1736 : int[] = prim::ListConstruct(%1732, %1733, %1734, %1735), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1737 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.7, %1736, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1738 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1737, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1739 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.7, %1738, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
  %1740 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1741 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1740, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1742 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%1741, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.7 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%1742, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1744 : int = aten::size(%ending_input.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1745 : int = aten::size(%ending_input.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1746 : int = aten::size(%ending_input.7, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1747 : int = aten::size(%ending_input.7, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1748 : int[] = prim::ListConstruct(%1744, %1745, %1746, %1747), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1749 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.7, %1748, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1750 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%1749, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1751 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.7, %1750, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
  %1752 : Bool(17:512, 512:1) = aten::ne(%1569, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1753 : Bool(17:512, 512:1) = aten::slice(%1752, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1754 : Bool(17:512, 512:1) = aten::slice(%1753, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1755 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1754, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.4 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1755, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:272:0
  %1757 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.4, %query.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.4 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%1757, %remove_from_windowed_attention_mask.4, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:275:0
  %1759 : int = aten::size(%float_mask.4, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1760 : int = aten::size(%float_mask.4, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1761 : int = aten::size(%float_mask.4, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1762 : int = aten::size(%float_mask.4, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1763 : int[] = prim::ListConstruct(%1759, %1760, %1761, %1762), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %query.8 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%1763, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:280:0
  %1765 : int = aten::size(%query.8, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.15 : Long() = prim::NumToTensor(%1765), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1767 : int = aten::size(%query.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.16 : Long() = prim::NumToTensor(%1767), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1769 : int = aten::size(%query.8, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.11 : Long() = prim::NumToTensor(%1769), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1771 : int = aten::size(%query.8, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:472:0
  %1772 : Long() = aten::floor_divide(%seq_len.16, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.11 : Long() = aten::sub(%1772, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:478:0
  %1774 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.8, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1775 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1776 : int = aten::Int(%1775), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1777 : int[] = prim::ListConstruct(%1776, %1767, %1771), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.39 : Float(17:512, 512:1, 1:1) = aten::reshape(%1774, %1777), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:481:0
  %1779 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.4, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1780 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1781 : int = aten::Int(%1780), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1782 : int[] = prim::ListConstruct(%1781, %1767, %1771), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.41 : Float(17:512, 512:1, 1:1) = aten::reshape(%1779, %1782), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:482:0
  %1784 : int = aten::size(%hidden_states.39, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1785 : int = aten::size(%hidden_states.39, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1786 : Long() = prim::NumToTensor(%1785), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1787 : Long() = aten::floor_divide(%1786, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1788 : int = aten::Int(%1787), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1789 : int = aten::size(%hidden_states.39, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1790 : int[] = prim::ListConstruct(%1784, %1788, %68, %1789), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.40 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.39, %1790), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1792 : int = aten::size(%hidden_states.40, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1793 : int = aten::size(%hidden_states.40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1794 : Long() = prim::NumToTensor(%1793), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1795 : int = aten::size(%hidden_states.40, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1796 : int = aten::size(%hidden_states.40, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1797 : Long() = aten::mul(%1794, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1798 : Long() = aten::sub(%1797, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1799 : int = aten::Int(%1798), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1800 : int[] = prim::ListConstruct(%1792, %1799, %1795, %1796), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1801 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1802 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.40, %1800, %1801, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1803 : int = aten::size(%hidden_states.41, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:442:0
  %1804 : int = aten::size(%hidden_states.41, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:443:0
  %1805 : Long() = prim::NumToTensor(%1804), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1806 : Long() = aten::floor_divide(%1805, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1807 : int = aten::Int(%1806), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1808 : int = aten::size(%hidden_states.41, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:445:0
  %1809 : int[] = prim::ListConstruct(%1803, %1807, %68, %1808), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states.42 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.41, %1809), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:441:0
  %1811 : int = aten::size(%hidden_states.42, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1812 : int = aten::size(%hidden_states.42, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1813 : Long() = prim::NumToTensor(%1812), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1814 : int = aten::size(%hidden_states.42, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1815 : int = aten::size(%hidden_states.42, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:449:0
  %1816 : Long() = aten::mul(%1813, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1817 : Long() = aten::sub(%1816, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:450:0
  %1818 : int = aten::Int(%1817), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1819 : int[] = prim::ListConstruct(%1811, %1818, %1814, %1815), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1820 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1821 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.42, %1819, %1820, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:454:0
  %1822 : Tensor[] = prim::ListConstruct(%1802, %1821), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.45 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %1822), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1824 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %hidden_states_padded.8 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.45, %1824, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1826 : int = aten::size(%hidden_states_padded.8, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1827 : int = aten::size(%hidden_states_padded.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1828 : int = aten::size(%hidden_states_padded.8, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1829 : int = aten::size(%hidden_states_padded.8, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:401:0
  %1830 : int[] = prim::ListConstruct(%1826, %1827, %1828, %1829), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_chunked_attention_scores.8 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.8, %1830), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:400:0
  %1832 : Long() = aten::mul(%batch_size.15, %num_heads.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1833 : int = aten::Int(%1832), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1834 : Long() = aten::add(%chunks_count.11, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:504:0
  %1835 : int = aten::Int(%1834), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1836 : int[] = prim::ListConstruct(%1833, %1835, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %diagonal_attention_scores.8 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.8, %1836, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:503:0
  %1838 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1839 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1838, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1840 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1839, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1841 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%1840, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1842 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1843 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1842, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1844 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1843, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1845 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%1844, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1846 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1847 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%1841, %1846), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1848 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%1845, %1847, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:509:0
  %1849 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1850 : Float(17:262656, 512:513, 513:1) = aten::select(%1849, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1851 : Float(17:262656, 256:513, 513:1) = aten::slice(%1850, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1852 : Float(17:262656, 256:513, 257:1) = aten::slice(%1851, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1853 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1854 : Float(17:262656, 256:513, 513:1) = aten::select(%1853, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1855 : Float(17:262656, 256:513, 513:1) = aten::slice(%1854, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1856 : Float(17:262656, 256:513, 257:1) = aten::slice(%1855, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1857 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1858 : Float(17:262656, 256:513, 257:1) = aten::view(%1852, %1857), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1859 : Float(17:262656, 256:513, 257:1) = aten::copy_(%1856, %1858, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:512:0
  %1860 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1861 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%1860, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1862 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%1861, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1863 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%1862, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1864 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1865 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1864, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1866 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%1865, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1867 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%1866, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1868 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1869 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%1863, %1868), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1870 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%1867, %1869, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:516:0
  %1871 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1872 : Float(17:262656, 512:513, 513:1) = aten::select(%1871, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1873 : Float(17:262656, 255:513, 513:1) = aten::slice(%1872, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1874 : Float(17:262656, 255:513, 255:1) = aten::slice(%1873, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1875 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1876 : Float(17:262656, 256:513, 513:1) = aten::select(%1875, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1877 : Float(17:262656, 255:513, 513:1) = aten::slice(%1876, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1878 : Float(17:262656, 255:513, 255:1) = aten::slice(%1877, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1879 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1880 : Float(17:262656, 255:513, 255:1) = aten::view(%1874, %1879), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1881 : Float(17:262656, 255:513, 255:1) = aten::copy_(%1878, %1880, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:520:0
  %1882 : int[] = prim::ListConstruct(%1765, %1769, %1767, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1883 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.8, %1882), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.11 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%1883, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:525:0
  %1885 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1886 : Float(256:257, 257:1) = aten::ones(%1885, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1887 : Float(256:257, 257:1) = aten::tril(%1886, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1888 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %beginning_mask_2d.8 : Float(256:257, 257:1) = aten::flip(%1887, %1888), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:458:0
  %1890 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.8, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1891 : Float(1:65792, 256:257, 257:1) = aten::slice(%1890, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1892 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%1891, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.8 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%1892, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:459:0
  %1894 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %ending_mask.8 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.8, %1894), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:460:0
  %1896 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1897 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1896, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1898 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1897, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.8 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1898, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:461:0
  %1900 : int = aten::size(%beginning_input.8, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1901 : int = aten::size(%beginning_input.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1902 : int = aten::size(%beginning_input.8, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1903 : int = aten::size(%beginning_input.8, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1904 : int[] = prim::ListConstruct(%1900, %1901, %1902, %1903), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1905 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.8, %1904, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:462:0
  %1906 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1905, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1907 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.8, %1906, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:463:0
  %1908 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1909 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1908, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1910 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%1909, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.8 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%1910, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:464:0
  %1912 : int = aten::size(%ending_input.8, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1913 : int = aten::size(%ending_input.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1914 : int = aten::size(%ending_input.8, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1915 : int = aten::size(%ending_input.8, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1916 : int[] = prim::ListConstruct(%1912, %1913, %1914, %1915), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1917 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.8, %1916, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:465:0
  %1918 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%1917, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:22:0
  %1919 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.8, %1918, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.4 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.10, %input_tensor.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.4 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.4, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:1500:0
  %1922 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.4, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1923 : Bool(17:512, 512:1) = aten::slice(%1922, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1924 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%1923, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %1925 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%1924, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %input.46 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.4, %1925, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.8 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.46, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:973:0
  %1928 : int[] = prim::ListConstruct(%1587, %1588, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1929 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.4, %1928), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
  %value.4 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%1929, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:331:0
  %1931 : int = aten::size(%value.4, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.16 : Long() = prim::NumToTensor(%1931), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1933 : int = aten::size(%value.4, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.17 : Long() = prim::NumToTensor(%1933), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1935 : int = aten::size(%value.4, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.12 : Long() = prim::NumToTensor(%1935), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1937 : int = aten::size(%value.4, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:537:0
  %1938 : Long() = aten::floor_divide(%seq_len.17, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %chunks_count.12 : Long() = aten::sub(%1938, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:542:0
  %1940 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.8, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
  %1941 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:546:0
  %1942 : int = aten::Int(%1941), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1943 : Long() = aten::floor_divide(%seq_len.17, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/tensor.py:424:0
  %1944 : int = aten::Int(%1943), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1945 : int[] = prim::ListConstruct(%1942, %1944, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.16 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%1940, %1945), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:545:0
  %1947 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.4, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1948 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1949 : int = aten::Int(%1948), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1950 : int[] = prim::ListConstruct(%1949, %1933, %1937), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %input.47 : Float(204:64, 512:13056, 64:1) = aten::reshape(%1947, %1950), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:550:0
  %1952 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %padded_value.4 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.47, %1952, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1954 : Long() = aten::mul(%batch_size.16, %num_heads.12), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
  %1955 : int = aten::Int(%1954), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1956 : Long() = aten::add(%chunks_count.12, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:556:0
  %1957 : int = aten::Int(%1956), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1958 : int[] = prim::ListConstruct(%1955, %1957, %59, %1937), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1959 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1960 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.4, %1958, %1959, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:564:0
  %1961 : int = aten::size(%chunked_hidden_states.16, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %1962 : int = aten::size(%chunked_hidden_states.16, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %1963 : int = aten::size(%chunked_hidden_states.16, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.4 : Long() = prim::NumToTensor(%1963), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1965 : int = aten::size(%chunked_hidden_states.16, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.4 : Long() = prim::NumToTensor(%1965), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1967 : Long() = aten::add(%window_overlap.4, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:422:0
  %1968 : int = aten::Int(%1967), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1969 : int[] = prim::ListConstruct(%74, %1968), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.17 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.16, %1969, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/nn/functional.py:3552:0
  %1971 : int[] = prim::ListConstruct(%1961, %1962, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.18 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.17, %1971), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:424:0
  %1973 : Long() = aten::neg(%window_overlap.4), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:428:0
  %1974 : int = aten::Int(%1973), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1975 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %1976 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%1975, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.19 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%1976, %66, %74, %1974, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:427:0
  %1978 : Long() = aten::add(%window_overlap.4, %hidden_dim.4, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:431:0
  %1979 : int = aten::Int(%1978), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1980 : int[] = prim::ListConstruct(%1961, %1962, %1963, %1979), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %chunked_hidden_states.20 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.19, %1980), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:430:0
  %1982 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1983 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1982, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1984 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%1983, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1985 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%1984, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:433:0
  %1986 : Tensor[] = prim::ListConstruct(%1985, %1960), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %context.4 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %1986), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # torch/functional.py:327:0
  %1988 : int[] = prim::ListConstruct(%1931, %1935, %1933, %1937), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1989 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.4, %1988), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.7 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%1989, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:569:0
  %1991 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.7, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %1992 : int[] = prim::ListConstruct(%1587, %1588, %1589), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self
  %1993 : Float(512:13056, 17:768, 768:1) = aten::reshape(%1991, %1992), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.8 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%1993, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:350:0
  %input.48 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.8, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.self # transformers/modeling_longformer.py:374:0
  %1996 : __torch__.torch.nn.modules.normalization.___torch_mangle_7154.LayerNorm = prim::GetAttr[name="LayerNorm"](%1563)
  %1997 : __torch__.torch.nn.modules.linear.___torch_mangle_7153.Linear = prim::GetAttr[name="dense"](%1563)
  %1998 : Tensor = prim::GetAttr[name="bias"](%1997)
  %1999 : Tensor = prim::GetAttr[name="weight"](%1997)
  %2000 : Float(768:1, 768:768) = aten::t(%1999), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.48, %2000), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.22, %1998, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.43 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.49, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.43, %hidden_states.34, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output # transformers/modeling_longformer.py:758:0
  %2005 : Tensor = prim::GetAttr[name="bias"](%1996)
  %2006 : Tensor = prim::GetAttr[name="weight"](%1996)
  %2007 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.12 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.50, %2007, %2006, %2005, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.attention/__module.longformer.encoder.layer.3.attention.output/__module.longformer.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2009 : __torch__.torch.nn.modules.linear.___torch_mangle_7158.Linear = prim::GetAttr[name="dense"](%1561)
  %2010 : Tensor = prim::GetAttr[name="bias"](%2009)
  %2011 : Tensor = prim::GetAttr[name="weight"](%2009)
  %2012 : Float(768:1, 3072:768) = aten::t(%2011), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.12, %2012), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.23, %2010, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate/__module.longformer.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %2016 : __torch__.torch.nn.modules.normalization.___torch_mangle_7161.LayerNorm = prim::GetAttr[name="LayerNorm"](%1560)
  %2017 : __torch__.torch.nn.modules.linear.___torch_mangle_7160.Linear = prim::GetAttr[name="dense"](%1560)
  %2018 : Tensor = prim::GetAttr[name="bias"](%2017)
  %2019 : Tensor = prim::GetAttr[name="weight"](%2017)
  %2020 : Float(3072:1, 768:3072) = aten::t(%2019), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.52, %2020), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.24, %2018, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.44 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.53, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.44, %input_tensor.12, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output # transformers/modeling_longformer.py:830:0
  %2025 : Tensor = prim::GetAttr[name="bias"](%2016)
  %2026 : Tensor = prim::GetAttr[name="weight"](%2016)
  %2027 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.LayerNorm
  %hidden_states.45 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.54, %2027, %2026, %2025, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.3/__module.longformer.encoder.layer.3.output/__module.longformer.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %2029 : __torch__.transformers.modeling_longformer.___torch_mangle_7182.LongformerOutput = prim::GetAttr[name="output"](%144)
  %2030 : __torch__.transformers.modeling_longformer.___torch_mangle_7178.LongformerIntermediate = prim::GetAttr[name="intermediate"](%144)
  %2031 : __torch__.transformers.modeling_longformer.___torch_mangle_7176.LongformerAttention = prim::GetAttr[name="attention"](%144)
  %2032 : __torch__.transformers.modeling_longformer.___torch_mangle_7175.LongformerSelfOutput = prim::GetAttr[name="output"](%2031)
  %2033 : __torch__.transformers.modeling_longformer.___torch_mangle_7171.LongformerSelfAttention = prim::GetAttr[name="self"](%2031)
  %2034 : __torch__.torch.nn.modules.linear.___torch_mangle_7167.Linear = prim::GetAttr[name="value"](%2033)
  %2035 : __torch__.torch.nn.modules.linear.___torch_mangle_7166.Linear = prim::GetAttr[name="key"](%2033)
  %2036 : __torch__.torch.nn.modules.linear.___torch_mangle_7165.Linear = prim::GetAttr[name="query"](%2033)
  %2037 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
  %2038 : Float(17:512, 512:1) = aten::squeeze(%2037, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.5 : Bool(17:512, 512:1) = aten::lt(%2038, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %input.55 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.45, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:248:0
  %2041 : Tensor = prim::GetAttr[name="bias"](%2036)
  %2042 : Tensor = prim::GetAttr[name="weight"](%2036)
  %2043 : Float(768:1, 768:768) = aten::t(%2042), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2043), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.25, %2041, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %2046 : Tensor = prim::GetAttr[name="bias"](%2035)
  %2047 : Tensor = prim::GetAttr[name="weight"](%2035)
  %2048 : Float(768:1, 768:768) = aten::t(%2047), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.26, %2046, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %2051 : Tensor = prim::GetAttr[name="bias"](%2034)
  %2052 : Tensor = prim::GetAttr[name="weight"](%2034)
  %2053 : Float(768:1, 768:768) = aten::t(%2052), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.55, %2053), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.5 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.27, %2051, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self/__module.longformer.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %2056 : int = aten::size(%input.55, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %2057 : int = aten::size(%input.55, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %2058 : int = aten::size(%input.55, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.9, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:261:0
  %2060 : int[] = prim::ListConstruct(%2056, %2057, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2061 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.10, %2060), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
  %query.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2061, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:263:0
  %2063 : int[] = prim::ListConstruct(%2056, %2057, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2064 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.5, %2063), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
  %key.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2064, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:264:0
  %2066 : int = aten::size(%query.9, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.18 : Long() = prim::NumToTensor(%2066), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2068 : int = aten::size(%query.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.19 : Long() = prim::NumToTensor(%2068), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2070 : int = aten::size(%query.9, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.13 : Long() = prim::NumToTensor(%2070), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2072 : int = aten::size(%query.9, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %2073 : Long() = aten::floor_divide(%seq_len.19, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.13 : Long() = aten::sub(%2073, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
  %2075 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.9, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2076 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2077 : int = aten::Int(%2076), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2078 : int[] = prim::ListConstruct(%2077, %2068, %2072), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.46 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2075, %2078), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2080 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.5, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2081 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2082 : int = aten::Int(%2081), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2083 : int[] = prim::ListConstruct(%2082, %2068, %2072), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.48 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2080, %2083), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2085 : int = aten::size(%hidden_states.46, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2086 : int = aten::size(%hidden_states.46, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2087 : Long() = prim::NumToTensor(%2086), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2088 : Long() = aten::floor_divide(%2087, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2089 : int = aten::Int(%2088), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2090 : int = aten::size(%hidden_states.46, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2091 : int[] = prim::ListConstruct(%2085, %2089, %68, %2090), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.47 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.46, %2091), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2093 : int = aten::size(%hidden_states.47, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2094 : int = aten::size(%hidden_states.47, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2095 : Long() = prim::NumToTensor(%2094), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2096 : int = aten::size(%hidden_states.47, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2097 : int = aten::size(%hidden_states.47, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2098 : Long() = aten::mul(%2095, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2099 : Long() = aten::sub(%2098, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2100 : int = aten::Int(%2099), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2101 : int[] = prim::ListConstruct(%2093, %2100, %2096, %2097), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2102 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2103 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.47, %2101, %2102, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2104 : int = aten::size(%hidden_states.48, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2105 : int = aten::size(%hidden_states.48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2106 : Long() = prim::NumToTensor(%2105), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2107 : Long() = aten::floor_divide(%2106, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2108 : int = aten::Int(%2107), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2109 : int = aten::size(%hidden_states.48, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2110 : int[] = prim::ListConstruct(%2104, %2108, %68, %2109), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.49 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.48, %2110), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2112 : int = aten::size(%hidden_states.49, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2113 : int = aten::size(%hidden_states.49, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2114 : Long() = prim::NumToTensor(%2113), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2115 : int = aten::size(%hidden_states.49, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2116 : int = aten::size(%hidden_states.49, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2117 : Long() = aten::mul(%2114, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2118 : Long() = aten::sub(%2117, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2119 : int = aten::Int(%2118), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2120 : int[] = prim::ListConstruct(%2112, %2119, %2115, %2116), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2121 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2122 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.49, %2120, %2121, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2123 : Tensor[] = prim::ListConstruct(%2103, %2122), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.56 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %2123), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2125 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states_padded.9 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.56, %2125, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2127 : int = aten::size(%hidden_states_padded.9, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2128 : int = aten::size(%hidden_states_padded.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2129 : int = aten::size(%hidden_states_padded.9, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2130 : int = aten::size(%hidden_states_padded.9, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2131 : int[] = prim::ListConstruct(%2127, %2128, %2129, %2130), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_chunked_attention_scores.9 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.9, %2131), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:400:0
  %2133 : Long() = aten::mul(%batch_size.18, %num_heads.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2134 : int = aten::Int(%2133), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2135 : Long() = aten::add(%chunks_count.13, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2136 : int = aten::Int(%2135), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2137 : int[] = prim::ListConstruct(%2134, %2136, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_attention_scores.9 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.9, %2137, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
  %2139 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2140 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2139, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2141 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2140, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2142 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%2141, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2143 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2144 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2143, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2145 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2144, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2146 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%2145, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2147 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2148 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%2142, %2147), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2149 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2146, %2148, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2150 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2151 : Float(204:262656, 512:513, 513:1) = aten::select(%2150, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2152 : Float(204:262656, 256:513, 513:1) = aten::slice(%2151, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2153 : Float(204:262656, 256:513, 257:1) = aten::slice(%2152, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2154 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2155 : Float(204:262656, 256:513, 513:1) = aten::select(%2154, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2156 : Float(204:262656, 256:513, 513:1) = aten::slice(%2155, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2157 : Float(204:262656, 256:513, 257:1) = aten::slice(%2156, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2158 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2159 : Float(204:262656, 256:513, 257:1) = aten::view(%2153, %2158), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2160 : Float(204:262656, 256:513, 257:1) = aten::copy_(%2157, %2159, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2161 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2162 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2161, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2163 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2162, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2164 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%2163, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2165 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2166 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2165, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2167 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2166, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2168 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%2167, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2169 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2170 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%2164, %2169), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2171 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2168, %2170, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2172 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2173 : Float(204:262656, 512:513, 513:1) = aten::select(%2172, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2174 : Float(204:262656, 255:513, 513:1) = aten::slice(%2173, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2175 : Float(204:262656, 255:513, 255:1) = aten::slice(%2174, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2176 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2177 : Float(204:262656, 256:513, 513:1) = aten::select(%2176, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2178 : Float(204:262656, 255:513, 513:1) = aten::slice(%2177, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2179 : Float(204:262656, 255:513, 255:1) = aten::slice(%2178, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2180 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2181 : Float(204:262656, 255:513, 255:1) = aten::view(%2175, %2180), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2182 : Float(204:262656, 255:513, 255:1) = aten::copy_(%2179, %2181, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2183 : int[] = prim::ListConstruct(%2066, %2070, %2068, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2184 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.9, %2183), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.13 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%2184, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %2186 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2187 : Float(256:257, 257:1) = aten::ones(%2186, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2188 : Float(256:257, 257:1) = aten::tril(%2187, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2189 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %beginning_mask_2d.9 : Float(256:257, 257:1) = aten::flip(%2188, %2189), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2191 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.9, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2192 : Float(1:65792, 256:257, 257:1) = aten::slice(%2191, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2193 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2192, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.9 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2193, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2195 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %ending_mask.9 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.9, %2195), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
  %2197 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2198 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2197, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2199 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2198, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.9 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2199, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2201 : int = aten::size(%beginning_input.9, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2202 : int = aten::size(%beginning_input.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2203 : int = aten::size(%beginning_input.9, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2204 : int = aten::size(%beginning_input.9, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2205 : int[] = prim::ListConstruct(%2201, %2202, %2203, %2204), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2206 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.9, %2205, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2207 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2206, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2208 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.9, %2207, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
  %2209 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2210 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2209, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2211 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2210, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.9 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2211, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2213 : int = aten::size(%ending_input.9, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2214 : int = aten::size(%ending_input.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2215 : int = aten::size(%ending_input.9, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2216 : int = aten::size(%ending_input.9, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2217 : int[] = prim::ListConstruct(%2213, %2214, %2215, %2216), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2218 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.9, %2217, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2219 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2218, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2220 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.9, %2219, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
  %2221 : Bool(17:512, 512:1) = aten::ne(%2038, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2222 : Bool(17:512, 512:1) = aten::slice(%2221, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2223 : Bool(17:512, 512:1) = aten::slice(%2222, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2224 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2223, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.5 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2224, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:272:0
  %2226 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.5, %query.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.5 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%2226, %remove_from_windowed_attention_mask.5, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:275:0
  %2228 : int = aten::size(%float_mask.5, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2229 : int = aten::size(%float_mask.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2230 : int = aten::size(%float_mask.5, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2231 : int = aten::size(%float_mask.5, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2232 : int[] = prim::ListConstruct(%2228, %2229, %2230, %2231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %query.10 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%2232, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:280:0
  %2234 : int = aten::size(%query.10, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.19 : Long() = prim::NumToTensor(%2234), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2236 : int = aten::size(%query.10, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.20 : Long() = prim::NumToTensor(%2236), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2238 : int = aten::size(%query.10, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.14 : Long() = prim::NumToTensor(%2238), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2240 : int = aten::size(%query.10, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:472:0
  %2241 : Long() = aten::floor_divide(%seq_len.20, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.14 : Long() = aten::sub(%2241, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:478:0
  %2243 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.10, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2244 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2245 : int = aten::Int(%2244), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2246 : int[] = prim::ListConstruct(%2245, %2236, %2240), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.50 : Float(17:512, 512:1, 1:1) = aten::reshape(%2243, %2246), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:481:0
  %2248 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.5, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2249 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2250 : int = aten::Int(%2249), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2251 : int[] = prim::ListConstruct(%2250, %2236, %2240), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.52 : Float(17:512, 512:1, 1:1) = aten::reshape(%2248, %2251), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:482:0
  %2253 : int = aten::size(%hidden_states.50, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2254 : int = aten::size(%hidden_states.50, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2255 : Long() = prim::NumToTensor(%2254), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2256 : Long() = aten::floor_divide(%2255, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2257 : int = aten::Int(%2256), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2258 : int = aten::size(%hidden_states.50, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2259 : int[] = prim::ListConstruct(%2253, %2257, %68, %2258), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.51 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.50, %2259), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2261 : int = aten::size(%hidden_states.51, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2262 : int = aten::size(%hidden_states.51, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2263 : Long() = prim::NumToTensor(%2262), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2264 : int = aten::size(%hidden_states.51, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2265 : int = aten::size(%hidden_states.51, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2266 : Long() = aten::mul(%2263, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2267 : Long() = aten::sub(%2266, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2268 : int = aten::Int(%2267), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2269 : int[] = prim::ListConstruct(%2261, %2268, %2264, %2265), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2270 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2271 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.51, %2269, %2270, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2272 : int = aten::size(%hidden_states.52, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:442:0
  %2273 : int = aten::size(%hidden_states.52, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:443:0
  %2274 : Long() = prim::NumToTensor(%2273), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2275 : Long() = aten::floor_divide(%2274, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2276 : int = aten::Int(%2275), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2277 : int = aten::size(%hidden_states.52, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:445:0
  %2278 : int[] = prim::ListConstruct(%2272, %2276, %68, %2277), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states.53 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.52, %2278), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:441:0
  %2280 : int = aten::size(%hidden_states.53, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2281 : int = aten::size(%hidden_states.53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2282 : Long() = prim::NumToTensor(%2281), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2283 : int = aten::size(%hidden_states.53, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2284 : int = aten::size(%hidden_states.53, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:449:0
  %2285 : Long() = aten::mul(%2282, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2286 : Long() = aten::sub(%2285, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:450:0
  %2287 : int = aten::Int(%2286), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2288 : int[] = prim::ListConstruct(%2280, %2287, %2283, %2284), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2289 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2290 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.53, %2288, %2289, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:454:0
  %2291 : Tensor[] = prim::ListConstruct(%2271, %2290), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.57 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %2291), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2293 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %hidden_states_padded.10 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.57, %2293, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2295 : int = aten::size(%hidden_states_padded.10, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2296 : int = aten::size(%hidden_states_padded.10, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2297 : int = aten::size(%hidden_states_padded.10, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2298 : int = aten::size(%hidden_states_padded.10, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:401:0
  %2299 : int[] = prim::ListConstruct(%2295, %2296, %2297, %2298), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_chunked_attention_scores.10 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.10, %2299), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:400:0
  %2301 : Long() = aten::mul(%batch_size.19, %num_heads.14), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2302 : int = aten::Int(%2301), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2303 : Long() = aten::add(%chunks_count.14, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:504:0
  %2304 : int = aten::Int(%2303), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2305 : int[] = prim::ListConstruct(%2302, %2304, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %diagonal_attention_scores.10 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.10, %2305, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:503:0
  %2307 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2308 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2307, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2309 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2308, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2310 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%2309, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2311 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2312 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2311, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2313 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2312, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2314 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%2313, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2315 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2316 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%2310, %2315), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2317 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2314, %2316, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:509:0
  %2318 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2319 : Float(17:262656, 512:513, 513:1) = aten::select(%2318, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2320 : Float(17:262656, 256:513, 513:1) = aten::slice(%2319, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2321 : Float(17:262656, 256:513, 257:1) = aten::slice(%2320, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2322 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2323 : Float(17:262656, 256:513, 513:1) = aten::select(%2322, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2324 : Float(17:262656, 256:513, 513:1) = aten::slice(%2323, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2325 : Float(17:262656, 256:513, 257:1) = aten::slice(%2324, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2326 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2327 : Float(17:262656, 256:513, 257:1) = aten::view(%2321, %2326), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2328 : Float(17:262656, 256:513, 257:1) = aten::copy_(%2325, %2327, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:512:0
  %2329 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2330 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2329, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2331 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2330, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2332 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%2331, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2333 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2334 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2333, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2335 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2334, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2336 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%2335, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2337 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2338 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%2332, %2337), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2339 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2336, %2338, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:516:0
  %2340 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2341 : Float(17:262656, 512:513, 513:1) = aten::select(%2340, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2342 : Float(17:262656, 255:513, 513:1) = aten::slice(%2341, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2343 : Float(17:262656, 255:513, 255:1) = aten::slice(%2342, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2344 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2345 : Float(17:262656, 256:513, 513:1) = aten::select(%2344, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2346 : Float(17:262656, 255:513, 513:1) = aten::slice(%2345, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2347 : Float(17:262656, 255:513, 255:1) = aten::slice(%2346, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2348 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2349 : Float(17:262656, 255:513, 255:1) = aten::view(%2343, %2348), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2350 : Float(17:262656, 255:513, 255:1) = aten::copy_(%2347, %2349, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:520:0
  %2351 : int[] = prim::ListConstruct(%2234, %2238, %2236, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2352 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.10, %2351), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.14 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%2352, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:525:0
  %2354 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2355 : Float(256:257, 257:1) = aten::ones(%2354, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2356 : Float(256:257, 257:1) = aten::tril(%2355, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2357 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %beginning_mask_2d.10 : Float(256:257, 257:1) = aten::flip(%2356, %2357), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:458:0
  %2359 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.10, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2360 : Float(1:65792, 256:257, 257:1) = aten::slice(%2359, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2361 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2360, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.10 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2361, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:459:0
  %2363 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %ending_mask.10 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.10, %2363), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:460:0
  %2365 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2366 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2365, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2367 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2366, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.10 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2367, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:461:0
  %2369 : int = aten::size(%beginning_input.10, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2370 : int = aten::size(%beginning_input.10, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2371 : int = aten::size(%beginning_input.10, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2372 : int = aten::size(%beginning_input.10, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2373 : int[] = prim::ListConstruct(%2369, %2370, %2371, %2372), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2374 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.10, %2373, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:462:0
  %2375 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2374, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2376 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.10, %2375, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:463:0
  %2377 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2378 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2377, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2379 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2378, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.10 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2379, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:464:0
  %2381 : int = aten::size(%ending_input.10, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2382 : int = aten::size(%ending_input.10, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2383 : int = aten::size(%ending_input.10, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2384 : int = aten::size(%ending_input.10, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2385 : int[] = prim::ListConstruct(%2381, %2382, %2383, %2384), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2386 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.10, %2385, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:465:0
  %2387 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2386, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:22:0
  %2388 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.10, %2387, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.5 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.13, %input_tensor.14, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.5 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.5, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:1500:0
  %2391 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.5, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2392 : Bool(17:512, 512:1) = aten::slice(%2391, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2393 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2392, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %2394 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2393, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %input.58 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.5, %2394, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.58, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:973:0
  %2397 : int[] = prim::ListConstruct(%2056, %2057, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2398 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.5, %2397), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
  %value.5 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2398, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:331:0
  %2400 : int = aten::size(%value.5, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.20 : Long() = prim::NumToTensor(%2400), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2402 : int = aten::size(%value.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.21 : Long() = prim::NumToTensor(%2402), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2404 : int = aten::size(%value.5, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.15 : Long() = prim::NumToTensor(%2404), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2406 : int = aten::size(%value.5, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:537:0
  %2407 : Long() = aten::floor_divide(%seq_len.21, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %chunks_count.15 : Long() = aten::sub(%2407, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:542:0
  %2409 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.10, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
  %2410 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:546:0
  %2411 : int = aten::Int(%2410), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2412 : Long() = aten::floor_divide(%seq_len.21, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/tensor.py:424:0
  %2413 : int = aten::Int(%2412), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2414 : int[] = prim::ListConstruct(%2411, %2413, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.21 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%2409, %2414), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:545:0
  %2416 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.5, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2417 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2418 : int = aten::Int(%2417), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2419 : int[] = prim::ListConstruct(%2418, %2402, %2406), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %input.59 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2416, %2419), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:550:0
  %2421 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %padded_value.5 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.59, %2421, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2423 : Long() = aten::mul(%batch_size.20, %num_heads.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
  %2424 : int = aten::Int(%2423), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2425 : Long() = aten::add(%chunks_count.15, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:556:0
  %2426 : int = aten::Int(%2425), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2427 : int[] = prim::ListConstruct(%2424, %2426, %59, %2406), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2428 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2429 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.5, %2427, %2428, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:564:0
  %2430 : int = aten::size(%chunked_hidden_states.21, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %2431 : int = aten::size(%chunked_hidden_states.21, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %2432 : int = aten::size(%chunked_hidden_states.21, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.5 : Long() = prim::NumToTensor(%2432), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2434 : int = aten::size(%chunked_hidden_states.21, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.5 : Long() = prim::NumToTensor(%2434), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2436 : Long() = aten::add(%window_overlap.5, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:422:0
  %2437 : int = aten::Int(%2436), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2438 : int[] = prim::ListConstruct(%74, %2437), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.22 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.21, %2438, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/nn/functional.py:3552:0
  %2440 : int[] = prim::ListConstruct(%2430, %2431, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.23 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.22, %2440), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:424:0
  %2442 : Long() = aten::neg(%window_overlap.5), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:428:0
  %2443 : int = aten::Int(%2442), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2444 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %2445 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%2444, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.24 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%2445, %66, %74, %2443, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:427:0
  %2447 : Long() = aten::add(%window_overlap.5, %hidden_dim.5, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:431:0
  %2448 : int = aten::Int(%2447), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2449 : int[] = prim::ListConstruct(%2430, %2431, %2432, %2448), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %chunked_hidden_states.25 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.24, %2449), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:430:0
  %2451 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.25, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2452 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2451, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2453 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2452, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2454 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%2453, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:433:0
  %2455 : Tensor[] = prim::ListConstruct(%2454, %2429), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %context.5 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %2455), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # torch/functional.py:327:0
  %2457 : int[] = prim::ListConstruct(%2400, %2404, %2402, %2406), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2458 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.5, %2457), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.9 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%2458, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:569:0
  %2460 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.9, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %2461 : int[] = prim::ListConstruct(%2056, %2057, %2058), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self
  %2462 : Float(512:13056, 17:768, 768:1) = aten::reshape(%2460, %2461), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.10 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%2462, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:350:0
  %input.60 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.10, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.self # transformers/modeling_longformer.py:374:0
  %2465 : __torch__.torch.nn.modules.normalization.___torch_mangle_7173.LayerNorm = prim::GetAttr[name="LayerNorm"](%2032)
  %2466 : __torch__.torch.nn.modules.linear.___torch_mangle_7172.Linear = prim::GetAttr[name="dense"](%2032)
  %2467 : Tensor = prim::GetAttr[name="bias"](%2466)
  %2468 : Tensor = prim::GetAttr[name="weight"](%2466)
  %2469 : Float(768:1, 768:768) = aten::t(%2468), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.60, %2469), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.28, %2467, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.54 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.61, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.62 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.54, %hidden_states.45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output # transformers/modeling_longformer.py:758:0
  %2474 : Tensor = prim::GetAttr[name="bias"](%2465)
  %2475 : Tensor = prim::GetAttr[name="weight"](%2465)
  %2476 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.15 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.62, %2476, %2475, %2474, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.attention/__module.longformer.encoder.layer.4.attention.output/__module.longformer.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2478 : __torch__.torch.nn.modules.linear.___torch_mangle_7177.Linear = prim::GetAttr[name="dense"](%2030)
  %2479 : Tensor = prim::GetAttr[name="bias"](%2478)
  %2480 : Tensor = prim::GetAttr[name="weight"](%2478)
  %2481 : Float(768:1, 3072:768) = aten::t(%2480), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.15, %2481), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.29, %2479, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate/__module.longformer.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.64 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %2485 : __torch__.torch.nn.modules.normalization.___torch_mangle_7180.LayerNorm = prim::GetAttr[name="LayerNorm"](%2029)
  %2486 : __torch__.torch.nn.modules.linear.___torch_mangle_7179.Linear = prim::GetAttr[name="dense"](%2029)
  %2487 : Tensor = prim::GetAttr[name="bias"](%2486)
  %2488 : Tensor = prim::GetAttr[name="weight"](%2486)
  %2489 : Float(3072:1, 768:3072) = aten::t(%2488), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.64, %2489), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.65 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.30, %2487, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.55 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.65, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.66 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.55, %input_tensor.15, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output # transformers/modeling_longformer.py:830:0
  %2494 : Tensor = prim::GetAttr[name="bias"](%2485)
  %2495 : Tensor = prim::GetAttr[name="weight"](%2485)
  %2496 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.LayerNorm
  %hidden_states.56 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.66, %2496, %2495, %2494, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.4/__module.longformer.encoder.layer.4.output/__module.longformer.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %2498 : __torch__.transformers.modeling_longformer.___torch_mangle_7201.LongformerOutput = prim::GetAttr[name="output"](%142)
  %2499 : __torch__.transformers.modeling_longformer.___torch_mangle_7197.LongformerIntermediate = prim::GetAttr[name="intermediate"](%142)
  %2500 : __torch__.transformers.modeling_longformer.___torch_mangle_7195.LongformerAttention = prim::GetAttr[name="attention"](%142)
  %2501 : __torch__.transformers.modeling_longformer.___torch_mangle_7194.LongformerSelfOutput = prim::GetAttr[name="output"](%2500)
  %2502 : __torch__.transformers.modeling_longformer.___torch_mangle_7190.LongformerSelfAttention = prim::GetAttr[name="self"](%2500)
  %2503 : __torch__.torch.nn.modules.linear.___torch_mangle_7186.Linear = prim::GetAttr[name="value"](%2502)
  %2504 : __torch__.torch.nn.modules.linear.___torch_mangle_7185.Linear = prim::GetAttr[name="key"](%2502)
  %2505 : __torch__.torch.nn.modules.linear.___torch_mangle_7184.Linear = prim::GetAttr[name="query"](%2502)
  %2506 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
  %2507 : Float(17:512, 512:1) = aten::squeeze(%2506, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.6 : Bool(17:512, 512:1) = aten::lt(%2507, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %input.67 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.56, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:248:0
  %2510 : Tensor = prim::GetAttr[name="bias"](%2505)
  %2511 : Tensor = prim::GetAttr[name="weight"](%2505)
  %2512 : Float(768:1, 768:768) = aten::t(%2511), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2512), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.31, %2510, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %2515 : Tensor = prim::GetAttr[name="bias"](%2504)
  %2516 : Tensor = prim::GetAttr[name="weight"](%2504)
  %2517 : Float(768:1, 768:768) = aten::t(%2516), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.32, %2515, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %2520 : Tensor = prim::GetAttr[name="bias"](%2503)
  %2521 : Tensor = prim::GetAttr[name="weight"](%2503)
  %2522 : Float(768:1, 768:768) = aten::t(%2521), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.67, %2522), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.6 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.33, %2520, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self/__module.longformer.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %2525 : int = aten::size(%input.67, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %2526 : int = aten::size(%input.67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %2527 : int = aten::size(%input.67, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.12 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.11, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:261:0
  %2529 : int[] = prim::ListConstruct(%2525, %2526, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2530 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.12, %2529), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
  %query.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2530, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:263:0
  %2532 : int[] = prim::ListConstruct(%2525, %2526, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2533 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.6, %2532), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
  %key.6 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2533, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:264:0
  %2535 : int = aten::size(%query.11, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.22 : Long() = prim::NumToTensor(%2535), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2537 : int = aten::size(%query.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.23 : Long() = prim::NumToTensor(%2537), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2539 : int = aten::size(%query.11, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.16 : Long() = prim::NumToTensor(%2539), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2541 : int = aten::size(%query.11, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %2542 : Long() = aten::floor_divide(%seq_len.23, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.16 : Long() = aten::sub(%2542, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
  %2544 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.11, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2545 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2546 : int = aten::Int(%2545), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2547 : int[] = prim::ListConstruct(%2546, %2537, %2541), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.57 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2544, %2547), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2549 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.6, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2550 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2551 : int = aten::Int(%2550), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2552 : int[] = prim::ListConstruct(%2551, %2537, %2541), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.59 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2549, %2552), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2554 : int = aten::size(%hidden_states.57, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2555 : int = aten::size(%hidden_states.57, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2556 : Long() = prim::NumToTensor(%2555), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2557 : Long() = aten::floor_divide(%2556, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2558 : int = aten::Int(%2557), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2559 : int = aten::size(%hidden_states.57, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2560 : int[] = prim::ListConstruct(%2554, %2558, %68, %2559), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.58 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.57, %2560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2562 : int = aten::size(%hidden_states.58, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2563 : int = aten::size(%hidden_states.58, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2564 : Long() = prim::NumToTensor(%2563), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2565 : int = aten::size(%hidden_states.58, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2566 : int = aten::size(%hidden_states.58, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2567 : Long() = aten::mul(%2564, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2568 : Long() = aten::sub(%2567, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2569 : int = aten::Int(%2568), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2570 : int[] = prim::ListConstruct(%2562, %2569, %2565, %2566), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2571 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2572 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.58, %2570, %2571, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2573 : int = aten::size(%hidden_states.59, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2574 : int = aten::size(%hidden_states.59, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2575 : Long() = prim::NumToTensor(%2574), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2576 : Long() = aten::floor_divide(%2575, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2577 : int = aten::Int(%2576), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2578 : int = aten::size(%hidden_states.59, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2579 : int[] = prim::ListConstruct(%2573, %2577, %68, %2578), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.60 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.59, %2579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2581 : int = aten::size(%hidden_states.60, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2582 : int = aten::size(%hidden_states.60, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2583 : Long() = prim::NumToTensor(%2582), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2584 : int = aten::size(%hidden_states.60, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2585 : int = aten::size(%hidden_states.60, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2586 : Long() = aten::mul(%2583, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2587 : Long() = aten::sub(%2586, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2588 : int = aten::Int(%2587), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2589 : int[] = prim::ListConstruct(%2581, %2588, %2584, %2585), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2590 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2591 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.60, %2589, %2590, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2592 : Tensor[] = prim::ListConstruct(%2572, %2591), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.68 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %2592), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2594 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states_padded.11 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.68, %2594, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2596 : int = aten::size(%hidden_states_padded.11, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2597 : int = aten::size(%hidden_states_padded.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2598 : int = aten::size(%hidden_states_padded.11, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2599 : int = aten::size(%hidden_states_padded.11, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2600 : int[] = prim::ListConstruct(%2596, %2597, %2598, %2599), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_chunked_attention_scores.11 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.11, %2600), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:400:0
  %2602 : Long() = aten::mul(%batch_size.22, %num_heads.16), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2603 : int = aten::Int(%2602), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2604 : Long() = aten::add(%chunks_count.16, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2605 : int = aten::Int(%2604), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2606 : int[] = prim::ListConstruct(%2603, %2605, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_attention_scores.11 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.11, %2606, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
  %2608 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2609 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2608, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2610 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2609, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2611 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%2610, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2612 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2613 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2612, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2614 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2613, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2615 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%2614, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2616 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2617 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%2611, %2616), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2618 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2615, %2617, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2619 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2620 : Float(204:262656, 512:513, 513:1) = aten::select(%2619, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2621 : Float(204:262656, 256:513, 513:1) = aten::slice(%2620, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2622 : Float(204:262656, 256:513, 257:1) = aten::slice(%2621, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2623 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2624 : Float(204:262656, 256:513, 513:1) = aten::select(%2623, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2625 : Float(204:262656, 256:513, 513:1) = aten::slice(%2624, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2626 : Float(204:262656, 256:513, 257:1) = aten::slice(%2625, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2627 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2628 : Float(204:262656, 256:513, 257:1) = aten::view(%2622, %2627), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2629 : Float(204:262656, 256:513, 257:1) = aten::copy_(%2626, %2628, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2630 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2631 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%2630, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2632 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%2631, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2633 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%2632, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2634 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2635 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2634, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2636 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%2635, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2637 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%2636, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2638 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2639 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%2633, %2638), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2640 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2637, %2639, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2641 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2642 : Float(204:262656, 512:513, 513:1) = aten::select(%2641, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2643 : Float(204:262656, 255:513, 513:1) = aten::slice(%2642, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2644 : Float(204:262656, 255:513, 255:1) = aten::slice(%2643, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2645 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2646 : Float(204:262656, 256:513, 513:1) = aten::select(%2645, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2647 : Float(204:262656, 255:513, 513:1) = aten::slice(%2646, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2648 : Float(204:262656, 255:513, 255:1) = aten::slice(%2647, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2649 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2650 : Float(204:262656, 255:513, 255:1) = aten::view(%2644, %2649), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2651 : Float(204:262656, 255:513, 255:1) = aten::copy_(%2648, %2650, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2652 : int[] = prim::ListConstruct(%2535, %2539, %2537, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2653 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.11, %2652), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.16 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%2653, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %2655 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2656 : Float(256:257, 257:1) = aten::ones(%2655, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2657 : Float(256:257, 257:1) = aten::tril(%2656, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2658 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %beginning_mask_2d.11 : Float(256:257, 257:1) = aten::flip(%2657, %2658), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2660 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.11, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2661 : Float(1:65792, 256:257, 257:1) = aten::slice(%2660, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2662 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2661, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.11 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2662, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2664 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %ending_mask.11 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.11, %2664), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
  %2666 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2667 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2666, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2668 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2667, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.11 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2668, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2670 : int = aten::size(%beginning_input.11, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2671 : int = aten::size(%beginning_input.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2672 : int = aten::size(%beginning_input.11, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2673 : int = aten::size(%beginning_input.11, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2674 : int[] = prim::ListConstruct(%2670, %2671, %2672, %2673), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2675 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.11, %2674, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2676 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2675, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2677 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.11, %2676, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
  %2678 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2679 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2678, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2680 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%2679, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.11 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%2680, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2682 : int = aten::size(%ending_input.11, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2683 : int = aten::size(%ending_input.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2684 : int = aten::size(%ending_input.11, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2685 : int = aten::size(%ending_input.11, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2686 : int[] = prim::ListConstruct(%2682, %2683, %2684, %2685), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2687 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.11, %2686, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2688 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%2687, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2689 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.11, %2688, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
  %2690 : Bool(17:512, 512:1) = aten::ne(%2507, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2691 : Bool(17:512, 512:1) = aten::slice(%2690, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2692 : Bool(17:512, 512:1) = aten::slice(%2691, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2693 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2692, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.6 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2693, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:272:0
  %2695 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.6, %query.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.6 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%2695, %remove_from_windowed_attention_mask.6, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:275:0
  %2697 : int = aten::size(%float_mask.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2698 : int = aten::size(%float_mask.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2699 : int = aten::size(%float_mask.6, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2700 : int = aten::size(%float_mask.6, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2701 : int[] = prim::ListConstruct(%2697, %2698, %2699, %2700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %query.12 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%2701, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:280:0
  %2703 : int = aten::size(%query.12, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.23 : Long() = prim::NumToTensor(%2703), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2705 : int = aten::size(%query.12, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.24 : Long() = prim::NumToTensor(%2705), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2707 : int = aten::size(%query.12, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.17 : Long() = prim::NumToTensor(%2707), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2709 : int = aten::size(%query.12, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:472:0
  %2710 : Long() = aten::floor_divide(%seq_len.24, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.17 : Long() = aten::sub(%2710, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:478:0
  %2712 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.12, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2713 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2714 : int = aten::Int(%2713), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2715 : int[] = prim::ListConstruct(%2714, %2705, %2709), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.61 : Float(17:512, 512:1, 1:1) = aten::reshape(%2712, %2715), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:481:0
  %2717 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.6, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2718 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2719 : int = aten::Int(%2718), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2720 : int[] = prim::ListConstruct(%2719, %2705, %2709), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.63 : Float(17:512, 512:1, 1:1) = aten::reshape(%2717, %2720), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:482:0
  %2722 : int = aten::size(%hidden_states.61, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2723 : int = aten::size(%hidden_states.61, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2724 : Long() = prim::NumToTensor(%2723), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2725 : Long() = aten::floor_divide(%2724, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2726 : int = aten::Int(%2725), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2727 : int = aten::size(%hidden_states.61, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2728 : int[] = prim::ListConstruct(%2722, %2726, %68, %2727), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.62 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.61, %2728), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2730 : int = aten::size(%hidden_states.62, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2731 : int = aten::size(%hidden_states.62, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2732 : Long() = prim::NumToTensor(%2731), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2733 : int = aten::size(%hidden_states.62, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2734 : int = aten::size(%hidden_states.62, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2735 : Long() = aten::mul(%2732, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2736 : Long() = aten::sub(%2735, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2737 : int = aten::Int(%2736), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2738 : int[] = prim::ListConstruct(%2730, %2737, %2733, %2734), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2739 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2740 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.62, %2738, %2739, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2741 : int = aten::size(%hidden_states.63, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:442:0
  %2742 : int = aten::size(%hidden_states.63, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:443:0
  %2743 : Long() = prim::NumToTensor(%2742), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2744 : Long() = aten::floor_divide(%2743, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2745 : int = aten::Int(%2744), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2746 : int = aten::size(%hidden_states.63, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:445:0
  %2747 : int[] = prim::ListConstruct(%2741, %2745, %68, %2746), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states.64 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.63, %2747), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:441:0
  %2749 : int = aten::size(%hidden_states.64, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2750 : int = aten::size(%hidden_states.64, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2751 : Long() = prim::NumToTensor(%2750), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2752 : int = aten::size(%hidden_states.64, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2753 : int = aten::size(%hidden_states.64, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:449:0
  %2754 : Long() = aten::mul(%2751, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2755 : Long() = aten::sub(%2754, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:450:0
  %2756 : int = aten::Int(%2755), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2757 : int[] = prim::ListConstruct(%2749, %2756, %2752, %2753), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2758 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2759 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.64, %2757, %2758, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:454:0
  %2760 : Tensor[] = prim::ListConstruct(%2740, %2759), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.69 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %2760), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2762 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %hidden_states_padded.12 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.69, %2762, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2764 : int = aten::size(%hidden_states_padded.12, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2765 : int = aten::size(%hidden_states_padded.12, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2766 : int = aten::size(%hidden_states_padded.12, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2767 : int = aten::size(%hidden_states_padded.12, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:401:0
  %2768 : int[] = prim::ListConstruct(%2764, %2765, %2766, %2767), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_chunked_attention_scores.12 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.12, %2768), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:400:0
  %2770 : Long() = aten::mul(%batch_size.23, %num_heads.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2771 : int = aten::Int(%2770), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2772 : Long() = aten::add(%chunks_count.17, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:504:0
  %2773 : int = aten::Int(%2772), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2774 : int[] = prim::ListConstruct(%2771, %2773, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %diagonal_attention_scores.12 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.12, %2774, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:503:0
  %2776 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2777 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2776, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2778 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2777, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2779 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%2778, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2780 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2781 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2780, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2782 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2781, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2783 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%2782, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2784 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2785 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%2779, %2784), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2786 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%2783, %2785, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:509:0
  %2787 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2788 : Float(17:262656, 512:513, 513:1) = aten::select(%2787, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2789 : Float(17:262656, 256:513, 513:1) = aten::slice(%2788, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2790 : Float(17:262656, 256:513, 257:1) = aten::slice(%2789, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2791 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2792 : Float(17:262656, 256:513, 513:1) = aten::select(%2791, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2793 : Float(17:262656, 256:513, 513:1) = aten::slice(%2792, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2794 : Float(17:262656, 256:513, 257:1) = aten::slice(%2793, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2795 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2796 : Float(17:262656, 256:513, 257:1) = aten::view(%2790, %2795), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2797 : Float(17:262656, 256:513, 257:1) = aten::copy_(%2794, %2796, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:512:0
  %2798 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2799 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%2798, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2800 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%2799, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2801 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%2800, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2802 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2803 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2802, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2804 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%2803, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2805 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%2804, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2806 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2807 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%2801, %2806), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2808 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%2805, %2807, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:516:0
  %2809 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.12, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2810 : Float(17:262656, 512:513, 513:1) = aten::select(%2809, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2811 : Float(17:262656, 255:513, 513:1) = aten::slice(%2810, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2812 : Float(17:262656, 255:513, 255:1) = aten::slice(%2811, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2813 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.12, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2814 : Float(17:262656, 256:513, 513:1) = aten::select(%2813, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2815 : Float(17:262656, 255:513, 513:1) = aten::slice(%2814, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2816 : Float(17:262656, 255:513, 255:1) = aten::slice(%2815, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2817 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2818 : Float(17:262656, 255:513, 255:1) = aten::view(%2812, %2817), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2819 : Float(17:262656, 255:513, 255:1) = aten::copy_(%2816, %2818, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:520:0
  %2820 : int[] = prim::ListConstruct(%2703, %2707, %2705, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2821 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.12, %2820), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.17 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%2821, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:525:0
  %2823 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2824 : Float(256:257, 257:1) = aten::ones(%2823, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2825 : Float(256:257, 257:1) = aten::tril(%2824, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2826 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %beginning_mask_2d.12 : Float(256:257, 257:1) = aten::flip(%2825, %2826), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:458:0
  %2828 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.12, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2829 : Float(1:65792, 256:257, 257:1) = aten::slice(%2828, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2830 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%2829, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.12 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%2830, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:459:0
  %2832 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %ending_mask.12 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.12, %2832), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:460:0
  %2834 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2835 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2834, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2836 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2835, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.12 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2836, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:461:0
  %2838 : int = aten::size(%beginning_input.12, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2839 : int = aten::size(%beginning_input.12, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2840 : int = aten::size(%beginning_input.12, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2841 : int = aten::size(%beginning_input.12, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2842 : int[] = prim::ListConstruct(%2838, %2839, %2840, %2841), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2843 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.12, %2842, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:462:0
  %2844 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2843, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2845 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.12, %2844, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:463:0
  %2846 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2847 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2846, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2848 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%2847, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.12 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%2848, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:464:0
  %2850 : int = aten::size(%ending_input.12, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2851 : int = aten::size(%ending_input.12, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2852 : int = aten::size(%ending_input.12, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2853 : int = aten::size(%ending_input.12, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2854 : int[] = prim::ListConstruct(%2850, %2851, %2852, %2853), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2855 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.12, %2854, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:465:0
  %2856 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%2855, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:22:0
  %2857 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.12, %2856, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.6 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.16, %input_tensor.17, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.6 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.6, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:1500:0
  %2860 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.6, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2861 : Bool(17:512, 512:1) = aten::slice(%2860, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2862 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%2861, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %2863 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%2862, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %input.70 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.6, %2863, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.12 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.70, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:973:0
  %2866 : int[] = prim::ListConstruct(%2525, %2526, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2867 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.6, %2866), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
  %value.6 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2867, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:331:0
  %2869 : int = aten::size(%value.6, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.24 : Long() = prim::NumToTensor(%2869), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2871 : int = aten::size(%value.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.25 : Long() = prim::NumToTensor(%2871), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2873 : int = aten::size(%value.6, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.18 : Long() = prim::NumToTensor(%2873), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2875 : int = aten::size(%value.6, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:537:0
  %2876 : Long() = aten::floor_divide(%seq_len.25, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %chunks_count.18 : Long() = aten::sub(%2876, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:542:0
  %2878 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.12, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
  %2879 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:546:0
  %2880 : int = aten::Int(%2879), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2881 : Long() = aten::floor_divide(%seq_len.25, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/tensor.py:424:0
  %2882 : int = aten::Int(%2881), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2883 : int[] = prim::ListConstruct(%2880, %2882, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.26 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%2878, %2883), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:545:0
  %2885 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.6, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2886 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2887 : int = aten::Int(%2886), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2888 : int[] = prim::ListConstruct(%2887, %2871, %2875), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %input.71 : Float(204:64, 512:13056, 64:1) = aten::reshape(%2885, %2888), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:550:0
  %2890 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %padded_value.6 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.71, %2890, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2892 : Long() = aten::mul(%batch_size.24, %num_heads.18), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
  %2893 : int = aten::Int(%2892), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2894 : Long() = aten::add(%chunks_count.18, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:556:0
  %2895 : int = aten::Int(%2894), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2896 : int[] = prim::ListConstruct(%2893, %2895, %59, %2875), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2897 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2898 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.6, %2896, %2897, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:564:0
  %2899 : int = aten::size(%chunked_hidden_states.26, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %2900 : int = aten::size(%chunked_hidden_states.26, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %2901 : int = aten::size(%chunked_hidden_states.26, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.6 : Long() = prim::NumToTensor(%2901), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2903 : int = aten::size(%chunked_hidden_states.26, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.6 : Long() = prim::NumToTensor(%2903), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2905 : Long() = aten::add(%window_overlap.6, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:422:0
  %2906 : int = aten::Int(%2905), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2907 : int[] = prim::ListConstruct(%74, %2906), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.27 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.26, %2907, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/nn/functional.py:3552:0
  %2909 : int[] = prim::ListConstruct(%2899, %2900, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.28 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.27, %2909), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:424:0
  %2911 : Long() = aten::neg(%window_overlap.6), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:428:0
  %2912 : int = aten::Int(%2911), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2913 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.28, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %2914 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%2913, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.29 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%2914, %66, %74, %2912, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:427:0
  %2916 : Long() = aten::add(%window_overlap.6, %hidden_dim.6, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:431:0
  %2917 : int = aten::Int(%2916), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2918 : int[] = prim::ListConstruct(%2899, %2900, %2901, %2917), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %chunked_hidden_states.30 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.29, %2918), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:430:0
  %2920 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.30, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2921 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2920, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2922 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%2921, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2923 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%2922, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:433:0
  %2924 : Tensor[] = prim::ListConstruct(%2923, %2898), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %context.6 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %2924), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # torch/functional.py:327:0
  %2926 : int[] = prim::ListConstruct(%2869, %2873, %2871, %2875), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2927 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.6, %2926), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.11 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%2927, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:569:0
  %2929 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.11, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %2930 : int[] = prim::ListConstruct(%2525, %2526, %2527), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self
  %2931 : Float(512:13056, 17:768, 768:1) = aten::reshape(%2929, %2930), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.12 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%2931, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:350:0
  %input.72 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.12, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.self # transformers/modeling_longformer.py:374:0
  %2934 : __torch__.torch.nn.modules.normalization.___torch_mangle_7192.LayerNorm = prim::GetAttr[name="LayerNorm"](%2501)
  %2935 : __torch__.torch.nn.modules.linear.___torch_mangle_7191.Linear = prim::GetAttr[name="dense"](%2501)
  %2936 : Tensor = prim::GetAttr[name="bias"](%2935)
  %2937 : Tensor = prim::GetAttr[name="weight"](%2935)
  %2938 : Float(768:1, 768:768) = aten::t(%2937), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.72, %2938), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.34, %2936, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.65 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.73, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.65, %hidden_states.56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output # transformers/modeling_longformer.py:758:0
  %2943 : Tensor = prim::GetAttr[name="bias"](%2934)
  %2944 : Tensor = prim::GetAttr[name="weight"](%2934)
  %2945 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.18 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.74, %2945, %2944, %2943, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.attention/__module.longformer.encoder.layer.5.attention.output/__module.longformer.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2947 : __torch__.torch.nn.modules.linear.___torch_mangle_7196.Linear = prim::GetAttr[name="dense"](%2499)
  %2948 : Tensor = prim::GetAttr[name="bias"](%2947)
  %2949 : Tensor = prim::GetAttr[name="weight"](%2947)
  %2950 : Float(768:1, 3072:768) = aten::t(%2949), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.18, %2950), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.75 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.35, %2948, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate/__module.longformer.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.76 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.75), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %2954 : __torch__.torch.nn.modules.normalization.___torch_mangle_7199.LayerNorm = prim::GetAttr[name="LayerNorm"](%2498)
  %2955 : __torch__.torch.nn.modules.linear.___torch_mangle_7198.Linear = prim::GetAttr[name="dense"](%2498)
  %2956 : Tensor = prim::GetAttr[name="bias"](%2955)
  %2957 : Tensor = prim::GetAttr[name="weight"](%2955)
  %2958 : Float(3072:1, 768:3072) = aten::t(%2957), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.76, %2958), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.77 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.36, %2956, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.66 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.77, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.66, %input_tensor.18, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output # transformers/modeling_longformer.py:830:0
  %2963 : Tensor = prim::GetAttr[name="bias"](%2954)
  %2964 : Tensor = prim::GetAttr[name="weight"](%2954)
  %2965 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.LayerNorm
  %hidden_states.67 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.78, %2965, %2964, %2963, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.5/__module.longformer.encoder.layer.5.output/__module.longformer.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %2967 : __torch__.transformers.modeling_longformer.___torch_mangle_7220.LongformerOutput = prim::GetAttr[name="output"](%140)
  %2968 : __torch__.transformers.modeling_longformer.___torch_mangle_7216.LongformerIntermediate = prim::GetAttr[name="intermediate"](%140)
  %2969 : __torch__.transformers.modeling_longformer.___torch_mangle_7214.LongformerAttention = prim::GetAttr[name="attention"](%140)
  %2970 : __torch__.transformers.modeling_longformer.___torch_mangle_7213.LongformerSelfOutput = prim::GetAttr[name="output"](%2969)
  %2971 : __torch__.transformers.modeling_longformer.___torch_mangle_7209.LongformerSelfAttention = prim::GetAttr[name="self"](%2969)
  %2972 : __torch__.torch.nn.modules.linear.___torch_mangle_7205.Linear = prim::GetAttr[name="value"](%2971)
  %2973 : __torch__.torch.nn.modules.linear.___torch_mangle_7204.Linear = prim::GetAttr[name="key"](%2971)
  %2974 : __torch__.torch.nn.modules.linear.___torch_mangle_7203.Linear = prim::GetAttr[name="query"](%2971)
  %2975 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
  %2976 : Float(17:512, 512:1) = aten::squeeze(%2975, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.7 : Bool(17:512, 512:1) = aten::lt(%2976, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %input.79 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.67, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:248:0
  %2979 : Tensor = prim::GetAttr[name="bias"](%2974)
  %2980 : Tensor = prim::GetAttr[name="weight"](%2974)
  %2981 : Float(768:1, 768:768) = aten::t(%2980), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2981), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.13 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.37, %2979, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %2984 : Tensor = prim::GetAttr[name="bias"](%2973)
  %2985 : Tensor = prim::GetAttr[name="weight"](%2973)
  %2986 : Float(768:1, 768:768) = aten::t(%2985), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2986), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.38, %2984, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %2989 : Tensor = prim::GetAttr[name="bias"](%2972)
  %2990 : Tensor = prim::GetAttr[name="weight"](%2972)
  %2991 : Float(768:1, 768:768) = aten::t(%2990), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.79, %2991), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.7 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.39, %2989, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self/__module.longformer.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %2994 : int = aten::size(%input.79, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %2995 : int = aten::size(%input.79, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %2996 : int = aten::size(%input.79, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.14 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.13, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:261:0
  %2998 : int[] = prim::ListConstruct(%2994, %2995, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %2999 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.14, %2998), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
  %query.13 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%2999, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:263:0
  %3001 : int[] = prim::ListConstruct(%2994, %2995, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3002 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.7, %3001), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
  %key.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3002, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:264:0
  %3004 : int = aten::size(%query.13, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.26 : Long() = prim::NumToTensor(%3004), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3006 : int = aten::size(%query.13, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.27 : Long() = prim::NumToTensor(%3006), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3008 : int = aten::size(%query.13, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.19 : Long() = prim::NumToTensor(%3008), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3010 : int = aten::size(%query.13, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %3011 : Long() = aten::floor_divide(%seq_len.27, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.19 : Long() = aten::sub(%3011, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
  %3013 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.13, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3014 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3015 : int = aten::Int(%3014), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3016 : int[] = prim::ListConstruct(%3015, %3006, %3010), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.68 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3013, %3016), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3018 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.7, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3019 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3020 : int = aten::Int(%3019), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3021 : int[] = prim::ListConstruct(%3020, %3006, %3010), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.70 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3018, %3021), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3023 : int = aten::size(%hidden_states.68, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3024 : int = aten::size(%hidden_states.68, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3025 : Long() = prim::NumToTensor(%3024), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3026 : Long() = aten::floor_divide(%3025, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3027 : int = aten::Int(%3026), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3028 : int = aten::size(%hidden_states.68, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3029 : int[] = prim::ListConstruct(%3023, %3027, %68, %3028), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.69 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.68, %3029), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3031 : int = aten::size(%hidden_states.69, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3032 : int = aten::size(%hidden_states.69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3033 : Long() = prim::NumToTensor(%3032), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3034 : int = aten::size(%hidden_states.69, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3035 : int = aten::size(%hidden_states.69, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3036 : Long() = aten::mul(%3033, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3037 : Long() = aten::sub(%3036, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3038 : int = aten::Int(%3037), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3039 : int[] = prim::ListConstruct(%3031, %3038, %3034, %3035), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3040 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3041 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.69, %3039, %3040, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3042 : int = aten::size(%hidden_states.70, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3043 : int = aten::size(%hidden_states.70, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3044 : Long() = prim::NumToTensor(%3043), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3045 : Long() = aten::floor_divide(%3044, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3046 : int = aten::Int(%3045), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3047 : int = aten::size(%hidden_states.70, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3048 : int[] = prim::ListConstruct(%3042, %3046, %68, %3047), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.71 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.70, %3048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3050 : int = aten::size(%hidden_states.71, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3051 : int = aten::size(%hidden_states.71, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3052 : Long() = prim::NumToTensor(%3051), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3053 : int = aten::size(%hidden_states.71, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3054 : int = aten::size(%hidden_states.71, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3055 : Long() = aten::mul(%3052, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3056 : Long() = aten::sub(%3055, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3057 : int = aten::Int(%3056), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3058 : int[] = prim::ListConstruct(%3050, %3057, %3053, %3054), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3059 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3060 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.71, %3058, %3059, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3061 : Tensor[] = prim::ListConstruct(%3041, %3060), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.80 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %3061), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3063 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states_padded.13 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.80, %3063, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3065 : int = aten::size(%hidden_states_padded.13, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3066 : int = aten::size(%hidden_states_padded.13, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3067 : int = aten::size(%hidden_states_padded.13, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3068 : int = aten::size(%hidden_states_padded.13, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3069 : int[] = prim::ListConstruct(%3065, %3066, %3067, %3068), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_chunked_attention_scores.13 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.13, %3069), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:400:0
  %3071 : Long() = aten::mul(%batch_size.26, %num_heads.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3072 : int = aten::Int(%3071), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3073 : Long() = aten::add(%chunks_count.19, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3074 : int = aten::Int(%3073), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3075 : int[] = prim::ListConstruct(%3072, %3074, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_attention_scores.13 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.13, %3075, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
  %3077 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3078 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3077, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3079 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3078, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3080 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%3079, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3081 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3082 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3081, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3083 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3082, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3084 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%3083, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3085 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3086 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%3080, %3085), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3087 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3084, %3086, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3088 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3089 : Float(204:262656, 512:513, 513:1) = aten::select(%3088, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3090 : Float(204:262656, 256:513, 513:1) = aten::slice(%3089, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3091 : Float(204:262656, 256:513, 257:1) = aten::slice(%3090, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3092 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3093 : Float(204:262656, 256:513, 513:1) = aten::select(%3092, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3094 : Float(204:262656, 256:513, 513:1) = aten::slice(%3093, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3095 : Float(204:262656, 256:513, 257:1) = aten::slice(%3094, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3096 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3097 : Float(204:262656, 256:513, 257:1) = aten::view(%3091, %3096), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3098 : Float(204:262656, 256:513, 257:1) = aten::copy_(%3095, %3097, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3099 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3100 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3099, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3101 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3100, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3102 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%3101, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3103 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3104 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3103, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3105 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3104, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3106 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%3105, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3107 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3108 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%3102, %3107), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3109 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3106, %3108, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3110 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3111 : Float(204:262656, 512:513, 513:1) = aten::select(%3110, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3112 : Float(204:262656, 255:513, 513:1) = aten::slice(%3111, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3113 : Float(204:262656, 255:513, 255:1) = aten::slice(%3112, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3114 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.13, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3115 : Float(204:262656, 256:513, 513:1) = aten::select(%3114, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3116 : Float(204:262656, 255:513, 513:1) = aten::slice(%3115, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3117 : Float(204:262656, 255:513, 255:1) = aten::slice(%3116, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3118 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3119 : Float(204:262656, 255:513, 255:1) = aten::view(%3113, %3118), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3120 : Float(204:262656, 255:513, 255:1) = aten::copy_(%3117, %3119, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3121 : int[] = prim::ListConstruct(%3004, %3008, %3006, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3122 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.13, %3121), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.19 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%3122, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %3124 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3125 : Float(256:257, 257:1) = aten::ones(%3124, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3126 : Float(256:257, 257:1) = aten::tril(%3125, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3127 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %beginning_mask_2d.13 : Float(256:257, 257:1) = aten::flip(%3126, %3127), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3129 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.13, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3130 : Float(1:65792, 256:257, 257:1) = aten::slice(%3129, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3131 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3130, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.13 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3131, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3133 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %ending_mask.13 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.13, %3133), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
  %3135 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3136 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3135, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3137 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3136, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.13 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3137, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3139 : int = aten::size(%beginning_input.13, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3140 : int = aten::size(%beginning_input.13, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3141 : int = aten::size(%beginning_input.13, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3142 : int = aten::size(%beginning_input.13, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3143 : int[] = prim::ListConstruct(%3139, %3140, %3141, %3142), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3144 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.13, %3143, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3145 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3144, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3146 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.13, %3145, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
  %3147 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3148 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3147, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3149 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3148, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.13 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3149, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3151 : int = aten::size(%ending_input.13, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3152 : int = aten::size(%ending_input.13, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3153 : int = aten::size(%ending_input.13, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3154 : int = aten::size(%ending_input.13, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3155 : int[] = prim::ListConstruct(%3151, %3152, %3153, %3154), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3156 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.13, %3155, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3157 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3156, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3158 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.13, %3157, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
  %3159 : Bool(17:512, 512:1) = aten::ne(%2976, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3160 : Bool(17:512, 512:1) = aten::slice(%3159, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3161 : Bool(17:512, 512:1) = aten::slice(%3160, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3162 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3161, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.7 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3162, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:272:0
  %3164 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.7, %query.13), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.7 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%3164, %remove_from_windowed_attention_mask.7, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:275:0
  %3166 : int = aten::size(%float_mask.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3167 : int = aten::size(%float_mask.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3168 : int = aten::size(%float_mask.7, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3169 : int = aten::size(%float_mask.7, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3170 : int[] = prim::ListConstruct(%3166, %3167, %3168, %3169), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %query.14 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%3170, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:280:0
  %3172 : int = aten::size(%query.14, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.27 : Long() = prim::NumToTensor(%3172), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3174 : int = aten::size(%query.14, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.28 : Long() = prim::NumToTensor(%3174), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3176 : int = aten::size(%query.14, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.20 : Long() = prim::NumToTensor(%3176), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3178 : int = aten::size(%query.14, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:472:0
  %3179 : Long() = aten::floor_divide(%seq_len.28, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.20 : Long() = aten::sub(%3179, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:478:0
  %3181 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.14, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3182 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3183 : int = aten::Int(%3182), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3184 : int[] = prim::ListConstruct(%3183, %3174, %3178), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.72 : Float(17:512, 512:1, 1:1) = aten::reshape(%3181, %3184), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:481:0
  %3186 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.7, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3187 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3188 : int = aten::Int(%3187), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3189 : int[] = prim::ListConstruct(%3188, %3174, %3178), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.74 : Float(17:512, 512:1, 1:1) = aten::reshape(%3186, %3189), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:482:0
  %3191 : int = aten::size(%hidden_states.72, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3192 : int = aten::size(%hidden_states.72, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3193 : Long() = prim::NumToTensor(%3192), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3194 : Long() = aten::floor_divide(%3193, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3195 : int = aten::Int(%3194), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3196 : int = aten::size(%hidden_states.72, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3197 : int[] = prim::ListConstruct(%3191, %3195, %68, %3196), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.73 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.72, %3197), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3199 : int = aten::size(%hidden_states.73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3200 : int = aten::size(%hidden_states.73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3201 : Long() = prim::NumToTensor(%3200), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3202 : int = aten::size(%hidden_states.73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3203 : int = aten::size(%hidden_states.73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3204 : Long() = aten::mul(%3201, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3205 : Long() = aten::sub(%3204, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3206 : int = aten::Int(%3205), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3207 : int[] = prim::ListConstruct(%3199, %3206, %3202, %3203), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3208 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3209 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.73, %3207, %3208, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3210 : int = aten::size(%hidden_states.74, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:442:0
  %3211 : int = aten::size(%hidden_states.74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:443:0
  %3212 : Long() = prim::NumToTensor(%3211), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3213 : Long() = aten::floor_divide(%3212, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3214 : int = aten::Int(%3213), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3215 : int = aten::size(%hidden_states.74, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:445:0
  %3216 : int[] = prim::ListConstruct(%3210, %3214, %68, %3215), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states.75 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.74, %3216), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:441:0
  %3218 : int = aten::size(%hidden_states.75, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3219 : int = aten::size(%hidden_states.75, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3220 : Long() = prim::NumToTensor(%3219), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3221 : int = aten::size(%hidden_states.75, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3222 : int = aten::size(%hidden_states.75, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:449:0
  %3223 : Long() = aten::mul(%3220, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3224 : Long() = aten::sub(%3223, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:450:0
  %3225 : int = aten::Int(%3224), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3226 : int[] = prim::ListConstruct(%3218, %3225, %3221, %3222), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3227 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3228 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.75, %3226, %3227, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:454:0
  %3229 : Tensor[] = prim::ListConstruct(%3209, %3228), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.81 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %3229), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3231 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %hidden_states_padded.14 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.81, %3231, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3233 : int = aten::size(%hidden_states_padded.14, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3234 : int = aten::size(%hidden_states_padded.14, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3235 : int = aten::size(%hidden_states_padded.14, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3236 : int = aten::size(%hidden_states_padded.14, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:401:0
  %3237 : int[] = prim::ListConstruct(%3233, %3234, %3235, %3236), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_chunked_attention_scores.14 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.14, %3237), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:400:0
  %3239 : Long() = aten::mul(%batch_size.27, %num_heads.20), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3240 : int = aten::Int(%3239), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3241 : Long() = aten::add(%chunks_count.20, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:504:0
  %3242 : int = aten::Int(%3241), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3243 : int[] = prim::ListConstruct(%3240, %3242, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %diagonal_attention_scores.14 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.14, %3243, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:503:0
  %3245 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3246 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3245, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3247 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3246, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3248 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%3247, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3249 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3250 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3249, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3251 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3250, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3252 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%3251, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3253 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3254 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%3248, %3253), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3255 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3252, %3254, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:509:0
  %3256 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3257 : Float(17:262656, 512:513, 513:1) = aten::select(%3256, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3258 : Float(17:262656, 256:513, 513:1) = aten::slice(%3257, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3259 : Float(17:262656, 256:513, 257:1) = aten::slice(%3258, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3260 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3261 : Float(17:262656, 256:513, 513:1) = aten::select(%3260, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3262 : Float(17:262656, 256:513, 513:1) = aten::slice(%3261, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3263 : Float(17:262656, 256:513, 257:1) = aten::slice(%3262, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3264 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3265 : Float(17:262656, 256:513, 257:1) = aten::view(%3259, %3264), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3266 : Float(17:262656, 256:513, 257:1) = aten::copy_(%3263, %3265, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:512:0
  %3267 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3268 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3267, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3269 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3268, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3270 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%3269, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3271 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3272 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3271, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3273 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3272, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3274 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%3273, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3275 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3276 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%3270, %3275), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3277 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3274, %3276, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:516:0
  %3278 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3279 : Float(17:262656, 512:513, 513:1) = aten::select(%3278, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3280 : Float(17:262656, 255:513, 513:1) = aten::slice(%3279, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3281 : Float(17:262656, 255:513, 255:1) = aten::slice(%3280, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3282 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.14, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3283 : Float(17:262656, 256:513, 513:1) = aten::select(%3282, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3284 : Float(17:262656, 255:513, 513:1) = aten::slice(%3283, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3285 : Float(17:262656, 255:513, 255:1) = aten::slice(%3284, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3286 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3287 : Float(17:262656, 255:513, 255:1) = aten::view(%3281, %3286), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3288 : Float(17:262656, 255:513, 255:1) = aten::copy_(%3285, %3287, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:520:0
  %3289 : int[] = prim::ListConstruct(%3172, %3176, %3174, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3290 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.14, %3289), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.20 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%3290, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:525:0
  %3292 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3293 : Float(256:257, 257:1) = aten::ones(%3292, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3294 : Float(256:257, 257:1) = aten::tril(%3293, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3295 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %beginning_mask_2d.14 : Float(256:257, 257:1) = aten::flip(%3294, %3295), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:458:0
  %3297 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.14, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3298 : Float(1:65792, 256:257, 257:1) = aten::slice(%3297, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3299 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3298, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.14 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3299, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:459:0
  %3301 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %ending_mask.14 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.14, %3301), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:460:0
  %3303 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3304 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3303, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3305 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3304, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.14 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3305, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:461:0
  %3307 : int = aten::size(%beginning_input.14, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3308 : int = aten::size(%beginning_input.14, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3309 : int = aten::size(%beginning_input.14, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3310 : int = aten::size(%beginning_input.14, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3311 : int[] = prim::ListConstruct(%3307, %3308, %3309, %3310), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3312 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.14, %3311, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:462:0
  %3313 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3312, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3314 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.14, %3313, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:463:0
  %3315 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3316 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3315, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3317 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3316, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.14 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3317, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:464:0
  %3319 : int = aten::size(%ending_input.14, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3320 : int = aten::size(%ending_input.14, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3321 : int = aten::size(%ending_input.14, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3322 : int = aten::size(%ending_input.14, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3323 : int[] = prim::ListConstruct(%3319, %3320, %3321, %3322), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3324 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.14, %3323, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:465:0
  %3325 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3324, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:22:0
  %3326 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.14, %3325, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.7 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.19, %input_tensor.20, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.7 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.7, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:1500:0
  %3329 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.7, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3330 : Bool(17:512, 512:1) = aten::slice(%3329, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3331 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3330, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %3332 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3331, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %input.82 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.7, %3332, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.14 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.82, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:973:0
  %3335 : int[] = prim::ListConstruct(%2994, %2995, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3336 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.7, %3335), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
  %value.7 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3336, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:331:0
  %3338 : int = aten::size(%value.7, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.28 : Long() = prim::NumToTensor(%3338), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3340 : int = aten::size(%value.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.29 : Long() = prim::NumToTensor(%3340), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3342 : int = aten::size(%value.7, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.21 : Long() = prim::NumToTensor(%3342), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3344 : int = aten::size(%value.7, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:537:0
  %3345 : Long() = aten::floor_divide(%seq_len.29, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %chunks_count.21 : Long() = aten::sub(%3345, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:542:0
  %3347 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.14, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
  %3348 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:546:0
  %3349 : int = aten::Int(%3348), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3350 : Long() = aten::floor_divide(%seq_len.29, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/tensor.py:424:0
  %3351 : int = aten::Int(%3350), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3352 : int[] = prim::ListConstruct(%3349, %3351, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.31 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%3347, %3352), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:545:0
  %3354 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.7, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3355 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3356 : int = aten::Int(%3355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3357 : int[] = prim::ListConstruct(%3356, %3340, %3344), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %input.83 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3354, %3357), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:550:0
  %3359 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %padded_value.7 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.83, %3359, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3361 : Long() = aten::mul(%batch_size.28, %num_heads.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
  %3362 : int = aten::Int(%3361), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3363 : Long() = aten::add(%chunks_count.21, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:556:0
  %3364 : int = aten::Int(%3363), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3365 : int[] = prim::ListConstruct(%3362, %3364, %59, %3344), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3366 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3367 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.7, %3365, %3366, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:564:0
  %3368 : int = aten::size(%chunked_hidden_states.31, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %3369 : int = aten::size(%chunked_hidden_states.31, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %3370 : int = aten::size(%chunked_hidden_states.31, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.7 : Long() = prim::NumToTensor(%3370), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3372 : int = aten::size(%chunked_hidden_states.31, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.7 : Long() = prim::NumToTensor(%3372), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3374 : Long() = aten::add(%window_overlap.7, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:422:0
  %3375 : int = aten::Int(%3374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3376 : int[] = prim::ListConstruct(%74, %3375), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.32 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.31, %3376, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/nn/functional.py:3552:0
  %3378 : int[] = prim::ListConstruct(%3368, %3369, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.33 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.32, %3378), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:424:0
  %3380 : Long() = aten::neg(%window_overlap.7), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:428:0
  %3381 : int = aten::Int(%3380), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3382 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.33, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %3383 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%3382, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.34 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%3383, %66, %74, %3381, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:427:0
  %3385 : Long() = aten::add(%window_overlap.7, %hidden_dim.7, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:431:0
  %3386 : int = aten::Int(%3385), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3387 : int[] = prim::ListConstruct(%3368, %3369, %3370, %3386), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %chunked_hidden_states.35 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.34, %3387), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:430:0
  %3389 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.35, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3390 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3389, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3391 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3390, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3392 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%3391, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:433:0
  %3393 : Tensor[] = prim::ListConstruct(%3392, %3367), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %context.7 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %3393), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # torch/functional.py:327:0
  %3395 : int[] = prim::ListConstruct(%3338, %3342, %3340, %3344), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3396 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.7, %3395), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.13 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%3396, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:569:0
  %3398 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.13, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %3399 : int[] = prim::ListConstruct(%2994, %2995, %2996), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self
  %3400 : Float(512:13056, 17:768, 768:1) = aten::reshape(%3398, %3399), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.14 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%3400, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:350:0
  %input.84 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.14, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.self # transformers/modeling_longformer.py:374:0
  %3403 : __torch__.torch.nn.modules.normalization.___torch_mangle_7211.LayerNorm = prim::GetAttr[name="LayerNorm"](%2970)
  %3404 : __torch__.torch.nn.modules.linear.___torch_mangle_7210.Linear = prim::GetAttr[name="dense"](%2970)
  %3405 : Tensor = prim::GetAttr[name="bias"](%3404)
  %3406 : Tensor = prim::GetAttr[name="weight"](%3404)
  %3407 : Float(768:1, 768:768) = aten::t(%3406), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.84, %3407), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.85 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.40, %3405, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.76 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.85, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.86 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.76, %hidden_states.67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output # transformers/modeling_longformer.py:758:0
  %3412 : Tensor = prim::GetAttr[name="bias"](%3403)
  %3413 : Tensor = prim::GetAttr[name="weight"](%3403)
  %3414 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.21 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.86, %3414, %3413, %3412, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.attention/__module.longformer.encoder.layer.6.attention.output/__module.longformer.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %3416 : __torch__.torch.nn.modules.linear.___torch_mangle_7215.Linear = prim::GetAttr[name="dense"](%2968)
  %3417 : Tensor = prim::GetAttr[name="bias"](%3416)
  %3418 : Tensor = prim::GetAttr[name="weight"](%3416)
  %3419 : Float(768:1, 3072:768) = aten::t(%3418), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.21, %3419), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.87 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.41, %3417, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate/__module.longformer.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.88 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.87), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %3423 : __torch__.torch.nn.modules.normalization.___torch_mangle_7218.LayerNorm = prim::GetAttr[name="LayerNorm"](%2967)
  %3424 : __torch__.torch.nn.modules.linear.___torch_mangle_7217.Linear = prim::GetAttr[name="dense"](%2967)
  %3425 : Tensor = prim::GetAttr[name="bias"](%3424)
  %3426 : Tensor = prim::GetAttr[name="weight"](%3424)
  %3427 : Float(3072:1, 768:3072) = aten::t(%3426), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.88, %3427), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.42, %3425, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.77 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.89, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.77, %input_tensor.21, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output # transformers/modeling_longformer.py:830:0
  %3432 : Tensor = prim::GetAttr[name="bias"](%3423)
  %3433 : Tensor = prim::GetAttr[name="weight"](%3423)
  %3434 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.LayerNorm
  %hidden_states.78 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.90, %3434, %3433, %3432, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.6/__module.longformer.encoder.layer.6.output/__module.longformer.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %3436 : __torch__.transformers.modeling_longformer.___torch_mangle_7239.LongformerOutput = prim::GetAttr[name="output"](%138)
  %3437 : __torch__.transformers.modeling_longformer.___torch_mangle_7235.LongformerIntermediate = prim::GetAttr[name="intermediate"](%138)
  %3438 : __torch__.transformers.modeling_longformer.___torch_mangle_7233.LongformerAttention = prim::GetAttr[name="attention"](%138)
  %3439 : __torch__.transformers.modeling_longformer.___torch_mangle_7232.LongformerSelfOutput = prim::GetAttr[name="output"](%3438)
  %3440 : __torch__.transformers.modeling_longformer.___torch_mangle_7228.LongformerSelfAttention = prim::GetAttr[name="self"](%3438)
  %3441 : __torch__.torch.nn.modules.linear.___torch_mangle_7224.Linear = prim::GetAttr[name="value"](%3440)
  %3442 : __torch__.torch.nn.modules.linear.___torch_mangle_7223.Linear = prim::GetAttr[name="key"](%3440)
  %3443 : __torch__.torch.nn.modules.linear.___torch_mangle_7222.Linear = prim::GetAttr[name="query"](%3440)
  %3444 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
  %3445 : Float(17:512, 512:1) = aten::squeeze(%3444, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.8 : Bool(17:512, 512:1) = aten::lt(%3445, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %input.91 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.78, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:248:0
  %3448 : Tensor = prim::GetAttr[name="bias"](%3443)
  %3449 : Tensor = prim::GetAttr[name="weight"](%3443)
  %3450 : Float(768:1, 768:768) = aten::t(%3449), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3450), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.15 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.43, %3448, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %3453 : Tensor = prim::GetAttr[name="bias"](%3442)
  %3454 : Tensor = prim::GetAttr[name="weight"](%3442)
  %3455 : Float(768:1, 768:768) = aten::t(%3454), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3455), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.44, %3453, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %3458 : Tensor = prim::GetAttr[name="bias"](%3441)
  %3459 : Tensor = prim::GetAttr[name="weight"](%3441)
  %3460 : Float(768:1, 768:768) = aten::t(%3459), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.91, %3460), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.8 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.45, %3458, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self/__module.longformer.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %3463 : int = aten::size(%input.91, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %3464 : int = aten::size(%input.91, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %3465 : int = aten::size(%input.91, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.16 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.15, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:261:0
  %3467 : int[] = prim::ListConstruct(%3463, %3464, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3468 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.16, %3467), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
  %query.15 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3468, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:263:0
  %3470 : int[] = prim::ListConstruct(%3463, %3464, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3471 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.8, %3470), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
  %key.8 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3471, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:264:0
  %3473 : int = aten::size(%query.15, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.30 : Long() = prim::NumToTensor(%3473), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3475 : int = aten::size(%query.15, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.31 : Long() = prim::NumToTensor(%3475), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3477 : int = aten::size(%query.15, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.22 : Long() = prim::NumToTensor(%3477), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3479 : int = aten::size(%query.15, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %3480 : Long() = aten::floor_divide(%seq_len.31, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.22 : Long() = aten::sub(%3480, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
  %3482 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.15, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3483 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3484 : int = aten::Int(%3483), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3485 : int[] = prim::ListConstruct(%3484, %3475, %3479), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.79 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3482, %3485), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3487 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.8, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3488 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3489 : int = aten::Int(%3488), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3490 : int[] = prim::ListConstruct(%3489, %3475, %3479), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.81 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3487, %3490), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3492 : int = aten::size(%hidden_states.79, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3493 : int = aten::size(%hidden_states.79, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3494 : Long() = prim::NumToTensor(%3493), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3495 : Long() = aten::floor_divide(%3494, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3496 : int = aten::Int(%3495), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3497 : int = aten::size(%hidden_states.79, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3498 : int[] = prim::ListConstruct(%3492, %3496, %68, %3497), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.80 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.79, %3498), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3500 : int = aten::size(%hidden_states.80, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3501 : int = aten::size(%hidden_states.80, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3502 : Long() = prim::NumToTensor(%3501), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3503 : int = aten::size(%hidden_states.80, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3504 : int = aten::size(%hidden_states.80, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3505 : Long() = aten::mul(%3502, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3506 : Long() = aten::sub(%3505, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3507 : int = aten::Int(%3506), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3508 : int[] = prim::ListConstruct(%3500, %3507, %3503, %3504), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3509 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3510 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.80, %3508, %3509, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3511 : int = aten::size(%hidden_states.81, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3512 : int = aten::size(%hidden_states.81, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3513 : Long() = prim::NumToTensor(%3512), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3514 : Long() = aten::floor_divide(%3513, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3515 : int = aten::Int(%3514), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3516 : int = aten::size(%hidden_states.81, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3517 : int[] = prim::ListConstruct(%3511, %3515, %68, %3516), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.82 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.81, %3517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3519 : int = aten::size(%hidden_states.82, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3520 : int = aten::size(%hidden_states.82, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3521 : Long() = prim::NumToTensor(%3520), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3522 : int = aten::size(%hidden_states.82, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3523 : int = aten::size(%hidden_states.82, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3524 : Long() = aten::mul(%3521, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3525 : Long() = aten::sub(%3524, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3526 : int = aten::Int(%3525), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3527 : int[] = prim::ListConstruct(%3519, %3526, %3522, %3523), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3528 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3529 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.82, %3527, %3528, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3530 : Tensor[] = prim::ListConstruct(%3510, %3529), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.92 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %3530), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3532 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states_padded.15 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.92, %3532, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3534 : int = aten::size(%hidden_states_padded.15, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3535 : int = aten::size(%hidden_states_padded.15, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3536 : int = aten::size(%hidden_states_padded.15, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3537 : int = aten::size(%hidden_states_padded.15, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3538 : int[] = prim::ListConstruct(%3534, %3535, %3536, %3537), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_chunked_attention_scores.15 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.15, %3538), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:400:0
  %3540 : Long() = aten::mul(%batch_size.30, %num_heads.22), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3541 : int = aten::Int(%3540), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3542 : Long() = aten::add(%chunks_count.22, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3543 : int = aten::Int(%3542), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3544 : int[] = prim::ListConstruct(%3541, %3543, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_attention_scores.15 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.15, %3544, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
  %3546 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3547 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3546, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3548 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3547, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3549 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%3548, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3550 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3551 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3550, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3552 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3551, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3553 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%3552, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3554 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3555 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%3549, %3554), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3556 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3553, %3555, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3557 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3558 : Float(204:262656, 512:513, 513:1) = aten::select(%3557, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3559 : Float(204:262656, 256:513, 513:1) = aten::slice(%3558, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3560 : Float(204:262656, 256:513, 257:1) = aten::slice(%3559, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3561 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3562 : Float(204:262656, 256:513, 513:1) = aten::select(%3561, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3563 : Float(204:262656, 256:513, 513:1) = aten::slice(%3562, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3564 : Float(204:262656, 256:513, 257:1) = aten::slice(%3563, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3565 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3566 : Float(204:262656, 256:513, 257:1) = aten::view(%3560, %3565), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3567 : Float(204:262656, 256:513, 257:1) = aten::copy_(%3564, %3566, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3568 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3569 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%3568, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3570 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%3569, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3571 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%3570, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3572 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3573 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3572, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3574 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%3573, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3575 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%3574, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3576 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3577 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%3571, %3576), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3578 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3575, %3577, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3579 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3580 : Float(204:262656, 512:513, 513:1) = aten::select(%3579, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3581 : Float(204:262656, 255:513, 513:1) = aten::slice(%3580, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3582 : Float(204:262656, 255:513, 255:1) = aten::slice(%3581, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3583 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.15, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3584 : Float(204:262656, 256:513, 513:1) = aten::select(%3583, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3585 : Float(204:262656, 255:513, 513:1) = aten::slice(%3584, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3586 : Float(204:262656, 255:513, 255:1) = aten::slice(%3585, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3587 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3588 : Float(204:262656, 255:513, 255:1) = aten::view(%3582, %3587), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3589 : Float(204:262656, 255:513, 255:1) = aten::copy_(%3586, %3588, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3590 : int[] = prim::ListConstruct(%3473, %3477, %3475, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3591 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.15, %3590), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.22 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%3591, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %3593 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3594 : Float(256:257, 257:1) = aten::ones(%3593, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3595 : Float(256:257, 257:1) = aten::tril(%3594, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3596 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %beginning_mask_2d.15 : Float(256:257, 257:1) = aten::flip(%3595, %3596), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3598 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.15, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3599 : Float(1:65792, 256:257, 257:1) = aten::slice(%3598, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3600 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3599, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.15 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3600, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3602 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %ending_mask.15 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.15, %3602), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
  %3604 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3605 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3604, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3606 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3605, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.15 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3606, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3608 : int = aten::size(%beginning_input.15, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3609 : int = aten::size(%beginning_input.15, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3610 : int = aten::size(%beginning_input.15, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3611 : int = aten::size(%beginning_input.15, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3612 : int[] = prim::ListConstruct(%3608, %3609, %3610, %3611), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3613 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.15, %3612, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3614 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3613, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3615 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.15, %3614, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
  %3616 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3617 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3616, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3618 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%3617, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.15 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%3618, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3620 : int = aten::size(%ending_input.15, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3621 : int = aten::size(%ending_input.15, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3622 : int = aten::size(%ending_input.15, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3623 : int = aten::size(%ending_input.15, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3624 : int[] = prim::ListConstruct(%3620, %3621, %3622, %3623), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3625 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.15, %3624, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3626 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%3625, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3627 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.15, %3626, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
  %3628 : Bool(17:512, 512:1) = aten::ne(%3445, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3629 : Bool(17:512, 512:1) = aten::slice(%3628, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3630 : Bool(17:512, 512:1) = aten::slice(%3629, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3631 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3630, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.8 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3631, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:272:0
  %3633 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.8, %query.15), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.8 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%3633, %remove_from_windowed_attention_mask.8, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:275:0
  %3635 : int = aten::size(%float_mask.8, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3636 : int = aten::size(%float_mask.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3637 : int = aten::size(%float_mask.8, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3638 : int = aten::size(%float_mask.8, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3639 : int[] = prim::ListConstruct(%3635, %3636, %3637, %3638), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %query.16 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%3639, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:280:0
  %3641 : int = aten::size(%query.16, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.31 : Long() = prim::NumToTensor(%3641), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3643 : int = aten::size(%query.16, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.32 : Long() = prim::NumToTensor(%3643), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3645 : int = aten::size(%query.16, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.23 : Long() = prim::NumToTensor(%3645), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3647 : int = aten::size(%query.16, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:472:0
  %3648 : Long() = aten::floor_divide(%seq_len.32, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.23 : Long() = aten::sub(%3648, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:478:0
  %3650 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.16, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3651 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3652 : int = aten::Int(%3651), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3653 : int[] = prim::ListConstruct(%3652, %3643, %3647), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.83 : Float(17:512, 512:1, 1:1) = aten::reshape(%3650, %3653), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:481:0
  %3655 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.8, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3656 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3657 : int = aten::Int(%3656), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3658 : int[] = prim::ListConstruct(%3657, %3643, %3647), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.85 : Float(17:512, 512:1, 1:1) = aten::reshape(%3655, %3658), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:482:0
  %3660 : int = aten::size(%hidden_states.83, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3661 : int = aten::size(%hidden_states.83, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3662 : Long() = prim::NumToTensor(%3661), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3663 : Long() = aten::floor_divide(%3662, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3664 : int = aten::Int(%3663), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3665 : int = aten::size(%hidden_states.83, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3666 : int[] = prim::ListConstruct(%3660, %3664, %68, %3665), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.84 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.83, %3666), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3668 : int = aten::size(%hidden_states.84, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3669 : int = aten::size(%hidden_states.84, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3670 : Long() = prim::NumToTensor(%3669), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3671 : int = aten::size(%hidden_states.84, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3672 : int = aten::size(%hidden_states.84, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3673 : Long() = aten::mul(%3670, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3674 : Long() = aten::sub(%3673, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3675 : int = aten::Int(%3674), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3676 : int[] = prim::ListConstruct(%3668, %3675, %3671, %3672), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3677 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3678 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.84, %3676, %3677, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3679 : int = aten::size(%hidden_states.85, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:442:0
  %3680 : int = aten::size(%hidden_states.85, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:443:0
  %3681 : Long() = prim::NumToTensor(%3680), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3682 : Long() = aten::floor_divide(%3681, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3683 : int = aten::Int(%3682), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3684 : int = aten::size(%hidden_states.85, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:445:0
  %3685 : int[] = prim::ListConstruct(%3679, %3683, %68, %3684), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states.86 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.85, %3685), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:441:0
  %3687 : int = aten::size(%hidden_states.86, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3688 : int = aten::size(%hidden_states.86, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3689 : Long() = prim::NumToTensor(%3688), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3690 : int = aten::size(%hidden_states.86, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3691 : int = aten::size(%hidden_states.86, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:449:0
  %3692 : Long() = aten::mul(%3689, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3693 : Long() = aten::sub(%3692, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:450:0
  %3694 : int = aten::Int(%3693), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3695 : int[] = prim::ListConstruct(%3687, %3694, %3690, %3691), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3696 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3697 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.86, %3695, %3696, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:454:0
  %3698 : Tensor[] = prim::ListConstruct(%3678, %3697), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.93 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %3698), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3700 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %hidden_states_padded.16 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.93, %3700, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3702 : int = aten::size(%hidden_states_padded.16, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3703 : int = aten::size(%hidden_states_padded.16, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3704 : int = aten::size(%hidden_states_padded.16, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3705 : int = aten::size(%hidden_states_padded.16, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:401:0
  %3706 : int[] = prim::ListConstruct(%3702, %3703, %3704, %3705), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_chunked_attention_scores.16 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.16, %3706), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:400:0
  %3708 : Long() = aten::mul(%batch_size.31, %num_heads.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3709 : int = aten::Int(%3708), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3710 : Long() = aten::add(%chunks_count.23, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:504:0
  %3711 : int = aten::Int(%3710), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3712 : int[] = prim::ListConstruct(%3709, %3711, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %diagonal_attention_scores.16 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.16, %3712, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:503:0
  %3714 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3715 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3714, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3716 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3715, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3717 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%3716, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3718 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3719 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3718, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3720 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3719, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3721 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%3720, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3722 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3723 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%3717, %3722), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3724 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%3721, %3723, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:509:0
  %3725 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3726 : Float(17:262656, 512:513, 513:1) = aten::select(%3725, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3727 : Float(17:262656, 256:513, 513:1) = aten::slice(%3726, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3728 : Float(17:262656, 256:513, 257:1) = aten::slice(%3727, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3729 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3730 : Float(17:262656, 256:513, 513:1) = aten::select(%3729, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3731 : Float(17:262656, 256:513, 513:1) = aten::slice(%3730, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3732 : Float(17:262656, 256:513, 257:1) = aten::slice(%3731, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3733 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3734 : Float(17:262656, 256:513, 257:1) = aten::view(%3728, %3733), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3735 : Float(17:262656, 256:513, 257:1) = aten::copy_(%3732, %3734, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:512:0
  %3736 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3737 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%3736, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3738 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%3737, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3739 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%3738, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3740 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3741 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3740, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3742 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%3741, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3743 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%3742, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3744 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3745 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%3739, %3744), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3746 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%3743, %3745, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:516:0
  %3747 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3748 : Float(17:262656, 512:513, 513:1) = aten::select(%3747, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3749 : Float(17:262656, 255:513, 513:1) = aten::slice(%3748, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3750 : Float(17:262656, 255:513, 255:1) = aten::slice(%3749, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3751 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.16, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3752 : Float(17:262656, 256:513, 513:1) = aten::select(%3751, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3753 : Float(17:262656, 255:513, 513:1) = aten::slice(%3752, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3754 : Float(17:262656, 255:513, 255:1) = aten::slice(%3753, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3755 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3756 : Float(17:262656, 255:513, 255:1) = aten::view(%3750, %3755), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3757 : Float(17:262656, 255:513, 255:1) = aten::copy_(%3754, %3756, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:520:0
  %3758 : int[] = prim::ListConstruct(%3641, %3645, %3643, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3759 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.16, %3758), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.23 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%3759, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:525:0
  %3761 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3762 : Float(256:257, 257:1) = aten::ones(%3761, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3763 : Float(256:257, 257:1) = aten::tril(%3762, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3764 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %beginning_mask_2d.16 : Float(256:257, 257:1) = aten::flip(%3763, %3764), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:458:0
  %3766 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.16, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3767 : Float(1:65792, 256:257, 257:1) = aten::slice(%3766, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3768 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%3767, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.16 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%3768, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:459:0
  %3770 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %ending_mask.16 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.16, %3770), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:460:0
  %3772 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3773 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3772, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3774 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3773, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.16 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3774, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:461:0
  %3776 : int = aten::size(%beginning_input.16, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3777 : int = aten::size(%beginning_input.16, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3778 : int = aten::size(%beginning_input.16, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3779 : int = aten::size(%beginning_input.16, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3780 : int[] = prim::ListConstruct(%3776, %3777, %3778, %3779), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3781 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.16, %3780, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:462:0
  %3782 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3781, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3783 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.16, %3782, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:463:0
  %3784 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3785 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3784, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3786 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%3785, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.16 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%3786, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:464:0
  %3788 : int = aten::size(%ending_input.16, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3789 : int = aten::size(%ending_input.16, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3790 : int = aten::size(%ending_input.16, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3791 : int = aten::size(%ending_input.16, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3792 : int[] = prim::ListConstruct(%3788, %3789, %3790, %3791), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3793 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.16, %3792, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:465:0
  %3794 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%3793, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:22:0
  %3795 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.16, %3794, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.8 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.22, %input_tensor.23, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.8 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.8, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:1500:0
  %3798 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.8, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3799 : Bool(17:512, 512:1) = aten::slice(%3798, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3800 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%3799, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %3801 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%3800, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %input.94 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.8, %3801, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.16 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.94, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:973:0
  %3804 : int[] = prim::ListConstruct(%3463, %3464, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3805 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.8, %3804), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
  %value.8 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3805, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:331:0
  %3807 : int = aten::size(%value.8, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.32 : Long() = prim::NumToTensor(%3807), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3809 : int = aten::size(%value.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.33 : Long() = prim::NumToTensor(%3809), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3811 : int = aten::size(%value.8, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.24 : Long() = prim::NumToTensor(%3811), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3813 : int = aten::size(%value.8, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:537:0
  %3814 : Long() = aten::floor_divide(%seq_len.33, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %chunks_count.24 : Long() = aten::sub(%3814, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:542:0
  %3816 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.16, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
  %3817 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:546:0
  %3818 : int = aten::Int(%3817), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3819 : Long() = aten::floor_divide(%seq_len.33, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/tensor.py:424:0
  %3820 : int = aten::Int(%3819), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3821 : int[] = prim::ListConstruct(%3818, %3820, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.36 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%3816, %3821), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:545:0
  %3823 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.8, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3824 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3825 : int = aten::Int(%3824), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3826 : int[] = prim::ListConstruct(%3825, %3809, %3813), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %input.95 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3823, %3826), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:550:0
  %3828 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %padded_value.8 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.95, %3828, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3830 : Long() = aten::mul(%batch_size.32, %num_heads.24), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
  %3831 : int = aten::Int(%3830), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3832 : Long() = aten::add(%chunks_count.24, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:556:0
  %3833 : int = aten::Int(%3832), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3834 : int[] = prim::ListConstruct(%3831, %3833, %59, %3813), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3835 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3836 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.8, %3834, %3835, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:564:0
  %3837 : int = aten::size(%chunked_hidden_states.36, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %3838 : int = aten::size(%chunked_hidden_states.36, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %3839 : int = aten::size(%chunked_hidden_states.36, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.8 : Long() = prim::NumToTensor(%3839), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3841 : int = aten::size(%chunked_hidden_states.36, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.8 : Long() = prim::NumToTensor(%3841), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3843 : Long() = aten::add(%window_overlap.8, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:422:0
  %3844 : int = aten::Int(%3843), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3845 : int[] = prim::ListConstruct(%74, %3844), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.37 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.36, %3845, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/nn/functional.py:3552:0
  %3847 : int[] = prim::ListConstruct(%3837, %3838, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.38 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.37, %3847), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:424:0
  %3849 : Long() = aten::neg(%window_overlap.8), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:428:0
  %3850 : int = aten::Int(%3849), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3851 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.38, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %3852 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%3851, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.39 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%3852, %66, %74, %3850, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:427:0
  %3854 : Long() = aten::add(%window_overlap.8, %hidden_dim.8, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:431:0
  %3855 : int = aten::Int(%3854), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3856 : int[] = prim::ListConstruct(%3837, %3838, %3839, %3855), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %chunked_hidden_states.40 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.39, %3856), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:430:0
  %3858 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.40, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3859 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3858, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3860 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%3859, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3861 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%3860, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:433:0
  %3862 : Tensor[] = prim::ListConstruct(%3861, %3836), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %context.8 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %3862), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # torch/functional.py:327:0
  %3864 : int[] = prim::ListConstruct(%3807, %3811, %3809, %3813), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3865 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.8, %3864), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.15 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%3865, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:569:0
  %3867 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.15, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %3868 : int[] = prim::ListConstruct(%3463, %3464, %3465), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self
  %3869 : Float(512:13056, 17:768, 768:1) = aten::reshape(%3867, %3868), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.16 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%3869, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:350:0
  %input.96 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.16, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.self # transformers/modeling_longformer.py:374:0
  %3872 : __torch__.torch.nn.modules.normalization.___torch_mangle_7230.LayerNorm = prim::GetAttr[name="LayerNorm"](%3439)
  %3873 : __torch__.torch.nn.modules.linear.___torch_mangle_7229.Linear = prim::GetAttr[name="dense"](%3439)
  %3874 : Tensor = prim::GetAttr[name="bias"](%3873)
  %3875 : Tensor = prim::GetAttr[name="weight"](%3873)
  %3876 : Float(768:1, 768:768) = aten::t(%3875), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.96, %3876), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.97 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.46, %3874, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.87 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.97, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.98 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.87, %hidden_states.78, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output # transformers/modeling_longformer.py:758:0
  %3881 : Tensor = prim::GetAttr[name="bias"](%3872)
  %3882 : Tensor = prim::GetAttr[name="weight"](%3872)
  %3883 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.24 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.98, %3883, %3882, %3881, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.attention/__module.longformer.encoder.layer.7.attention.output/__module.longformer.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %3885 : __torch__.torch.nn.modules.linear.___torch_mangle_7234.Linear = prim::GetAttr[name="dense"](%3437)
  %3886 : Tensor = prim::GetAttr[name="bias"](%3885)
  %3887 : Tensor = prim::GetAttr[name="weight"](%3885)
  %3888 : Float(768:1, 3072:768) = aten::t(%3887), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.24, %3888), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.47, %3886, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate/__module.longformer.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.100 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.99), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %3892 : __torch__.torch.nn.modules.normalization.___torch_mangle_7237.LayerNorm = prim::GetAttr[name="LayerNorm"](%3436)
  %3893 : __torch__.torch.nn.modules.linear.___torch_mangle_7236.Linear = prim::GetAttr[name="dense"](%3436)
  %3894 : Tensor = prim::GetAttr[name="bias"](%3893)
  %3895 : Tensor = prim::GetAttr[name="weight"](%3893)
  %3896 : Float(3072:1, 768:3072) = aten::t(%3895), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.100, %3896), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.48, %3894, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.88 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.101, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.102 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.88, %input_tensor.24, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output # transformers/modeling_longformer.py:830:0
  %3901 : Tensor = prim::GetAttr[name="bias"](%3892)
  %3902 : Tensor = prim::GetAttr[name="weight"](%3892)
  %3903 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.LayerNorm
  %hidden_states.89 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.102, %3903, %3902, %3901, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.7/__module.longformer.encoder.layer.7.output/__module.longformer.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %3905 : __torch__.transformers.modeling_longformer.___torch_mangle_7258.LongformerOutput = prim::GetAttr[name="output"](%136)
  %3906 : __torch__.transformers.modeling_longformer.___torch_mangle_7254.LongformerIntermediate = prim::GetAttr[name="intermediate"](%136)
  %3907 : __torch__.transformers.modeling_longformer.___torch_mangle_7252.LongformerAttention = prim::GetAttr[name="attention"](%136)
  %3908 : __torch__.transformers.modeling_longformer.___torch_mangle_7251.LongformerSelfOutput = prim::GetAttr[name="output"](%3907)
  %3909 : __torch__.transformers.modeling_longformer.___torch_mangle_7247.LongformerSelfAttention = prim::GetAttr[name="self"](%3907)
  %3910 : __torch__.torch.nn.modules.linear.___torch_mangle_7243.Linear = prim::GetAttr[name="value"](%3909)
  %3911 : __torch__.torch.nn.modules.linear.___torch_mangle_7242.Linear = prim::GetAttr[name="key"](%3909)
  %3912 : __torch__.torch.nn.modules.linear.___torch_mangle_7241.Linear = prim::GetAttr[name="query"](%3909)
  %3913 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
  %3914 : Float(17:512, 512:1) = aten::squeeze(%3913, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.9 : Bool(17:512, 512:1) = aten::lt(%3914, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %input.103 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.89, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:248:0
  %3917 : Tensor = prim::GetAttr[name="bias"](%3912)
  %3918 : Tensor = prim::GetAttr[name="weight"](%3912)
  %3919 : Float(768:1, 768:768) = aten::t(%3918), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3919), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.17 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.49, %3917, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %3922 : Tensor = prim::GetAttr[name="bias"](%3911)
  %3923 : Tensor = prim::GetAttr[name="weight"](%3911)
  %3924 : Float(768:1, 768:768) = aten::t(%3923), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3924), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.50, %3922, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %3927 : Tensor = prim::GetAttr[name="bias"](%3910)
  %3928 : Tensor = prim::GetAttr[name="weight"](%3910)
  %3929 : Float(768:1, 768:768) = aten::t(%3928), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.103, %3929), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.9 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.51, %3927, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self/__module.longformer.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %3932 : int = aten::size(%input.103, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %3933 : int = aten::size(%input.103, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %3934 : int = aten::size(%input.103, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.18 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.17, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:261:0
  %3936 : int[] = prim::ListConstruct(%3932, %3933, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3937 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.18, %3936), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
  %query.17 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3937, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:263:0
  %3939 : int[] = prim::ListConstruct(%3932, %3933, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3940 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.9, %3939), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
  %key.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%3940, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:264:0
  %3942 : int = aten::size(%query.17, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.34 : Long() = prim::NumToTensor(%3942), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3944 : int = aten::size(%query.17, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.35 : Long() = prim::NumToTensor(%3944), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3946 : int = aten::size(%query.17, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.25 : Long() = prim::NumToTensor(%3946), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3948 : int = aten::size(%query.17, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %3949 : Long() = aten::floor_divide(%seq_len.35, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.25 : Long() = aten::sub(%3949, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
  %3951 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.17, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3952 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3953 : int = aten::Int(%3952), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3954 : int[] = prim::ListConstruct(%3953, %3944, %3948), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.90 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3951, %3954), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %3956 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.9, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3957 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3958 : int = aten::Int(%3957), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3959 : int[] = prim::ListConstruct(%3958, %3944, %3948), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.92 : Float(204:64, 512:13056, 64:1) = aten::reshape(%3956, %3959), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %3961 : int = aten::size(%hidden_states.90, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %3962 : int = aten::size(%hidden_states.90, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %3963 : Long() = prim::NumToTensor(%3962), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3964 : Long() = aten::floor_divide(%3963, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %3965 : int = aten::Int(%3964), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3966 : int = aten::size(%hidden_states.90, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %3967 : int[] = prim::ListConstruct(%3961, %3965, %68, %3966), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.91 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.90, %3967), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %3969 : int = aten::size(%hidden_states.91, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3970 : int = aten::size(%hidden_states.91, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3971 : Long() = prim::NumToTensor(%3970), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3972 : int = aten::size(%hidden_states.91, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3973 : int = aten::size(%hidden_states.91, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3974 : Long() = aten::mul(%3971, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3975 : Long() = aten::sub(%3974, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3976 : int = aten::Int(%3975), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3977 : int[] = prim::ListConstruct(%3969, %3976, %3972, %3973), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3978 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3979 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.91, %3977, %3978, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %3980 : int = aten::size(%hidden_states.92, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %3981 : int = aten::size(%hidden_states.92, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %3982 : Long() = prim::NumToTensor(%3981), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3983 : Long() = aten::floor_divide(%3982, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %3984 : int = aten::Int(%3983), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3985 : int = aten::size(%hidden_states.92, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %3986 : int[] = prim::ListConstruct(%3980, %3984, %68, %3985), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.93 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.92, %3986), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %3988 : int = aten::size(%hidden_states.93, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3989 : int = aten::size(%hidden_states.93, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3990 : Long() = prim::NumToTensor(%3989), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3991 : int = aten::size(%hidden_states.93, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3992 : int = aten::size(%hidden_states.93, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %3993 : Long() = aten::mul(%3990, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3994 : Long() = aten::sub(%3993, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %3995 : int = aten::Int(%3994), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3996 : int[] = prim::ListConstruct(%3988, %3995, %3991, %3992), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3997 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %3998 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.93, %3996, %3997, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %3999 : Tensor[] = prim::ListConstruct(%3979, %3998), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.104 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %3999), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %4001 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states_padded.17 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.104, %4001, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4003 : int = aten::size(%hidden_states_padded.17, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4004 : int = aten::size(%hidden_states_padded.17, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4005 : int = aten::size(%hidden_states_padded.17, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4006 : int = aten::size(%hidden_states_padded.17, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4007 : int[] = prim::ListConstruct(%4003, %4004, %4005, %4006), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_chunked_attention_scores.17 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.17, %4007), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:400:0
  %4009 : Long() = aten::mul(%batch_size.34, %num_heads.25), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %4010 : int = aten::Int(%4009), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4011 : Long() = aten::add(%chunks_count.25, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %4012 : int = aten::Int(%4011), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4013 : int[] = prim::ListConstruct(%4010, %4012, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_attention_scores.17 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.17, %4013, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
  %4015 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4016 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4015, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4017 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4016, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4018 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%4017, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4019 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4020 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4019, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4021 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4020, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4022 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%4021, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4023 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4024 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%4018, %4023), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4025 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4022, %4024, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4026 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4027 : Float(204:262656, 512:513, 513:1) = aten::select(%4026, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4028 : Float(204:262656, 256:513, 513:1) = aten::slice(%4027, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4029 : Float(204:262656, 256:513, 257:1) = aten::slice(%4028, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4030 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4031 : Float(204:262656, 256:513, 513:1) = aten::select(%4030, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4032 : Float(204:262656, 256:513, 513:1) = aten::slice(%4031, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4033 : Float(204:262656, 256:513, 257:1) = aten::slice(%4032, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4034 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4035 : Float(204:262656, 256:513, 257:1) = aten::view(%4029, %4034), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4036 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4033, %4035, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4037 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4038 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4037, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4039 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4038, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4040 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4039, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4041 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4042 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4041, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4043 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4042, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4044 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4043, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4045 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4046 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4040, %4045), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4047 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4044, %4046, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4048 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4049 : Float(204:262656, 512:513, 513:1) = aten::select(%4048, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4050 : Float(204:262656, 255:513, 513:1) = aten::slice(%4049, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4051 : Float(204:262656, 255:513, 255:1) = aten::slice(%4050, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4052 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.17, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4053 : Float(204:262656, 256:513, 513:1) = aten::select(%4052, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4054 : Float(204:262656, 255:513, 513:1) = aten::slice(%4053, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4055 : Float(204:262656, 255:513, 255:1) = aten::slice(%4054, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4056 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4057 : Float(204:262656, 255:513, 255:1) = aten::view(%4051, %4056), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4058 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4055, %4057, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4059 : int[] = prim::ListConstruct(%3942, %3946, %3944, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4060 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.17, %4059), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.25 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4060, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %4062 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4063 : Float(256:257, 257:1) = aten::ones(%4062, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4064 : Float(256:257, 257:1) = aten::tril(%4063, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4065 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %beginning_mask_2d.17 : Float(256:257, 257:1) = aten::flip(%4064, %4065), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4067 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.17, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4068 : Float(1:65792, 256:257, 257:1) = aten::slice(%4067, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4069 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4068, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.17 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4069, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4071 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %ending_mask.17 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.17, %4071), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
  %4073 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.25, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4074 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4073, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4075 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4074, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.17 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4075, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4077 : int = aten::size(%beginning_input.17, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4078 : int = aten::size(%beginning_input.17, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4079 : int = aten::size(%beginning_input.17, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4080 : int = aten::size(%beginning_input.17, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4081 : int[] = prim::ListConstruct(%4077, %4078, %4079, %4080), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4082 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.17, %4081, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4083 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4082, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4084 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.17, %4083, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
  %4085 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.25, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4086 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4085, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4087 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4086, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.17 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4087, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4089 : int = aten::size(%ending_input.17, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4090 : int = aten::size(%ending_input.17, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4091 : int = aten::size(%ending_input.17, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4092 : int = aten::size(%ending_input.17, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4093 : int[] = prim::ListConstruct(%4089, %4090, %4091, %4092), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4094 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.17, %4093, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4095 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4094, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4096 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.17, %4095, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
  %4097 : Bool(17:512, 512:1) = aten::ne(%3914, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4098 : Bool(17:512, 512:1) = aten::slice(%4097, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4099 : Bool(17:512, 512:1) = aten::slice(%4098, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4100 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4099, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.9 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4100, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:272:0
  %4102 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.9, %query.17), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.9 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%4102, %remove_from_windowed_attention_mask.9, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:275:0
  %4104 : int = aten::size(%float_mask.9, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4105 : int = aten::size(%float_mask.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4106 : int = aten::size(%float_mask.9, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4107 : int = aten::size(%float_mask.9, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4108 : int[] = prim::ListConstruct(%4104, %4105, %4106, %4107), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %query.18 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%4108, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:280:0
  %4110 : int = aten::size(%query.18, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.35 : Long() = prim::NumToTensor(%4110), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4112 : int = aten::size(%query.18, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.36 : Long() = prim::NumToTensor(%4112), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4114 : int = aten::size(%query.18, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.26 : Long() = prim::NumToTensor(%4114), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4116 : int = aten::size(%query.18, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:472:0
  %4117 : Long() = aten::floor_divide(%seq_len.36, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.26 : Long() = aten::sub(%4117, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:478:0
  %4119 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.18, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4120 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4121 : int = aten::Int(%4120), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4122 : int[] = prim::ListConstruct(%4121, %4112, %4116), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.94 : Float(17:512, 512:1, 1:1) = aten::reshape(%4119, %4122), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:481:0
  %4124 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.9, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4125 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4126 : int = aten::Int(%4125), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4127 : int[] = prim::ListConstruct(%4126, %4112, %4116), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.96 : Float(17:512, 512:1, 1:1) = aten::reshape(%4124, %4127), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:482:0
  %4129 : int = aten::size(%hidden_states.94, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %4130 : int = aten::size(%hidden_states.94, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %4131 : Long() = prim::NumToTensor(%4130), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4132 : Long() = aten::floor_divide(%4131, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4133 : int = aten::Int(%4132), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4134 : int = aten::size(%hidden_states.94, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %4135 : int[] = prim::ListConstruct(%4129, %4133, %68, %4134), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.95 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.94, %4135), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %4137 : int = aten::size(%hidden_states.95, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4138 : int = aten::size(%hidden_states.95, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4139 : Long() = prim::NumToTensor(%4138), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4140 : int = aten::size(%hidden_states.95, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4141 : int = aten::size(%hidden_states.95, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4142 : Long() = aten::mul(%4139, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4143 : Long() = aten::sub(%4142, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4144 : int = aten::Int(%4143), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4145 : int[] = prim::ListConstruct(%4137, %4144, %4140, %4141), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4146 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4147 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.95, %4145, %4146, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %4148 : int = aten::size(%hidden_states.96, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:442:0
  %4149 : int = aten::size(%hidden_states.96, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:443:0
  %4150 : Long() = prim::NumToTensor(%4149), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4151 : Long() = aten::floor_divide(%4150, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4152 : int = aten::Int(%4151), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4153 : int = aten::size(%hidden_states.96, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:445:0
  %4154 : int[] = prim::ListConstruct(%4148, %4152, %68, %4153), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states.97 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.96, %4154), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:441:0
  %4156 : int = aten::size(%hidden_states.97, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4157 : int = aten::size(%hidden_states.97, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4158 : Long() = prim::NumToTensor(%4157), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4159 : int = aten::size(%hidden_states.97, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4160 : int = aten::size(%hidden_states.97, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:449:0
  %4161 : Long() = aten::mul(%4158, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4162 : Long() = aten::sub(%4161, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:450:0
  %4163 : int = aten::Int(%4162), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4164 : int[] = prim::ListConstruct(%4156, %4163, %4159, %4160), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4165 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4166 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.97, %4164, %4165, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:454:0
  %4167 : Tensor[] = prim::ListConstruct(%4147, %4166), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.105 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %4167), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %4169 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %hidden_states_padded.18 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.105, %4169, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4171 : int = aten::size(%hidden_states_padded.18, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4172 : int = aten::size(%hidden_states_padded.18, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4173 : int = aten::size(%hidden_states_padded.18, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4174 : int = aten::size(%hidden_states_padded.18, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:401:0
  %4175 : int[] = prim::ListConstruct(%4171, %4172, %4173, %4174), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_chunked_attention_scores.18 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.18, %4175), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:400:0
  %4177 : Long() = aten::mul(%batch_size.35, %num_heads.26), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %4178 : int = aten::Int(%4177), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4179 : Long() = aten::add(%chunks_count.26, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:504:0
  %4180 : int = aten::Int(%4179), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4181 : int[] = prim::ListConstruct(%4178, %4180, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %diagonal_attention_scores.18 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.18, %4181, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:503:0
  %4183 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4184 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4183, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4185 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4184, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4186 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%4185, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4187 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4188 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4187, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4189 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4188, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4190 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%4189, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4191 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4192 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%4186, %4191), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4193 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4190, %4192, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:509:0
  %4194 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4195 : Float(17:262656, 512:513, 513:1) = aten::select(%4194, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4196 : Float(17:262656, 256:513, 513:1) = aten::slice(%4195, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4197 : Float(17:262656, 256:513, 257:1) = aten::slice(%4196, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4198 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4199 : Float(17:262656, 256:513, 513:1) = aten::select(%4198, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4200 : Float(17:262656, 256:513, 513:1) = aten::slice(%4199, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4201 : Float(17:262656, 256:513, 257:1) = aten::slice(%4200, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4202 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4203 : Float(17:262656, 256:513, 257:1) = aten::view(%4197, %4202), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4204 : Float(17:262656, 256:513, 257:1) = aten::copy_(%4201, %4203, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:512:0
  %4205 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4206 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4205, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4207 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4206, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4208 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%4207, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4209 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4210 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4209, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4211 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4210, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4212 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%4211, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4213 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4214 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%4208, %4213), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4215 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4212, %4214, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:516:0
  %4216 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4217 : Float(17:262656, 512:513, 513:1) = aten::select(%4216, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4218 : Float(17:262656, 255:513, 513:1) = aten::slice(%4217, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4219 : Float(17:262656, 255:513, 255:1) = aten::slice(%4218, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4220 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.18, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4221 : Float(17:262656, 256:513, 513:1) = aten::select(%4220, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4222 : Float(17:262656, 255:513, 513:1) = aten::slice(%4221, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4223 : Float(17:262656, 255:513, 255:1) = aten::slice(%4222, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4224 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4225 : Float(17:262656, 255:513, 255:1) = aten::view(%4219, %4224), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4226 : Float(17:262656, 255:513, 255:1) = aten::copy_(%4223, %4225, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:520:0
  %4227 : int[] = prim::ListConstruct(%4110, %4114, %4112, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4228 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.18, %4227), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.26 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%4228, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:525:0
  %4230 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4231 : Float(256:257, 257:1) = aten::ones(%4230, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4232 : Float(256:257, 257:1) = aten::tril(%4231, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4233 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %beginning_mask_2d.18 : Float(256:257, 257:1) = aten::flip(%4232, %4233), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:458:0
  %4235 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.18, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4236 : Float(1:65792, 256:257, 257:1) = aten::slice(%4235, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4237 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4236, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.18 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4237, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:459:0
  %4239 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %ending_mask.18 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.18, %4239), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:460:0
  %4241 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.26, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4242 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4241, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4243 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4242, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.18 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4243, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:461:0
  %4245 : int = aten::size(%beginning_input.18, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4246 : int = aten::size(%beginning_input.18, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4247 : int = aten::size(%beginning_input.18, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4248 : int = aten::size(%beginning_input.18, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4249 : int[] = prim::ListConstruct(%4245, %4246, %4247, %4248), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4250 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.18, %4249, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:462:0
  %4251 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4250, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4252 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.18, %4251, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:463:0
  %4253 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.26, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4254 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4253, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4255 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4254, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.18 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4255, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:464:0
  %4257 : int = aten::size(%ending_input.18, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4258 : int = aten::size(%ending_input.18, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4259 : int = aten::size(%ending_input.18, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4260 : int = aten::size(%ending_input.18, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4261 : int[] = prim::ListConstruct(%4257, %4258, %4259, %4260), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4262 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.18, %4261, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:465:0
  %4263 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4262, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:22:0
  %4264 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.18, %4263, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.9 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.25, %input_tensor.26, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.9 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.9, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:1500:0
  %4267 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.9, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4268 : Bool(17:512, 512:1) = aten::slice(%4267, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4269 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4268, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %4270 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4269, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %input.106 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.9, %4270, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.18 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.106, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:973:0
  %4273 : int[] = prim::ListConstruct(%3932, %3933, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4274 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.9, %4273), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
  %value.9 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4274, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:331:0
  %4276 : int = aten::size(%value.9, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.36 : Long() = prim::NumToTensor(%4276), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4278 : int = aten::size(%value.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.37 : Long() = prim::NumToTensor(%4278), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4280 : int = aten::size(%value.9, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.27 : Long() = prim::NumToTensor(%4280), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4282 : int = aten::size(%value.9, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:537:0
  %4283 : Long() = aten::floor_divide(%seq_len.37, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %chunks_count.27 : Long() = aten::sub(%4283, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:542:0
  %4285 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.18, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
  %4286 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:546:0
  %4287 : int = aten::Int(%4286), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4288 : Long() = aten::floor_divide(%seq_len.37, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/tensor.py:424:0
  %4289 : int = aten::Int(%4288), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4290 : int[] = prim::ListConstruct(%4287, %4289, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.41 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%4285, %4290), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:545:0
  %4292 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.9, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4293 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4294 : int = aten::Int(%4293), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4295 : int[] = prim::ListConstruct(%4294, %4278, %4282), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %input.107 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4292, %4295), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:550:0
  %4297 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %padded_value.9 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.107, %4297, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4299 : Long() = aten::mul(%batch_size.36, %num_heads.27), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
  %4300 : int = aten::Int(%4299), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4301 : Long() = aten::add(%chunks_count.27, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:556:0
  %4302 : int = aten::Int(%4301), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4303 : int[] = prim::ListConstruct(%4300, %4302, %59, %4282), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4304 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4305 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.9, %4303, %4304, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:564:0
  %4306 : int = aten::size(%chunked_hidden_states.41, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %4307 : int = aten::size(%chunked_hidden_states.41, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %4308 : int = aten::size(%chunked_hidden_states.41, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.9 : Long() = prim::NumToTensor(%4308), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4310 : int = aten::size(%chunked_hidden_states.41, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.9 : Long() = prim::NumToTensor(%4310), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4312 : Long() = aten::add(%window_overlap.9, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:422:0
  %4313 : int = aten::Int(%4312), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4314 : int[] = prim::ListConstruct(%74, %4313), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.42 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.41, %4314, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/nn/functional.py:3552:0
  %4316 : int[] = prim::ListConstruct(%4306, %4307, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.43 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.42, %4316), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:424:0
  %4318 : Long() = aten::neg(%window_overlap.9), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:428:0
  %4319 : int = aten::Int(%4318), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4320 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.43, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %4321 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%4320, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.44 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%4321, %66, %74, %4319, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:427:0
  %4323 : Long() = aten::add(%window_overlap.9, %hidden_dim.9, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:431:0
  %4324 : int = aten::Int(%4323), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4325 : int[] = prim::ListConstruct(%4306, %4307, %4308, %4324), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %chunked_hidden_states.45 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.44, %4325), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:430:0
  %4327 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.45, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4328 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4327, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4329 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4328, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4330 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%4329, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:433:0
  %4331 : Tensor[] = prim::ListConstruct(%4330, %4305), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %context.9 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %4331), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # torch/functional.py:327:0
  %4333 : int[] = prim::ListConstruct(%4276, %4280, %4278, %4282), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4334 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.9, %4333), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.17 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%4334, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:569:0
  %4336 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.17, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %4337 : int[] = prim::ListConstruct(%3932, %3933, %3934), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self
  %4338 : Float(512:13056, 17:768, 768:1) = aten::reshape(%4336, %4337), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.18 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%4338, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:350:0
  %input.108 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.18, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.self # transformers/modeling_longformer.py:374:0
  %4341 : __torch__.torch.nn.modules.normalization.___torch_mangle_7249.LayerNorm = prim::GetAttr[name="LayerNorm"](%3908)
  %4342 : __torch__.torch.nn.modules.linear.___torch_mangle_7248.Linear = prim::GetAttr[name="dense"](%3908)
  %4343 : Tensor = prim::GetAttr[name="bias"](%4342)
  %4344 : Tensor = prim::GetAttr[name="weight"](%4342)
  %4345 : Float(768:1, 768:768) = aten::t(%4344), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.108, %4345), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.52, %4343, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.98 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.109, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.98, %hidden_states.89, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output # transformers/modeling_longformer.py:758:0
  %4350 : Tensor = prim::GetAttr[name="bias"](%4341)
  %4351 : Tensor = prim::GetAttr[name="weight"](%4341)
  %4352 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.27 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.110, %4352, %4351, %4350, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.attention/__module.longformer.encoder.layer.8.attention.output/__module.longformer.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %4354 : __torch__.torch.nn.modules.linear.___torch_mangle_7253.Linear = prim::GetAttr[name="dense"](%3906)
  %4355 : Tensor = prim::GetAttr[name="bias"](%4354)
  %4356 : Tensor = prim::GetAttr[name="weight"](%4354)
  %4357 : Float(768:1, 3072:768) = aten::t(%4356), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.27, %4357), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.53, %4355, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate/__module.longformer.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.111), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %4361 : __torch__.torch.nn.modules.normalization.___torch_mangle_7256.LayerNorm = prim::GetAttr[name="LayerNorm"](%3905)
  %4362 : __torch__.torch.nn.modules.linear.___torch_mangle_7255.Linear = prim::GetAttr[name="dense"](%3905)
  %4363 : Tensor = prim::GetAttr[name="bias"](%4362)
  %4364 : Tensor = prim::GetAttr[name="weight"](%4362)
  %4365 : Float(3072:1, 768:3072) = aten::t(%4364), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.112, %4365), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.54, %4363, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.99 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.113, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.99, %input_tensor.27, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output # transformers/modeling_longformer.py:830:0
  %4370 : Tensor = prim::GetAttr[name="bias"](%4361)
  %4371 : Tensor = prim::GetAttr[name="weight"](%4361)
  %4372 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.LayerNorm
  %hidden_states.100 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.114, %4372, %4371, %4370, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.8/__module.longformer.encoder.layer.8.output/__module.longformer.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %4374 : __torch__.transformers.modeling_longformer.___torch_mangle_7277.LongformerOutput = prim::GetAttr[name="output"](%134)
  %4375 : __torch__.transformers.modeling_longformer.___torch_mangle_7273.LongformerIntermediate = prim::GetAttr[name="intermediate"](%134)
  %4376 : __torch__.transformers.modeling_longformer.___torch_mangle_7271.LongformerAttention = prim::GetAttr[name="attention"](%134)
  %4377 : __torch__.transformers.modeling_longformer.___torch_mangle_7270.LongformerSelfOutput = prim::GetAttr[name="output"](%4376)
  %4378 : __torch__.transformers.modeling_longformer.___torch_mangle_7266.LongformerSelfAttention = prim::GetAttr[name="self"](%4376)
  %4379 : __torch__.torch.nn.modules.linear.___torch_mangle_7262.Linear = prim::GetAttr[name="value"](%4378)
  %4380 : __torch__.torch.nn.modules.linear.___torch_mangle_7261.Linear = prim::GetAttr[name="key"](%4378)
  %4381 : __torch__.torch.nn.modules.linear.___torch_mangle_7260.Linear = prim::GetAttr[name="query"](%4378)
  %4382 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
  %4383 : Float(17:512, 512:1) = aten::squeeze(%4382, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.10 : Bool(17:512, 512:1) = aten::lt(%4383, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %input.115 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.100, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:248:0
  %4386 : Tensor = prim::GetAttr[name="bias"](%4381)
  %4387 : Tensor = prim::GetAttr[name="weight"](%4381)
  %4388 : Float(768:1, 768:768) = aten::t(%4387), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4388), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.19 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.55, %4386, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %4391 : Tensor = prim::GetAttr[name="bias"](%4380)
  %4392 : Tensor = prim::GetAttr[name="weight"](%4380)
  %4393 : Float(768:1, 768:768) = aten::t(%4392), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4393), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.56, %4391, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %4396 : Tensor = prim::GetAttr[name="bias"](%4379)
  %4397 : Tensor = prim::GetAttr[name="weight"](%4379)
  %4398 : Float(768:1, 768:768) = aten::t(%4397), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.115, %4398), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.10 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.57, %4396, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self/__module.longformer.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %4401 : int = aten::size(%input.115, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %4402 : int = aten::size(%input.115, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %4403 : int = aten::size(%input.115, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.20 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.19, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:261:0
  %4405 : int[] = prim::ListConstruct(%4401, %4402, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4406 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.20, %4405), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
  %query.19 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4406, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:263:0
  %4408 : int[] = prim::ListConstruct(%4401, %4402, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4409 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.10, %4408), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
  %key.10 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4409, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:264:0
  %4411 : int = aten::size(%query.19, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.38 : Long() = prim::NumToTensor(%4411), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4413 : int = aten::size(%query.19, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.39 : Long() = prim::NumToTensor(%4413), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4415 : int = aten::size(%query.19, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.28 : Long() = prim::NumToTensor(%4415), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4417 : int = aten::size(%query.19, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %4418 : Long() = aten::floor_divide(%seq_len.39, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.28 : Long() = aten::sub(%4418, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
  %4420 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.19, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4421 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4422 : int = aten::Int(%4421), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4423 : int[] = prim::ListConstruct(%4422, %4413, %4417), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.101 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4420, %4423), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4425 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.10, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4426 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4427 : int = aten::Int(%4426), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4428 : int[] = prim::ListConstruct(%4427, %4413, %4417), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.103 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4425, %4428), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4430 : int = aten::size(%hidden_states.101, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4431 : int = aten::size(%hidden_states.101, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4432 : Long() = prim::NumToTensor(%4431), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4433 : Long() = aten::floor_divide(%4432, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4434 : int = aten::Int(%4433), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4435 : int = aten::size(%hidden_states.101, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4436 : int[] = prim::ListConstruct(%4430, %4434, %68, %4435), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.102 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.101, %4436), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4438 : int = aten::size(%hidden_states.102, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4439 : int = aten::size(%hidden_states.102, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4440 : Long() = prim::NumToTensor(%4439), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4441 : int = aten::size(%hidden_states.102, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4442 : int = aten::size(%hidden_states.102, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4443 : Long() = aten::mul(%4440, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4444 : Long() = aten::sub(%4443, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4445 : int = aten::Int(%4444), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4446 : int[] = prim::ListConstruct(%4438, %4445, %4441, %4442), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4447 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4448 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.102, %4446, %4447, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4449 : int = aten::size(%hidden_states.103, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4450 : int = aten::size(%hidden_states.103, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4451 : Long() = prim::NumToTensor(%4450), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4452 : Long() = aten::floor_divide(%4451, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4453 : int = aten::Int(%4452), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4454 : int = aten::size(%hidden_states.103, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4455 : int[] = prim::ListConstruct(%4449, %4453, %68, %4454), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.104 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.103, %4455), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4457 : int = aten::size(%hidden_states.104, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4458 : int = aten::size(%hidden_states.104, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4459 : Long() = prim::NumToTensor(%4458), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4460 : int = aten::size(%hidden_states.104, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4461 : int = aten::size(%hidden_states.104, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4462 : Long() = aten::mul(%4459, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4463 : Long() = aten::sub(%4462, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4464 : int = aten::Int(%4463), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4465 : int[] = prim::ListConstruct(%4457, %4464, %4460, %4461), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4466 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4467 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.104, %4465, %4466, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4468 : Tensor[] = prim::ListConstruct(%4448, %4467), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.116 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %4468), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4470 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states_padded.19 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.116, %4470, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4472 : int = aten::size(%hidden_states_padded.19, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4473 : int = aten::size(%hidden_states_padded.19, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4474 : int = aten::size(%hidden_states_padded.19, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4475 : int = aten::size(%hidden_states_padded.19, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4476 : int[] = prim::ListConstruct(%4472, %4473, %4474, %4475), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_chunked_attention_scores.19 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.19, %4476), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:400:0
  %4478 : Long() = aten::mul(%batch_size.38, %num_heads.28), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4479 : int = aten::Int(%4478), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4480 : Long() = aten::add(%chunks_count.28, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4481 : int = aten::Int(%4480), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4482 : int[] = prim::ListConstruct(%4479, %4481, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_attention_scores.19 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.19, %4482, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
  %4484 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4485 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4484, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4486 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4485, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4487 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%4486, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4488 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4489 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4488, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4490 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4489, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4491 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%4490, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4492 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4493 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%4487, %4492), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4494 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4491, %4493, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4495 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4496 : Float(204:262656, 512:513, 513:1) = aten::select(%4495, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4497 : Float(204:262656, 256:513, 513:1) = aten::slice(%4496, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4498 : Float(204:262656, 256:513, 257:1) = aten::slice(%4497, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4499 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4500 : Float(204:262656, 256:513, 513:1) = aten::select(%4499, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4501 : Float(204:262656, 256:513, 513:1) = aten::slice(%4500, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4502 : Float(204:262656, 256:513, 257:1) = aten::slice(%4501, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4503 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4504 : Float(204:262656, 256:513, 257:1) = aten::view(%4498, %4503), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4505 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4502, %4504, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4506 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4507 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4506, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4508 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4507, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4509 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4508, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4510 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4511 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4510, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4512 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4511, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4513 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4512, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4514 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4515 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4509, %4514), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4516 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4513, %4515, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4517 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4518 : Float(204:262656, 512:513, 513:1) = aten::select(%4517, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4519 : Float(204:262656, 255:513, 513:1) = aten::slice(%4518, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4520 : Float(204:262656, 255:513, 255:1) = aten::slice(%4519, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4521 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.19, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4522 : Float(204:262656, 256:513, 513:1) = aten::select(%4521, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4523 : Float(204:262656, 255:513, 513:1) = aten::slice(%4522, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4524 : Float(204:262656, 255:513, 255:1) = aten::slice(%4523, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4525 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4526 : Float(204:262656, 255:513, 255:1) = aten::view(%4520, %4525), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4527 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4524, %4526, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4528 : int[] = prim::ListConstruct(%4411, %4415, %4413, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4529 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.19, %4528), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.28 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4529, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %4531 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4532 : Float(256:257, 257:1) = aten::ones(%4531, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4533 : Float(256:257, 257:1) = aten::tril(%4532, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4534 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %beginning_mask_2d.19 : Float(256:257, 257:1) = aten::flip(%4533, %4534), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4536 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.19, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4537 : Float(1:65792, 256:257, 257:1) = aten::slice(%4536, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4538 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4537, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.19 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4538, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4540 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %ending_mask.19 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.19, %4540), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
  %4542 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.28, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4543 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4542, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4544 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4543, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.19 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4544, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4546 : int = aten::size(%beginning_input.19, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4547 : int = aten::size(%beginning_input.19, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4548 : int = aten::size(%beginning_input.19, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4549 : int = aten::size(%beginning_input.19, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4550 : int[] = prim::ListConstruct(%4546, %4547, %4548, %4549), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4551 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.19, %4550, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4552 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4551, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4553 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.19, %4552, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
  %4554 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.28, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4555 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4554, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4556 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%4555, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.19 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%4556, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4558 : int = aten::size(%ending_input.19, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4559 : int = aten::size(%ending_input.19, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4560 : int = aten::size(%ending_input.19, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4561 : int = aten::size(%ending_input.19, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4562 : int[] = prim::ListConstruct(%4558, %4559, %4560, %4561), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4563 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.19, %4562, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4564 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%4563, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4565 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.19, %4564, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
  %4566 : Bool(17:512, 512:1) = aten::ne(%4383, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4567 : Bool(17:512, 512:1) = aten::slice(%4566, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4568 : Bool(17:512, 512:1) = aten::slice(%4567, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4569 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4568, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.10 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4569, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:272:0
  %4571 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.10, %query.19), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.10 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%4571, %remove_from_windowed_attention_mask.10, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:275:0
  %4573 : int = aten::size(%float_mask.10, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4574 : int = aten::size(%float_mask.10, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4575 : int = aten::size(%float_mask.10, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4576 : int = aten::size(%float_mask.10, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4577 : int[] = prim::ListConstruct(%4573, %4574, %4575, %4576), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %query.20 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%4577, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:280:0
  %4579 : int = aten::size(%query.20, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.39 : Long() = prim::NumToTensor(%4579), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4581 : int = aten::size(%query.20, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.40 : Long() = prim::NumToTensor(%4581), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4583 : int = aten::size(%query.20, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.29 : Long() = prim::NumToTensor(%4583), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4585 : int = aten::size(%query.20, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:472:0
  %4586 : Long() = aten::floor_divide(%seq_len.40, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.29 : Long() = aten::sub(%4586, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:478:0
  %4588 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.20, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4589 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4590 : int = aten::Int(%4589), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4591 : int[] = prim::ListConstruct(%4590, %4581, %4585), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.105 : Float(17:512, 512:1, 1:1) = aten::reshape(%4588, %4591), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:481:0
  %4593 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.10, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4594 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4595 : int = aten::Int(%4594), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4596 : int[] = prim::ListConstruct(%4595, %4581, %4585), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.107 : Float(17:512, 512:1, 1:1) = aten::reshape(%4593, %4596), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:482:0
  %4598 : int = aten::size(%hidden_states.105, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4599 : int = aten::size(%hidden_states.105, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4600 : Long() = prim::NumToTensor(%4599), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4601 : Long() = aten::floor_divide(%4600, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4602 : int = aten::Int(%4601), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4603 : int = aten::size(%hidden_states.105, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4604 : int[] = prim::ListConstruct(%4598, %4602, %68, %4603), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.106 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.105, %4604), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4606 : int = aten::size(%hidden_states.106, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4607 : int = aten::size(%hidden_states.106, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4608 : Long() = prim::NumToTensor(%4607), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4609 : int = aten::size(%hidden_states.106, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4610 : int = aten::size(%hidden_states.106, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4611 : Long() = aten::mul(%4608, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4612 : Long() = aten::sub(%4611, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4613 : int = aten::Int(%4612), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4614 : int[] = prim::ListConstruct(%4606, %4613, %4609, %4610), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4615 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4616 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.106, %4614, %4615, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4617 : int = aten::size(%hidden_states.107, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:442:0
  %4618 : int = aten::size(%hidden_states.107, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:443:0
  %4619 : Long() = prim::NumToTensor(%4618), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4620 : Long() = aten::floor_divide(%4619, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4621 : int = aten::Int(%4620), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4622 : int = aten::size(%hidden_states.107, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:445:0
  %4623 : int[] = prim::ListConstruct(%4617, %4621, %68, %4622), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states.108 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.107, %4623), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:441:0
  %4625 : int = aten::size(%hidden_states.108, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4626 : int = aten::size(%hidden_states.108, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4627 : Long() = prim::NumToTensor(%4626), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4628 : int = aten::size(%hidden_states.108, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4629 : int = aten::size(%hidden_states.108, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:449:0
  %4630 : Long() = aten::mul(%4627, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4631 : Long() = aten::sub(%4630, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:450:0
  %4632 : int = aten::Int(%4631), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4633 : int[] = prim::ListConstruct(%4625, %4632, %4628, %4629), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4634 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4635 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.108, %4633, %4634, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:454:0
  %4636 : Tensor[] = prim::ListConstruct(%4616, %4635), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.117 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %4636), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4638 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %hidden_states_padded.20 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.117, %4638, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4640 : int = aten::size(%hidden_states_padded.20, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4641 : int = aten::size(%hidden_states_padded.20, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4642 : int = aten::size(%hidden_states_padded.20, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4643 : int = aten::size(%hidden_states_padded.20, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:401:0
  %4644 : int[] = prim::ListConstruct(%4640, %4641, %4642, %4643), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_chunked_attention_scores.20 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.20, %4644), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:400:0
  %4646 : Long() = aten::mul(%batch_size.39, %num_heads.29), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4647 : int = aten::Int(%4646), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4648 : Long() = aten::add(%chunks_count.29, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:504:0
  %4649 : int = aten::Int(%4648), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4650 : int[] = prim::ListConstruct(%4647, %4649, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %diagonal_attention_scores.20 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.20, %4650, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:503:0
  %4652 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4653 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4652, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4654 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4653, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4655 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%4654, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4656 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4657 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4656, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4658 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4657, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4659 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%4658, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4660 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4661 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%4655, %4660), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4662 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4659, %4661, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:509:0
  %4663 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4664 : Float(17:262656, 512:513, 513:1) = aten::select(%4663, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4665 : Float(17:262656, 256:513, 513:1) = aten::slice(%4664, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4666 : Float(17:262656, 256:513, 257:1) = aten::slice(%4665, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4667 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4668 : Float(17:262656, 256:513, 513:1) = aten::select(%4667, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4669 : Float(17:262656, 256:513, 513:1) = aten::slice(%4668, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4670 : Float(17:262656, 256:513, 257:1) = aten::slice(%4669, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4671 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4672 : Float(17:262656, 256:513, 257:1) = aten::view(%4666, %4671), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4673 : Float(17:262656, 256:513, 257:1) = aten::copy_(%4670, %4672, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:512:0
  %4674 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4675 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%4674, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4676 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%4675, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4677 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%4676, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4678 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4679 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4678, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4680 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%4679, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4681 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%4680, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4682 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4683 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%4677, %4682), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4684 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4681, %4683, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:516:0
  %4685 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4686 : Float(17:262656, 512:513, 513:1) = aten::select(%4685, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4687 : Float(17:262656, 255:513, 513:1) = aten::slice(%4686, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4688 : Float(17:262656, 255:513, 255:1) = aten::slice(%4687, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4689 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.20, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4690 : Float(17:262656, 256:513, 513:1) = aten::select(%4689, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4691 : Float(17:262656, 255:513, 513:1) = aten::slice(%4690, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4692 : Float(17:262656, 255:513, 255:1) = aten::slice(%4691, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4693 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4694 : Float(17:262656, 255:513, 255:1) = aten::view(%4688, %4693), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4695 : Float(17:262656, 255:513, 255:1) = aten::copy_(%4692, %4694, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:520:0
  %4696 : int[] = prim::ListConstruct(%4579, %4583, %4581, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4697 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.20, %4696), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.29 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%4697, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:525:0
  %4699 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4700 : Float(256:257, 257:1) = aten::ones(%4699, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4701 : Float(256:257, 257:1) = aten::tril(%4700, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4702 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %beginning_mask_2d.20 : Float(256:257, 257:1) = aten::flip(%4701, %4702), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:458:0
  %4704 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.20, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4705 : Float(1:65792, 256:257, 257:1) = aten::slice(%4704, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4706 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%4705, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.20 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%4706, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:459:0
  %4708 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %ending_mask.20 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.20, %4708), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:460:0
  %4710 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.29, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4711 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4710, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4712 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4711, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.20 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4712, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:461:0
  %4714 : int = aten::size(%beginning_input.20, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4715 : int = aten::size(%beginning_input.20, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4716 : int = aten::size(%beginning_input.20, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4717 : int = aten::size(%beginning_input.20, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4718 : int[] = prim::ListConstruct(%4714, %4715, %4716, %4717), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4719 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.20, %4718, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:462:0
  %4720 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4719, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4721 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.20, %4720, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:463:0
  %4722 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.29, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4723 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4722, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4724 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%4723, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.20 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%4724, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:464:0
  %4726 : int = aten::size(%ending_input.20, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4727 : int = aten::size(%ending_input.20, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4728 : int = aten::size(%ending_input.20, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4729 : int = aten::size(%ending_input.20, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4730 : int[] = prim::ListConstruct(%4726, %4727, %4728, %4729), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4731 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.20, %4730, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:465:0
  %4732 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%4731, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:22:0
  %4733 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.20, %4732, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.10 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.28, %input_tensor.29, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.10 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.10, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:1500:0
  %4736 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.10, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4737 : Bool(17:512, 512:1) = aten::slice(%4736, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4738 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%4737, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %4739 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%4738, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %input.118 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.10, %4739, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.20 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.118, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:973:0
  %4742 : int[] = prim::ListConstruct(%4401, %4402, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4743 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.10, %4742), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
  %value.10 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4743, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:331:0
  %4745 : int = aten::size(%value.10, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.40 : Long() = prim::NumToTensor(%4745), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4747 : int = aten::size(%value.10, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.41 : Long() = prim::NumToTensor(%4747), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4749 : int = aten::size(%value.10, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.30 : Long() = prim::NumToTensor(%4749), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4751 : int = aten::size(%value.10, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:537:0
  %4752 : Long() = aten::floor_divide(%seq_len.41, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %chunks_count.30 : Long() = aten::sub(%4752, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:542:0
  %4754 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.20, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
  %4755 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:546:0
  %4756 : int = aten::Int(%4755), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4757 : Long() = aten::floor_divide(%seq_len.41, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/tensor.py:424:0
  %4758 : int = aten::Int(%4757), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4759 : int[] = prim::ListConstruct(%4756, %4758, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.46 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%4754, %4759), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:545:0
  %4761 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.10, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4762 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4763 : int = aten::Int(%4762), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4764 : int[] = prim::ListConstruct(%4763, %4747, %4751), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %input.119 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4761, %4764), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:550:0
  %4766 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %padded_value.10 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.119, %4766, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4768 : Long() = aten::mul(%batch_size.40, %num_heads.30), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
  %4769 : int = aten::Int(%4768), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4770 : Long() = aten::add(%chunks_count.30, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:556:0
  %4771 : int = aten::Int(%4770), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4772 : int[] = prim::ListConstruct(%4769, %4771, %59, %4751), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4773 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4774 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.10, %4772, %4773, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:564:0
  %4775 : int = aten::size(%chunked_hidden_states.46, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %4776 : int = aten::size(%chunked_hidden_states.46, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %4777 : int = aten::size(%chunked_hidden_states.46, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.10 : Long() = prim::NumToTensor(%4777), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4779 : int = aten::size(%chunked_hidden_states.46, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.10 : Long() = prim::NumToTensor(%4779), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4781 : Long() = aten::add(%window_overlap.10, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:422:0
  %4782 : int = aten::Int(%4781), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4783 : int[] = prim::ListConstruct(%74, %4782), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.47 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.46, %4783, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/nn/functional.py:3552:0
  %4785 : int[] = prim::ListConstruct(%4775, %4776, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.48 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.47, %4785), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:424:0
  %4787 : Long() = aten::neg(%window_overlap.10), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:428:0
  %4788 : int = aten::Int(%4787), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4789 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.48, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %4790 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%4789, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.49 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%4790, %66, %74, %4788, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:427:0
  %4792 : Long() = aten::add(%window_overlap.10, %hidden_dim.10, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:431:0
  %4793 : int = aten::Int(%4792), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4794 : int[] = prim::ListConstruct(%4775, %4776, %4777, %4793), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %chunked_hidden_states.50 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.49, %4794), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:430:0
  %4796 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.50, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4797 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4796, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4798 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%4797, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4799 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%4798, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:433:0
  %4800 : Tensor[] = prim::ListConstruct(%4799, %4774), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %context.10 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %4800), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # torch/functional.py:327:0
  %4802 : int[] = prim::ListConstruct(%4745, %4749, %4747, %4751), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4803 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.10, %4802), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.19 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%4803, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:569:0
  %4805 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.19, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %4806 : int[] = prim::ListConstruct(%4401, %4402, %4403), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self
  %4807 : Float(512:13056, 17:768, 768:1) = aten::reshape(%4805, %4806), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.20 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%4807, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:350:0
  %input.120 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.20, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.self # transformers/modeling_longformer.py:374:0
  %4810 : __torch__.torch.nn.modules.normalization.___torch_mangle_7268.LayerNorm = prim::GetAttr[name="LayerNorm"](%4377)
  %4811 : __torch__.torch.nn.modules.linear.___torch_mangle_7267.Linear = prim::GetAttr[name="dense"](%4377)
  %4812 : Tensor = prim::GetAttr[name="bias"](%4811)
  %4813 : Tensor = prim::GetAttr[name="weight"](%4811)
  %4814 : Float(768:1, 768:768) = aten::t(%4813), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.120, %4814), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.58, %4812, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.109 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.121, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.122 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.109, %hidden_states.100, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output # transformers/modeling_longformer.py:758:0
  %4819 : Tensor = prim::GetAttr[name="bias"](%4810)
  %4820 : Tensor = prim::GetAttr[name="weight"](%4810)
  %4821 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.30 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.122, %4821, %4820, %4819, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.attention/__module.longformer.encoder.layer.9.attention.output/__module.longformer.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %4823 : __torch__.torch.nn.modules.linear.___torch_mangle_7272.Linear = prim::GetAttr[name="dense"](%4375)
  %4824 : Tensor = prim::GetAttr[name="bias"](%4823)
  %4825 : Tensor = prim::GetAttr[name="weight"](%4823)
  %4826 : Float(768:1, 3072:768) = aten::t(%4825), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.30, %4826), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.59, %4824, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate/__module.longformer.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.124 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.123), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %4830 : __torch__.torch.nn.modules.normalization.___torch_mangle_7275.LayerNorm = prim::GetAttr[name="LayerNorm"](%4374)
  %4831 : __torch__.torch.nn.modules.linear.___torch_mangle_7274.Linear = prim::GetAttr[name="dense"](%4374)
  %4832 : Tensor = prim::GetAttr[name="bias"](%4831)
  %4833 : Tensor = prim::GetAttr[name="weight"](%4831)
  %4834 : Float(3072:1, 768:3072) = aten::t(%4833), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.124, %4834), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.125 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.60, %4832, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.110 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.125, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.126 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.110, %input_tensor.30, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output # transformers/modeling_longformer.py:830:0
  %4839 : Tensor = prim::GetAttr[name="bias"](%4830)
  %4840 : Tensor = prim::GetAttr[name="weight"](%4830)
  %4841 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.LayerNorm
  %hidden_states.111 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.126, %4841, %4840, %4839, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.9/__module.longformer.encoder.layer.9.output/__module.longformer.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %4843 : __torch__.transformers.modeling_longformer.___torch_mangle_7296.LongformerOutput = prim::GetAttr[name="output"](%132)
  %4844 : __torch__.transformers.modeling_longformer.___torch_mangle_7292.LongformerIntermediate = prim::GetAttr[name="intermediate"](%132)
  %4845 : __torch__.transformers.modeling_longformer.___torch_mangle_7290.LongformerAttention = prim::GetAttr[name="attention"](%132)
  %4846 : __torch__.transformers.modeling_longformer.___torch_mangle_7289.LongformerSelfOutput = prim::GetAttr[name="output"](%4845)
  %4847 : __torch__.transformers.modeling_longformer.___torch_mangle_7285.LongformerSelfAttention = prim::GetAttr[name="self"](%4845)
  %4848 : __torch__.torch.nn.modules.linear.___torch_mangle_7281.Linear = prim::GetAttr[name="value"](%4847)
  %4849 : __torch__.torch.nn.modules.linear.___torch_mangle_7280.Linear = prim::GetAttr[name="key"](%4847)
  %4850 : __torch__.torch.nn.modules.linear.___torch_mangle_7279.Linear = prim::GetAttr[name="query"](%4847)
  %4851 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
  %4852 : Float(17:512, 512:1) = aten::squeeze(%4851, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked.11 : Bool(17:512, 512:1) = aten::lt(%4852, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %input.127 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.111, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:248:0
  %4855 : Tensor = prim::GetAttr[name="bias"](%4850)
  %4856 : Tensor = prim::GetAttr[name="weight"](%4850)
  %4857 : Float(768:1, 768:768) = aten::t(%4856), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4857), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.21 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.61, %4855, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %4860 : Tensor = prim::GetAttr[name="bias"](%4849)
  %4861 : Tensor = prim::GetAttr[name="weight"](%4849)
  %4862 : Float(768:1, 768:768) = aten::t(%4861), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4862), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.62, %4860, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %4865 : Tensor = prim::GetAttr[name="bias"](%4848)
  %4866 : Tensor = prim::GetAttr[name="weight"](%4848)
  %4867 : Float(768:1, 768:768) = aten::t(%4866), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.127, %4867), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors.11 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.63, %4865, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self/__module.longformer.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %4870 : int = aten::size(%input.127, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %4871 : int = aten::size(%input.127, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %4872 : int = aten::size(%input.127, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors.22 : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.21, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:261:0
  %4874 : int[] = prim::ListConstruct(%4870, %4871, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4875 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors.22, %4874), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
  %query.21 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4875, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:263:0
  %4877 : int[] = prim::ListConstruct(%4870, %4871, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4878 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors.11, %4877), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
  %key.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%4878, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:264:0
  %4880 : int = aten::size(%query.21, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.42 : Long() = prim::NumToTensor(%4880), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4882 : int = aten::size(%query.21, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.43 : Long() = prim::NumToTensor(%4882), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4884 : int = aten::size(%query.21, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.31 : Long() = prim::NumToTensor(%4884), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4886 : int = aten::size(%query.21, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %4887 : Long() = aten::floor_divide(%seq_len.43, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.31 : Long() = aten::sub(%4887, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
  %4889 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.21, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4890 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4891 : int = aten::Int(%4890), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4892 : int[] = prim::ListConstruct(%4891, %4882, %4886), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.112 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4889, %4892), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %4894 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key.11, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4895 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4896 : int = aten::Int(%4895), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4897 : int[] = prim::ListConstruct(%4896, %4882, %4886), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.114 : Float(204:64, 512:13056, 64:1) = aten::reshape(%4894, %4897), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %4899 : int = aten::size(%hidden_states.112, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %4900 : int = aten::size(%hidden_states.112, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %4901 : Long() = prim::NumToTensor(%4900), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4902 : Long() = aten::floor_divide(%4901, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %4903 : int = aten::Int(%4902), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4904 : int = aten::size(%hidden_states.112, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %4905 : int[] = prim::ListConstruct(%4899, %4903, %68, %4904), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.113 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.112, %4905), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %4907 : int = aten::size(%hidden_states.113, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4908 : int = aten::size(%hidden_states.113, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4909 : Long() = prim::NumToTensor(%4908), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4910 : int = aten::size(%hidden_states.113, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4911 : int = aten::size(%hidden_states.113, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4912 : Long() = aten::mul(%4909, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4913 : Long() = aten::sub(%4912, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4914 : int = aten::Int(%4913), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4915 : int[] = prim::ListConstruct(%4907, %4914, %4910, %4911), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4916 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4917 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.113, %4915, %4916, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %4918 : int = aten::size(%hidden_states.114, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %4919 : int = aten::size(%hidden_states.114, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %4920 : Long() = prim::NumToTensor(%4919), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4921 : Long() = aten::floor_divide(%4920, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %4922 : int = aten::Int(%4921), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4923 : int = aten::size(%hidden_states.114, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %4924 : int[] = prim::ListConstruct(%4918, %4922, %68, %4923), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.115 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.114, %4924), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %4926 : int = aten::size(%hidden_states.115, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4927 : int = aten::size(%hidden_states.115, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4928 : Long() = prim::NumToTensor(%4927), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4929 : int = aten::size(%hidden_states.115, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4930 : int = aten::size(%hidden_states.115, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %4931 : Long() = aten::mul(%4928, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4932 : Long() = aten::sub(%4931, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %4933 : int = aten::Int(%4932), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4934 : int[] = prim::ListConstruct(%4926, %4933, %4929, %4930), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4935 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4936 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.115, %4934, %4935, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %4937 : Tensor[] = prim::ListConstruct(%4917, %4936), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.128 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %4937), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %4939 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states_padded.21 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.128, %4939, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %4941 : int = aten::size(%hidden_states_padded.21, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4942 : int = aten::size(%hidden_states_padded.21, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4943 : int = aten::size(%hidden_states_padded.21, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4944 : int = aten::size(%hidden_states_padded.21, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %4945 : int[] = prim::ListConstruct(%4941, %4942, %4943, %4944), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_chunked_attention_scores.21 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.21, %4945), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:400:0
  %4947 : Long() = aten::mul(%batch_size.42, %num_heads.31), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %4948 : int = aten::Int(%4947), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4949 : Long() = aten::add(%chunks_count.31, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %4950 : int = aten::Int(%4949), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4951 : int[] = prim::ListConstruct(%4948, %4950, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_attention_scores.21 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.21, %4951, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
  %4953 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4954 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4953, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4955 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4954, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4956 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%4955, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4957 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4958 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4957, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4959 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4958, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4960 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%4959, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4961 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4962 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%4956, %4961), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4963 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%4960, %4962, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %4964 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4965 : Float(204:262656, 512:513, 513:1) = aten::select(%4964, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4966 : Float(204:262656, 256:513, 513:1) = aten::slice(%4965, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4967 : Float(204:262656, 256:513, 257:1) = aten::slice(%4966, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4968 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4969 : Float(204:262656, 256:513, 513:1) = aten::select(%4968, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4970 : Float(204:262656, 256:513, 513:1) = aten::slice(%4969, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4971 : Float(204:262656, 256:513, 257:1) = aten::slice(%4970, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4972 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4973 : Float(204:262656, 256:513, 257:1) = aten::view(%4967, %4972), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4974 : Float(204:262656, 256:513, 257:1) = aten::copy_(%4971, %4973, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %4975 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4976 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%4975, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4977 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%4976, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4978 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%4977, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4979 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4980 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4979, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4981 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%4980, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4982 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%4981, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4983 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4984 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%4978, %4983), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4985 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%4982, %4984, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %4986 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.21, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4987 : Float(204:262656, 512:513, 513:1) = aten::select(%4986, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4988 : Float(204:262656, 255:513, 513:1) = aten::slice(%4987, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4989 : Float(204:262656, 255:513, 255:1) = aten::slice(%4988, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4990 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.21, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4991 : Float(204:262656, 256:513, 513:1) = aten::select(%4990, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4992 : Float(204:262656, 255:513, 513:1) = aten::slice(%4991, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4993 : Float(204:262656, 255:513, 255:1) = aten::slice(%4992, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4994 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4995 : Float(204:262656, 255:513, 255:1) = aten::view(%4989, %4994), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4996 : Float(204:262656, 255:513, 255:1) = aten::copy_(%4993, %4995, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %4997 : int[] = prim::ListConstruct(%4880, %4884, %4882, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %4998 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.21, %4997), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.31 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%4998, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %5000 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5001 : Float(256:257, 257:1) = aten::ones(%5000, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5002 : Float(256:257, 257:1) = aten::tril(%5001, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5003 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %beginning_mask_2d.21 : Float(256:257, 257:1) = aten::flip(%5002, %5003), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5005 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.21, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5006 : Float(1:65792, 256:257, 257:1) = aten::slice(%5005, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5007 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5006, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.21 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5007, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5009 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %ending_mask.21 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.21, %5009), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
  %5011 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.31, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5012 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5011, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5013 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5012, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.21 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5013, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5015 : int = aten::size(%beginning_input.21, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5016 : int = aten::size(%beginning_input.21, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5017 : int = aten::size(%beginning_input.21, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5018 : int = aten::size(%beginning_input.21, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5019 : int[] = prim::ListConstruct(%5015, %5016, %5017, %5018), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5020 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.21, %5019, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5021 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5020, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5022 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.21, %5021, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
  %5023 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.31, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5024 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5023, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5025 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5024, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.21 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5025, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5027 : int = aten::size(%ending_input.21, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5028 : int = aten::size(%ending_input.21, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5029 : int = aten::size(%ending_input.21, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5030 : int = aten::size(%ending_input.21, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5031 : int[] = prim::ListConstruct(%5027, %5028, %5029, %5030), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5032 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.21, %5031, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5033 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5032, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5034 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.21, %5033, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
  %5035 : Bool(17:512, 512:1) = aten::ne(%4852, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5036 : Bool(17:512, 512:1) = aten::slice(%5035, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5037 : Bool(17:512, 512:1) = aten::slice(%5036, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5038 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5037, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask.11 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5038, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:272:0
  %5040 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask.11, %query.21), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask.11 : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%5040, %remove_from_windowed_attention_mask.11, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:275:0
  %5042 : int = aten::size(%float_mask.11, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5043 : int = aten::size(%float_mask.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5044 : int = aten::size(%float_mask.11, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5045 : int = aten::size(%float_mask.11, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5046 : int[] = prim::ListConstruct(%5042, %5043, %5044, %5045), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %query.22 : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%5046, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:280:0
  %5048 : int = aten::size(%query.22, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.43 : Long() = prim::NumToTensor(%5048), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5050 : int = aten::size(%query.22, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.44 : Long() = prim::NumToTensor(%5050), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5052 : int = aten::size(%query.22, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.32 : Long() = prim::NumToTensor(%5052), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5054 : int = aten::size(%query.22, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:472:0
  %5055 : Long() = aten::floor_divide(%seq_len.44, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.32 : Long() = aten::sub(%5055, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:478:0
  %5057 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query.22, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5058 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5059 : int = aten::Int(%5058), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5060 : int[] = prim::ListConstruct(%5059, %5050, %5054), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.116 : Float(17:512, 512:1, 1:1) = aten::reshape(%5057, %5060), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:481:0
  %5062 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask.11, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5063 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5064 : int = aten::Int(%5063), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5065 : int[] = prim::ListConstruct(%5064, %5050, %5054), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.118 : Float(17:512, 512:1, 1:1) = aten::reshape(%5062, %5065), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:482:0
  %5067 : int = aten::size(%hidden_states.116, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %5068 : int = aten::size(%hidden_states.116, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %5069 : Long() = prim::NumToTensor(%5068), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5070 : Long() = aten::floor_divide(%5069, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5071 : int = aten::Int(%5070), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5072 : int = aten::size(%hidden_states.116, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %5073 : int[] = prim::ListConstruct(%5067, %5071, %68, %5072), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.117 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.116, %5073), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %5075 : int = aten::size(%hidden_states.117, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5076 : int = aten::size(%hidden_states.117, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5077 : Long() = prim::NumToTensor(%5076), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5078 : int = aten::size(%hidden_states.117, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5079 : int = aten::size(%hidden_states.117, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5080 : Long() = aten::mul(%5077, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5081 : Long() = aten::sub(%5080, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5082 : int = aten::Int(%5081), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5083 : int[] = prim::ListConstruct(%5075, %5082, %5078, %5079), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5084 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5085 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.117, %5083, %5084, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %5086 : int = aten::size(%hidden_states.118, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:442:0
  %5087 : int = aten::size(%hidden_states.118, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:443:0
  %5088 : Long() = prim::NumToTensor(%5087), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5089 : Long() = aten::floor_divide(%5088, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5090 : int = aten::Int(%5089), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5091 : int = aten::size(%hidden_states.118, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:445:0
  %5092 : int[] = prim::ListConstruct(%5086, %5090, %68, %5091), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states.119 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.118, %5092), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:441:0
  %5094 : int = aten::size(%hidden_states.119, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5095 : int = aten::size(%hidden_states.119, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5096 : Long() = prim::NumToTensor(%5095), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5097 : int = aten::size(%hidden_states.119, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5098 : int = aten::size(%hidden_states.119, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:449:0
  %5099 : Long() = aten::mul(%5096, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5100 : Long() = aten::sub(%5099, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:450:0
  %5101 : int = aten::Int(%5100), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5102 : int[] = prim::ListConstruct(%5094, %5101, %5097, %5098), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5103 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5104 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.119, %5102, %5103, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:454:0
  %5105 : Tensor[] = prim::ListConstruct(%5085, %5104), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.129 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %5105), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %5107 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %hidden_states_padded.22 : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.129, %5107, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5109 : int = aten::size(%hidden_states_padded.22, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5110 : int = aten::size(%hidden_states_padded.22, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5111 : int = aten::size(%hidden_states_padded.22, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5112 : int = aten::size(%hidden_states_padded.22, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:401:0
  %5113 : int[] = prim::ListConstruct(%5109, %5110, %5111, %5112), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_chunked_attention_scores.22 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.22, %5113), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:400:0
  %5115 : Long() = aten::mul(%batch_size.43, %num_heads.32), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %5116 : int = aten::Int(%5115), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5117 : Long() = aten::add(%chunks_count.32, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:504:0
  %5118 : int = aten::Int(%5117), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5119 : int[] = prim::ListConstruct(%5116, %5118, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %diagonal_attention_scores.22 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.22, %5119, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:503:0
  %5121 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5122 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5121, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5123 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5122, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5124 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%5123, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5125 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5126 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5125, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5127 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5126, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5128 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%5127, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5129 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5130 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%5124, %5129), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5131 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5128, %5130, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:509:0
  %5132 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5133 : Float(17:262656, 512:513, 513:1) = aten::select(%5132, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5134 : Float(17:262656, 256:513, 513:1) = aten::slice(%5133, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5135 : Float(17:262656, 256:513, 257:1) = aten::slice(%5134, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5136 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5137 : Float(17:262656, 256:513, 513:1) = aten::select(%5136, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5138 : Float(17:262656, 256:513, 513:1) = aten::slice(%5137, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5139 : Float(17:262656, 256:513, 257:1) = aten::slice(%5138, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5140 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5141 : Float(17:262656, 256:513, 257:1) = aten::view(%5135, %5140), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5142 : Float(17:262656, 256:513, 257:1) = aten::copy_(%5139, %5141, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:512:0
  %5143 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5144 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5143, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5145 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5144, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5146 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%5145, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5147 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5148 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5147, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5149 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5148, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5150 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%5149, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5151 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5152 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%5146, %5151), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5153 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5150, %5152, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:516:0
  %5154 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5155 : Float(17:262656, 512:513, 513:1) = aten::select(%5154, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5156 : Float(17:262656, 255:513, 513:1) = aten::slice(%5155, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5157 : Float(17:262656, 255:513, 255:1) = aten::slice(%5156, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5158 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.22, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5159 : Float(17:262656, 256:513, 513:1) = aten::select(%5158, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5160 : Float(17:262656, 255:513, 513:1) = aten::slice(%5159, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5161 : Float(17:262656, 255:513, 255:1) = aten::slice(%5160, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5162 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5163 : Float(17:262656, 255:513, 255:1) = aten::view(%5157, %5162), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5164 : Float(17:262656, 255:513, 255:1) = aten::copy_(%5161, %5163, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:520:0
  %5165 : int[] = prim::ListConstruct(%5048, %5052, %5050, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5166 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.22, %5165), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.32 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%5166, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:525:0
  %5168 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5169 : Float(256:257, 257:1) = aten::ones(%5168, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5170 : Float(256:257, 257:1) = aten::tril(%5169, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5171 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %beginning_mask_2d.22 : Float(256:257, 257:1) = aten::flip(%5170, %5171), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:458:0
  %5173 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.22, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5174 : Float(1:65792, 256:257, 257:1) = aten::slice(%5173, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5175 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5174, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.22 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5175, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:459:0
  %5177 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %ending_mask.22 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.22, %5177), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:460:0
  %5179 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.32, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5180 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5179, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5181 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5180, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.22 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5181, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:461:0
  %5183 : int = aten::size(%beginning_input.22, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5184 : int = aten::size(%beginning_input.22, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5185 : int = aten::size(%beginning_input.22, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5186 : int = aten::size(%beginning_input.22, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5187 : int[] = prim::ListConstruct(%5183, %5184, %5185, %5186), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5188 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask.22, %5187, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:462:0
  %5189 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5188, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5190 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input.22, %5189, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:463:0
  %5191 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.32, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5192 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5191, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5193 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5192, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.22 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5193, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:464:0
  %5195 : int = aten::size(%ending_input.22, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5196 : int = aten::size(%ending_input.22, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5197 : int = aten::size(%ending_input.22, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5198 : int = aten::size(%ending_input.22, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5199 : int[] = prim::ListConstruct(%5195, %5196, %5197, %5198), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5200 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask.22, %5199, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:465:0
  %5201 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5200, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:22:0
  %5202 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input.22, %5201, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores.11 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.31, %input_tensor.32, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32.11 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores.11, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:1500:0
  %5205 : Bool(17:512, 512:1) = aten::slice(%is_index_masked.11, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5206 : Bool(17:512, 512:1) = aten::slice(%5205, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5207 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5206, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %5208 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5207, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %input.130 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32.11, %5208, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs.22 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.130, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:973:0
  %5211 : int[] = prim::ListConstruct(%4870, %4871, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5212 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors.11, %5211), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
  %value.11 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5212, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:331:0
  %5214 : int = aten::size(%value.11, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size.44 : Long() = prim::NumToTensor(%5214), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5216 : int = aten::size(%value.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len.45 : Long() = prim::NumToTensor(%5216), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5218 : int = aten::size(%value.11, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads.33 : Long() = prim::NumToTensor(%5218), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5220 : int = aten::size(%value.11, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:537:0
  %5221 : Long() = aten::floor_divide(%seq_len.45, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %chunks_count.33 : Long() = aten::sub(%5221, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:542:0
  %5223 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs.22, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
  %5224 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:546:0
  %5225 : int = aten::Int(%5224), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5226 : Long() = aten::floor_divide(%seq_len.45, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/tensor.py:424:0
  %5227 : int = aten::Int(%5226), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5228 : int[] = prim::ListConstruct(%5225, %5227, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.51 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%5223, %5228), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:545:0
  %5230 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value.11, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5231 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5232 : int = aten::Int(%5231), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5233 : int[] = prim::ListConstruct(%5232, %5216, %5220), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %input.131 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5230, %5233), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:550:0
  %5235 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %padded_value.11 : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.131, %5235, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5237 : Long() = aten::mul(%batch_size.44, %num_heads.33), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
  %5238 : int = aten::Int(%5237), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5239 : Long() = aten::add(%chunks_count.33, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:556:0
  %5240 : int = aten::Int(%5239), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5241 : int[] = prim::ListConstruct(%5238, %5240, %59, %5220), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5242 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5243 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value.11, %5241, %5242, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:564:0
  %5244 : int = aten::size(%chunked_hidden_states.51, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %5245 : int = aten::size(%chunked_hidden_states.51, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %5246 : int = aten::size(%chunked_hidden_states.51, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap.11 : Long() = prim::NumToTensor(%5246), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5248 : int = aten::size(%chunked_hidden_states.51, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim.11 : Long() = prim::NumToTensor(%5248), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5250 : Long() = aten::add(%window_overlap.11, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:422:0
  %5251 : int = aten::Int(%5250), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5252 : int[] = prim::ListConstruct(%74, %5251), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.52 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.51, %5252, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/nn/functional.py:3552:0
  %5254 : int[] = prim::ListConstruct(%5244, %5245, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.53 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.52, %5254), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:424:0
  %5256 : Long() = aten::neg(%window_overlap.11), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:428:0
  %5257 : int = aten::Int(%5256), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5258 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.53, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %5259 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%5258, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.54 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%5259, %66, %74, %5257, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:427:0
  %5261 : Long() = aten::add(%window_overlap.11, %hidden_dim.11, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:431:0
  %5262 : int = aten::Int(%5261), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5263 : int[] = prim::ListConstruct(%5244, %5245, %5246, %5262), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %chunked_hidden_states.55 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.54, %5263), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:430:0
  %5265 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states.55, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5266 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5265, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5267 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5266, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5268 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%5267, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:433:0
  %5269 : Tensor[] = prim::ListConstruct(%5268, %5243), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %context.11 : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %5269), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # torch/functional.py:327:0
  %5271 : int[] = prim::ListConstruct(%5214, %5218, %5216, %5220), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5272 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context.11, %5271), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.21 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%5272, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:569:0
  %5274 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.21, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %5275 : int[] = prim::ListConstruct(%4870, %4871, %4872), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self
  %5276 : Float(512:13056, 17:768, 768:1) = aten::reshape(%5274, %5275), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output.22 : Float(512:13056, 17:768, 768:1) = aten::contiguous(%5276, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:350:0
  %input.132 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output.22, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.self # transformers/modeling_longformer.py:374:0
  %5279 : __torch__.torch.nn.modules.normalization.___torch_mangle_7287.LayerNorm = prim::GetAttr[name="LayerNorm"](%4846)
  %5280 : __torch__.torch.nn.modules.linear.___torch_mangle_7286.Linear = prim::GetAttr[name="dense"](%4846)
  %5281 : Tensor = prim::GetAttr[name="bias"](%5280)
  %5282 : Tensor = prim::GetAttr[name="weight"](%5280)
  %5283 : Float(768:1, 768:768) = aten::t(%5282), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.132, %5283), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.133 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.64, %5281, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.120 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.133, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.134 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.120, %hidden_states.111, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output # transformers/modeling_longformer.py:758:0
  %5288 : Tensor = prim::GetAttr[name="bias"](%5279)
  %5289 : Tensor = prim::GetAttr[name="weight"](%5279)
  %5290 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.33 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.134, %5290, %5289, %5288, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.attention/__module.longformer.encoder.layer.10.attention.output/__module.longformer.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %5292 : __torch__.torch.nn.modules.linear.___torch_mangle_7291.Linear = prim::GetAttr[name="dense"](%4844)
  %5293 : Tensor = prim::GetAttr[name="bias"](%5292)
  %5294 : Tensor = prim::GetAttr[name="weight"](%5292)
  %5295 : Float(768:1, 3072:768) = aten::t(%5294), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor.33, %5295), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.135 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.65, %5293, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate/__module.longformer.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.136 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.135), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %5299 : __torch__.torch.nn.modules.normalization.___torch_mangle_7294.LayerNorm = prim::GetAttr[name="LayerNorm"](%4843)
  %5300 : __torch__.torch.nn.modules.linear.___torch_mangle_7293.Linear = prim::GetAttr[name="dense"](%4843)
  %5301 : Tensor = prim::GetAttr[name="bias"](%5300)
  %5302 : Tensor = prim::GetAttr[name="weight"](%5300)
  %5303 : Float(3072:1, 768:3072) = aten::t(%5302), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.136, %5303), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.137 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.66, %5301, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.121 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.137, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.138 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.121, %input_tensor.33, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output # transformers/modeling_longformer.py:830:0
  %5308 : Tensor = prim::GetAttr[name="bias"](%5299)
  %5309 : Tensor = prim::GetAttr[name="weight"](%5299)
  %5310 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.LayerNorm
  %hidden_states.122 : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.138, %5310, %5309, %5308, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.10/__module.longformer.encoder.layer.10.output/__module.longformer.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %5312 : __torch__.transformers.modeling_longformer.___torch_mangle_7315.LongformerOutput = prim::GetAttr[name="output"](%130)
  %5313 : __torch__.transformers.modeling_longformer.___torch_mangle_7311.LongformerIntermediate = prim::GetAttr[name="intermediate"](%130)
  %5314 : __torch__.transformers.modeling_longformer.___torch_mangle_7309.LongformerAttention = prim::GetAttr[name="attention"](%130)
  %5315 : __torch__.transformers.modeling_longformer.___torch_mangle_7308.LongformerSelfOutput = prim::GetAttr[name="output"](%5314)
  %5316 : __torch__.transformers.modeling_longformer.___torch_mangle_7304.LongformerSelfAttention = prim::GetAttr[name="self"](%5314)
  %5317 : __torch__.torch.nn.modules.linear.___torch_mangle_7300.Linear = prim::GetAttr[name="value"](%5316)
  %5318 : __torch__.torch.nn.modules.linear.___torch_mangle_7299.Linear = prim::GetAttr[name="key"](%5316)
  %5319 : __torch__.torch.nn.modules.linear.___torch_mangle_7298.Linear = prim::GetAttr[name="query"](%5316)
  %5320 : Float(17:512, 1:512, 512:1) = aten::squeeze(%attention_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
  %5321 : Float(17:512, 512:1) = aten::squeeze(%5320, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:241:0
  %is_index_masked : Bool(17:512, 512:1) = aten::lt(%5321, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %input.139 : Float(512:768, 17:393216, 768:1) = aten::transpose(%hidden_states.122, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:248:0
  %5324 : Tensor = prim::GetAttr[name="bias"](%5319)
  %5325 : Tensor = prim::GetAttr[name="weight"](%5319)
  %5326 : Float(768:1, 768:768) = aten::t(%5325), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5326), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %query_vectors.23 : Float(512:13056, 17:768, 768:1) = aten::add_(%output.67, %5324, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %5329 : Tensor = prim::GetAttr[name="bias"](%5318)
  %5330 : Tensor = prim::GetAttr[name="weight"](%5318)
  %5331 : Float(768:1, 768:768) = aten::t(%5330), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5331), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %key_vectors : Float(512:13056, 17:768, 768:1) = aten::add_(%output.68, %5329, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %5334 : Tensor = prim::GetAttr[name="bias"](%5317)
  %5335 : Tensor = prim::GetAttr[name="weight"](%5317)
  %5336 : Float(768:1, 768:768) = aten::t(%5335), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(512:13056, 17:768, 768:1) = aten::matmul(%input.139, %5336), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %value_vectors : Float(512:13056, 17:768, 768:1) = aten::add_(%output.69, %5334, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self/__module.longformer.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %5339 : int = aten::size(%input.139, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %5340 : int = aten::size(%input.139, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %5341 : int = aten::size(%input.139, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:255:0
  %query_vectors : Float(512:13056, 17:768, 768:1) = aten::div_(%query_vectors.23, %55), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:261:0
  %5343 : int[] = prim::ListConstruct(%5339, %5340, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5344 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%query_vectors, %5343), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
  %query.23 : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5344, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:263:0
  %5346 : int[] = prim::ListConstruct(%5339, %5340, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5347 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%key_vectors, %5346), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
  %key : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5347, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:264:0
  %5349 : int = aten::size(%query.23, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.46 : Long() = prim::NumToTensor(%5349), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5351 : int = aten::size(%query.23, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.47 : Long() = prim::NumToTensor(%5351), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5353 : int = aten::size(%query.23, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.34 : Long() = prim::NumToTensor(%5353), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5355 : int = aten::size(%query.23, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %5356 : Long() = aten::floor_divide(%seq_len.47, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count.34 : Long() = aten::sub(%5356, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
  %5358 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%query.23, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5359 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5360 : int = aten::Int(%5359), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5361 : int[] = prim::ListConstruct(%5360, %5351, %5355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.123 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5358, %5361), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5363 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%key, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5364 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5365 : int = aten::Int(%5364), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5366 : int[] = prim::ListConstruct(%5365, %5351, %5355), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.125 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5363, %5366), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5368 : int = aten::size(%hidden_states.123, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5369 : int = aten::size(%hidden_states.123, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5370 : Long() = prim::NumToTensor(%5369), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5371 : Long() = aten::floor_divide(%5370, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5372 : int = aten::Int(%5371), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5373 : int = aten::size(%hidden_states.123, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5374 : int[] = prim::ListConstruct(%5368, %5372, %68, %5373), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.124 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.123, %5374), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5376 : int = aten::size(%hidden_states.124, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5377 : int = aten::size(%hidden_states.124, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5378 : Long() = prim::NumToTensor(%5377), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5379 : int = aten::size(%hidden_states.124, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5380 : int = aten::size(%hidden_states.124, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5381 : Long() = aten::mul(%5378, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5382 : Long() = aten::sub(%5381, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5383 : int = aten::Int(%5382), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5384 : int[] = prim::ListConstruct(%5376, %5383, %5379, %5380), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5385 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5386 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.124, %5384, %5385, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5387 : int = aten::size(%hidden_states.125, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5388 : int = aten::size(%hidden_states.125, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5389 : Long() = prim::NumToTensor(%5388), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5390 : Long() = aten::floor_divide(%5389, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5391 : int = aten::Int(%5390), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5392 : int = aten::size(%hidden_states.125, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5393 : int[] = prim::ListConstruct(%5387, %5391, %68, %5392), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.126 : Float(204:64, 1:6684672, 512:13056, 64:1) = aten::view(%hidden_states.125, %5393), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5395 : int = aten::size(%hidden_states.126, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5396 : int = aten::size(%hidden_states.126, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5397 : Long() = prim::NumToTensor(%5396), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5398 : int = aten::size(%hidden_states.126, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5399 : int = aten::size(%hidden_states.126, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5400 : Long() = aten::mul(%5397, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5401 : Long() = aten::sub(%5400, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5402 : int = aten::Int(%5401), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5403 : int[] = prim::ListConstruct(%5395, %5402, %5398, %5399), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5404 : int[] = prim::ListConstruct(%53, %49, %48, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5405 : Float(204:64, 1:3342336, 512:13056, 64:1) = aten::as_strided(%hidden_states.126, %5403, %5404, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5406 : Tensor[] = prim::ListConstruct(%5386, %5405), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.140 : Float(204:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %5406), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5408 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states_padded.23 : Float(204:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.140, %5408, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5410 : int = aten::size(%hidden_states_padded.23, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5411 : int = aten::size(%hidden_states_padded.23, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5412 : int = aten::size(%hidden_states_padded.23, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5413 : int = aten::size(%hidden_states_padded.23, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5414 : int[] = prim::ListConstruct(%5410, %5411, %5412, %5413), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_chunked_attention_scores.23 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded.23, %5414), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:400:0
  %5416 : Long() = aten::mul(%batch_size.46, %num_heads.34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5417 : int = aten::Int(%5416), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5418 : Long() = aten::add(%chunks_count.34, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5419 : int = aten::Int(%5418), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5420 : int[] = prim::ListConstruct(%5417, %5419, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_attention_scores.23 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores.23, %5420, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
  %5422 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5423 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%5422, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5424 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%5423, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5425 : Float(204:262656, 1:262656, 256:513, 257:1) = aten::slice(%5424, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5426 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5427 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5426, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5428 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5427, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5429 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::slice(%5428, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5430 : int[] = prim::ListConstruct(%42, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5431 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::view(%5425, %5430), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5432 : Float(204:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5429, %5431, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5433 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5434 : Float(204:262656, 512:513, 513:1) = aten::select(%5433, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5435 : Float(204:262656, 256:513, 513:1) = aten::slice(%5434, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5436 : Float(204:262656, 256:513, 257:1) = aten::slice(%5435, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5437 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5438 : Float(204:262656, 256:513, 513:1) = aten::select(%5437, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5439 : Float(204:262656, 256:513, 513:1) = aten::slice(%5438, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5440 : Float(204:262656, 256:513, 257:1) = aten::slice(%5439, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5441 : int[] = prim::ListConstruct(%42, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5442 : Float(204:262656, 256:513, 257:1) = aten::view(%5436, %5441), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5443 : Float(204:262656, 256:513, 257:1) = aten::copy_(%5440, %5442, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5444 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5445 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%5444, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5446 : Float(204:262656, 1:262656, 256:513, 513:1) = aten::slice(%5445, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5447 : Float(204:262656, 1:262656, 256:513, 256:1) = aten::slice(%5446, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5448 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5449 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5448, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5450 : Float(204:262656, 1:131328, 256:513, 513:1) = aten::slice(%5449, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5451 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::slice(%5450, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5452 : int[] = prim::ListConstruct(%42, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5453 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::view(%5447, %5452), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5454 : Float(204:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5451, %5453, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5455 : Float(204:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5456 : Float(204:262656, 512:513, 513:1) = aten::select(%5455, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5457 : Float(204:262656, 255:513, 513:1) = aten::slice(%5456, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5458 : Float(204:262656, 255:513, 255:1) = aten::slice(%5457, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5459 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores.23, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5460 : Float(204:262656, 256:513, 513:1) = aten::select(%5459, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5461 : Float(204:262656, 255:513, 513:1) = aten::slice(%5460, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5462 : Float(204:262656, 255:513, 255:1) = aten::slice(%5461, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5463 : int[] = prim::ListConstruct(%42, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5464 : Float(204:262656, 255:513, 255:1) = aten::view(%5458, %5463), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5465 : Float(204:262656, 255:513, 255:1) = aten::copy_(%5462, %5464, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5466 : int[] = prim::ListConstruct(%5349, %5353, %5351, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5467 : Float(17:3151872, 12:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores.23, %5466), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.34 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::transpose(%5467, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %5469 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5470 : Float(256:257, 257:1) = aten::ones(%5469, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5471 : Float(256:257, 257:1) = aten::tril(%5470, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5472 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %beginning_mask_2d.23 : Float(256:257, 257:1) = aten::flip(%5471, %5472), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5474 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d.23, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5475 : Float(1:65792, 256:257, 257:1) = aten::slice(%5474, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5476 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5475, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask.23 : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5476, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5478 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %ending_mask.23 : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask.23, %5478), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
  %5480 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.34, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5481 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5480, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5482 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5481, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input.23 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5482, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5484 : int = aten::size(%beginning_input.23, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5485 : int = aten::size(%beginning_input.23, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5486 : int = aten::size(%beginning_input.23, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5487 : int = aten::size(%beginning_input.23, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5488 : int[] = prim::ListConstruct(%5484, %5485, %5486, %5487), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5489 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%beginning_mask.23, %5488, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5490 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5489, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5491 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%beginning_input.23, %5490, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
  %5492 : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::slice(%input_tensor.34, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5493 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5492, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5494 : Float(17:3151872, 256:513, 12:262656, 513:1) = aten::slice(%5493, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input.23 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::slice(%5494, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5496 : int = aten::size(%ending_input.23, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5497 : int = aten::size(%ending_input.23, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5498 : int = aten::size(%ending_input.23, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5499 : int = aten::size(%ending_input.23, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5500 : int[] = prim::ListConstruct(%5496, %5497, %5498, %5499), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5501 : Float(17:0, 256:257, 12:0, 257:1) = aten::expand(%ending_mask.23, %5500, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5502 : Bool(17:789504, 256:3084, 12:257, 257:1) = aten::eq(%5501, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5503 : Float(17:3151872, 256:513, 12:262656, 257:1) = aten::masked_fill_(%ending_input.23, %5502, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
  %5504 : Bool(17:512, 512:1) = aten::ne(%5321, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5505 : Bool(17:512, 512:1) = aten::slice(%5504, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5506 : Bool(17:512, 512:1) = aten::slice(%5505, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5507 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5506, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %remove_from_windowed_attention_mask : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5507, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:272:0
  %5509 : Float(17:512, 512:1, 1:1, 1:1) = aten::type_as(%remove_from_windowed_attention_mask, %query.23), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
  %float_mask : Float(17:512, 512:1, 1:1, 1:1) = aten::masked_fill(%5509, %remove_from_windowed_attention_mask, %36), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:275:0
  %5511 : int = aten::size(%float_mask, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5512 : int = aten::size(%float_mask, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5513 : int = aten::size(%float_mask, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5514 : int = aten::size(%float_mask, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5515 : int[] = prim::ListConstruct(%5511, %5512, %5513, %5514), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %query : Float(17:512, 512:1, 1:1, 1:1) = aten::ones(%5515, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:280:0
  %5517 : int = aten::size(%query, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %batch_size.47 : Long() = prim::NumToTensor(%5517), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5519 : int = aten::size(%query, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %seq_len.48 : Long() = prim::NumToTensor(%5519), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5521 : int = aten::size(%query, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %num_heads.35 : Long() = prim::NumToTensor(%5521), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5523 : int = aten::size(%query, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:472:0
  %5524 : Long() = aten::floor_divide(%seq_len.48, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count.35 : Long() = aten::sub(%5524, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:478:0
  %5526 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%query, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5527 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5528 : int = aten::Int(%5527), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5529 : int[] = prim::ListConstruct(%5528, %5519, %5523), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.127 : Float(17:512, 512:1, 1:1) = aten::reshape(%5526, %5529), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:481:0
  %5531 : Float(17:512, 1:1, 512:1, 1:1) = aten::transpose(%float_mask, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5532 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5533 : int = aten::Int(%5532), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5534 : int[] = prim::ListConstruct(%5533, %5519, %5523), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.129 : Float(17:512, 512:1, 1:1) = aten::reshape(%5531, %5534), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:482:0
  %5536 : int = aten::size(%hidden_states.127, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5537 : int = aten::size(%hidden_states.127, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5538 : Long() = prim::NumToTensor(%5537), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5539 : Long() = aten::floor_divide(%5538, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5540 : int = aten::Int(%5539), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5541 : int = aten::size(%hidden_states.127, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5542 : int[] = prim::ListConstruct(%5536, %5540, %68, %5541), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.128 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.127, %5542), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5544 : int = aten::size(%hidden_states.128, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5545 : int = aten::size(%hidden_states.128, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5546 : Long() = prim::NumToTensor(%5545), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5547 : int = aten::size(%hidden_states.128, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5548 : int = aten::size(%hidden_states.128, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5549 : Long() = aten::mul(%5546, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5550 : Long() = aten::sub(%5549, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5551 : int = aten::Int(%5550), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5552 : int[] = prim::ListConstruct(%5544, %5551, %5547, %5548), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5553 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5554 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.128, %5552, %5553, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5555 : int = aten::size(%hidden_states.129, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:442:0
  %5556 : int = aten::size(%hidden_states.129, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:443:0
  %5557 : Long() = prim::NumToTensor(%5556), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5558 : Long() = aten::floor_divide(%5557, %51), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5559 : int = aten::Int(%5558), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5560 : int = aten::size(%hidden_states.129, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:445:0
  %5561 : int[] = prim::ListConstruct(%5555, %5559, %68, %5560), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states.130 : Float(17:512, 1:512, 512:1, 1:1) = aten::view(%hidden_states.129, %5561), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:441:0
  %5563 : int = aten::size(%hidden_states.130, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5564 : int = aten::size(%hidden_states.130, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5565 : Long() = prim::NumToTensor(%5564), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5566 : int = aten::size(%hidden_states.130, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5567 : int = aten::size(%hidden_states.130, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:449:0
  %5568 : Long() = aten::mul(%5565, %50), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5569 : Long() = aten::sub(%5568, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:450:0
  %5570 : int = aten::Int(%5569), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5571 : int[] = prim::ListConstruct(%5563, %5570, %5566, %5567), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5572 : int[] = prim::ListConstruct(%68, %45, %73, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5573 : Float(17:512, 1:256, 512:1, 1:1) = aten::as_strided(%hidden_states.130, %5571, %5572, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:454:0
  %5574 : Tensor[] = prim::ListConstruct(%5554, %5573), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.141 : Float(17:262144, 1:262144, 512:512, 512:1) = aten::einsum(%47, %5574), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5576 : int[] = prim::ListConstruct(%74, %74, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %hidden_states_padded : Float(17:262656, 1:262656, 513:512, 512:1) = aten::constant_pad_nd(%input.141, %5576, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5578 : int = aten::size(%hidden_states_padded, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5579 : int = aten::size(%hidden_states_padded, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5580 : int = aten::size(%hidden_states_padded, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5581 : int = aten::size(%hidden_states_padded, %46), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:401:0
  %5582 : int[] = prim::ListConstruct(%5578, %5579, %5580, %5581), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_chunked_attention_scores : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%hidden_states_padded, %5582), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:400:0
  %5584 : Long() = aten::mul(%batch_size.47, %num_heads.35), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5585 : int = aten::Int(%5584), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5586 : Long() = aten::add(%chunks_count.35, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:504:0
  %5587 : int = aten::Int(%5586), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5588 : int[] = prim::ListConstruct(%5585, %5587, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %diagonal_attention_scores : Float(17:262656, 2:131328, 256:513, 513:1) = aten::new_empty(%diagonal_chunked_attention_scores, %5588, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:503:0
  %5590 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5591 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5590, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5592 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5591, %66, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5593 : Float(17:262656, 1:262656, 256:513, 257:1) = aten::slice(%5592, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5594 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5595 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5594, %73, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5596 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5595, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5597 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::slice(%5596, %65, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5598 : int[] = prim::ListConstruct(%35, %73, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5599 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::view(%5593, %5598), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5600 : Float(17:262656, 1:131328, 256:513, 257:1) = aten::copy_(%5597, %5599, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:509:0
  %5601 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5602 : Float(17:262656, 512:513, 513:1) = aten::select(%5601, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5603 : Float(17:262656, 256:513, 513:1) = aten::slice(%5602, %73, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5604 : Float(17:262656, 256:513, 257:1) = aten::slice(%5603, %66, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5605 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5606 : Float(17:262656, 256:513, 513:1) = aten::select(%5605, %73, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5607 : Float(17:262656, 256:513, 513:1) = aten::slice(%5606, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5608 : Float(17:262656, 256:513, 257:1) = aten::slice(%5607, %66, %45, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5609 : int[] = prim::ListConstruct(%35, %45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5610 : Float(17:262656, 256:513, 257:1) = aten::view(%5604, %5609), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5611 : Float(17:262656, 256:513, 257:1) = aten::copy_(%5608, %5610, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:512:0
  %5612 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5613 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%5612, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5614 : Float(17:262656, 1:262656, 256:513, 513:1) = aten::slice(%5613, %66, %41, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5615 : Float(17:262656, 1:262656, 256:513, 256:1) = aten::slice(%5614, %65, %43, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5616 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5617 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5616, %73, %73, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5618 : Float(17:262656, 1:131328, 256:513, 513:1) = aten::slice(%5617, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5619 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::slice(%5618, %65, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5620 : int[] = prim::ListConstruct(%35, %73, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5621 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::view(%5615, %5620), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5622 : Float(17:262656, 1:131328, 256:513, 256:1) = aten::copy_(%5619, %5621, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:516:0
  %5623 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::slice(%diagonal_chunked_attention_scores, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5624 : Float(17:262656, 512:513, 513:1) = aten::select(%5623, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5625 : Float(17:262656, 255:513, 513:1) = aten::slice(%5624, %73, %74, %40, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5626 : Float(17:262656, 255:513, 255:1) = aten::slice(%5625, %66, %39, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5627 : Float(17:262656, 2:131328, 256:513, 513:1) = aten::slice(%diagonal_attention_scores, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5628 : Float(17:262656, 256:513, 513:1) = aten::select(%5627, %73, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5629 : Float(17:262656, 255:513, 513:1) = aten::slice(%5628, %73, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5630 : Float(17:262656, 255:513, 255:1) = aten::slice(%5629, %66, %73, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5631 : int[] = prim::ListConstruct(%35, %40, %40), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5632 : Float(17:262656, 255:513, 255:1) = aten::view(%5626, %5631), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5633 : Float(17:262656, 255:513, 255:1) = aten::copy_(%5630, %5632, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:520:0
  %5634 : int[] = prim::ListConstruct(%5517, %5521, %5519, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5635 : Float(17:262656, 1:262656, 512:513, 513:1) = aten::view(%diagonal_attention_scores, %5634), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %input_tensor.35 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::transpose(%5635, %66, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:525:0
  %5637 : int[] = prim::ListConstruct(%45, %43), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5638 : Float(256:257, 257:1) = aten::ones(%5637, %64, %74, %71, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5639 : Float(256:257, 257:1) = aten::tril(%5638, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5640 : int[] = prim::ListConstruct(%74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %beginning_mask_2d : Float(256:257, 257:1) = aten::flip(%5639, %5640), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:458:0
  %5642 : Float(1:65792, 256:257, 257:1) = aten::unsqueeze(%beginning_mask_2d, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5643 : Float(1:65792, 256:257, 257:1) = aten::slice(%5642, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5644 : Float(1:65792, 256:257, 1:257, 257:1) = aten::unsqueeze(%5643, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %beginning_mask : Float(1:65792, 256:257, 1:257, 257:1) = aten::slice(%5644, %65, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:459:0
  %5646 : int[] = prim::ListConstruct(%73, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %ending_mask : Float(1:65792, 256:257, 1:257, 257:1) = aten::flip(%beginning_mask, %5646), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:460:0
  %5648 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.35, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5649 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5648, %73, %74, %45, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5650 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5649, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %beginning_input : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5650, %65, %74, %43, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:461:0
  %5652 : int = aten::size(%beginning_input, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5653 : int = aten::size(%beginning_input, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5654 : int = aten::size(%beginning_input, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5655 : int = aten::size(%beginning_input, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5656 : int[] = prim::ListConstruct(%5652, %5653, %5654, %5655), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5657 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%beginning_mask, %5656, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:462:0
  %5658 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5657, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5659 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%beginning_input, %5658, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:463:0
  %5660 : Float(17:262656, 512:513, 1:262656, 513:1) = aten::slice(%input_tensor.35, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5661 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5660, %73, %37, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5662 : Float(17:262656, 256:513, 1:262656, 513:1) = aten::slice(%5661, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %ending_input : Float(17:262656, 256:513, 1:262656, 257:1) = aten::slice(%5662, %65, %41, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:464:0
  %5664 : int = aten::size(%ending_input, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5665 : int = aten::size(%ending_input, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5666 : int = aten::size(%ending_input, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5667 : int = aten::size(%ending_input, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5668 : int[] = prim::ListConstruct(%5664, %5665, %5666, %5667), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5669 : Float(17:0, 256:257, 1:257, 257:1) = aten::expand(%ending_mask, %5668, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:465:0
  %5670 : Bool(17:65792, 256:257, 1:257, 257:1) = aten::eq(%5669, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:22:0
  %5671 : Float(17:262656, 256:513, 1:262656, 257:1) = aten::masked_fill_(%ending_input, %5670, %38), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:466:0
  %attn_scores : Float(17:3151872, 512:513, 12:262656, 513:1) = aten::add_(%input_tensor.34, %input_tensor.35, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:284:0
  %attn_probs_fp32 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::softmax(%attn_scores, %56, %64), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:1500:0
  %5674 : Bool(17:512, 512:1) = aten::slice(%is_index_masked, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5675 : Bool(17:512, 512:1) = aten::slice(%5674, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5676 : Bool(17:512, 512:1, 1:1) = aten::unsqueeze(%5675, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %5677 : Bool(17:512, 512:1, 1:1, 1:1) = aten::unsqueeze(%5676, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %input.142 : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::masked_fill(%attn_probs_fp32, %5677, %34), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:326:0
  %attn_probs : Float(17:3151872, 512:6156, 12:513, 513:1) = aten::dropout(%input.142, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:973:0
  %5680 : int[] = prim::ListConstruct(%5339, %5340, %54, %53), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5681 : Float(512:13056, 17:768, 12:64, 64:1) = aten::view(%value_vectors, %5680), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
  %value : Float(17:768, 512:13056, 12:64, 64:1) = aten::transpose(%5681, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:331:0
  %5683 : int = aten::size(%value, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %batch_size : Long() = prim::NumToTensor(%5683), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5685 : int = aten::size(%value, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %seq_len : Long() = prim::NumToTensor(%5685), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5687 : int = aten::size(%value, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %num_heads : Long() = prim::NumToTensor(%5687), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5689 : int = aten::size(%value, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:537:0
  %5690 : Long() = aten::floor_divide(%seq_len, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %chunks_count : Long() = aten::sub(%5690, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:542:0
  %5692 : Float(17:3151872, 12:513, 512:6156, 513:1) = aten::transpose(%attn_probs, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
  %5693 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:546:0
  %5694 : int = aten::Int(%5693), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5695 : Long() = aten::floor_divide(%seq_len, %52), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/tensor.py:424:0
  %5696 : int = aten::Int(%5695), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5697 : int[] = prim::ListConstruct(%5694, %5696, %45, %44), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.56 : Float(204:262656, 2:131328, 256:513, 513:1) = aten::reshape(%5692, %5697), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:545:0
  %5699 : Float(17:768, 12:64, 512:13056, 64:1) = aten::transpose(%value, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5700 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5701 : int = aten::Int(%5700), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5702 : int[] = prim::ListConstruct(%5701, %5685, %5689), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %input.143 : Float(204:64, 512:13056, 64:1) = aten::reshape(%5699, %5702), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:550:0
  %5704 : int[] = prim::ListConstruct(%74, %74, %45, %45), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %padded_value : Float(204:65536, 1024:64, 64:1) = aten::constant_pad_nd(%input.143, %5704, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5706 : Long() = aten::mul(%batch_size, %num_heads), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
  %5707 : int = aten::Int(%5706), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5708 : Long() = aten::add(%chunks_count, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:556:0
  %5709 : int = aten::Int(%5708), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5710 : int[] = prim::ListConstruct(%5707, %5709, %59, %5689), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5711 : int[] = prim::ListConstruct(%33, %32, %53, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5712 : Float(204:65536, 2:16384, 768:64, 64:1) = aten::as_strided(%padded_value, %5710, %5711, %63), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:564:0
  %5713 : int = aten::size(%chunked_hidden_states.56, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %5714 : int = aten::size(%chunked_hidden_states.56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %5715 : int = aten::size(%chunked_hidden_states.56, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %window_overlap : Long() = prim::NumToTensor(%5715), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5717 : int = aten::size(%chunked_hidden_states.56, %65), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:420:0
  %hidden_dim : Long() = prim::NumToTensor(%5717), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5719 : Long() = aten::add(%window_overlap, %69, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:422:0
  %5720 : int = aten::Int(%5719), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5721 : int[] = prim::ListConstruct(%74, %5720), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.57 : Float(204:394240, 2:197120, 256:770, 770:1) = aten::constant_pad_nd(%chunked_hidden_states.56, %5721, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/nn/functional.py:3552:0
  %5723 : int[] = prim::ListConstruct(%5713, %5714, %56), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states.58 : Float(204:394240, 2:197120, 197120:1) = aten::view(%chunked_hidden_states.57, %5723), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:424:0
  %5725 : Long() = aten::neg(%window_overlap), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:428:0
  %5726 : int = aten::Int(%5725), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5727 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%chunked_hidden_states.58, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %5728 : Float(204:394240, 2:197120, 197120:1) = aten::slice(%5727, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %chunked_hidden_states.59 : Float(204:394240, 2:197120, 196864:1) = aten::slice(%5728, %66, %74, %5726, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:427:0
  %5730 : Long() = aten::add(%window_overlap, %hidden_dim, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:431:0
  %5731 : int = aten::Int(%5730), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5732 : int[] = prim::ListConstruct(%5713, %5714, %5715, %5731), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %chunked_hidden_states : Float(204:394240, 2:197120, 256:769, 769:1) = aten::view(%chunked_hidden_states.59, %5732), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:430:0
  %5734 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%chunked_hidden_states, %74, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5735 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5734, %73, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5736 : Float(204:394240, 2:197120, 256:769, 769:1) = aten::slice(%5735, %66, %74, %67, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5737 : Float(204:394240, 2:197120, 256:769, 768:1) = aten::slice(%5736, %65, %74, %56, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:433:0
  %5738 : Tensor[] = prim::ListConstruct(%5737, %5712), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %context : Float(204:32768, 2:16384, 256:64, 64:1) = aten::einsum(%31, %5738), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # torch/functional.py:327:0
  %5740 : int[] = prim::ListConstruct(%5683, %5687, %5685, %5689), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5741 : Float(17:393216, 12:32768, 512:64, 64:1) = aten::view(%context, %5740), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
  %attn_output.23 : Float(17:393216, 512:64, 12:32768, 64:1) = aten::transpose(%5741, %73, %66), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:569:0
  %5743 : Float(512:64, 17:393216, 12:32768, 64:1) = aten::transpose(%attn_output.23, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %5744 : int[] = prim::ListConstruct(%5339, %5340, %5341), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self
  %5745 : Float(512:13056, 17:768, 768:1) = aten::reshape(%5743, %5744), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %attn_output : Float(512:13056, 17:768, 768:1) = aten::contiguous(%5745, %74), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:350:0
  %input.144 : Float(17:768, 512:13056, 768:1) = aten::transpose(%attn_output, %74, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.self # transformers/modeling_longformer.py:374:0
  %5748 : __torch__.torch.nn.modules.normalization.___torch_mangle_7306.LayerNorm = prim::GetAttr[name="LayerNorm"](%5315)
  %5749 : __torch__.torch.nn.modules.linear.___torch_mangle_7305.Linear = prim::GetAttr[name="dense"](%5315)
  %5750 : Tensor = prim::GetAttr[name="bias"](%5749)
  %5751 : Tensor = prim::GetAttr[name="weight"](%5749)
  %5752 : Float(768:1, 768:768) = aten::t(%5751), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.144, %5752), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.145 : Float(17:393216, 512:768, 768:1) = aten::add_(%output.70, %5750, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.131 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.145, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.131, %hidden_states.122, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output # transformers/modeling_longformer.py:758:0
  %5757 : Tensor = prim::GetAttr[name="bias"](%5748)
  %5758 : Tensor = prim::GetAttr[name="weight"](%5748)
  %5759 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.146, %5759, %5758, %5757, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.attention/__module.longformer.encoder.layer.11.attention.output/__module.longformer.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %5761 : __torch__.torch.nn.modules.linear.___torch_mangle_7310.Linear = prim::GetAttr[name="dense"](%5313)
  %5762 : Tensor = prim::GetAttr[name="bias"](%5761)
  %5763 : Tensor = prim::GetAttr[name="weight"](%5761)
  %5764 : Float(768:1, 3072:768) = aten::t(%5763), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:1572864, 512:3072, 3072:1) = aten::matmul(%input_tensor, %5764), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.147 : Float(17:1572864, 512:3072, 3072:1) = aten::add_(%output.71, %5762, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate/__module.longformer.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.148 : Float(17:1572864, 512:3072, 3072:1) = aten::gelu(%input.147), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %5768 : __torch__.torch.nn.modules.normalization.___torch_mangle_7313.LayerNorm = prim::GetAttr[name="LayerNorm"](%5312)
  %5769 : __torch__.torch.nn.modules.linear.___torch_mangle_7312.Linear = prim::GetAttr[name="dense"](%5312)
  %5770 : Tensor = prim::GetAttr[name="bias"](%5769)
  %5771 : Tensor = prim::GetAttr[name="weight"](%5769)
  %5772 : Float(3072:1, 768:3072) = aten::t(%5771), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output : Float(17:393216, 512:768, 768:1) = aten::matmul(%input.148, %5772), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.149 : Float(17:393216, 512:768, 768:1) = aten::add_(%output, %5770, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.132 : Float(17:393216, 512:768, 768:1) = aten::dropout(%input.149, %60, %70), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(17:393216, 512:768, 768:1) = aten::add(%hidden_states.132, %input_tensor, %73), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output # transformers/modeling_longformer.py:830:0
  %5777 : Tensor = prim::GetAttr[name="bias"](%5768)
  %5778 : Tensor = prim::GetAttr[name="weight"](%5768)
  %5779 : int[] = prim::ListConstruct(%59), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.LayerNorm
  %sequence_output : Float(17:393216, 512:768, 768:1) = aten::layer_norm(%input.150, %5779, %5778, %5777, %58, %57), scope: __module.longformer/__module.longformer.encoder/__module.longformer.encoder.layer.11/__module.longformer.encoder.layer.11.output/__module.longformer.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %5781 : Long() = aten::neg(%padding_len), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %5782 : int = aten::Int(%5781), scope: __module.longformer
  %5783 : Float(17:393216, 512:768, 768:1) = aten::slice(%sequence_output, %74, %74, %67, %73), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %hidden_states.133 : Float(17:393216, 13:768, 768:1) = aten::slice(%5783, %73, %74, %5782, %73), scope: __module.longformer # transformers/modeling_longformer.py:1275:0
  %5785 : float = prim::Constant[value=0.10000000000000001](), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %5786 : bool = prim::Constant[value=0](), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %5787 : int = prim::Constant[value=1](), scope: __module.classifier # transformers/modeling_longformer.py:1488:0
  %5788 : int = prim::Constant[value=9223372036854775807](), scope: __module.classifier # transformers/modeling_longformer.py:1488:0
  %5789 : int = prim::Constant[value=0](), scope: __module.classifier # transformers/modeling_longformer.py:1488:0
  %5790 : __torch__.torch.nn.modules.linear.___torch_mangle_7322.Linear = prim::GetAttr[name="out_proj"](%3)
  %5791 : __torch__.torch.nn.modules.linear.___torch_mangle_7320.Linear = prim::GetAttr[name="dense"](%3)
  %5792 : Float(17:393216, 13:768, 768:1) = aten::slice(%hidden_states.133, %5789, %5789, %5788, %5787), scope: __module.classifier # transformers/modeling_longformer.py:1488:0
  %5793 : Float(17:393216, 768:1) = aten::select(%5792, %5787, %5789), scope: __module.classifier # transformers/modeling_longformer.py:1488:0
  %input.151 : Float(17:393216, 768:1) = aten::slice(%5793, %5787, %5789, %5788, %5787), scope: __module.classifier # transformers/modeling_longformer.py:1488:0
  %input.152 : Float(17:393216, 768:1) = aten::dropout(%input.151, %5785, %5786), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %5796 : Tensor = prim::GetAttr[name="bias"](%5791)
  %5797 : Tensor = prim::GetAttr[name="weight"](%5791)
  %5798 : Float(768:1, 768:768) = aten::t(%5797), scope: __module.classifier/__module.classifier.dense # torch/nn/functional.py:1674:0
  %hidden_states : Float(17:768, 768:1) = aten::addmm(%5796, %input.152, %5798, %5787, %5787), scope: __module.classifier/__module.classifier.dense # torch/nn/functional.py:1674:0
  %input.153 : Float(17:768, 768:1) = aten::tanh(%hidden_states), scope: __module.classifier # transformers/modeling_longformer.py:1491:0
  %input : Float(17:768, 768:1) = aten::dropout(%input.153, %5785, %5786), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %5802 : Tensor = prim::GetAttr[name="bias"](%5790)
  %5803 : Tensor = prim::GetAttr[name="weight"](%5790)
  %5804 : Float(768:1, 2:768) = aten::t(%5803), scope: __module.classifier/__module.classifier.out_proj # torch/nn/functional.py:1674:0
  %5805 : Float(17:2, 2:1) = aten::addmm(%5802, %input, %5804, %5787, %5787), scope: __module.classifier/__module.classifier.out_proj # torch/nn/functional.py:1674:0
  %30 : (Float(17:2, 2:1)) = prim::TupleConstruct(%5805)
  return (%30)
