graph(%self.1 : __torch__.transformers.modeling_xlm.XLMForQuestionAnswering,
      %input_ids : Long(17:13, 13:1),
      %padding_mask : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_utils.SQuADHead = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_xlm.___torch_mangle_35748.XLMModel = prim::GetAttr[name="transformer"](%self.1)
  %13 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %14 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %15 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %16 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %17 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %18 : None = prim::Constant(), scope: __module.transformer
  %19 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlm.py:580:0
  %20 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
  %21 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %22 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %23 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %24 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %25 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %26 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %27 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlm.py:546:0
  %28 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlm.py:546:0
  %29 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlm.py:517:0
  %30 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %31 : __torch__.torch.nn.modules.normalization.___torch_mangle_35746.LayerNorm = prim::GetAttr[name="11"](%30)
  %32 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %33 : __torch__.transformers.modeling_xlm.___torch_mangle_35733.TransformerFFN = prim::GetAttr[name="11"](%32)
  %34 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %35 : __torch__.torch.nn.modules.normalization.___torch_mangle_35696.LayerNorm = prim::GetAttr[name="11"](%34)
  %36 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %37 : __torch__.transformers.modeling_xlm.___torch_mangle_35683.MultiHeadAttention = prim::GetAttr[name="11"](%36)
  %38 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %39 : __torch__.torch.nn.modules.normalization.___torch_mangle_35745.LayerNorm = prim::GetAttr[name="10"](%38)
  %40 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %41 : __torch__.transformers.modeling_xlm.___torch_mangle_35730.TransformerFFN = prim::GetAttr[name="10"](%40)
  %42 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %43 : __torch__.torch.nn.modules.normalization.___torch_mangle_35695.LayerNorm = prim::GetAttr[name="10"](%42)
  %44 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %45 : __torch__.transformers.modeling_xlm.___torch_mangle_35678.MultiHeadAttention = prim::GetAttr[name="10"](%44)
  %46 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %47 : __torch__.torch.nn.modules.normalization.___torch_mangle_35744.LayerNorm = prim::GetAttr[name="9"](%46)
  %48 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %49 : __torch__.transformers.modeling_xlm.___torch_mangle_35727.TransformerFFN = prim::GetAttr[name="9"](%48)
  %50 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %51 : __torch__.torch.nn.modules.normalization.___torch_mangle_35694.LayerNorm = prim::GetAttr[name="9"](%50)
  %52 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %53 : __torch__.transformers.modeling_xlm.___torch_mangle_35673.MultiHeadAttention = prim::GetAttr[name="9"](%52)
  %54 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %55 : __torch__.torch.nn.modules.normalization.___torch_mangle_35743.LayerNorm = prim::GetAttr[name="8"](%54)
  %56 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %57 : __torch__.transformers.modeling_xlm.___torch_mangle_35724.TransformerFFN = prim::GetAttr[name="8"](%56)
  %58 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %59 : __torch__.torch.nn.modules.normalization.___torch_mangle_35693.LayerNorm = prim::GetAttr[name="8"](%58)
  %60 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %61 : __torch__.transformers.modeling_xlm.___torch_mangle_35668.MultiHeadAttention = prim::GetAttr[name="8"](%60)
  %62 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %63 : __torch__.torch.nn.modules.normalization.___torch_mangle_35742.LayerNorm = prim::GetAttr[name="7"](%62)
  %64 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %65 : __torch__.transformers.modeling_xlm.___torch_mangle_35721.TransformerFFN = prim::GetAttr[name="7"](%64)
  %66 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %67 : __torch__.torch.nn.modules.normalization.___torch_mangle_35692.LayerNorm = prim::GetAttr[name="7"](%66)
  %68 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %69 : __torch__.transformers.modeling_xlm.___torch_mangle_35663.MultiHeadAttention = prim::GetAttr[name="7"](%68)
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %71 : __torch__.torch.nn.modules.normalization.___torch_mangle_35741.LayerNorm = prim::GetAttr[name="6"](%70)
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %73 : __torch__.transformers.modeling_xlm.___torch_mangle_35718.TransformerFFN = prim::GetAttr[name="6"](%72)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %75 : __torch__.torch.nn.modules.normalization.___torch_mangle_35691.LayerNorm = prim::GetAttr[name="6"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %77 : __torch__.transformers.modeling_xlm.___torch_mangle_35658.MultiHeadAttention = prim::GetAttr[name="6"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %79 : __torch__.torch.nn.modules.normalization.___torch_mangle_35740.LayerNorm = prim::GetAttr[name="5"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %81 : __torch__.transformers.modeling_xlm.___torch_mangle_35715.TransformerFFN = prim::GetAttr[name="5"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %83 : __torch__.torch.nn.modules.normalization.___torch_mangle_35690.LayerNorm = prim::GetAttr[name="5"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %85 : __torch__.transformers.modeling_xlm.___torch_mangle_35653.MultiHeadAttention = prim::GetAttr[name="5"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %87 : __torch__.torch.nn.modules.normalization.___torch_mangle_35739.LayerNorm = prim::GetAttr[name="4"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %89 : __torch__.transformers.modeling_xlm.___torch_mangle_35712.TransformerFFN = prim::GetAttr[name="4"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %91 : __torch__.torch.nn.modules.normalization.___torch_mangle_35689.LayerNorm = prim::GetAttr[name="4"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %93 : __torch__.transformers.modeling_xlm.___torch_mangle_35648.MultiHeadAttention = prim::GetAttr[name="4"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %95 : __torch__.torch.nn.modules.normalization.___torch_mangle_35738.LayerNorm = prim::GetAttr[name="3"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %97 : __torch__.transformers.modeling_xlm.___torch_mangle_35709.TransformerFFN = prim::GetAttr[name="3"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %99 : __torch__.torch.nn.modules.normalization.___torch_mangle_35688.LayerNorm = prim::GetAttr[name="3"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %101 : __torch__.transformers.modeling_xlm.___torch_mangle_35643.MultiHeadAttention = prim::GetAttr[name="3"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %103 : __torch__.torch.nn.modules.normalization.___torch_mangle_35737.LayerNorm = prim::GetAttr[name="2"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %105 : __torch__.transformers.modeling_xlm.___torch_mangle_35706.TransformerFFN = prim::GetAttr[name="2"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %107 : __torch__.torch.nn.modules.normalization.___torch_mangle_35687.LayerNorm = prim::GetAttr[name="2"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %109 : __torch__.transformers.modeling_xlm.___torch_mangle_35638.MultiHeadAttention = prim::GetAttr[name="2"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %111 : __torch__.torch.nn.modules.normalization.___torch_mangle_35736.LayerNorm = prim::GetAttr[name="1"](%110)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %113 : __torch__.transformers.modeling_xlm.___torch_mangle_35703.TransformerFFN = prim::GetAttr[name="1"](%112)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %115 : __torch__.torch.nn.modules.normalization.___torch_mangle_35686.LayerNorm = prim::GetAttr[name="1"](%114)
  %116 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %117 : __torch__.transformers.modeling_xlm.___torch_mangle_35633.MultiHeadAttention = prim::GetAttr[name="1"](%116)
  %118 : __torch__.torch.nn.modules.container.___torch_mangle_35747.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %119 : __torch__.torch.nn.modules.normalization.___torch_mangle_35735.LayerNorm = prim::GetAttr[name="0"](%118)
  %120 : __torch__.torch.nn.modules.container.___torch_mangle_35734.ModuleList = prim::GetAttr[name="ffns"](%4)
  %121 : __torch__.transformers.modeling_xlm.___torch_mangle_35700.TransformerFFN = prim::GetAttr[name="0"](%120)
  %122 : __torch__.torch.nn.modules.container.___torch_mangle_35697.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %123 : __torch__.torch.nn.modules.normalization.___torch_mangle_35685.LayerNorm = prim::GetAttr[name="0"](%122)
  %124 : __torch__.torch.nn.modules.container.___torch_mangle_35684.ModuleList = prim::GetAttr[name="attentions"](%4)
  %125 : __torch__.transformers.modeling_xlm.___torch_mangle_35628.MultiHeadAttention = prim::GetAttr[name="0"](%124)
  %126 : __torch__.torch.nn.modules.normalization.___torch_mangle_35623.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%4)
  %127 : __torch__.torch.nn.modules.sparse.___torch_mangle_35621.Embedding = prim::GetAttr[name="position_embeddings"](%4)
  %128 : __torch__.torch.nn.modules.sparse.___torch_mangle_35622.Embedding = prim::GetAttr[name="embeddings"](%4)
  %129 : Tensor = prim::GetAttr[name="position_ids"](%4)
  %130 : int = aten::size(%input_ids, %29), scope: __module.transformer # transformers/modeling_xlm.py:517:0
  %131 : Long(1:512, 512:1) = aten::slice(%129, %28, %28, %27, %29), scope: __module.transformer # transformers/modeling_xlm.py:546:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%131, %29, %28, %130, %29), scope: __module.transformer # transformers/modeling_xlm.py:546:0
  %133 : Tensor = prim::GetAttr[name="weight"](%128)
  %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%133, %input_ids, %25, %26, %26), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %135 : Tensor = prim::GetAttr[name="weight"](%127)
  %136 : Float(1:26624, 13:2048, 2048:1) = aten::embedding(%135, %input.1, %24, %26, %26), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %137 : Float(17:0, 13:2048, 2048:1) = aten::expand_as(%136, %inputs_embeds), scope: __module.transformer # transformers/modeling_xlm.py:573:0
  %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%inputs_embeds, %137, %29), scope: __module.transformer # transformers/modeling_xlm.py:573:0
  %139 : Tensor = prim::GetAttr[name="bias"](%126)
  %140 : Tensor = prim::GetAttr[name="weight"](%126)
  %141 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm_emb
  %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %141, %140, %139, %22, %23), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.3, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %144 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:580:0
  %145 : Float(17:13, 13:1, 1:1) = aten::to(%144, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:580:0
  %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %145), scope: __module.transformer # transformers/modeling_xlm.py:580:0
  %147 : __torch__.torch.nn.modules.linear.___torch_mangle_35627.Linear = prim::GetAttr[name="out_lin"](%125)
  %148 : __torch__.torch.nn.modules.linear.___torch_mangle_35626.Linear = prim::GetAttr[name="v_lin"](%125)
  %149 : __torch__.torch.nn.modules.linear.___torch_mangle_35625.Linear = prim::GetAttr[name="k_lin"](%125)
  %150 : __torch__.torch.nn.modules.linear.___torch_mangle_35624.Linear = prim::GetAttr[name="q_lin"](%125)
  %151 : int = aten::size(%input.4, %28), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %152 : int = aten::size(%input.4, %29), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %153 : Tensor = prim::GetAttr[name="bias"](%150)
  %154 : Tensor = prim::GetAttr[name="weight"](%150)
  %155 : Float(2048:1, 2048:2048) = aten::t(%154), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %155), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %153, %29), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1678:0
  %158 : int[] = prim::ListConstruct(%151, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.0
  %159 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.1, %158), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%159, %29, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %161 : Tensor = prim::GetAttr[name="bias"](%149)
  %162 : Tensor = prim::GetAttr[name="weight"](%149)
  %163 : Float(2048:1, 2048:2048) = aten::t(%162), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %163), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %161, %29), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1678:0
  %166 : int[] = prim::ListConstruct(%151, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.0
  %167 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.2, %166), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%167, %29, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %169 : Tensor = prim::GetAttr[name="bias"](%148)
  %170 : Tensor = prim::GetAttr[name="weight"](%148)
  %171 : Float(2048:1, 2048:2048) = aten::t(%170), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %171), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %169, %29), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1678:0
  %174 : int[] = prim::ListConstruct(%151, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.0
  %175 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.3, %174), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%175, %29, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %15), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %178 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %25, %16), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %178), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %180 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.0 # torch/tensor.py:22:0
  %181 : int[] = prim::ListConstruct(%151, %29, %29, %152), scope: __module.transformer/__module.transformer.attentions.0
  %182 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%180, %181), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%182, %scores.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %17), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
  %186 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %24, %18), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:1498:0
  %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%186, %20, %26), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
  %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:200:0
  %189 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %29, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %190 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%189, %28), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %191 : int[] = prim::ListConstruct(%151, %24, %21), scope: __module.transformer/__module.transformer.attentions.0
  %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%190, %191), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %193 : Tensor = prim::GetAttr[name="bias"](%147)
  %194 : Tensor = prim::GetAttr[name="weight"](%147)
  %195 : Float(2048:1, 2048:2048) = aten::t(%194), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %195), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %193, %29), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1678:0
  %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.8, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %200 : Tensor = prim::GetAttr[name="bias"](%123)
  %201 : Tensor = prim::GetAttr[name="weight"](%123)
  %202 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.0
  %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %202, %201, %200, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
  %204 : __torch__.torch.nn.modules.linear.___torch_mangle_35699.Linear = prim::GetAttr[name="lin2"](%121)
  %205 : __torch__.torch.nn.modules.linear.___torch_mangle_35698.Linear = prim::GetAttr[name="lin1"](%121)
  %206 : Tensor = prim::GetAttr[name="bias"](%205)
  %207 : Tensor = prim::GetAttr[name="weight"](%205)
  %208 : Float(2048:1, 8192:2048) = aten::t(%207), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.1, %208), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %206, %29), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1678:0
  %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.10), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:1369:0
  %212 : Tensor = prim::GetAttr[name="bias"](%204)
  %213 : Tensor = prim::GetAttr[name="weight"](%204)
  %214 : Float(8192:1, 2048:8192) = aten::t(%213), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %214), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %212, %29), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1678:0
  %217 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.12, %20, %26), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
  %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.1, %217, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %219 : Tensor = prim::GetAttr[name="bias"](%119)
  %220 : Tensor = prim::GetAttr[name="weight"](%119)
  %221 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.0
  %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %221, %220, %219, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
  %223 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %224 : Float(17:13, 13:1, 1:1) = aten::to(%223, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.2, %224), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %226 : __torch__.torch.nn.modules.linear.___torch_mangle_35632.Linear = prim::GetAttr[name="out_lin"](%117)
  %227 : __torch__.torch.nn.modules.linear.___torch_mangle_35631.Linear = prim::GetAttr[name="v_lin"](%117)
  %228 : __torch__.torch.nn.modules.linear.___torch_mangle_35630.Linear = prim::GetAttr[name="k_lin"](%117)
  %229 : __torch__.torch.nn.modules.linear.___torch_mangle_35629.Linear = prim::GetAttr[name="q_lin"](%117)
  %230 : int = aten::size(%input.14, %28), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %231 : int = aten::size(%input.14, %29), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %232 : Tensor = prim::GetAttr[name="bias"](%229)
  %233 : Tensor = prim::GetAttr[name="weight"](%229)
  %234 : Float(2048:1, 2048:2048) = aten::t(%233), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %234), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %232, %29), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1678:0
  %237 : int[] = prim::ListConstruct(%230, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.1
  %238 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.5, %237), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%238, %29, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %240 : Tensor = prim::GetAttr[name="bias"](%228)
  %241 : Tensor = prim::GetAttr[name="weight"](%228)
  %242 : Float(2048:1, 2048:2048) = aten::t(%241), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %242), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %240, %29), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1678:0
  %245 : int[] = prim::ListConstruct(%230, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.1
  %246 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.6, %245), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%246, %29, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %248 : Tensor = prim::GetAttr[name="bias"](%227)
  %249 : Tensor = prim::GetAttr[name="weight"](%227)
  %250 : Float(2048:1, 2048:2048) = aten::t(%249), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %250), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %248, %29), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1678:0
  %253 : int[] = prim::ListConstruct(%230, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.1
  %254 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.7, %253), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%254, %29, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %15), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:188:0
  %257 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %25, %16), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %257), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %259 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.1 # torch/tensor.py:22:0
  %260 : int[] = prim::ListConstruct(%230, %29, %29, %231), scope: __module.transformer/__module.transformer.attentions.1
  %261 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%259, %260), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%261, %scores.3), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %17), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:191:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
  %265 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %24, %18), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:1498:0
  %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%265, %20, %26), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
  %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:200:0
  %268 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %29, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %269 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%268, %28), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %270 : int[] = prim::ListConstruct(%230, %24, %21), scope: __module.transformer/__module.transformer.attentions.1
  %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%269, %270), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %272 : Tensor = prim::GetAttr[name="bias"](%226)
  %273 : Tensor = prim::GetAttr[name="weight"](%226)
  %274 : Float(2048:1, 2048:2048) = aten::t(%273), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %274), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %272, %29), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1678:0
  %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.18, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %279 : Tensor = prim::GetAttr[name="bias"](%115)
  %280 : Tensor = prim::GetAttr[name="weight"](%115)
  %281 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.1
  %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %281, %280, %279, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
  %283 : __torch__.torch.nn.modules.linear.___torch_mangle_35702.Linear = prim::GetAttr[name="lin2"](%113)
  %284 : __torch__.torch.nn.modules.linear.___torch_mangle_35701.Linear = prim::GetAttr[name="lin1"](%113)
  %285 : Tensor = prim::GetAttr[name="bias"](%284)
  %286 : Tensor = prim::GetAttr[name="weight"](%284)
  %287 : Float(2048:1, 8192:2048) = aten::t(%286), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.2, %287), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %285, %29), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1678:0
  %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.20), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:1369:0
  %291 : Tensor = prim::GetAttr[name="bias"](%283)
  %292 : Tensor = prim::GetAttr[name="weight"](%283)
  %293 : Float(8192:1, 2048:8192) = aten::t(%292), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %293), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %291, %29), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1678:0
  %296 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.22, %20, %26), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
  %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.2, %296, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %298 : Tensor = prim::GetAttr[name="bias"](%111)
  %299 : Tensor = prim::GetAttr[name="weight"](%111)
  %300 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.1
  %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %300, %299, %298, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
  %302 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %303 : Float(17:13, 13:1, 1:1) = aten::to(%302, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.3, %303), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %305 : __torch__.torch.nn.modules.linear.___torch_mangle_35637.Linear = prim::GetAttr[name="out_lin"](%109)
  %306 : __torch__.torch.nn.modules.linear.___torch_mangle_35636.Linear = prim::GetAttr[name="v_lin"](%109)
  %307 : __torch__.torch.nn.modules.linear.___torch_mangle_35635.Linear = prim::GetAttr[name="k_lin"](%109)
  %308 : __torch__.torch.nn.modules.linear.___torch_mangle_35634.Linear = prim::GetAttr[name="q_lin"](%109)
  %309 : int = aten::size(%input.24, %28), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %310 : int = aten::size(%input.24, %29), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %311 : Tensor = prim::GetAttr[name="bias"](%308)
  %312 : Tensor = prim::GetAttr[name="weight"](%308)
  %313 : Float(2048:1, 2048:2048) = aten::t(%312), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %313), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %311, %29), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1678:0
  %316 : int[] = prim::ListConstruct(%309, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.2
  %317 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.9, %316), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%317, %29, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %319 : Tensor = prim::GetAttr[name="bias"](%307)
  %320 : Tensor = prim::GetAttr[name="weight"](%307)
  %321 : Float(2048:1, 2048:2048) = aten::t(%320), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %321), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %319, %29), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1678:0
  %324 : int[] = prim::ListConstruct(%309, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.2
  %325 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.10, %324), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%325, %29, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %327 : Tensor = prim::GetAttr[name="bias"](%306)
  %328 : Tensor = prim::GetAttr[name="weight"](%306)
  %329 : Float(2048:1, 2048:2048) = aten::t(%328), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %329), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %327, %29), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1678:0
  %332 : int[] = prim::ListConstruct(%309, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.2
  %333 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.11, %332), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%333, %29, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %15), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:188:0
  %336 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %25, %16), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %336), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %338 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.2 # torch/tensor.py:22:0
  %339 : int[] = prim::ListConstruct(%309, %29, %29, %310), scope: __module.transformer/__module.transformer.attentions.2
  %340 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%338, %339), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%340, %scores.5), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %17), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:191:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
  %344 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %24, %18), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:1498:0
  %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%344, %20, %26), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
  %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:200:0
  %347 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %29, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %348 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%347, %28), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %349 : int[] = prim::ListConstruct(%309, %24, %21), scope: __module.transformer/__module.transformer.attentions.2
  %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%348, %349), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %351 : Tensor = prim::GetAttr[name="bias"](%305)
  %352 : Tensor = prim::GetAttr[name="weight"](%305)
  %353 : Float(2048:1, 2048:2048) = aten::t(%352), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %353), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %351, %29), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1678:0
  %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.28, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %358 : Tensor = prim::GetAttr[name="bias"](%107)
  %359 : Tensor = prim::GetAttr[name="weight"](%107)
  %360 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.2
  %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %360, %359, %358, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
  %362 : __torch__.torch.nn.modules.linear.___torch_mangle_35705.Linear = prim::GetAttr[name="lin2"](%105)
  %363 : __torch__.torch.nn.modules.linear.___torch_mangle_35704.Linear = prim::GetAttr[name="lin1"](%105)
  %364 : Tensor = prim::GetAttr[name="bias"](%363)
  %365 : Tensor = prim::GetAttr[name="weight"](%363)
  %366 : Float(2048:1, 8192:2048) = aten::t(%365), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.3, %366), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %364, %29), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1678:0
  %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.30), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:1369:0
  %370 : Tensor = prim::GetAttr[name="bias"](%362)
  %371 : Tensor = prim::GetAttr[name="weight"](%362)
  %372 : Float(8192:1, 2048:8192) = aten::t(%371), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %372), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %370, %29), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1678:0
  %375 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.32, %20, %26), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
  %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.3, %375, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %377 : Tensor = prim::GetAttr[name="bias"](%103)
  %378 : Tensor = prim::GetAttr[name="weight"](%103)
  %379 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.2
  %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %379, %378, %377, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
  %381 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %382 : Float(17:13, 13:1, 1:1) = aten::to(%381, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.4, %382), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %384 : __torch__.torch.nn.modules.linear.___torch_mangle_35642.Linear = prim::GetAttr[name="out_lin"](%101)
  %385 : __torch__.torch.nn.modules.linear.___torch_mangle_35641.Linear = prim::GetAttr[name="v_lin"](%101)
  %386 : __torch__.torch.nn.modules.linear.___torch_mangle_35640.Linear = prim::GetAttr[name="k_lin"](%101)
  %387 : __torch__.torch.nn.modules.linear.___torch_mangle_35639.Linear = prim::GetAttr[name="q_lin"](%101)
  %388 : int = aten::size(%input.34, %28), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %389 : int = aten::size(%input.34, %29), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %390 : Tensor = prim::GetAttr[name="bias"](%387)
  %391 : Tensor = prim::GetAttr[name="weight"](%387)
  %392 : Float(2048:1, 2048:2048) = aten::t(%391), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %392), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %390, %29), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1678:0
  %395 : int[] = prim::ListConstruct(%388, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.3
  %396 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.13, %395), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%396, %29, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %398 : Tensor = prim::GetAttr[name="bias"](%386)
  %399 : Tensor = prim::GetAttr[name="weight"](%386)
  %400 : Float(2048:1, 2048:2048) = aten::t(%399), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %400), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %398, %29), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1678:0
  %403 : int[] = prim::ListConstruct(%388, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.3
  %404 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.14, %403), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%404, %29, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %406 : Tensor = prim::GetAttr[name="bias"](%385)
  %407 : Tensor = prim::GetAttr[name="weight"](%385)
  %408 : Float(2048:1, 2048:2048) = aten::t(%407), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %408), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %406, %29), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1678:0
  %411 : int[] = prim::ListConstruct(%388, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.3
  %412 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.15, %411), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%412, %29, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %15), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:188:0
  %415 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %25, %16), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %415), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %417 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.3 # torch/tensor.py:22:0
  %418 : int[] = prim::ListConstruct(%388, %29, %29, %389), scope: __module.transformer/__module.transformer.attentions.3
  %419 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%417, %418), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%419, %scores.7), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %17), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:191:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
  %423 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %24, %18), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:1498:0
  %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%423, %20, %26), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
  %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:200:0
  %426 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %29, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %427 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%426, %28), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %428 : int[] = prim::ListConstruct(%388, %24, %21), scope: __module.transformer/__module.transformer.attentions.3
  %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%427, %428), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %430 : Tensor = prim::GetAttr[name="bias"](%384)
  %431 : Tensor = prim::GetAttr[name="weight"](%384)
  %432 : Float(2048:1, 2048:2048) = aten::t(%431), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %432), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %430, %29), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1678:0
  %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.38, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %437 : Tensor = prim::GetAttr[name="bias"](%99)
  %438 : Tensor = prim::GetAttr[name="weight"](%99)
  %439 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.3
  %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %439, %438, %437, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
  %441 : __torch__.torch.nn.modules.linear.___torch_mangle_35708.Linear = prim::GetAttr[name="lin2"](%97)
  %442 : __torch__.torch.nn.modules.linear.___torch_mangle_35707.Linear = prim::GetAttr[name="lin1"](%97)
  %443 : Tensor = prim::GetAttr[name="bias"](%442)
  %444 : Tensor = prim::GetAttr[name="weight"](%442)
  %445 : Float(2048:1, 8192:2048) = aten::t(%444), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.4, %445), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %443, %29), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1678:0
  %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.40), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:1369:0
  %449 : Tensor = prim::GetAttr[name="bias"](%441)
  %450 : Tensor = prim::GetAttr[name="weight"](%441)
  %451 : Float(8192:1, 2048:8192) = aten::t(%450), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %451), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %449, %29), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1678:0
  %454 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.42, %20, %26), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
  %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.4, %454, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %456 : Tensor = prim::GetAttr[name="bias"](%95)
  %457 : Tensor = prim::GetAttr[name="weight"](%95)
  %458 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.3
  %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %458, %457, %456, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
  %460 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %461 : Float(17:13, 13:1, 1:1) = aten::to(%460, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.5, %461), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %463 : __torch__.torch.nn.modules.linear.___torch_mangle_35647.Linear = prim::GetAttr[name="out_lin"](%93)
  %464 : __torch__.torch.nn.modules.linear.___torch_mangle_35646.Linear = prim::GetAttr[name="v_lin"](%93)
  %465 : __torch__.torch.nn.modules.linear.___torch_mangle_35645.Linear = prim::GetAttr[name="k_lin"](%93)
  %466 : __torch__.torch.nn.modules.linear.___torch_mangle_35644.Linear = prim::GetAttr[name="q_lin"](%93)
  %467 : int = aten::size(%input.44, %28), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %468 : int = aten::size(%input.44, %29), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %469 : Tensor = prim::GetAttr[name="bias"](%466)
  %470 : Tensor = prim::GetAttr[name="weight"](%466)
  %471 : Float(2048:1, 2048:2048) = aten::t(%470), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %471), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %469, %29), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1678:0
  %474 : int[] = prim::ListConstruct(%467, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.4
  %475 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.17, %474), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%475, %29, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %477 : Tensor = prim::GetAttr[name="bias"](%465)
  %478 : Tensor = prim::GetAttr[name="weight"](%465)
  %479 : Float(2048:1, 2048:2048) = aten::t(%478), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %479), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %477, %29), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1678:0
  %482 : int[] = prim::ListConstruct(%467, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.4
  %483 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.18, %482), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%483, %29, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %485 : Tensor = prim::GetAttr[name="bias"](%464)
  %486 : Tensor = prim::GetAttr[name="weight"](%464)
  %487 : Float(2048:1, 2048:2048) = aten::t(%486), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %487), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %485, %29), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1678:0
  %490 : int[] = prim::ListConstruct(%467, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.4
  %491 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.19, %490), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%491, %29, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %15), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:188:0
  %494 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %25, %16), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %494), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %496 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.4 # torch/tensor.py:22:0
  %497 : int[] = prim::ListConstruct(%467, %29, %29, %468), scope: __module.transformer/__module.transformer.attentions.4
  %498 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%496, %497), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%498, %scores.9), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %17), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:191:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
  %502 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %24, %18), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:1498:0
  %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%502, %20, %26), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
  %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:200:0
  %505 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %29, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %506 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%505, %28), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %507 : int[] = prim::ListConstruct(%467, %24, %21), scope: __module.transformer/__module.transformer.attentions.4
  %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%506, %507), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %509 : Tensor = prim::GetAttr[name="bias"](%463)
  %510 : Tensor = prim::GetAttr[name="weight"](%463)
  %511 : Float(2048:1, 2048:2048) = aten::t(%510), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %511), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %509, %29), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1678:0
  %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.48, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %516 : Tensor = prim::GetAttr[name="bias"](%91)
  %517 : Tensor = prim::GetAttr[name="weight"](%91)
  %518 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.4
  %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %518, %517, %516, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
  %520 : __torch__.torch.nn.modules.linear.___torch_mangle_35711.Linear = prim::GetAttr[name="lin2"](%89)
  %521 : __torch__.torch.nn.modules.linear.___torch_mangle_35710.Linear = prim::GetAttr[name="lin1"](%89)
  %522 : Tensor = prim::GetAttr[name="bias"](%521)
  %523 : Tensor = prim::GetAttr[name="weight"](%521)
  %524 : Float(2048:1, 8192:2048) = aten::t(%523), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.5, %524), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %522, %29), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1678:0
  %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.50), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:1369:0
  %528 : Tensor = prim::GetAttr[name="bias"](%520)
  %529 : Tensor = prim::GetAttr[name="weight"](%520)
  %530 : Float(8192:1, 2048:8192) = aten::t(%529), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %530), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %528, %29), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1678:0
  %533 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.52, %20, %26), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
  %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.5, %533, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %535 : Tensor = prim::GetAttr[name="bias"](%87)
  %536 : Tensor = prim::GetAttr[name="weight"](%87)
  %537 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.4
  %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %537, %536, %535, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
  %539 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %540 : Float(17:13, 13:1, 1:1) = aten::to(%539, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.6, %540), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %542 : __torch__.torch.nn.modules.linear.___torch_mangle_35652.Linear = prim::GetAttr[name="out_lin"](%85)
  %543 : __torch__.torch.nn.modules.linear.___torch_mangle_35651.Linear = prim::GetAttr[name="v_lin"](%85)
  %544 : __torch__.torch.nn.modules.linear.___torch_mangle_35650.Linear = prim::GetAttr[name="k_lin"](%85)
  %545 : __torch__.torch.nn.modules.linear.___torch_mangle_35649.Linear = prim::GetAttr[name="q_lin"](%85)
  %546 : int = aten::size(%input.54, %28), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %547 : int = aten::size(%input.54, %29), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %548 : Tensor = prim::GetAttr[name="bias"](%545)
  %549 : Tensor = prim::GetAttr[name="weight"](%545)
  %550 : Float(2048:1, 2048:2048) = aten::t(%549), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %550), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %548, %29), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1678:0
  %553 : int[] = prim::ListConstruct(%546, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.5
  %554 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.21, %553), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%554, %29, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %556 : Tensor = prim::GetAttr[name="bias"](%544)
  %557 : Tensor = prim::GetAttr[name="weight"](%544)
  %558 : Float(2048:1, 2048:2048) = aten::t(%557), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %558), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %556, %29), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1678:0
  %561 : int[] = prim::ListConstruct(%546, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.5
  %562 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.22, %561), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%562, %29, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %564 : Tensor = prim::GetAttr[name="bias"](%543)
  %565 : Tensor = prim::GetAttr[name="weight"](%543)
  %566 : Float(2048:1, 2048:2048) = aten::t(%565), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %566), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %564, %29), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1678:0
  %569 : int[] = prim::ListConstruct(%546, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.5
  %570 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.23, %569), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%570, %29, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %15), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:188:0
  %573 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %25, %16), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %573), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %575 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.5 # torch/tensor.py:22:0
  %576 : int[] = prim::ListConstruct(%546, %29, %29, %547), scope: __module.transformer/__module.transformer.attentions.5
  %577 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%575, %576), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%577, %scores.11), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %17), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:191:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
  %581 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %24, %18), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:1498:0
  %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%581, %20, %26), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
  %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:200:0
  %584 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %29, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %585 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%584, %28), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %586 : int[] = prim::ListConstruct(%546, %24, %21), scope: __module.transformer/__module.transformer.attentions.5
  %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%585, %586), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %588 : Tensor = prim::GetAttr[name="bias"](%542)
  %589 : Tensor = prim::GetAttr[name="weight"](%542)
  %590 : Float(2048:1, 2048:2048) = aten::t(%589), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %590), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %588, %29), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1678:0
  %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.58, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %595 : Tensor = prim::GetAttr[name="bias"](%83)
  %596 : Tensor = prim::GetAttr[name="weight"](%83)
  %597 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.5
  %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %597, %596, %595, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
  %599 : __torch__.torch.nn.modules.linear.___torch_mangle_35714.Linear = prim::GetAttr[name="lin2"](%81)
  %600 : __torch__.torch.nn.modules.linear.___torch_mangle_35713.Linear = prim::GetAttr[name="lin1"](%81)
  %601 : Tensor = prim::GetAttr[name="bias"](%600)
  %602 : Tensor = prim::GetAttr[name="weight"](%600)
  %603 : Float(2048:1, 8192:2048) = aten::t(%602), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.6, %603), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %601, %29), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1678:0
  %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.60), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:1369:0
  %607 : Tensor = prim::GetAttr[name="bias"](%599)
  %608 : Tensor = prim::GetAttr[name="weight"](%599)
  %609 : Float(8192:1, 2048:8192) = aten::t(%608), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %609), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %607, %29), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1678:0
  %612 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.62, %20, %26), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
  %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.6, %612, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %614 : Tensor = prim::GetAttr[name="bias"](%79)
  %615 : Tensor = prim::GetAttr[name="weight"](%79)
  %616 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.5
  %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %616, %615, %614, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
  %618 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %619 : Float(17:13, 13:1, 1:1) = aten::to(%618, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.7, %619), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %621 : __torch__.torch.nn.modules.linear.___torch_mangle_35657.Linear = prim::GetAttr[name="out_lin"](%77)
  %622 : __torch__.torch.nn.modules.linear.___torch_mangle_35656.Linear = prim::GetAttr[name="v_lin"](%77)
  %623 : __torch__.torch.nn.modules.linear.___torch_mangle_35655.Linear = prim::GetAttr[name="k_lin"](%77)
  %624 : __torch__.torch.nn.modules.linear.___torch_mangle_35654.Linear = prim::GetAttr[name="q_lin"](%77)
  %625 : int = aten::size(%input.64, %28), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %626 : int = aten::size(%input.64, %29), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %627 : Tensor = prim::GetAttr[name="bias"](%624)
  %628 : Tensor = prim::GetAttr[name="weight"](%624)
  %629 : Float(2048:1, 2048:2048) = aten::t(%628), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %629), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %627, %29), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1678:0
  %632 : int[] = prim::ListConstruct(%625, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.6
  %633 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.25, %632), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%633, %29, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %635 : Tensor = prim::GetAttr[name="bias"](%623)
  %636 : Tensor = prim::GetAttr[name="weight"](%623)
  %637 : Float(2048:1, 2048:2048) = aten::t(%636), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %637), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %635, %29), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1678:0
  %640 : int[] = prim::ListConstruct(%625, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.6
  %641 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.26, %640), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%641, %29, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %643 : Tensor = prim::GetAttr[name="bias"](%622)
  %644 : Tensor = prim::GetAttr[name="weight"](%622)
  %645 : Float(2048:1, 2048:2048) = aten::t(%644), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %645), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %643, %29), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1678:0
  %648 : int[] = prim::ListConstruct(%625, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.6
  %649 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.27, %648), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%649, %29, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %15), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:188:0
  %652 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %25, %16), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %652), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %654 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.6 # torch/tensor.py:22:0
  %655 : int[] = prim::ListConstruct(%625, %29, %29, %626), scope: __module.transformer/__module.transformer.attentions.6
  %656 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%654, %655), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%656, %scores.13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %17), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:191:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
  %660 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %24, %18), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:1498:0
  %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%660, %20, %26), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
  %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:200:0
  %663 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %29, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %664 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%663, %28), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %665 : int[] = prim::ListConstruct(%625, %24, %21), scope: __module.transformer/__module.transformer.attentions.6
  %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%664, %665), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %667 : Tensor = prim::GetAttr[name="bias"](%621)
  %668 : Tensor = prim::GetAttr[name="weight"](%621)
  %669 : Float(2048:1, 2048:2048) = aten::t(%668), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %669), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %667, %29), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1678:0
  %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.68, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %674 : Tensor = prim::GetAttr[name="bias"](%75)
  %675 : Tensor = prim::GetAttr[name="weight"](%75)
  %676 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.6
  %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %676, %675, %674, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
  %678 : __torch__.torch.nn.modules.linear.___torch_mangle_35717.Linear = prim::GetAttr[name="lin2"](%73)
  %679 : __torch__.torch.nn.modules.linear.___torch_mangle_35716.Linear = prim::GetAttr[name="lin1"](%73)
  %680 : Tensor = prim::GetAttr[name="bias"](%679)
  %681 : Tensor = prim::GetAttr[name="weight"](%679)
  %682 : Float(2048:1, 8192:2048) = aten::t(%681), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.7, %682), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %680, %29), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1678:0
  %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:1369:0
  %686 : Tensor = prim::GetAttr[name="bias"](%678)
  %687 : Tensor = prim::GetAttr[name="weight"](%678)
  %688 : Float(8192:1, 2048:8192) = aten::t(%687), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %688), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %686, %29), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1678:0
  %691 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.72, %20, %26), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
  %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.7, %691, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %693 : Tensor = prim::GetAttr[name="bias"](%71)
  %694 : Tensor = prim::GetAttr[name="weight"](%71)
  %695 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.6
  %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %695, %694, %693, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
  %697 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %698 : Float(17:13, 13:1, 1:1) = aten::to(%697, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.8, %698), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %700 : __torch__.torch.nn.modules.linear.___torch_mangle_35662.Linear = prim::GetAttr[name="out_lin"](%69)
  %701 : __torch__.torch.nn.modules.linear.___torch_mangle_35661.Linear = prim::GetAttr[name="v_lin"](%69)
  %702 : __torch__.torch.nn.modules.linear.___torch_mangle_35660.Linear = prim::GetAttr[name="k_lin"](%69)
  %703 : __torch__.torch.nn.modules.linear.___torch_mangle_35659.Linear = prim::GetAttr[name="q_lin"](%69)
  %704 : int = aten::size(%input.74, %28), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %705 : int = aten::size(%input.74, %29), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %706 : Tensor = prim::GetAttr[name="bias"](%703)
  %707 : Tensor = prim::GetAttr[name="weight"](%703)
  %708 : Float(2048:1, 2048:2048) = aten::t(%707), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %708), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %706, %29), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1678:0
  %711 : int[] = prim::ListConstruct(%704, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.7
  %712 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.29, %711), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%712, %29, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %714 : Tensor = prim::GetAttr[name="bias"](%702)
  %715 : Tensor = prim::GetAttr[name="weight"](%702)
  %716 : Float(2048:1, 2048:2048) = aten::t(%715), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %716), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %714, %29), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1678:0
  %719 : int[] = prim::ListConstruct(%704, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.7
  %720 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.30, %719), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%720, %29, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %722 : Tensor = prim::GetAttr[name="bias"](%701)
  %723 : Tensor = prim::GetAttr[name="weight"](%701)
  %724 : Float(2048:1, 2048:2048) = aten::t(%723), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %724), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %722, %29), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1678:0
  %727 : int[] = prim::ListConstruct(%704, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.7
  %728 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.31, %727), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%728, %29, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:188:0
  %731 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %25, %16), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %731), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %733 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.7 # torch/tensor.py:22:0
  %734 : int[] = prim::ListConstruct(%704, %29, %29, %705), scope: __module.transformer/__module.transformer.attentions.7
  %735 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%733, %734), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%735, %scores.15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %17), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:191:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
  %739 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %24, %18), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:1498:0
  %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%739, %20, %26), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
  %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:200:0
  %742 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %29, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %743 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%742, %28), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %744 : int[] = prim::ListConstruct(%704, %24, %21), scope: __module.transformer/__module.transformer.attentions.7
  %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%743, %744), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %746 : Tensor = prim::GetAttr[name="bias"](%700)
  %747 : Tensor = prim::GetAttr[name="weight"](%700)
  %748 : Float(2048:1, 2048:2048) = aten::t(%747), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %748), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %746, %29), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1678:0
  %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.78, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %753 : Tensor = prim::GetAttr[name="bias"](%67)
  %754 : Tensor = prim::GetAttr[name="weight"](%67)
  %755 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.7
  %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %755, %754, %753, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
  %757 : __torch__.torch.nn.modules.linear.___torch_mangle_35720.Linear = prim::GetAttr[name="lin2"](%65)
  %758 : __torch__.torch.nn.modules.linear.___torch_mangle_35719.Linear = prim::GetAttr[name="lin1"](%65)
  %759 : Tensor = prim::GetAttr[name="bias"](%758)
  %760 : Tensor = prim::GetAttr[name="weight"](%758)
  %761 : Float(2048:1, 8192:2048) = aten::t(%760), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.8, %761), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %759, %29), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1678:0
  %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.80), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:1369:0
  %765 : Tensor = prim::GetAttr[name="bias"](%757)
  %766 : Tensor = prim::GetAttr[name="weight"](%757)
  %767 : Float(8192:1, 2048:8192) = aten::t(%766), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %767), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %765, %29), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1678:0
  %770 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.82, %20, %26), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
  %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.8, %770, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %772 : Tensor = prim::GetAttr[name="bias"](%63)
  %773 : Tensor = prim::GetAttr[name="weight"](%63)
  %774 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.7
  %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %774, %773, %772, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
  %776 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %777 : Float(17:13, 13:1, 1:1) = aten::to(%776, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.9, %777), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %779 : __torch__.torch.nn.modules.linear.___torch_mangle_35667.Linear = prim::GetAttr[name="out_lin"](%61)
  %780 : __torch__.torch.nn.modules.linear.___torch_mangle_35666.Linear = prim::GetAttr[name="v_lin"](%61)
  %781 : __torch__.torch.nn.modules.linear.___torch_mangle_35665.Linear = prim::GetAttr[name="k_lin"](%61)
  %782 : __torch__.torch.nn.modules.linear.___torch_mangle_35664.Linear = prim::GetAttr[name="q_lin"](%61)
  %783 : int = aten::size(%input.84, %28), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %784 : int = aten::size(%input.84, %29), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %785 : Tensor = prim::GetAttr[name="bias"](%782)
  %786 : Tensor = prim::GetAttr[name="weight"](%782)
  %787 : Float(2048:1, 2048:2048) = aten::t(%786), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %787), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %785, %29), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1678:0
  %790 : int[] = prim::ListConstruct(%783, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.8
  %791 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.33, %790), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%791, %29, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %793 : Tensor = prim::GetAttr[name="bias"](%781)
  %794 : Tensor = prim::GetAttr[name="weight"](%781)
  %795 : Float(2048:1, 2048:2048) = aten::t(%794), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %795), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %793, %29), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1678:0
  %798 : int[] = prim::ListConstruct(%783, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.8
  %799 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.34, %798), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%799, %29, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %801 : Tensor = prim::GetAttr[name="bias"](%780)
  %802 : Tensor = prim::GetAttr[name="weight"](%780)
  %803 : Float(2048:1, 2048:2048) = aten::t(%802), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %803), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %801, %29), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1678:0
  %806 : int[] = prim::ListConstruct(%783, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.8
  %807 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.35, %806), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%807, %29, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %15), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:188:0
  %810 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %25, %16), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %810), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %812 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.8 # torch/tensor.py:22:0
  %813 : int[] = prim::ListConstruct(%783, %29, %29, %784), scope: __module.transformer/__module.transformer.attentions.8
  %814 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%812, %813), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%814, %scores.17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:191:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
  %818 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %24, %18), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:1498:0
  %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%818, %20, %26), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
  %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:200:0
  %821 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %29, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %822 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%821, %28), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %823 : int[] = prim::ListConstruct(%783, %24, %21), scope: __module.transformer/__module.transformer.attentions.8
  %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%822, %823), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %825 : Tensor = prim::GetAttr[name="bias"](%779)
  %826 : Tensor = prim::GetAttr[name="weight"](%779)
  %827 : Float(2048:1, 2048:2048) = aten::t(%826), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %827), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %825, %29), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1678:0
  %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.88, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %832 : Tensor = prim::GetAttr[name="bias"](%59)
  %833 : Tensor = prim::GetAttr[name="weight"](%59)
  %834 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.8
  %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %834, %833, %832, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
  %836 : __torch__.torch.nn.modules.linear.___torch_mangle_35723.Linear = prim::GetAttr[name="lin2"](%57)
  %837 : __torch__.torch.nn.modules.linear.___torch_mangle_35722.Linear = prim::GetAttr[name="lin1"](%57)
  %838 : Tensor = prim::GetAttr[name="bias"](%837)
  %839 : Tensor = prim::GetAttr[name="weight"](%837)
  %840 : Float(2048:1, 8192:2048) = aten::t(%839), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.9, %840), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %838, %29), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1678:0
  %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.90), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:1369:0
  %844 : Tensor = prim::GetAttr[name="bias"](%836)
  %845 : Tensor = prim::GetAttr[name="weight"](%836)
  %846 : Float(8192:1, 2048:8192) = aten::t(%845), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %846), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %844, %29), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1678:0
  %849 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.92, %20, %26), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
  %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.9, %849, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %851 : Tensor = prim::GetAttr[name="bias"](%55)
  %852 : Tensor = prim::GetAttr[name="weight"](%55)
  %853 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.8
  %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %853, %852, %851, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
  %855 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %856 : Float(17:13, 13:1, 1:1) = aten::to(%855, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.10, %856), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %858 : __torch__.torch.nn.modules.linear.___torch_mangle_35672.Linear = prim::GetAttr[name="out_lin"](%53)
  %859 : __torch__.torch.nn.modules.linear.___torch_mangle_35671.Linear = prim::GetAttr[name="v_lin"](%53)
  %860 : __torch__.torch.nn.modules.linear.___torch_mangle_35670.Linear = prim::GetAttr[name="k_lin"](%53)
  %861 : __torch__.torch.nn.modules.linear.___torch_mangle_35669.Linear = prim::GetAttr[name="q_lin"](%53)
  %862 : int = aten::size(%input.94, %28), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %863 : int = aten::size(%input.94, %29), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %864 : Tensor = prim::GetAttr[name="bias"](%861)
  %865 : Tensor = prim::GetAttr[name="weight"](%861)
  %866 : Float(2048:1, 2048:2048) = aten::t(%865), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %866), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %864, %29), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1678:0
  %869 : int[] = prim::ListConstruct(%862, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.9
  %870 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.37, %869), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%870, %29, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %872 : Tensor = prim::GetAttr[name="bias"](%860)
  %873 : Tensor = prim::GetAttr[name="weight"](%860)
  %874 : Float(2048:1, 2048:2048) = aten::t(%873), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %874), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %872, %29), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1678:0
  %877 : int[] = prim::ListConstruct(%862, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.9
  %878 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.38, %877), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%878, %29, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %880 : Tensor = prim::GetAttr[name="bias"](%859)
  %881 : Tensor = prim::GetAttr[name="weight"](%859)
  %882 : Float(2048:1, 2048:2048) = aten::t(%881), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %882), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %880, %29), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1678:0
  %885 : int[] = prim::ListConstruct(%862, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.9
  %886 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.39, %885), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%886, %29, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %15), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:188:0
  %889 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %25, %16), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %889), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %891 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.9 # torch/tensor.py:22:0
  %892 : int[] = prim::ListConstruct(%862, %29, %29, %863), scope: __module.transformer/__module.transformer.attentions.9
  %893 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%891, %892), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%893, %scores.19), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %17), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:191:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
  %897 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %24, %18), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:1498:0
  %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%897, %20, %26), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
  %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:200:0
  %900 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %29, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %901 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%900, %28), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %902 : int[] = prim::ListConstruct(%862, %24, %21), scope: __module.transformer/__module.transformer.attentions.9
  %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%901, %902), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %904 : Tensor = prim::GetAttr[name="bias"](%858)
  %905 : Tensor = prim::GetAttr[name="weight"](%858)
  %906 : Float(2048:1, 2048:2048) = aten::t(%905), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %906), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %904, %29), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1678:0
  %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.98, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %911 : Tensor = prim::GetAttr[name="bias"](%51)
  %912 : Tensor = prim::GetAttr[name="weight"](%51)
  %913 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.9
  %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %913, %912, %911, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
  %915 : __torch__.torch.nn.modules.linear.___torch_mangle_35726.Linear = prim::GetAttr[name="lin2"](%49)
  %916 : __torch__.torch.nn.modules.linear.___torch_mangle_35725.Linear = prim::GetAttr[name="lin1"](%49)
  %917 : Tensor = prim::GetAttr[name="bias"](%916)
  %918 : Tensor = prim::GetAttr[name="weight"](%916)
  %919 : Float(2048:1, 8192:2048) = aten::t(%918), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.10, %919), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %917, %29), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1678:0
  %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.100), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:1369:0
  %923 : Tensor = prim::GetAttr[name="bias"](%915)
  %924 : Tensor = prim::GetAttr[name="weight"](%915)
  %925 : Float(8192:1, 2048:8192) = aten::t(%924), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %925), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %923, %29), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1678:0
  %928 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.102, %20, %26), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
  %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.10, %928, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %930 : Tensor = prim::GetAttr[name="bias"](%47)
  %931 : Tensor = prim::GetAttr[name="weight"](%47)
  %932 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.9
  %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %932, %931, %930, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
  %934 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %935 : Float(17:13, 13:1, 1:1) = aten::to(%934, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.11, %935), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %937 : __torch__.torch.nn.modules.linear.___torch_mangle_35677.Linear = prim::GetAttr[name="out_lin"](%45)
  %938 : __torch__.torch.nn.modules.linear.___torch_mangle_35676.Linear = prim::GetAttr[name="v_lin"](%45)
  %939 : __torch__.torch.nn.modules.linear.___torch_mangle_35675.Linear = prim::GetAttr[name="k_lin"](%45)
  %940 : __torch__.torch.nn.modules.linear.___torch_mangle_35674.Linear = prim::GetAttr[name="q_lin"](%45)
  %941 : int = aten::size(%input.104, %28), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %942 : int = aten::size(%input.104, %29), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %943 : Tensor = prim::GetAttr[name="bias"](%940)
  %944 : Tensor = prim::GetAttr[name="weight"](%940)
  %945 : Float(2048:1, 2048:2048) = aten::t(%944), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %945), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %943, %29), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1678:0
  %948 : int[] = prim::ListConstruct(%941, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.10
  %949 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.41, %948), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%949, %29, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %951 : Tensor = prim::GetAttr[name="bias"](%939)
  %952 : Tensor = prim::GetAttr[name="weight"](%939)
  %953 : Float(2048:1, 2048:2048) = aten::t(%952), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %953), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %951, %29), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1678:0
  %956 : int[] = prim::ListConstruct(%941, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.10
  %957 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.42, %956), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%957, %29, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %959 : Tensor = prim::GetAttr[name="bias"](%938)
  %960 : Tensor = prim::GetAttr[name="weight"](%938)
  %961 : Float(2048:1, 2048:2048) = aten::t(%960), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %961), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %959, %29), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1678:0
  %964 : int[] = prim::ListConstruct(%941, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.10
  %965 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.43, %964), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%965, %29, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %15), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:188:0
  %968 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %25, %16), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %968), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %970 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.10 # torch/tensor.py:22:0
  %971 : int[] = prim::ListConstruct(%941, %29, %29, %942), scope: __module.transformer/__module.transformer.attentions.10
  %972 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%970, %971), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%972, %scores.21), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %17), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:191:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
  %976 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %24, %18), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:1498:0
  %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%976, %20, %26), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
  %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:200:0
  %979 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %29, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %980 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%979, %28), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %981 : int[] = prim::ListConstruct(%941, %24, %21), scope: __module.transformer/__module.transformer.attentions.10
  %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%980, %981), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %983 : Tensor = prim::GetAttr[name="bias"](%937)
  %984 : Tensor = prim::GetAttr[name="weight"](%937)
  %985 : Float(2048:1, 2048:2048) = aten::t(%984), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %985), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %983, %29), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1678:0
  %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.108, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %990 : Tensor = prim::GetAttr[name="bias"](%43)
  %991 : Tensor = prim::GetAttr[name="weight"](%43)
  %992 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.10
  %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %992, %991, %990, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
  %994 : __torch__.torch.nn.modules.linear.___torch_mangle_35729.Linear = prim::GetAttr[name="lin2"](%41)
  %995 : __torch__.torch.nn.modules.linear.___torch_mangle_35728.Linear = prim::GetAttr[name="lin1"](%41)
  %996 : Tensor = prim::GetAttr[name="bias"](%995)
  %997 : Tensor = prim::GetAttr[name="weight"](%995)
  %998 : Float(2048:1, 8192:2048) = aten::t(%997), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.11, %998), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %996, %29), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1678:0
  %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.110), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:1369:0
  %1002 : Tensor = prim::GetAttr[name="bias"](%994)
  %1003 : Tensor = prim::GetAttr[name="weight"](%994)
  %1004 : Float(8192:1, 2048:8192) = aten::t(%1003), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %1004), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %1002, %29), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1678:0
  %1007 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.112, %20, %26), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
  %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.11, %1007, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %1009 : Tensor = prim::GetAttr[name="bias"](%39)
  %1010 : Tensor = prim::GetAttr[name="weight"](%39)
  %1011 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.10
  %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %1011, %1010, %1009, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1013 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %1014 : Float(17:13, 13:1, 1:1) = aten::to(%1013, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.12, %1014), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %1016 : __torch__.torch.nn.modules.linear.___torch_mangle_35682.Linear = prim::GetAttr[name="out_lin"](%37)
  %1017 : __torch__.torch.nn.modules.linear.___torch_mangle_35681.Linear = prim::GetAttr[name="v_lin"](%37)
  %1018 : __torch__.torch.nn.modules.linear.___torch_mangle_35680.Linear = prim::GetAttr[name="k_lin"](%37)
  %1019 : __torch__.torch.nn.modules.linear.___torch_mangle_35679.Linear = prim::GetAttr[name="q_lin"](%37)
  %1020 : int = aten::size(%input.114, %28), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1021 : int = aten::size(%input.114, %29), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1022 : Tensor = prim::GetAttr[name="bias"](%1019)
  %1023 : Tensor = prim::GetAttr[name="weight"](%1019)
  %1024 : Float(2048:1, 2048:2048) = aten::t(%1023), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1024), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %1022, %29), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1678:0
  %1027 : int[] = prim::ListConstruct(%1020, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.11
  %1028 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.45, %1027), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1028, %29, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1030 : Tensor = prim::GetAttr[name="bias"](%1018)
  %1031 : Tensor = prim::GetAttr[name="weight"](%1018)
  %1032 : Float(2048:1, 2048:2048) = aten::t(%1031), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1032), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %1030, %29), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1678:0
  %1035 : int[] = prim::ListConstruct(%1020, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.11
  %1036 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.46, %1035), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1036, %29, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1038 : Tensor = prim::GetAttr[name="bias"](%1017)
  %1039 : Tensor = prim::GetAttr[name="weight"](%1017)
  %1040 : Float(2048:1, 2048:2048) = aten::t(%1039), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1040), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %1038, %29), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1678:0
  %1043 : int[] = prim::ListConstruct(%1020, %24, %13, %14), scope: __module.transformer/__module.transformer.attentions.11
  %1044 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.47, %1043), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1044, %29, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %15), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:188:0
  %1047 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %25, %16), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %1047), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %1049 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %28), scope: __module.transformer/__module.transformer.attentions.11 # torch/tensor.py:22:0
  %1050 : int[] = prim::ListConstruct(%1020, %29, %29, %1021), scope: __module.transformer/__module.transformer.attentions.11
  %1051 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1049, %1050), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1051, %scores.23), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %17), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:191:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %19, %26, %26, %18), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
  %1055 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %24, %18), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:1498:0
  %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1055, %20, %26), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
  %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:200:0
  %1058 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %29, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1059 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1058, %28), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1060 : int[] = prim::ListConstruct(%1020, %24, %21), scope: __module.transformer/__module.transformer.attentions.11
  %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1059, %1060), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1062 : Tensor = prim::GetAttr[name="bias"](%1016)
  %1063 : Tensor = prim::GetAttr[name="weight"](%1016)
  %1064 : Float(2048:1, 2048:2048) = aten::t(%1063), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %1064), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %1062, %29), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1678:0
  %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.118, %20, %26), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %29), scope: __module.transformer # transformers/modeling_xlm.py:601:0
  %1069 : Tensor = prim::GetAttr[name="bias"](%35)
  %1070 : Tensor = prim::GetAttr[name="weight"](%35)
  %1071 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm1.11
  %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %1071, %1070, %1069, %22, %23), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1073 : __torch__.torch.nn.modules.linear.___torch_mangle_35732.Linear = prim::GetAttr[name="lin2"](%33)
  %1074 : __torch__.torch.nn.modules.linear.___torch_mangle_35731.Linear = prim::GetAttr[name="lin1"](%33)
  %1075 : Tensor = prim::GetAttr[name="bias"](%1074)
  %1076 : Tensor = prim::GetAttr[name="weight"](%1074)
  %1077 : Float(2048:1, 8192:2048) = aten::t(%1076), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor, %1077), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %1075, %29), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1678:0
  %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.120), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:1369:0
  %1081 : Tensor = prim::GetAttr[name="bias"](%1073)
  %1082 : Tensor = prim::GetAttr[name="weight"](%1073)
  %1083 : Float(8192:1, 2048:8192) = aten::t(%1082), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %output.72 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %1083), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.72, %1081, %29), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1678:0
  %1086 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.122, %20, %26), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
  %input.123 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor, %1086, %29), scope: __module.transformer # transformers/modeling_xlm.py:612:0
  %1088 : Tensor = prim::GetAttr[name="bias"](%31)
  %1089 : Tensor = prim::GetAttr[name="weight"](%31)
  %1090 : int[] = prim::ListConstruct(%21), scope: __module.transformer/__module.transformer.layer_norm2.11
  %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.123, %1090, %1089, %1088, %22, %23), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1092 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %24), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %1093 : Float(17:13, 13:1, 1:1) = aten::to(%1092, %19, %26, %26, %18), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %input.124 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor, %1093), scope: __module.transformer # transformers/modeling_xlm.py:614:0
  %1095 : int = prim::Constant[value=0](), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %1096 : int = prim::Constant[value=9223372036854775807](), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %1097 : str = prim::Constant[value="blh,bl->bh"](), scope: __module.qa_outputs # torch/functional.py:327:0
  %1098 : int = prim::Constant[value=25](), scope: __module.qa_outputs # transformers/modeling_utils.py:1395:0
  %1099 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %1100 : int = prim::Constant[value=2048](), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %1101 : int = prim::Constant[value=-2](), scope: __module.qa_outputs # transformers/modeling_utils.py:1382:0
  %1102 : bool = prim::Constant[value=0](), scope: __module.qa_outputs # transformers/modeling_utils.py:1381:0
  %1103 : bool = prim::Constant[value=1](), scope: __module.qa_outputs # transformers/modeling_utils.py:1378:0
  %1104 : int = prim::Constant[value=5](), scope: __module.qa_outputs # transformers/modeling_utils.py:1378:0
  %1105 : None = prim::Constant(), scope: __module.qa_outputs
  %1106 : int = prim::Constant[value=2](), scope: __module.qa_outputs # transformers/modeling_utils.py:1375:0
  %1107 : int = prim::Constant[value=1](), scope: __module.qa_outputs/__module.qa_outputs.start_logits/__module.qa_outputs.start_logits.dense # torch/nn/functional.py:1678:0
  %1108 : int = prim::Constant[value=-1](), scope: __module.qa_outputs/__module.qa_outputs.start_logits # transformers/modeling_utils.py:1126:0
  %1109 : __torch__.transformers.modeling_utils.PoolerAnswerClass = prim::GetAttr[name="answer_class"](%3)
  %1110 : __torch__.transformers.modeling_utils.PoolerEndLogits = prim::GetAttr[name="end_logits"](%3)
  %1111 : __torch__.transformers.modeling_utils.PoolerStartLogits = prim::GetAttr[name="start_logits"](%3)
  %1112 : __torch__.torch.nn.modules.linear.___torch_mangle_35749.Linear = prim::GetAttr[name="dense"](%1111)
  %1113 : Tensor = prim::GetAttr[name="bias"](%1112)
  %1114 : Tensor = prim::GetAttr[name="weight"](%1112)
  %1115 : Float(2048:1, 1:2048) = aten::t(%1114), scope: __module.qa_outputs/__module.qa_outputs.start_logits/__module.qa_outputs.start_logits.dense # torch/nn/functional.py:1676:0
  %output.73 : Float(17:13, 13:1, 1:1) = aten::matmul(%input.124, %1115), scope: __module.qa_outputs/__module.qa_outputs.start_logits/__module.qa_outputs.start_logits.dense # torch/nn/functional.py:1676:0
  %1117 : Float(17:13, 13:1, 1:1) = aten::add_(%output.73, %1113, %1107), scope: __module.qa_outputs/__module.qa_outputs.start_logits/__module.qa_outputs.start_logits.dense # torch/nn/functional.py:1678:0
  %input.125 : Float(17:13, 13:1) = aten::squeeze(%1117, %1108), scope: __module.qa_outputs/__module.qa_outputs.start_logits # transformers/modeling_utils.py:1126:0
  %1119 : int = aten::size(%input.124, %1107), scope: __module.qa_outputs # transformers/modeling_utils.py:1375:0
  %1120 : int = aten::size(%input.124, %1106), scope: __module.qa_outputs # transformers/modeling_utils.py:1375:0
  %start_log_probs : Float(17:13, 13:1) = aten::softmax(%input.125, %1108, %1105), scope: __module.qa_outputs # torch/nn/functional.py:1498:0
  %1122 : Float(17:5, 5:1), %start_top_index : Long(17:5, 5:1) = aten::topk(%start_log_probs, %1104, %1108, %1103, %1103), scope: __module.qa_outputs # transformers/modeling_utils.py:1378:0
  %1124 : Long(17:5, 5:1, 1:1) = aten::unsqueeze(%start_top_index, %1108), scope: __module.qa_outputs # transformers/modeling_utils.py:1381:0
  %1125 : int[] = prim::ListConstruct(%1108, %1108, %1120), scope: __module.qa_outputs
  %start_top_index_exp : Long(17:5, 5:1, 2048:0) = aten::expand(%1124, %1125, %1102), scope: __module.qa_outputs # transformers/modeling_utils.py:1381:0
  %start_states.1 : Float(17:10240, 5:2048, 2048:1) = aten::gather(%input.124, %1101, %start_top_index_exp, %1102), scope: __module.qa_outputs # transformers/modeling_utils.py:1382:0
  %1128 : Float(17:10240, 1:10240, 5:2048, 2048:1) = aten::unsqueeze(%start_states.1, %1107), scope: __module.qa_outputs # transformers/modeling_utils.py:1383:0
  %1129 : int[] = prim::ListConstruct(%1108, %1119, %1108, %1108), scope: __module.qa_outputs
  %start_states.2 : Float(17:10240, 13:0, 5:2048, 2048:1) = aten::expand(%1128, %1129, %1102), scope: __module.qa_outputs # transformers/modeling_utils.py:1383:0
  %1131 : Float(17:26624, 13:2048, 1:2048, 2048:1) = aten::unsqueeze(%input.124, %1106), scope: __module.qa_outputs # transformers/modeling_utils.py:1385:0
  %hidden_states : Float(17:26624, 13:2048, 5:0, 2048:1) = aten::expand_as(%1131, %start_states.2), scope: __module.qa_outputs # transformers/modeling_utils.py:1385:0
  %1133 : __torch__.torch.nn.modules.linear.___torch_mangle_35753.Linear = prim::GetAttr[name="dense_1"](%1110)
  %1134 : __torch__.torch.nn.modules.normalization.___torch_mangle_35752.LayerNorm = prim::GetAttr[name="LayerNorm"](%1110)
  %1135 : __torch__.torch.nn.modules.linear.___torch_mangle_35750.Linear = prim::GetAttr[name="dense_0"](%1110)
  %1136 : Tensor[] = prim::ListConstruct(%hidden_states, %start_states.2), scope: __module.qa_outputs/__module.qa_outputs.end_logits
  %input.126 : Float(17:266240, 13:20480, 5:4096, 4096:1) = aten::cat(%1136, %1108), scope: __module.qa_outputs/__module.qa_outputs.end_logits # transformers/modeling_utils.py:1190:0
  %1138 : Tensor = prim::GetAttr[name="bias"](%1135)
  %1139 : Tensor = prim::GetAttr[name="weight"](%1135)
  %1140 : Float(4096:1, 2048:4096) = aten::t(%1139), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_0 # torch/nn/functional.py:1676:0
  %output.74 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::matmul(%input.126, %1140), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_0 # torch/nn/functional.py:1676:0
  %input.127 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::add_(%output.74, %1138, %1107), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_0 # torch/nn/functional.py:1678:0
  %input.128 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::tanh(%input.127), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.activation # torch/nn/modules/activation.py:350:0
  %1144 : Tensor = prim::GetAttr[name="bias"](%1134)
  %1145 : Tensor = prim::GetAttr[name="weight"](%1134)
  %1146 : int[] = prim::ListConstruct(%1100), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.LayerNorm
  %input.129 : Float(17:133120, 13:10240, 5:2048, 2048:1) = aten::layer_norm(%input.128, %1146, %1145, %1144, %1099, %1103), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.LayerNorm # torch/nn/functional.py:2048:0
  %1148 : Tensor = prim::GetAttr[name="bias"](%1133)
  %1149 : Tensor = prim::GetAttr[name="weight"](%1133)
  %1150 : Float(2048:1, 1:2048) = aten::t(%1149), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_1 # torch/nn/functional.py:1676:0
  %output : Float(17:65, 13:5, 5:1, 1:1) = aten::matmul(%input.129, %1150), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_1 # torch/nn/functional.py:1676:0
  %1152 : Float(17:65, 13:5, 5:1, 1:1) = aten::add_(%output, %1148, %1107), scope: __module.qa_outputs/__module.qa_outputs.end_logits/__module.qa_outputs.end_logits.dense_1 # torch/nn/functional.py:1678:0
  %input.130 : Float(17:65, 13:5, 5:1) = aten::squeeze(%1152, %1108), scope: __module.qa_outputs/__module.qa_outputs.end_logits # transformers/modeling_utils.py:1193:0
  %end_log_probs : Float(17:65, 13:5, 5:1) = aten::softmax(%input.130, %1107, %1105), scope: __module.qa_outputs # torch/nn/functional.py:1498:0
  %end_top_log_probs : Float(17:25, 5:5, 5:1), %end_top_index : Long(17:25, 5:5, 5:1) = aten::topk(%end_log_probs, %1104, %1107, %1103, %1103), scope: __module.qa_outputs # transformers/modeling_utils.py:1392:0
  %1157 : int[] = prim::ListConstruct(%1108, %1098), scope: __module.qa_outputs
  %1158 : Float(17:25, 25:1) = aten::view(%end_top_log_probs, %1157), scope: __module.qa_outputs # transformers/modeling_utils.py:1395:0
  %1159 : int[] = prim::ListConstruct(%1108, %1098), scope: __module.qa_outputs
  %1160 : Long(17:25, 25:1) = aten::view(%end_top_index, %1159), scope: __module.qa_outputs # transformers/modeling_utils.py:1396:0
  %1161 : Tensor[] = prim::ListConstruct(%input.124, %start_log_probs), scope: __module.qa_outputs
  %start_states : Float(17:2048, 2048:1) = aten::einsum(%1097, %1161), scope: __module.qa_outputs # torch/functional.py:327:0
  %1163 : __torch__.torch.nn.modules.linear.___torch_mangle_35756.Linear = prim::GetAttr[name="dense_1"](%1109)
  %1164 : __torch__.torch.nn.modules.linear.___torch_mangle_35754.Linear = prim::GetAttr[name="dense_0"](%1109)
  %1165 : Float(17:26624, 13:2048, 2048:1) = aten::slice(%input.124, %1095, %1095, %1096, %1107), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %1166 : Float(17:26624, 2048:1) = aten::select(%1165, %1107, %1108), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %cls_token_state : Float(17:26624, 2048:1) = aten::slice(%1166, %1107, %1095, %1096, %1107), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1258:0
  %1168 : Tensor[] = prim::ListConstruct(%start_states, %cls_token_state), scope: __module.qa_outputs/__module.qa_outputs.answer_class
  %input.131 : Float(17:4096, 4096:1) = aten::cat(%1168, %1108), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1260:0
  %1170 : Tensor = prim::GetAttr[name="bias"](%1164)
  %1171 : Tensor = prim::GetAttr[name="weight"](%1164)
  %1172 : Float(4096:1, 2048:4096) = aten::t(%1171), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.dense_0 # torch/nn/functional.py:1674:0
  %input.132 : Float(17:2048, 2048:1) = aten::addmm(%1170, %input.131, %1172, %1107, %1107), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.dense_0 # torch/nn/functional.py:1674:0
  %input : Float(17:2048, 2048:1) = aten::tanh(%input.132), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.activation # torch/nn/modules/activation.py:350:0
  %1175 : Tensor = prim::GetAttr[name="weight"](%1163)
  %1176 : Float(2048:1, 1:2048) = aten::t(%1175), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.dense_1 # torch/nn/functional.py:1676:0
  %1177 : Float(17:1, 1:1) = aten::matmul(%input, %1176), scope: __module.qa_outputs/__module.qa_outputs.answer_class/__module.qa_outputs.answer_class.dense_1 # torch/nn/functional.py:1676:0
  %1178 : Float(17:1) = aten::squeeze(%1177, %1108), scope: __module.qa_outputs/__module.qa_outputs.answer_class # transformers/modeling_utils.py:1262:0
  %1179 : (Float(17:5, 5:1), Long(17:5, 5:1), Float(17:25, 25:1), Long(17:25, 25:1), Float(17:1)) = prim::TupleConstruct(%1122, %start_top_index, %1158, %1160, %1178)
  %7 : Float(17:5, 5:1), %8 : Long(17:5, 5:1), %9 : Float(17:25, 25:1), %10 : Long(17:25, 25:1), %11 : Float(17:1) = prim::TupleUnpack(%1179)
  %12 : (Float(17:5, 5:1), Long(17:5, 5:1), Float(17:25, 25:1), Long(17:25, 25:1), Float(17:1)) = prim::TupleConstruct(%7, %8, %9, %10, %11)
  return (%12)
