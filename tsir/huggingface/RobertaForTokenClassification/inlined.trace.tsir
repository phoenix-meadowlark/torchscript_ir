graph(%self.1 : __torch__.transformers.modeling_roberta.RobertaForTokenClassification,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_33315.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_33314.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_roberta.___torch_mangle_33313.RobertaModel = prim::GetAttr[name="roberta"](%self.1)
  %10 : Double() = prim::Constant[value={8}](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %11 : int = prim::Constant[value=-2](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %12 : int = prim::Constant[value=64](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %13 : int = prim::Constant[value=12](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %14 : Long() = prim::Constant[value={1}](), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %15 : int = prim::Constant[value=-1](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %16 : bool = prim::Constant[value=1](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %17 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %18 : int = prim::Constant[value=768](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %19 : float = prim::Constant[value=0.10000000000000001](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.dropout # torch/nn/functional.py:973:0
  %20 : Double() = prim::Constant[value={-10000}](), scope: __module.roberta # transformers/modeling_utils.py:258:0
  %21 : float = prim::Constant[value=1.](), scope: __module.roberta # torch/tensor.py:396:0
  %22 : None = prim::Constant(), scope: __module.roberta
  %23 : int = prim::Constant[value=6](), scope: __module.roberta # transformers/modeling_utils.py:257:0
  %24 : int = prim::Constant[value=3](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %25 : int = prim::Constant[value=2](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %26 : int = prim::Constant[value=9223372036854775807](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %27 : bool = prim::Constant[value=0](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %28 : Device = prim::Constant[value="cpu"](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %29 : int = prim::Constant[value=4](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %30 : int = prim::Constant[value=1](), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %31 : int = prim::Constant[value=0](), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %32 : __torch__.transformers.modeling_roberta.___torch_mangle_33312.RobertaEncoder = prim::GetAttr[name="encoder"](%5)
  %33 : __torch__.transformers.modeling_roberta.___torch_mangle_33106.RobertaEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %34 : int = aten::size(%input_ids, %31), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %35 : int = aten::size(%input_ids, %30), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %36 : int[] = prim::ListConstruct(%34, %35), scope: __module.roberta
  %input.2 : Long(17:13, 13:1) = aten::zeros(%36, %29, %31, %28, %27), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %38 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %31, %31, %26, %30), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %39 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%38, %30), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %40 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%39, %25), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%40, %24, %31, %26, %30), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %42 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %23, %27, %27, %22), scope: __module.roberta # transformers/modeling_utils.py:257:0
  %43 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%42, %21, %30), scope: __module.roberta # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%43, %20), scope: __module.roberta # transformers/modeling_utils.py:258:0
  %45 : __torch__.torch.nn.modules.normalization.___torch_mangle_33104.LayerNorm = prim::GetAttr[name="LayerNorm"](%33)
  %46 : __torch__.torch.nn.modules.sparse.___torch_mangle_33103.Embedding = prim::GetAttr[name="token_type_embeddings"](%33)
  %47 : __torch__.torch.nn.modules.sparse.___torch_mangle_33102.Embedding = prim::GetAttr[name="position_embeddings"](%33)
  %48 : __torch__.torch.nn.modules.sparse.___torch_mangle_33101.Embedding = prim::GetAttr[name="word_embeddings"](%33)
  %49 : Bool(17:13, 13:1) = aten::ne(%input_ids, %30), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1332:0
  %mask : Int(17:13, 13:1) = aten::to(%49, %24, %27, %27, %22), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1332:0
  %51 : Long(17:13, 13:1) = aten::cumsum(%mask, %30, %22), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %52 : Int(17:13, 13:1) = aten::type_as(%51, %mask), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %incremental_indices : Int(17:13, 13:1) = aten::mul(%52, %mask), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %54 : Long(17:13, 13:1) = aten::to(%incremental_indices, %29, %27, %27, %22), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %55 : Long(17:13, 13:1) = aten::add(%54, %14, %30), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %input.1 : Long(17:13, 13:1) = aten::to(%55, %29, %31, %28, %27, %27, %27, %22), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:98:0
  %57 : Tensor = prim::GetAttr[name="weight"](%48)
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::embedding(%57, %input_ids, %30, %27, %27), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %59 : Tensor = prim::GetAttr[name="weight"](%47)
  %position_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%59, %input.1, %30, %27, %27), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %61 : Tensor = prim::GetAttr[name="weight"](%46)
  %token_type_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%61, %input.2, %15, %27, %27), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %63 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %30), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:121:0
  %input.3 : Float(17:9984, 13:768, 768:1) = aten::add(%63, %token_type_embeddings, %30), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:121:0
  %65 : Tensor = prim::GetAttr[name="bias"](%45)
  %66 : Tensor = prim::GetAttr[name="weight"](%45)
  %67 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm
  %input.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %67, %66, %65, %17, %16), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.4, %19, %27), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.dropout # torch/nn/functional.py:973:0
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %71 : __torch__.transformers.modeling_roberta.___torch_mangle_33310.RobertaLayer = prim::GetAttr[name="11"](%70)
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %73 : __torch__.transformers.modeling_roberta.___torch_mangle_33293.RobertaLayer = prim::GetAttr[name="10"](%72)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %75 : __torch__.transformers.modeling_roberta.___torch_mangle_33276.RobertaLayer = prim::GetAttr[name="9"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %77 : __torch__.transformers.modeling_roberta.___torch_mangle_33259.RobertaLayer = prim::GetAttr[name="8"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %79 : __torch__.transformers.modeling_roberta.___torch_mangle_33242.RobertaLayer = prim::GetAttr[name="7"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %81 : __torch__.transformers.modeling_roberta.___torch_mangle_33225.RobertaLayer = prim::GetAttr[name="6"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %83 : __torch__.transformers.modeling_roberta.___torch_mangle_33208.RobertaLayer = prim::GetAttr[name="5"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %85 : __torch__.transformers.modeling_roberta.___torch_mangle_33191.RobertaLayer = prim::GetAttr[name="4"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %87 : __torch__.transformers.modeling_roberta.___torch_mangle_33174.RobertaLayer = prim::GetAttr[name="3"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %89 : __torch__.transformers.modeling_roberta.___torch_mangle_33157.RobertaLayer = prim::GetAttr[name="2"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %91 : __torch__.transformers.modeling_roberta.___torch_mangle_33140.RobertaLayer = prim::GetAttr[name="1"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_33311.ModuleList = prim::GetAttr[name="layer"](%32)
  %93 : __torch__.transformers.modeling_roberta.___torch_mangle_33123.RobertaLayer = prim::GetAttr[name="0"](%92)
  %94 : __torch__.transformers.modeling_roberta.___torch_mangle_33122.RobertaOutput = prim::GetAttr[name="output"](%93)
  %95 : __torch__.transformers.modeling_roberta.___torch_mangle_33118.RobertaIntermediate = prim::GetAttr[name="intermediate"](%93)
  %96 : __torch__.transformers.modeling_roberta.___torch_mangle_33116.RobertaAttention = prim::GetAttr[name="attention"](%93)
  %97 : __torch__.transformers.modeling_roberta.___torch_mangle_33115.RobertaSelfOutput = prim::GetAttr[name="output"](%96)
  %98 : __torch__.transformers.modeling_roberta.___torch_mangle_33111.RobertaSelfAttention = prim::GetAttr[name="self"](%96)
  %99 : __torch__.torch.nn.modules.linear.___torch_mangle_33109.Linear = prim::GetAttr[name="value"](%98)
  %100 : __torch__.torch.nn.modules.linear.___torch_mangle_33108.Linear = prim::GetAttr[name="key"](%98)
  %101 : __torch__.torch.nn.modules.linear.___torch_mangle_33107.Linear = prim::GetAttr[name="query"](%98)
  %102 : Tensor = prim::GetAttr[name="bias"](%101)
  %103 : Tensor = prim::GetAttr[name="weight"](%101)
  %104 : Float(768:1, 768:768) = aten::t(%103), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %104), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %102, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %107 : Tensor = prim::GetAttr[name="bias"](%100)
  %108 : Tensor = prim::GetAttr[name="weight"](%100)
  %109 : Float(768:1, 768:768) = aten::t(%108), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %109), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %107, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %112 : Tensor = prim::GetAttr[name="bias"](%99)
  %113 : Tensor = prim::GetAttr[name="weight"](%99)
  %114 : Float(768:1, 768:768) = aten::t(%113), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %114), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %112, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %117 : int = aten::size(%x.1, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %118 : int = aten::size(%x.1, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %119 : int[] = prim::ListConstruct(%117, %118, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %119), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %121 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %121), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %123 : int = aten::size(%x.3, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %124 : int = aten::size(%x.3, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %125 : int[] = prim::ListConstruct(%123, %124, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %125), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %127 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %127), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %129 : int = aten::size(%x.5, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %130 : int = aten::size(%x.5, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %131 : int[] = prim::ListConstruct(%129, %130, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %131), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %133 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %133), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %135 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %135), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:198:0
  %input.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:211:0
  %142 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %143 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %142), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%143, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %145 : int = aten::size(%context_layer.2, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %146 : int = aten::size(%context_layer.2, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %147 : int[] = prim::ListConstruct(%145, %146, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.2, %147), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:215:0
  %149 : __torch__.torch.nn.modules.normalization.___torch_mangle_33113.LayerNorm = prim::GetAttr[name="LayerNorm"](%97)
  %150 : __torch__.torch.nn.modules.linear.___torch_mangle_33112.Linear = prim::GetAttr[name="dense"](%97)
  %151 : Tensor = prim::GetAttr[name="bias"](%150)
  %152 : Tensor = prim::GetAttr[name="weight"](%150)
  %153 : Float(768:1, 768:768) = aten::t(%152), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.8, %153), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.4, %151, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.9, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output # transformers/modeling_roberta.py:232:0
  %158 : Tensor = prim::GetAttr[name="bias"](%149)
  %159 : Tensor = prim::GetAttr[name="weight"](%149)
  %160 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %160, %159, %158, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %162 : __torch__.torch.nn.modules.linear.___torch_mangle_33117.Linear = prim::GetAttr[name="dense"](%95)
  %163 : Tensor = prim::GetAttr[name="bias"](%162)
  %164 : Tensor = prim::GetAttr[name="weight"](%162)
  %165 : Float(768:1, 3072:768) = aten::t(%164), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %165), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.5, %163, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %169 : __torch__.torch.nn.modules.normalization.___torch_mangle_33120.LayerNorm = prim::GetAttr[name="LayerNorm"](%94)
  %170 : __torch__.torch.nn.modules.linear.___torch_mangle_33119.Linear = prim::GetAttr[name="dense"](%94)
  %171 : Tensor = prim::GetAttr[name="bias"](%170)
  %172 : Tensor = prim::GetAttr[name="weight"](%170)
  %173 : Float(3072:1, 768:3072) = aten::t(%172), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %173), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %171, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output # transformers/modeling_roberta.py:311:0
  %178 : Tensor = prim::GetAttr[name="bias"](%169)
  %179 : Tensor = prim::GetAttr[name="weight"](%169)
  %180 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.LayerNorm
  %input.15 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %180, %179, %178, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %182 : __torch__.transformers.modeling_roberta.___torch_mangle_33139.RobertaOutput = prim::GetAttr[name="output"](%91)
  %183 : __torch__.transformers.modeling_roberta.___torch_mangle_33135.RobertaIntermediate = prim::GetAttr[name="intermediate"](%91)
  %184 : __torch__.transformers.modeling_roberta.___torch_mangle_33133.RobertaAttention = prim::GetAttr[name="attention"](%91)
  %185 : __torch__.transformers.modeling_roberta.___torch_mangle_33132.RobertaSelfOutput = prim::GetAttr[name="output"](%184)
  %186 : __torch__.transformers.modeling_roberta.___torch_mangle_33128.RobertaSelfAttention = prim::GetAttr[name="self"](%184)
  %187 : __torch__.torch.nn.modules.linear.___torch_mangle_33126.Linear = prim::GetAttr[name="value"](%186)
  %188 : __torch__.torch.nn.modules.linear.___torch_mangle_33125.Linear = prim::GetAttr[name="key"](%186)
  %189 : __torch__.torch.nn.modules.linear.___torch_mangle_33124.Linear = prim::GetAttr[name="query"](%186)
  %190 : Tensor = prim::GetAttr[name="bias"](%189)
  %191 : Tensor = prim::GetAttr[name="weight"](%189)
  %192 : Float(768:1, 768:768) = aten::t(%191), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %192), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %190, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %195 : Tensor = prim::GetAttr[name="bias"](%188)
  %196 : Tensor = prim::GetAttr[name="weight"](%188)
  %197 : Float(768:1, 768:768) = aten::t(%196), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %197), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %195, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %200 : Tensor = prim::GetAttr[name="bias"](%187)
  %201 : Tensor = prim::GetAttr[name="weight"](%187)
  %202 : Float(768:1, 768:768) = aten::t(%201), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %202), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.9, %200, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %205 : int = aten::size(%x.7, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %206 : int = aten::size(%x.7, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %207 : int[] = prim::ListConstruct(%205, %206, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %207), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %209 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %209), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %211 : int = aten::size(%x.9, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %212 : int = aten::size(%x.9, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %213 : int[] = prim::ListConstruct(%211, %212, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %213), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %215 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %215), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %217 : int = aten::size(%x.11, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %218 : int = aten::size(%x.11, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %219 : int[] = prim::ListConstruct(%217, %218, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %219), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %221 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %221), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %223 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %223), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:195:0
  %input.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:198:0
  %input.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:211:0
  %230 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %231 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %230), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%231, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %233 : int = aten::size(%context_layer.4, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %234 : int = aten::size(%context_layer.4, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %235 : int[] = prim::ListConstruct(%233, %234, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.4, %235), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:215:0
  %237 : __torch__.torch.nn.modules.normalization.___torch_mangle_33130.LayerNorm = prim::GetAttr[name="LayerNorm"](%185)
  %238 : __torch__.torch.nn.modules.linear.___torch_mangle_33129.Linear = prim::GetAttr[name="dense"](%185)
  %239 : Tensor = prim::GetAttr[name="bias"](%238)
  %240 : Tensor = prim::GetAttr[name="weight"](%238)
  %241 : Float(768:1, 768:768) = aten::t(%240), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.18, %241), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %239, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.19, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output # transformers/modeling_roberta.py:232:0
  %246 : Tensor = prim::GetAttr[name="bias"](%237)
  %247 : Tensor = prim::GetAttr[name="weight"](%237)
  %248 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %248, %247, %246, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %250 : __torch__.torch.nn.modules.linear.___torch_mangle_33134.Linear = prim::GetAttr[name="dense"](%183)
  %251 : Tensor = prim::GetAttr[name="bias"](%250)
  %252 : Tensor = prim::GetAttr[name="weight"](%250)
  %253 : Float(768:1, 3072:768) = aten::t(%252), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %253), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.11, %251, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %257 : __torch__.torch.nn.modules.normalization.___torch_mangle_33137.LayerNorm = prim::GetAttr[name="LayerNorm"](%182)
  %258 : __torch__.torch.nn.modules.linear.___torch_mangle_33136.Linear = prim::GetAttr[name="dense"](%182)
  %259 : Tensor = prim::GetAttr[name="bias"](%258)
  %260 : Tensor = prim::GetAttr[name="weight"](%258)
  %261 : Float(3072:1, 768:3072) = aten::t(%260), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.22, %261), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %259, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.23, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output # transformers/modeling_roberta.py:311:0
  %266 : Tensor = prim::GetAttr[name="bias"](%257)
  %267 : Tensor = prim::GetAttr[name="weight"](%257)
  %268 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.LayerNorm
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %268, %267, %266, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %270 : __torch__.transformers.modeling_roberta.___torch_mangle_33156.RobertaOutput = prim::GetAttr[name="output"](%89)
  %271 : __torch__.transformers.modeling_roberta.___torch_mangle_33152.RobertaIntermediate = prim::GetAttr[name="intermediate"](%89)
  %272 : __torch__.transformers.modeling_roberta.___torch_mangle_33150.RobertaAttention = prim::GetAttr[name="attention"](%89)
  %273 : __torch__.transformers.modeling_roberta.___torch_mangle_33149.RobertaSelfOutput = prim::GetAttr[name="output"](%272)
  %274 : __torch__.transformers.modeling_roberta.___torch_mangle_33145.RobertaSelfAttention = prim::GetAttr[name="self"](%272)
  %275 : __torch__.torch.nn.modules.linear.___torch_mangle_33143.Linear = prim::GetAttr[name="value"](%274)
  %276 : __torch__.torch.nn.modules.linear.___torch_mangle_33142.Linear = prim::GetAttr[name="key"](%274)
  %277 : __torch__.torch.nn.modules.linear.___torch_mangle_33141.Linear = prim::GetAttr[name="query"](%274)
  %278 : Tensor = prim::GetAttr[name="bias"](%277)
  %279 : Tensor = prim::GetAttr[name="weight"](%277)
  %280 : Float(768:1, 768:768) = aten::t(%279), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %280), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %278, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %283 : Tensor = prim::GetAttr[name="bias"](%276)
  %284 : Tensor = prim::GetAttr[name="weight"](%276)
  %285 : Float(768:1, 768:768) = aten::t(%284), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %285), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.14, %283, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %288 : Tensor = prim::GetAttr[name="bias"](%275)
  %289 : Tensor = prim::GetAttr[name="weight"](%275)
  %290 : Float(768:1, 768:768) = aten::t(%289), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %290), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %288, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %293 : int = aten::size(%x.13, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %294 : int = aten::size(%x.13, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %295 : int[] = prim::ListConstruct(%293, %294, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %295), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %297 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %297), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %299 : int = aten::size(%x.15, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %300 : int = aten::size(%x.15, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %301 : int[] = prim::ListConstruct(%299, %300, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %301), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %303 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %303), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %305 : int = aten::size(%x.17, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %306 : int = aten::size(%x.17, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %307 : int[] = prim::ListConstruct(%305, %306, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %307), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %309 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %309), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %311 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %311), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:195:0
  %input.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:198:0
  %input.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:211:0
  %318 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %319 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %318), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%319, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %321 : int = aten::size(%context_layer.6, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %322 : int = aten::size(%context_layer.6, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %323 : int[] = prim::ListConstruct(%321, %322, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.6, %323), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:215:0
  %325 : __torch__.torch.nn.modules.normalization.___torch_mangle_33147.LayerNorm = prim::GetAttr[name="LayerNorm"](%273)
  %326 : __torch__.torch.nn.modules.linear.___torch_mangle_33146.Linear = prim::GetAttr[name="dense"](%273)
  %327 : Tensor = prim::GetAttr[name="bias"](%326)
  %328 : Tensor = prim::GetAttr[name="weight"](%326)
  %329 : Float(768:1, 768:768) = aten::t(%328), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.28, %329), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %327, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.29, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output # transformers/modeling_roberta.py:232:0
  %334 : Tensor = prim::GetAttr[name="bias"](%325)
  %335 : Tensor = prim::GetAttr[name="weight"](%325)
  %336 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %336, %335, %334, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %338 : __torch__.torch.nn.modules.linear.___torch_mangle_33151.Linear = prim::GetAttr[name="dense"](%271)
  %339 : Tensor = prim::GetAttr[name="bias"](%338)
  %340 : Tensor = prim::GetAttr[name="weight"](%338)
  %341 : Float(768:1, 3072:768) = aten::t(%340), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %341), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.17, %339, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %345 : __torch__.torch.nn.modules.normalization.___torch_mangle_33154.LayerNorm = prim::GetAttr[name="LayerNorm"](%270)
  %346 : __torch__.torch.nn.modules.linear.___torch_mangle_33153.Linear = prim::GetAttr[name="dense"](%270)
  %347 : Tensor = prim::GetAttr[name="bias"](%346)
  %348 : Tensor = prim::GetAttr[name="weight"](%346)
  %349 : Float(3072:1, 768:3072) = aten::t(%348), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.32, %349), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %347, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.33, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output # transformers/modeling_roberta.py:311:0
  %354 : Tensor = prim::GetAttr[name="bias"](%345)
  %355 : Tensor = prim::GetAttr[name="weight"](%345)
  %356 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.LayerNorm
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %356, %355, %354, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %358 : __torch__.transformers.modeling_roberta.___torch_mangle_33173.RobertaOutput = prim::GetAttr[name="output"](%87)
  %359 : __torch__.transformers.modeling_roberta.___torch_mangle_33169.RobertaIntermediate = prim::GetAttr[name="intermediate"](%87)
  %360 : __torch__.transformers.modeling_roberta.___torch_mangle_33167.RobertaAttention = prim::GetAttr[name="attention"](%87)
  %361 : __torch__.transformers.modeling_roberta.___torch_mangle_33166.RobertaSelfOutput = prim::GetAttr[name="output"](%360)
  %362 : __torch__.transformers.modeling_roberta.___torch_mangle_33162.RobertaSelfAttention = prim::GetAttr[name="self"](%360)
  %363 : __torch__.torch.nn.modules.linear.___torch_mangle_33160.Linear = prim::GetAttr[name="value"](%362)
  %364 : __torch__.torch.nn.modules.linear.___torch_mangle_33159.Linear = prim::GetAttr[name="key"](%362)
  %365 : __torch__.torch.nn.modules.linear.___torch_mangle_33158.Linear = prim::GetAttr[name="query"](%362)
  %366 : Tensor = prim::GetAttr[name="bias"](%365)
  %367 : Tensor = prim::GetAttr[name="weight"](%365)
  %368 : Float(768:1, 768:768) = aten::t(%367), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %368), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.19, %366, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %371 : Tensor = prim::GetAttr[name="bias"](%364)
  %372 : Tensor = prim::GetAttr[name="weight"](%364)
  %373 : Float(768:1, 768:768) = aten::t(%372), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %373), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %371, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %376 : Tensor = prim::GetAttr[name="bias"](%363)
  %377 : Tensor = prim::GetAttr[name="weight"](%363)
  %378 : Float(768:1, 768:768) = aten::t(%377), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %378), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %376, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %381 : int = aten::size(%x.19, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %382 : int = aten::size(%x.19, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %383 : int[] = prim::ListConstruct(%381, %382, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %383), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %385 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %385), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %387 : int = aten::size(%x.21, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %388 : int = aten::size(%x.21, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %389 : int[] = prim::ListConstruct(%387, %388, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %389), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %391 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %391), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %393 : int = aten::size(%x.23, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %394 : int = aten::size(%x.23, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %395 : int[] = prim::ListConstruct(%393, %394, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.24 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %395), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %397 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %397), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %399 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %399), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:195:0
  %input.36 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:198:0
  %input.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:211:0
  %406 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %407 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %406), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%407, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %409 : int = aten::size(%context_layer.8, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %410 : int = aten::size(%context_layer.8, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %411 : int[] = prim::ListConstruct(%409, %410, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %input.38 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.8, %411), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:215:0
  %413 : __torch__.torch.nn.modules.normalization.___torch_mangle_33164.LayerNorm = prim::GetAttr[name="LayerNorm"](%361)
  %414 : __torch__.torch.nn.modules.linear.___torch_mangle_33163.Linear = prim::GetAttr[name="dense"](%361)
  %415 : Tensor = prim::GetAttr[name="bias"](%414)
  %416 : Tensor = prim::GetAttr[name="weight"](%414)
  %417 : Float(768:1, 768:768) = aten::t(%416), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.38, %417), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %415, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.39, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output # transformers/modeling_roberta.py:232:0
  %422 : Tensor = prim::GetAttr[name="bias"](%413)
  %423 : Tensor = prim::GetAttr[name="weight"](%413)
  %424 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %424, %423, %422, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %426 : __torch__.torch.nn.modules.linear.___torch_mangle_33168.Linear = prim::GetAttr[name="dense"](%359)
  %427 : Tensor = prim::GetAttr[name="bias"](%426)
  %428 : Tensor = prim::GetAttr[name="weight"](%426)
  %429 : Float(768:1, 3072:768) = aten::t(%428), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %429), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.23, %427, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %433 : __torch__.torch.nn.modules.normalization.___torch_mangle_33171.LayerNorm = prim::GetAttr[name="LayerNorm"](%358)
  %434 : __torch__.torch.nn.modules.linear.___torch_mangle_33170.Linear = prim::GetAttr[name="dense"](%358)
  %435 : Tensor = prim::GetAttr[name="bias"](%434)
  %436 : Tensor = prim::GetAttr[name="weight"](%434)
  %437 : Float(3072:1, 768:3072) = aten::t(%436), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.42, %437), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.24, %435, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.43, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output # transformers/modeling_roberta.py:311:0
  %442 : Tensor = prim::GetAttr[name="bias"](%433)
  %443 : Tensor = prim::GetAttr[name="weight"](%433)
  %444 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.LayerNorm
  %input.45 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %444, %443, %442, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %446 : __torch__.transformers.modeling_roberta.___torch_mangle_33190.RobertaOutput = prim::GetAttr[name="output"](%85)
  %447 : __torch__.transformers.modeling_roberta.___torch_mangle_33186.RobertaIntermediate = prim::GetAttr[name="intermediate"](%85)
  %448 : __torch__.transformers.modeling_roberta.___torch_mangle_33184.RobertaAttention = prim::GetAttr[name="attention"](%85)
  %449 : __torch__.transformers.modeling_roberta.___torch_mangle_33183.RobertaSelfOutput = prim::GetAttr[name="output"](%448)
  %450 : __torch__.transformers.modeling_roberta.___torch_mangle_33179.RobertaSelfAttention = prim::GetAttr[name="self"](%448)
  %451 : __torch__.torch.nn.modules.linear.___torch_mangle_33177.Linear = prim::GetAttr[name="value"](%450)
  %452 : __torch__.torch.nn.modules.linear.___torch_mangle_33176.Linear = prim::GetAttr[name="key"](%450)
  %453 : __torch__.torch.nn.modules.linear.___torch_mangle_33175.Linear = prim::GetAttr[name="query"](%450)
  %454 : Tensor = prim::GetAttr[name="bias"](%453)
  %455 : Tensor = prim::GetAttr[name="weight"](%453)
  %456 : Float(768:1, 768:768) = aten::t(%455), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %456), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.25, %454, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %459 : Tensor = prim::GetAttr[name="bias"](%452)
  %460 : Tensor = prim::GetAttr[name="weight"](%452)
  %461 : Float(768:1, 768:768) = aten::t(%460), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %461), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.26, %459, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %464 : Tensor = prim::GetAttr[name="bias"](%451)
  %465 : Tensor = prim::GetAttr[name="weight"](%451)
  %466 : Float(768:1, 768:768) = aten::t(%465), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %466), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.27, %464, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %469 : int = aten::size(%x.25, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %470 : int = aten::size(%x.25, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %471 : int[] = prim::ListConstruct(%469, %470, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %471), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %473 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %473), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %475 : int = aten::size(%x.27, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %476 : int = aten::size(%x.27, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %477 : int[] = prim::ListConstruct(%475, %476, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.28 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %477), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %479 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %479), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %481 : int = aten::size(%x.29, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %482 : int = aten::size(%x.29, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %483 : int[] = prim::ListConstruct(%481, %482, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.30 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %483), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %485 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %485), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %487 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %487), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:195:0
  %input.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:198:0
  %input.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:211:0
  %494 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %495 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %494), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%495, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %497 : int = aten::size(%context_layer.10, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %498 : int = aten::size(%context_layer.10, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %499 : int[] = prim::ListConstruct(%497, %498, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %input.48 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.10, %499), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:215:0
  %501 : __torch__.torch.nn.modules.normalization.___torch_mangle_33181.LayerNorm = prim::GetAttr[name="LayerNorm"](%449)
  %502 : __torch__.torch.nn.modules.linear.___torch_mangle_33180.Linear = prim::GetAttr[name="dense"](%449)
  %503 : Tensor = prim::GetAttr[name="bias"](%502)
  %504 : Tensor = prim::GetAttr[name="weight"](%502)
  %505 : Float(768:1, 768:768) = aten::t(%504), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.48, %505), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.28, %503, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.49, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output # transformers/modeling_roberta.py:232:0
  %510 : Tensor = prim::GetAttr[name="bias"](%501)
  %511 : Tensor = prim::GetAttr[name="weight"](%501)
  %512 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %512, %511, %510, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %514 : __torch__.torch.nn.modules.linear.___torch_mangle_33185.Linear = prim::GetAttr[name="dense"](%447)
  %515 : Tensor = prim::GetAttr[name="bias"](%514)
  %516 : Tensor = prim::GetAttr[name="weight"](%514)
  %517 : Float(768:1, 3072:768) = aten::t(%516), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %517), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.29, %515, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %521 : __torch__.torch.nn.modules.normalization.___torch_mangle_33188.LayerNorm = prim::GetAttr[name="LayerNorm"](%446)
  %522 : __torch__.torch.nn.modules.linear.___torch_mangle_33187.Linear = prim::GetAttr[name="dense"](%446)
  %523 : Tensor = prim::GetAttr[name="bias"](%522)
  %524 : Tensor = prim::GetAttr[name="weight"](%522)
  %525 : Float(3072:1, 768:3072) = aten::t(%524), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.52, %525), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.30, %523, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.53, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output # transformers/modeling_roberta.py:311:0
  %530 : Tensor = prim::GetAttr[name="bias"](%521)
  %531 : Tensor = prim::GetAttr[name="weight"](%521)
  %532 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.LayerNorm
  %input.55 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %532, %531, %530, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %534 : __torch__.transformers.modeling_roberta.___torch_mangle_33207.RobertaOutput = prim::GetAttr[name="output"](%83)
  %535 : __torch__.transformers.modeling_roberta.___torch_mangle_33203.RobertaIntermediate = prim::GetAttr[name="intermediate"](%83)
  %536 : __torch__.transformers.modeling_roberta.___torch_mangle_33201.RobertaAttention = prim::GetAttr[name="attention"](%83)
  %537 : __torch__.transformers.modeling_roberta.___torch_mangle_33200.RobertaSelfOutput = prim::GetAttr[name="output"](%536)
  %538 : __torch__.transformers.modeling_roberta.___torch_mangle_33196.RobertaSelfAttention = prim::GetAttr[name="self"](%536)
  %539 : __torch__.torch.nn.modules.linear.___torch_mangle_33194.Linear = prim::GetAttr[name="value"](%538)
  %540 : __torch__.torch.nn.modules.linear.___torch_mangle_33193.Linear = prim::GetAttr[name="key"](%538)
  %541 : __torch__.torch.nn.modules.linear.___torch_mangle_33192.Linear = prim::GetAttr[name="query"](%538)
  %542 : Tensor = prim::GetAttr[name="bias"](%541)
  %543 : Tensor = prim::GetAttr[name="weight"](%541)
  %544 : Float(768:1, 768:768) = aten::t(%543), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %544), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.31, %542, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %547 : Tensor = prim::GetAttr[name="bias"](%540)
  %548 : Tensor = prim::GetAttr[name="weight"](%540)
  %549 : Float(768:1, 768:768) = aten::t(%548), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %549), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.32, %547, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %552 : Tensor = prim::GetAttr[name="bias"](%539)
  %553 : Tensor = prim::GetAttr[name="weight"](%539)
  %554 : Float(768:1, 768:768) = aten::t(%553), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %554), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.33, %552, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %557 : int = aten::size(%x.31, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %558 : int = aten::size(%x.31, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %559 : int[] = prim::ListConstruct(%557, %558, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.32 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %559), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %561 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %561), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %563 : int = aten::size(%x.33, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %564 : int = aten::size(%x.33, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %565 : int[] = prim::ListConstruct(%563, %564, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.34 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %565), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %567 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %567), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %569 : int = aten::size(%x.35, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %570 : int = aten::size(%x.35, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %571 : int[] = prim::ListConstruct(%569, %570, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.36 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %571), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %573 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %573), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %575 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %575), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:195:0
  %input.56 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:198:0
  %input.57 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:211:0
  %582 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %583 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %582), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%583, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %585 : int = aten::size(%context_layer.12, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %586 : int = aten::size(%context_layer.12, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %587 : int[] = prim::ListConstruct(%585, %586, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %input.58 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.12, %587), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:215:0
  %589 : __torch__.torch.nn.modules.normalization.___torch_mangle_33198.LayerNorm = prim::GetAttr[name="LayerNorm"](%537)
  %590 : __torch__.torch.nn.modules.linear.___torch_mangle_33197.Linear = prim::GetAttr[name="dense"](%537)
  %591 : Tensor = prim::GetAttr[name="bias"](%590)
  %592 : Tensor = prim::GetAttr[name="weight"](%590)
  %593 : Float(768:1, 768:768) = aten::t(%592), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.58, %593), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.34, %591, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.59, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output # transformers/modeling_roberta.py:232:0
  %598 : Tensor = prim::GetAttr[name="bias"](%589)
  %599 : Tensor = prim::GetAttr[name="weight"](%589)
  %600 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %600, %599, %598, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %602 : __torch__.torch.nn.modules.linear.___torch_mangle_33202.Linear = prim::GetAttr[name="dense"](%535)
  %603 : Tensor = prim::GetAttr[name="bias"](%602)
  %604 : Tensor = prim::GetAttr[name="weight"](%602)
  %605 : Float(768:1, 3072:768) = aten::t(%604), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %605), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.35, %603, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %609 : __torch__.torch.nn.modules.normalization.___torch_mangle_33205.LayerNorm = prim::GetAttr[name="LayerNorm"](%534)
  %610 : __torch__.torch.nn.modules.linear.___torch_mangle_33204.Linear = prim::GetAttr[name="dense"](%534)
  %611 : Tensor = prim::GetAttr[name="bias"](%610)
  %612 : Tensor = prim::GetAttr[name="weight"](%610)
  %613 : Float(3072:1, 768:3072) = aten::t(%612), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.62, %613), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.36, %611, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.63, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output # transformers/modeling_roberta.py:311:0
  %618 : Tensor = prim::GetAttr[name="bias"](%609)
  %619 : Tensor = prim::GetAttr[name="weight"](%609)
  %620 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.LayerNorm
  %input.65 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %620, %619, %618, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %622 : __torch__.transformers.modeling_roberta.___torch_mangle_33224.RobertaOutput = prim::GetAttr[name="output"](%81)
  %623 : __torch__.transformers.modeling_roberta.___torch_mangle_33220.RobertaIntermediate = prim::GetAttr[name="intermediate"](%81)
  %624 : __torch__.transformers.modeling_roberta.___torch_mangle_33218.RobertaAttention = prim::GetAttr[name="attention"](%81)
  %625 : __torch__.transformers.modeling_roberta.___torch_mangle_33217.RobertaSelfOutput = prim::GetAttr[name="output"](%624)
  %626 : __torch__.transformers.modeling_roberta.___torch_mangle_33213.RobertaSelfAttention = prim::GetAttr[name="self"](%624)
  %627 : __torch__.torch.nn.modules.linear.___torch_mangle_33211.Linear = prim::GetAttr[name="value"](%626)
  %628 : __torch__.torch.nn.modules.linear.___torch_mangle_33210.Linear = prim::GetAttr[name="key"](%626)
  %629 : __torch__.torch.nn.modules.linear.___torch_mangle_33209.Linear = prim::GetAttr[name="query"](%626)
  %630 : Tensor = prim::GetAttr[name="bias"](%629)
  %631 : Tensor = prim::GetAttr[name="weight"](%629)
  %632 : Float(768:1, 768:768) = aten::t(%631), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %632), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.37, %630, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %635 : Tensor = prim::GetAttr[name="bias"](%628)
  %636 : Tensor = prim::GetAttr[name="weight"](%628)
  %637 : Float(768:1, 768:768) = aten::t(%636), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %637), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.38, %635, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %640 : Tensor = prim::GetAttr[name="bias"](%627)
  %641 : Tensor = prim::GetAttr[name="weight"](%627)
  %642 : Float(768:1, 768:768) = aten::t(%641), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %642), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.39, %640, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %645 : int = aten::size(%x.37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %646 : int = aten::size(%x.37, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %647 : int[] = prim::ListConstruct(%645, %646, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.38 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %647), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %649 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %649), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %651 : int = aten::size(%x.39, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %652 : int = aten::size(%x.39, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %653 : int[] = prim::ListConstruct(%651, %652, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.40 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %653), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %655 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %655), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %657 : int = aten::size(%x.41, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %658 : int = aten::size(%x.41, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %659 : int[] = prim::ListConstruct(%657, %658, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.42 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %659), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %661 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %661), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %663 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %663), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.14 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:195:0
  %input.66 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:198:0
  %input.67 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:211:0
  %670 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %671 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %670), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%671, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %673 : int = aten::size(%context_layer.14, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %674 : int = aten::size(%context_layer.14, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %675 : int[] = prim::ListConstruct(%673, %674, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %input.68 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.14, %675), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:215:0
  %677 : __torch__.torch.nn.modules.normalization.___torch_mangle_33215.LayerNorm = prim::GetAttr[name="LayerNorm"](%625)
  %678 : __torch__.torch.nn.modules.linear.___torch_mangle_33214.Linear = prim::GetAttr[name="dense"](%625)
  %679 : Tensor = prim::GetAttr[name="bias"](%678)
  %680 : Tensor = prim::GetAttr[name="weight"](%678)
  %681 : Float(768:1, 768:768) = aten::t(%680), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.68, %681), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.40, %679, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.69, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output # transformers/modeling_roberta.py:232:0
  %686 : Tensor = prim::GetAttr[name="bias"](%677)
  %687 : Tensor = prim::GetAttr[name="weight"](%677)
  %688 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %688, %687, %686, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %690 : __torch__.torch.nn.modules.linear.___torch_mangle_33219.Linear = prim::GetAttr[name="dense"](%623)
  %691 : Tensor = prim::GetAttr[name="bias"](%690)
  %692 : Tensor = prim::GetAttr[name="weight"](%690)
  %693 : Float(768:1, 3072:768) = aten::t(%692), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %693), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.41, %691, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %697 : __torch__.torch.nn.modules.normalization.___torch_mangle_33222.LayerNorm = prim::GetAttr[name="LayerNorm"](%622)
  %698 : __torch__.torch.nn.modules.linear.___torch_mangle_33221.Linear = prim::GetAttr[name="dense"](%622)
  %699 : Tensor = prim::GetAttr[name="bias"](%698)
  %700 : Tensor = prim::GetAttr[name="weight"](%698)
  %701 : Float(3072:1, 768:3072) = aten::t(%700), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.72, %701), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.42, %699, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.73, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output # transformers/modeling_roberta.py:311:0
  %706 : Tensor = prim::GetAttr[name="bias"](%697)
  %707 : Tensor = prim::GetAttr[name="weight"](%697)
  %708 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.LayerNorm
  %input.75 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %708, %707, %706, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %710 : __torch__.transformers.modeling_roberta.___torch_mangle_33241.RobertaOutput = prim::GetAttr[name="output"](%79)
  %711 : __torch__.transformers.modeling_roberta.___torch_mangle_33237.RobertaIntermediate = prim::GetAttr[name="intermediate"](%79)
  %712 : __torch__.transformers.modeling_roberta.___torch_mangle_33235.RobertaAttention = prim::GetAttr[name="attention"](%79)
  %713 : __torch__.transformers.modeling_roberta.___torch_mangle_33234.RobertaSelfOutput = prim::GetAttr[name="output"](%712)
  %714 : __torch__.transformers.modeling_roberta.___torch_mangle_33230.RobertaSelfAttention = prim::GetAttr[name="self"](%712)
  %715 : __torch__.torch.nn.modules.linear.___torch_mangle_33228.Linear = prim::GetAttr[name="value"](%714)
  %716 : __torch__.torch.nn.modules.linear.___torch_mangle_33227.Linear = prim::GetAttr[name="key"](%714)
  %717 : __torch__.torch.nn.modules.linear.___torch_mangle_33226.Linear = prim::GetAttr[name="query"](%714)
  %718 : Tensor = prim::GetAttr[name="bias"](%717)
  %719 : Tensor = prim::GetAttr[name="weight"](%717)
  %720 : Float(768:1, 768:768) = aten::t(%719), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %720), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.43, %718, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %723 : Tensor = prim::GetAttr[name="bias"](%716)
  %724 : Tensor = prim::GetAttr[name="weight"](%716)
  %725 : Float(768:1, 768:768) = aten::t(%724), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %725), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.44, %723, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %728 : Tensor = prim::GetAttr[name="bias"](%715)
  %729 : Tensor = prim::GetAttr[name="weight"](%715)
  %730 : Float(768:1, 768:768) = aten::t(%729), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %730), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.45, %728, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %733 : int = aten::size(%x.43, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %734 : int = aten::size(%x.43, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %735 : int[] = prim::ListConstruct(%733, %734, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.44 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %735), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %737 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %737), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %739 : int = aten::size(%x.45, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %740 : int = aten::size(%x.45, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %741 : int[] = prim::ListConstruct(%739, %740, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.46 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %741), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %743 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %743), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %745 : int = aten::size(%x.47, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %746 : int = aten::size(%x.47, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %747 : int[] = prim::ListConstruct(%745, %746, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.48 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %747), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %749 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %749), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %751 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %751), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:195:0
  %input.76 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:198:0
  %input.77 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:211:0
  %758 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %759 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %758), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%759, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %761 : int = aten::size(%context_layer.16, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %762 : int = aten::size(%context_layer.16, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %763 : int[] = prim::ListConstruct(%761, %762, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %input.78 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.16, %763), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:215:0
  %765 : __torch__.torch.nn.modules.normalization.___torch_mangle_33232.LayerNorm = prim::GetAttr[name="LayerNorm"](%713)
  %766 : __torch__.torch.nn.modules.linear.___torch_mangle_33231.Linear = prim::GetAttr[name="dense"](%713)
  %767 : Tensor = prim::GetAttr[name="bias"](%766)
  %768 : Tensor = prim::GetAttr[name="weight"](%766)
  %769 : Float(768:1, 768:768) = aten::t(%768), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.78, %769), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.46, %767, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.79, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output # transformers/modeling_roberta.py:232:0
  %774 : Tensor = prim::GetAttr[name="bias"](%765)
  %775 : Tensor = prim::GetAttr[name="weight"](%765)
  %776 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %776, %775, %774, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %778 : __torch__.torch.nn.modules.linear.___torch_mangle_33236.Linear = prim::GetAttr[name="dense"](%711)
  %779 : Tensor = prim::GetAttr[name="bias"](%778)
  %780 : Tensor = prim::GetAttr[name="weight"](%778)
  %781 : Float(768:1, 3072:768) = aten::t(%780), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %781), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.47, %779, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %785 : __torch__.torch.nn.modules.normalization.___torch_mangle_33239.LayerNorm = prim::GetAttr[name="LayerNorm"](%710)
  %786 : __torch__.torch.nn.modules.linear.___torch_mangle_33238.Linear = prim::GetAttr[name="dense"](%710)
  %787 : Tensor = prim::GetAttr[name="bias"](%786)
  %788 : Tensor = prim::GetAttr[name="weight"](%786)
  %789 : Float(3072:1, 768:3072) = aten::t(%788), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.82, %789), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.48, %787, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.83, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output # transformers/modeling_roberta.py:311:0
  %794 : Tensor = prim::GetAttr[name="bias"](%785)
  %795 : Tensor = prim::GetAttr[name="weight"](%785)
  %796 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.LayerNorm
  %input.85 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %796, %795, %794, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %798 : __torch__.transformers.modeling_roberta.___torch_mangle_33258.RobertaOutput = prim::GetAttr[name="output"](%77)
  %799 : __torch__.transformers.modeling_roberta.___torch_mangle_33254.RobertaIntermediate = prim::GetAttr[name="intermediate"](%77)
  %800 : __torch__.transformers.modeling_roberta.___torch_mangle_33252.RobertaAttention = prim::GetAttr[name="attention"](%77)
  %801 : __torch__.transformers.modeling_roberta.___torch_mangle_33251.RobertaSelfOutput = prim::GetAttr[name="output"](%800)
  %802 : __torch__.transformers.modeling_roberta.___torch_mangle_33247.RobertaSelfAttention = prim::GetAttr[name="self"](%800)
  %803 : __torch__.torch.nn.modules.linear.___torch_mangle_33245.Linear = prim::GetAttr[name="value"](%802)
  %804 : __torch__.torch.nn.modules.linear.___torch_mangle_33244.Linear = prim::GetAttr[name="key"](%802)
  %805 : __torch__.torch.nn.modules.linear.___torch_mangle_33243.Linear = prim::GetAttr[name="query"](%802)
  %806 : Tensor = prim::GetAttr[name="bias"](%805)
  %807 : Tensor = prim::GetAttr[name="weight"](%805)
  %808 : Float(768:1, 768:768) = aten::t(%807), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %808), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.49, %806, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %811 : Tensor = prim::GetAttr[name="bias"](%804)
  %812 : Tensor = prim::GetAttr[name="weight"](%804)
  %813 : Float(768:1, 768:768) = aten::t(%812), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %813), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.50, %811, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %816 : Tensor = prim::GetAttr[name="bias"](%803)
  %817 : Tensor = prim::GetAttr[name="weight"](%803)
  %818 : Float(768:1, 768:768) = aten::t(%817), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %818), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.51, %816, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %821 : int = aten::size(%x.49, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %822 : int = aten::size(%x.49, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %823 : int[] = prim::ListConstruct(%821, %822, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.50 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %823), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %825 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %825), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %827 : int = aten::size(%x.51, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %828 : int = aten::size(%x.51, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %829 : int[] = prim::ListConstruct(%827, %828, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.52 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %829), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %831 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %831), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %833 : int = aten::size(%x.53, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %834 : int = aten::size(%x.53, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %835 : int[] = prim::ListConstruct(%833, %834, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %835), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %837 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %837), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %839 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %839), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.18 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:195:0
  %input.86 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:198:0
  %input.87 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:211:0
  %846 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %847 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %846), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%847, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %849 : int = aten::size(%context_layer.18, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %850 : int = aten::size(%context_layer.18, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %851 : int[] = prim::ListConstruct(%849, %850, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %input.88 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.18, %851), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:215:0
  %853 : __torch__.torch.nn.modules.normalization.___torch_mangle_33249.LayerNorm = prim::GetAttr[name="LayerNorm"](%801)
  %854 : __torch__.torch.nn.modules.linear.___torch_mangle_33248.Linear = prim::GetAttr[name="dense"](%801)
  %855 : Tensor = prim::GetAttr[name="bias"](%854)
  %856 : Tensor = prim::GetAttr[name="weight"](%854)
  %857 : Float(768:1, 768:768) = aten::t(%856), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.88, %857), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.52, %855, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.89, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output # transformers/modeling_roberta.py:232:0
  %862 : Tensor = prim::GetAttr[name="bias"](%853)
  %863 : Tensor = prim::GetAttr[name="weight"](%853)
  %864 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %864, %863, %862, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %866 : __torch__.torch.nn.modules.linear.___torch_mangle_33253.Linear = prim::GetAttr[name="dense"](%799)
  %867 : Tensor = prim::GetAttr[name="bias"](%866)
  %868 : Tensor = prim::GetAttr[name="weight"](%866)
  %869 : Float(768:1, 3072:768) = aten::t(%868), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %869), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.53, %867, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %873 : __torch__.torch.nn.modules.normalization.___torch_mangle_33256.LayerNorm = prim::GetAttr[name="LayerNorm"](%798)
  %874 : __torch__.torch.nn.modules.linear.___torch_mangle_33255.Linear = prim::GetAttr[name="dense"](%798)
  %875 : Tensor = prim::GetAttr[name="bias"](%874)
  %876 : Tensor = prim::GetAttr[name="weight"](%874)
  %877 : Float(3072:1, 768:3072) = aten::t(%876), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.92, %877), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.54, %875, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.93, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output # transformers/modeling_roberta.py:311:0
  %882 : Tensor = prim::GetAttr[name="bias"](%873)
  %883 : Tensor = prim::GetAttr[name="weight"](%873)
  %884 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.LayerNorm
  %input.95 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %884, %883, %882, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %886 : __torch__.transformers.modeling_roberta.___torch_mangle_33275.RobertaOutput = prim::GetAttr[name="output"](%75)
  %887 : __torch__.transformers.modeling_roberta.___torch_mangle_33271.RobertaIntermediate = prim::GetAttr[name="intermediate"](%75)
  %888 : __torch__.transformers.modeling_roberta.___torch_mangle_33269.RobertaAttention = prim::GetAttr[name="attention"](%75)
  %889 : __torch__.transformers.modeling_roberta.___torch_mangle_33268.RobertaSelfOutput = prim::GetAttr[name="output"](%888)
  %890 : __torch__.transformers.modeling_roberta.___torch_mangle_33264.RobertaSelfAttention = prim::GetAttr[name="self"](%888)
  %891 : __torch__.torch.nn.modules.linear.___torch_mangle_33262.Linear = prim::GetAttr[name="value"](%890)
  %892 : __torch__.torch.nn.modules.linear.___torch_mangle_33261.Linear = prim::GetAttr[name="key"](%890)
  %893 : __torch__.torch.nn.modules.linear.___torch_mangle_33260.Linear = prim::GetAttr[name="query"](%890)
  %894 : Tensor = prim::GetAttr[name="bias"](%893)
  %895 : Tensor = prim::GetAttr[name="weight"](%893)
  %896 : Float(768:1, 768:768) = aten::t(%895), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %896), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.55, %894, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %899 : Tensor = prim::GetAttr[name="bias"](%892)
  %900 : Tensor = prim::GetAttr[name="weight"](%892)
  %901 : Float(768:1, 768:768) = aten::t(%900), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %901), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.56, %899, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %904 : Tensor = prim::GetAttr[name="bias"](%891)
  %905 : Tensor = prim::GetAttr[name="weight"](%891)
  %906 : Float(768:1, 768:768) = aten::t(%905), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %906), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.57, %904, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %909 : int = aten::size(%x.55, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %910 : int = aten::size(%x.55, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %911 : int[] = prim::ListConstruct(%909, %910, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.56 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %911), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %913 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %913), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %915 : int = aten::size(%x.57, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %916 : int = aten::size(%x.57, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %917 : int[] = prim::ListConstruct(%915, %916, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.58 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %917), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %919 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %919), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %921 : int = aten::size(%x.59, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %922 : int = aten::size(%x.59, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %923 : int[] = prim::ListConstruct(%921, %922, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %923), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %925 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %925), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %927 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.19 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %927), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.20 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:195:0
  %input.96 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:198:0
  %input.97 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:211:0
  %934 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %935 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %934), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%935, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %937 : int = aten::size(%context_layer.20, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %938 : int = aten::size(%context_layer.20, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %939 : int[] = prim::ListConstruct(%937, %938, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %input.98 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.20, %939), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:215:0
  %941 : __torch__.torch.nn.modules.normalization.___torch_mangle_33266.LayerNorm = prim::GetAttr[name="LayerNorm"](%889)
  %942 : __torch__.torch.nn.modules.linear.___torch_mangle_33265.Linear = prim::GetAttr[name="dense"](%889)
  %943 : Tensor = prim::GetAttr[name="bias"](%942)
  %944 : Tensor = prim::GetAttr[name="weight"](%942)
  %945 : Float(768:1, 768:768) = aten::t(%944), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.98, %945), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.58, %943, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.99, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output # transformers/modeling_roberta.py:232:0
  %950 : Tensor = prim::GetAttr[name="bias"](%941)
  %951 : Tensor = prim::GetAttr[name="weight"](%941)
  %952 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %952, %951, %950, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %954 : __torch__.torch.nn.modules.linear.___torch_mangle_33270.Linear = prim::GetAttr[name="dense"](%887)
  %955 : Tensor = prim::GetAttr[name="bias"](%954)
  %956 : Tensor = prim::GetAttr[name="weight"](%954)
  %957 : Float(768:1, 3072:768) = aten::t(%956), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %957), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.59, %955, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %961 : __torch__.torch.nn.modules.normalization.___torch_mangle_33273.LayerNorm = prim::GetAttr[name="LayerNorm"](%886)
  %962 : __torch__.torch.nn.modules.linear.___torch_mangle_33272.Linear = prim::GetAttr[name="dense"](%886)
  %963 : Tensor = prim::GetAttr[name="bias"](%962)
  %964 : Tensor = prim::GetAttr[name="weight"](%962)
  %965 : Float(3072:1, 768:3072) = aten::t(%964), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.102, %965), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.60, %963, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.103, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output # transformers/modeling_roberta.py:311:0
  %970 : Tensor = prim::GetAttr[name="bias"](%961)
  %971 : Tensor = prim::GetAttr[name="weight"](%961)
  %972 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.LayerNorm
  %input.105 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %972, %971, %970, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %974 : __torch__.transformers.modeling_roberta.___torch_mangle_33292.RobertaOutput = prim::GetAttr[name="output"](%73)
  %975 : __torch__.transformers.modeling_roberta.___torch_mangle_33288.RobertaIntermediate = prim::GetAttr[name="intermediate"](%73)
  %976 : __torch__.transformers.modeling_roberta.___torch_mangle_33286.RobertaAttention = prim::GetAttr[name="attention"](%73)
  %977 : __torch__.transformers.modeling_roberta.___torch_mangle_33285.RobertaSelfOutput = prim::GetAttr[name="output"](%976)
  %978 : __torch__.transformers.modeling_roberta.___torch_mangle_33281.RobertaSelfAttention = prim::GetAttr[name="self"](%976)
  %979 : __torch__.torch.nn.modules.linear.___torch_mangle_33279.Linear = prim::GetAttr[name="value"](%978)
  %980 : __torch__.torch.nn.modules.linear.___torch_mangle_33278.Linear = prim::GetAttr[name="key"](%978)
  %981 : __torch__.torch.nn.modules.linear.___torch_mangle_33277.Linear = prim::GetAttr[name="query"](%978)
  %982 : Tensor = prim::GetAttr[name="bias"](%981)
  %983 : Tensor = prim::GetAttr[name="weight"](%981)
  %984 : Float(768:1, 768:768) = aten::t(%983), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %984), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.61, %982, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %987 : Tensor = prim::GetAttr[name="bias"](%980)
  %988 : Tensor = prim::GetAttr[name="weight"](%980)
  %989 : Float(768:1, 768:768) = aten::t(%988), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %989), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.62, %987, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %992 : Tensor = prim::GetAttr[name="bias"](%979)
  %993 : Tensor = prim::GetAttr[name="weight"](%979)
  %994 : Float(768:1, 768:768) = aten::t(%993), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %994), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %992, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %997 : int = aten::size(%x.61, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %998 : int = aten::size(%x.61, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %999 : int[] = prim::ListConstruct(%997, %998, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.62 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %999), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1001 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %1001), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1003 : int = aten::size(%x.63, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1004 : int = aten::size(%x.63, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1005 : int[] = prim::ListConstruct(%1003, %1004, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.64 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %1005), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1007 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1007), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1009 : int = aten::size(%x.65, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1010 : int = aten::size(%x.65, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1011 : int[] = prim::ListConstruct(%1009, %1010, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.66 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1011), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1013 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1013), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1015 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.21 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1015), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:195:0
  %input.106 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:198:0
  %input.107 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:211:0
  %1022 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %1023 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1022), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1023, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %1025 : int = aten::size(%context_layer.22, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1026 : int = aten::size(%context_layer.22, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1027 : int[] = prim::ListConstruct(%1025, %1026, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %input.108 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1027), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:215:0
  %1029 : __torch__.torch.nn.modules.normalization.___torch_mangle_33283.LayerNorm = prim::GetAttr[name="LayerNorm"](%977)
  %1030 : __torch__.torch.nn.modules.linear.___torch_mangle_33282.Linear = prim::GetAttr[name="dense"](%977)
  %1031 : Tensor = prim::GetAttr[name="bias"](%1030)
  %1032 : Tensor = prim::GetAttr[name="weight"](%1030)
  %1033 : Float(768:1, 768:768) = aten::t(%1032), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.108, %1033), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %1031, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.109, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output # transformers/modeling_roberta.py:232:0
  %1038 : Tensor = prim::GetAttr[name="bias"](%1029)
  %1039 : Tensor = prim::GetAttr[name="weight"](%1029)
  %1040 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1040, %1039, %1038, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1042 : __torch__.torch.nn.modules.linear.___torch_mangle_33287.Linear = prim::GetAttr[name="dense"](%975)
  %1043 : Tensor = prim::GetAttr[name="bias"](%1042)
  %1044 : Tensor = prim::GetAttr[name="weight"](%1042)
  %1045 : Float(768:1, 3072:768) = aten::t(%1044), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1045), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1043, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1049 : __torch__.torch.nn.modules.normalization.___torch_mangle_33290.LayerNorm = prim::GetAttr[name="LayerNorm"](%974)
  %1050 : __torch__.torch.nn.modules.linear.___torch_mangle_33289.Linear = prim::GetAttr[name="dense"](%974)
  %1051 : Tensor = prim::GetAttr[name="bias"](%1050)
  %1052 : Tensor = prim::GetAttr[name="weight"](%1050)
  %1053 : Float(3072:1, 768:3072) = aten::t(%1052), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.112, %1053), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.66, %1051, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.113, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output # transformers/modeling_roberta.py:311:0
  %1058 : Tensor = prim::GetAttr[name="bias"](%1049)
  %1059 : Tensor = prim::GetAttr[name="weight"](%1049)
  %1060 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.LayerNorm
  %input.115 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1060, %1059, %1058, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1062 : __torch__.transformers.modeling_roberta.___torch_mangle_33309.RobertaOutput = prim::GetAttr[name="output"](%71)
  %1063 : __torch__.transformers.modeling_roberta.___torch_mangle_33305.RobertaIntermediate = prim::GetAttr[name="intermediate"](%71)
  %1064 : __torch__.transformers.modeling_roberta.___torch_mangle_33303.RobertaAttention = prim::GetAttr[name="attention"](%71)
  %1065 : __torch__.transformers.modeling_roberta.___torch_mangle_33302.RobertaSelfOutput = prim::GetAttr[name="output"](%1064)
  %1066 : __torch__.transformers.modeling_roberta.___torch_mangle_33298.RobertaSelfAttention = prim::GetAttr[name="self"](%1064)
  %1067 : __torch__.torch.nn.modules.linear.___torch_mangle_33296.Linear = prim::GetAttr[name="value"](%1066)
  %1068 : __torch__.torch.nn.modules.linear.___torch_mangle_33295.Linear = prim::GetAttr[name="key"](%1066)
  %1069 : __torch__.torch.nn.modules.linear.___torch_mangle_33294.Linear = prim::GetAttr[name="query"](%1066)
  %1070 : Tensor = prim::GetAttr[name="bias"](%1069)
  %1071 : Tensor = prim::GetAttr[name="weight"](%1069)
  %1072 : Float(768:1, 768:768) = aten::t(%1071), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1072), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %1070, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1075 : Tensor = prim::GetAttr[name="bias"](%1068)
  %1076 : Tensor = prim::GetAttr[name="weight"](%1068)
  %1077 : Float(768:1, 768:768) = aten::t(%1076), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1077), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %1075, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1080 : Tensor = prim::GetAttr[name="bias"](%1067)
  %1081 : Tensor = prim::GetAttr[name="weight"](%1067)
  %1082 : Float(768:1, 768:768) = aten::t(%1081), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1082), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %1080, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1085 : int = aten::size(%x.67, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1086 : int = aten::size(%x.67, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1087 : int[] = prim::ListConstruct(%1085, %1086, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x.68 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1087), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1089 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %query_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1089), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1091 : int = aten::size(%x.69, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1092 : int = aten::size(%x.69, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1093 : int[] = prim::ListConstruct(%1091, %1092, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x.70 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1093), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1095 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %key_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1095), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1097 : int = aten::size(%x.71, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1098 : int = aten::size(%x.71, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1099 : int[] = prim::ListConstruct(%1097, %1098, %13, %12), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1099), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1101 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %value_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1101), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1103 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %15, %11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1103), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:195:0
  %input.116 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:198:0
  %input.117 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %15, %22), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:211:0
  %1110 : int[] = prim::ListConstruct(%31, %25, %30, %24), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %1111 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1110), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1111, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %1113 : int = aten::size(%context_layer, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1114 : int = aten::size(%context_layer, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1115 : int[] = prim::ListConstruct(%1113, %1114, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %input.118 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer, %1115), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:215:0
  %1117 : __torch__.torch.nn.modules.normalization.___torch_mangle_33300.LayerNorm = prim::GetAttr[name="LayerNorm"](%1065)
  %1118 : __torch__.torch.nn.modules.linear.___torch_mangle_33299.Linear = prim::GetAttr[name="dense"](%1065)
  %1119 : Tensor = prim::GetAttr[name="bias"](%1118)
  %1120 : Tensor = prim::GetAttr[name="weight"](%1118)
  %1121 : Float(768:1, 768:768) = aten::t(%1120), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.118, %1121), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %1119, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.119, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output # transformers/modeling_roberta.py:232:0
  %1126 : Tensor = prim::GetAttr[name="bias"](%1117)
  %1127 : Tensor = prim::GetAttr[name="weight"](%1117)
  %1128 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1128, %1127, %1126, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1130 : __torch__.torch.nn.modules.linear.___torch_mangle_33304.Linear = prim::GetAttr[name="dense"](%1063)
  %1131 : Tensor = prim::GetAttr[name="bias"](%1130)
  %1132 : Tensor = prim::GetAttr[name="weight"](%1130)
  %1133 : Float(768:1, 3072:768) = aten::t(%1132), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1133), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1131, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1137 : __torch__.torch.nn.modules.normalization.___torch_mangle_33307.LayerNorm = prim::GetAttr[name="LayerNorm"](%1062)
  %1138 : __torch__.torch.nn.modules.linear.___torch_mangle_33306.Linear = prim::GetAttr[name="dense"](%1062)
  %1139 : Tensor = prim::GetAttr[name="bias"](%1138)
  %1140 : Tensor = prim::GetAttr[name="weight"](%1138)
  %1141 : Float(3072:1, 768:3072) = aten::t(%1140), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.122, %1141), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %1139, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.123, %19, %27), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states, %input_tensor, %30), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output # transformers/modeling_roberta.py:311:0
  %1146 : Tensor = prim::GetAttr[name="bias"](%1137)
  %1147 : Tensor = prim::GetAttr[name="weight"](%1137)
  %1148 : int[] = prim::ListConstruct(%18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.LayerNorm
  %input.125 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1148, %1147, %1146, %17, %16), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1150 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1151 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.125, %1151, %1150), scope: __module.dropout # torch/nn/functional.py:973:0
  %1153 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
  %1154 : Tensor = prim::GetAttr[name="bias"](%3)
  %1155 : Tensor = prim::GetAttr[name="weight"](%3)
  %1156 : Float(768:1, 2:768) = aten::t(%1155), scope: __module.classifier # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1156), scope: __module.classifier # torch/nn/functional.py:1676:0
  %1158 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1154, %1153), scope: __module.classifier # torch/nn/functional.py:1678:0
  %9 : (Float(17:26, 13:2, 2:1)) = prim::TupleConstruct(%1158)
  return (%9)
