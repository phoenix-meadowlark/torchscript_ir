FlaubertModel(
  (position_embeddings): Embedding(512, 2048)
  (embeddings): Embedding(30145, 2048, padding_idx=2)
  (layer_norm_emb): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (6): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (7): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (8): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (9): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (10): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (11): MultiHeadAttention(
      (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
      (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (6): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (7): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (8): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (9): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (10): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (11): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (6): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (7): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (8): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (9): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (10): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
    (11): TransformerFFN(
      (lin1): Linear(in_features=2048, out_features=8192, bias=True)
      (lin2): Linear(in_features=8192, out_features=2048, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (6): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (7): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (8): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (9): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (10): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (11): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
  )
)

FlaubertModel._actual_script_module
FlaubertModel.forward
  graph(%self.1 : __torch__.transformers.modeling_flaubert.FlaubertModel,
        %input_ids : Long(17:13, 13:1),
        %padding_mask : Long(17:13, 13:1)):
    %2831 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2832 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="11"](%2831)
    %2827 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2828 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="11"](%2827)
    %2819 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2820 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="11"](%2819)
    %2815 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2816 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="11"](%2815)
    %2801 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2802 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="10"](%2801)
    %2797 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2798 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="10"](%2797)
    %2789 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2790 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="10"](%2789)
    %2785 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2786 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="10"](%2785)
    %2771 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2772 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="9"](%2771)
    %2767 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2768 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="9"](%2767)
    %2759 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2760 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="9"](%2759)
    %2755 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2756 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="9"](%2755)
    %2741 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2742 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="8"](%2741)
    %2737 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2738 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="8"](%2737)
    %2729 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2730 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="8"](%2729)
    %2725 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2726 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="8"](%2725)
    %2711 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2712 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="7"](%2711)
    %2707 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2708 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="7"](%2707)
    %2699 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2700 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="7"](%2699)
    %2695 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2696 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="7"](%2695)
    %2681 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2682 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="6"](%2681)
    %2677 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2678 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="6"](%2677)
    %2669 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2670 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="6"](%2669)
    %2665 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2666 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="6"](%2665)
    %2651 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2652 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="5"](%2651)
    %2647 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2648 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="5"](%2647)
    %2639 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2640 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="5"](%2639)
    %2635 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2636 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="5"](%2635)
    %2621 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2622 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="4"](%2621)
    %2617 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2618 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="4"](%2617)
    %2609 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2610 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="4"](%2609)
    %2605 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2606 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="4"](%2605)
    %2591 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2592 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="3"](%2591)
    %2587 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2588 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="3"](%2587)
    %2579 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2580 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="3"](%2579)
    %2575 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2576 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="3"](%2575)
    %2561 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2562 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="2"](%2561)
    %2557 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2558 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="2"](%2557)
    %2549 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2550 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="2"](%2549)
    %2545 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2546 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="2"](%2545)
    %2531 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2532 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="1"](%2531)
    %2527 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2528 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="1"](%2527)
    %2519 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2520 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="1"](%2519)
    %2515 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2516 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="1"](%2515)
    %2501 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
    %2502 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="0"](%2501)
    %2497 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.1)
    %2498 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="0"](%2497)
    %2489 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
    %2490 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="0"](%2489)
    %2485 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.1)
    %2486 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="0"](%2485)
    %2472 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%self.1)
    %2469 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="position_embeddings"](%self.1)
    %2467 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embeddings"](%self.1)
    %455 : int = prim::Constant[value=0]() # transformers/modeling_flaubert.py:174:0
    %456 : int = aten::size(%input_ids, %455) # transformers/modeling_flaubert.py:174:0
    %bs.1 : Long() = prim::NumToTensor(%456)
    %505 : int = aten::Int(%bs.1)
    %458 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:174:0
    %459 : int = aten::size(%input_ids, %458) # transformers/modeling_flaubert.py:174:0
    %slen : Long() = prim::NumToTensor(%459)
    %506 : int = aten::Int(%slen)
    %497 : Scalar = aten::ScalarImplicit(%slen)
    %498 : int = prim::Constant[value=4]() # transformers/modeling_flaubert.py:203:0
    %499 : int = prim::Constant[value=0]() # transformers/modeling_flaubert.py:203:0
    %500 : Device = prim::Constant[value="cpu"]() # transformers/modeling_flaubert.py:203:0
    %501 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:203:0
    %position_ids : Long(13:1) = aten::arange(%497, %498, %499, %500, %501) # transformers/modeling_flaubert.py:203:0
    %503 : int = prim::Constant[value=0]() # transformers/modeling_flaubert.py:204:0
    %504 : Long(1:13, 13:1) = aten::unsqueeze(%position_ids, %503) # transformers/modeling_flaubert.py:204:0
    %507 : int[] = prim::ListConstruct(%505, %506)
    %508 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:204:0
    %input.1 : Long(17:0, 13:1) = aten::expand(%504, %507, %508) # transformers/modeling_flaubert.py:204:0
    %2956 : Tensor = prim::CallMethod[name="forward"](%2467, %input_ids)
    %2957 : Tensor = prim::CallMethod[name="forward"](%2469, %input.1)
    %523 : Float(17:26624, 13:2048, 2048:1) = aten::expand_as(%2957, %2956) # transformers/modeling_flaubert.py:231:0
    %524 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:231:0
    %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2956, %523, %524) # transformers/modeling_flaubert.py:231:0
    %2958 : Tensor = prim::CallMethod[name="forward"](%2472, %input.2)
    %531 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %532 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2958, %531, %532) # torch/nn/functional.py:973:0
    %534 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:238:0
    %535 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %534) # transformers/modeling_flaubert.py:238:0
    %536 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:238:0
    %537 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:238:0
    %538 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:238:0
    %539 : None = prim::Constant()
    %540 : Float(17:13, 13:1, 1:1) = aten::to(%535, %536, %537, %538, %539) # transformers/modeling_flaubert.py:238:0
    %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %540) # transformers/modeling_flaubert.py:238:0
    %2959 : Tensor = prim::CallMethod[name="forward"](%2486, %input.4, %padding_mask)
    %634 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %635 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2959, %634, %635) # torch/nn/functional.py:973:0
    %637 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %637) # transformers/modeling_flaubert.py:265:0
    %2960 : Tensor = prim::CallMethod[name="forward"](%2490, %input.9)
    %2961 : Tensor = prim::CallMethod[name="forward"](%2498, %2960)
    %677 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2960, %2961, %677) # transformers/modeling_flaubert.py:285:0
    %2962 : Tensor = prim::CallMethod[name="forward"](%2502, %input.13)
    %684 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %685 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %684) # transformers/modeling_flaubert.py:291:0
    %686 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %687 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %688 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %689 : None = prim::Constant()
    %690 : Float(17:13, 13:1, 1:1) = aten::to(%685, %686, %687, %688, %689) # transformers/modeling_flaubert.py:291:0
    %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2962, %690) # transformers/modeling_flaubert.py:291:0
    %2963 : Tensor = prim::CallMethod[name="forward"](%2516, %input.14, %padding_mask)
    %784 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %785 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2963, %784, %785) # torch/nn/functional.py:973:0
    %787 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %787) # transformers/modeling_flaubert.py:265:0
    %2964 : Tensor = prim::CallMethod[name="forward"](%2520, %input.19)
    %2965 : Tensor = prim::CallMethod[name="forward"](%2528, %2964)
    %827 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2964, %2965, %827) # transformers/modeling_flaubert.py:285:0
    %2966 : Tensor = prim::CallMethod[name="forward"](%2532, %input.23)
    %834 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %835 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %834) # transformers/modeling_flaubert.py:291:0
    %836 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %837 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %838 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %839 : None = prim::Constant()
    %840 : Float(17:13, 13:1, 1:1) = aten::to(%835, %836, %837, %838, %839) # transformers/modeling_flaubert.py:291:0
    %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2966, %840) # transformers/modeling_flaubert.py:291:0
    %2967 : Tensor = prim::CallMethod[name="forward"](%2546, %input.24, %padding_mask)
    %934 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %935 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2967, %934, %935) # torch/nn/functional.py:973:0
    %937 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %937) # transformers/modeling_flaubert.py:265:0
    %2968 : Tensor = prim::CallMethod[name="forward"](%2550, %input.29)
    %2969 : Tensor = prim::CallMethod[name="forward"](%2558, %2968)
    %977 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2968, %2969, %977) # transformers/modeling_flaubert.py:285:0
    %2970 : Tensor = prim::CallMethod[name="forward"](%2562, %input.33)
    %984 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %985 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %984) # transformers/modeling_flaubert.py:291:0
    %986 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %987 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %988 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %989 : None = prim::Constant()
    %990 : Float(17:13, 13:1, 1:1) = aten::to(%985, %986, %987, %988, %989) # transformers/modeling_flaubert.py:291:0
    %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2970, %990) # transformers/modeling_flaubert.py:291:0
    %2971 : Tensor = prim::CallMethod[name="forward"](%2576, %input.34, %padding_mask)
    %1084 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1085 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2971, %1084, %1085) # torch/nn/functional.py:973:0
    %1087 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %1087) # transformers/modeling_flaubert.py:265:0
    %2972 : Tensor = prim::CallMethod[name="forward"](%2580, %input.39)
    %2973 : Tensor = prim::CallMethod[name="forward"](%2588, %2972)
    %1127 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2972, %2973, %1127) # transformers/modeling_flaubert.py:285:0
    %2974 : Tensor = prim::CallMethod[name="forward"](%2592, %input.43)
    %1134 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %1135 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1134) # transformers/modeling_flaubert.py:291:0
    %1136 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %1137 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1138 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1139 : None = prim::Constant()
    %1140 : Float(17:13, 13:1, 1:1) = aten::to(%1135, %1136, %1137, %1138, %1139) # transformers/modeling_flaubert.py:291:0
    %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2974, %1140) # transformers/modeling_flaubert.py:291:0
    %2975 : Tensor = prim::CallMethod[name="forward"](%2606, %input.44, %padding_mask)
    %1234 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1235 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2975, %1234, %1235) # torch/nn/functional.py:973:0
    %1237 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %1237) # transformers/modeling_flaubert.py:265:0
    %2976 : Tensor = prim::CallMethod[name="forward"](%2610, %input.49)
    %2977 : Tensor = prim::CallMethod[name="forward"](%2618, %2976)
    %1277 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2976, %2977, %1277) # transformers/modeling_flaubert.py:285:0
    %2978 : Tensor = prim::CallMethod[name="forward"](%2622, %input.53)
    %1284 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %1285 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1284) # transformers/modeling_flaubert.py:291:0
    %1286 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %1287 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1288 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1289 : None = prim::Constant()
    %1290 : Float(17:13, 13:1, 1:1) = aten::to(%1285, %1286, %1287, %1288, %1289) # transformers/modeling_flaubert.py:291:0
    %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2978, %1290) # transformers/modeling_flaubert.py:291:0
    %2979 : Tensor = prim::CallMethod[name="forward"](%2636, %input.54, %padding_mask)
    %1384 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1385 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2979, %1384, %1385) # torch/nn/functional.py:973:0
    %1387 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %1387) # transformers/modeling_flaubert.py:265:0
    %2980 : Tensor = prim::CallMethod[name="forward"](%2640, %input.59)
    %2981 : Tensor = prim::CallMethod[name="forward"](%2648, %2980)
    %1427 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2980, %2981, %1427) # transformers/modeling_flaubert.py:285:0
    %2982 : Tensor = prim::CallMethod[name="forward"](%2652, %input.63)
    %1434 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %1435 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1434) # transformers/modeling_flaubert.py:291:0
    %1436 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %1437 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1438 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1439 : None = prim::Constant()
    %1440 : Float(17:13, 13:1, 1:1) = aten::to(%1435, %1436, %1437, %1438, %1439) # transformers/modeling_flaubert.py:291:0
    %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2982, %1440) # transformers/modeling_flaubert.py:291:0
    %2983 : Tensor = prim::CallMethod[name="forward"](%2666, %input.64, %padding_mask)
    %1534 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1535 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2983, %1534, %1535) # torch/nn/functional.py:973:0
    %1537 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %1537) # transformers/modeling_flaubert.py:265:0
    %2984 : Tensor = prim::CallMethod[name="forward"](%2670, %input.69)
    %2985 : Tensor = prim::CallMethod[name="forward"](%2678, %2984)
    %1577 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2984, %2985, %1577) # transformers/modeling_flaubert.py:285:0
    %2986 : Tensor = prim::CallMethod[name="forward"](%2682, %input.73)
    %1584 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %1585 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1584) # transformers/modeling_flaubert.py:291:0
    %1586 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %1587 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1588 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1589 : None = prim::Constant()
    %1590 : Float(17:13, 13:1, 1:1) = aten::to(%1585, %1586, %1587, %1588, %1589) # transformers/modeling_flaubert.py:291:0
    %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2986, %1590) # transformers/modeling_flaubert.py:291:0
    %2987 : Tensor = prim::CallMethod[name="forward"](%2696, %input.74, %padding_mask)
    %1684 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1685 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2987, %1684, %1685) # torch/nn/functional.py:973:0
    %1687 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %1687) # transformers/modeling_flaubert.py:265:0
    %2988 : Tensor = prim::CallMethod[name="forward"](%2700, %input.79)
    %2989 : Tensor = prim::CallMethod[name="forward"](%2708, %2988)
    %1727 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2988, %2989, %1727) # transformers/modeling_flaubert.py:285:0
    %2990 : Tensor = prim::CallMethod[name="forward"](%2712, %input.83)
    %1734 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %1735 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1734) # transformers/modeling_flaubert.py:291:0
    %1736 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %1737 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1738 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1739 : None = prim::Constant()
    %1740 : Float(17:13, 13:1, 1:1) = aten::to(%1735, %1736, %1737, %1738, %1739) # transformers/modeling_flaubert.py:291:0
    %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2990, %1740) # transformers/modeling_flaubert.py:291:0
    %2991 : Tensor = prim::CallMethod[name="forward"](%2726, %input.84, %padding_mask)
    %1834 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1835 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2991, %1834, %1835) # torch/nn/functional.py:973:0
    %1837 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %1837) # transformers/modeling_flaubert.py:265:0
    %2992 : Tensor = prim::CallMethod[name="forward"](%2730, %input.89)
    %2993 : Tensor = prim::CallMethod[name="forward"](%2738, %2992)
    %1877 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2992, %2993, %1877) # transformers/modeling_flaubert.py:285:0
    %2994 : Tensor = prim::CallMethod[name="forward"](%2742, %input.93)
    %1884 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %1885 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %1884) # transformers/modeling_flaubert.py:291:0
    %1886 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %1887 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1888 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %1889 : None = prim::Constant()
    %1890 : Float(17:13, 13:1, 1:1) = aten::to(%1885, %1886, %1887, %1888, %1889) # transformers/modeling_flaubert.py:291:0
    %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2994, %1890) # transformers/modeling_flaubert.py:291:0
    %2995 : Tensor = prim::CallMethod[name="forward"](%2756, %input.94, %padding_mask)
    %1984 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %1985 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2995, %1984, %1985) # torch/nn/functional.py:973:0
    %1987 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %1987) # transformers/modeling_flaubert.py:265:0
    %2996 : Tensor = prim::CallMethod[name="forward"](%2760, %input.99)
    %2997 : Tensor = prim::CallMethod[name="forward"](%2768, %2996)
    %2027 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%2996, %2997, %2027) # transformers/modeling_flaubert.py:285:0
    %2998 : Tensor = prim::CallMethod[name="forward"](%2772, %input.103)
    %2034 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %2035 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %2034) # transformers/modeling_flaubert.py:291:0
    %2036 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %2037 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %2038 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %2039 : None = prim::Constant()
    %2040 : Float(17:13, 13:1, 1:1) = aten::to(%2035, %2036, %2037, %2038, %2039) # transformers/modeling_flaubert.py:291:0
    %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%2998, %2040) # transformers/modeling_flaubert.py:291:0
    %2999 : Tensor = prim::CallMethod[name="forward"](%2786, %input.104, %padding_mask)
    %2134 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %2135 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%2999, %2134, %2135) # torch/nn/functional.py:973:0
    %2137 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %2137) # transformers/modeling_flaubert.py:265:0
    %3000 : Tensor = prim::CallMethod[name="forward"](%2790, %input.109)
    %3001 : Tensor = prim::CallMethod[name="forward"](%2798, %3000)
    %2177 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%3000, %3001, %2177) # transformers/modeling_flaubert.py:285:0
    %3002 : Tensor = prim::CallMethod[name="forward"](%2802, %input.113)
    %2184 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %2185 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %2184) # transformers/modeling_flaubert.py:291:0
    %2186 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %2187 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %2188 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %2189 : None = prim::Constant()
    %2190 : Float(17:13, 13:1, 1:1) = aten::to(%2185, %2186, %2187, %2188, %2189) # transformers/modeling_flaubert.py:291:0
    %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%3002, %2190) # transformers/modeling_flaubert.py:291:0
    %3003 : Tensor = prim::CallMethod[name="forward"](%2816, %input.114, %padding_mask)
    %2284 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
    %2285 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
    %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%3003, %2284, %2285) # torch/nn/functional.py:973:0
    %2287 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
    %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %2287) # transformers/modeling_flaubert.py:265:0
    %3004 : Tensor = prim::CallMethod[name="forward"](%2820, %input.119)
    %3005 : Tensor = prim::CallMethod[name="forward"](%2828, %3004)
    %2327 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
    %input : Float(17:26624, 13:2048, 2048:1) = aten::add(%3004, %3005, %2327) # transformers/modeling_flaubert.py:285:0
    %3006 : Tensor = prim::CallMethod[name="forward"](%2832, %input)
    %2334 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
    %2335 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %2334) # transformers/modeling_flaubert.py:291:0
    %2336 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
    %2337 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %2338 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
    %2339 : None = prim::Constant()
    %2340 : Float(17:13, 13:1, 1:1) = aten::to(%2335, %2336, %2337, %2338, %2339) # transformers/modeling_flaubert.py:291:0
    %2341 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%3006, %2340) # transformers/modeling_flaubert.py:291:0
    %2342 : (Float(17:26624, 13:2048, 2048:1)) = prim::TupleConstruct(%2341)
    return (%2342)

FlaubertModel.embeddings
Embedding._actual_script_module
  graph(%self.2 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(17:13, 13:1)):
    %1 : Tensor = prim::GetAttr[name="weight"](%self.2)
    %7 : int = prim::Constant[value=2](), scope: __module.embeddings # torch/nn/functional.py:1814:0
    %8 : bool = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:1814:0
    %9 : bool = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:1814:0
    %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%1, %input_ids, %7, %8, %9), scope: __module.embeddings # torch/nn/functional.py:1814:0
    return (%inputs_embeds)

FlaubertModel.layer_norm_emb
LayerNorm._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.2 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.4)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm_emb
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
    %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %4, %2, %1, %5, %6), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
    return (%input.3)

FlaubertModel.position_embeddings
Embedding._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.1 : Long(17:0, 13:1)):
    %1 : Tensor = prim::GetAttr[name="weight"](%self.3)
    %2 : int = prim::Constant[value=-1](), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
    %3 : bool = prim::Constant[value=0](), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
    %5 : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%1, %input.1, %2, %3, %4), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
    return (%5)

ModuleList.*
  module had no methods with graph attrs.

MultiHeadAttention._actual_script_module
  graph(%self.5 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.4 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.5)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.5)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.5)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.5)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.4, %5), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
    %bs.2 : Long() = prim::NumToTensor(%6), scope: __module.attentions.0
    %9 : int = aten::Int(%bs.2), scope: __module.attentions.0
    %10 : int = aten::Int(%bs.2), scope: __module.attentions.0
    %11 : int = aten::Int(%bs.2), scope: __module.attentions.0
    %12 : int = aten::Int(%bs.2), scope: __module.attentions.0
    %13 : int = aten::Int(%bs.2), scope: __module.attentions.0
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.4, %14), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
    %qlen.1 : Long() = prim::NumToTensor(%15), scope: __module.attentions.0
    %17 : int = aten::Int(%qlen.1), scope: __module.attentions.0
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.4)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.0
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.4)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.0
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.4)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.0
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.0 # transformers/modeling_xlm.py:188:0
    %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %48), scope: __module.attentions.0 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %50, %51), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
    %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %52), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.0 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.0 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.0
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
    %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.1), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.0 # transformers/modeling_xlm.py:191:0
    %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %62), scope: __module.attentions.0 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.0
    %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %64, %65, %66, %67), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.0
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %69, %70), scope: __module.attentions.0 # torch/nn/functional.py:1498:0
    %input.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.5), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.0 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.0 # torch/nn/functional.py:973:0
    %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.6, %73, %74), scope: __module.attentions.0 # torch/nn/functional.py:973:0
    %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.attentions.0 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %77, %78), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.0
    %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.7)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.7 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.7)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.7)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1676:0
    %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1678:0
    %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %2, %6), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1678:0
    return (%x.2)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.9 : __torch__.torch.nn.modules.linear.Linear,
        %input.7 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.9)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.9)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1676:0
    %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %4), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1678:0
    %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %2, %6), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1678:0
    return (%input.8)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.6)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.6)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1676:0
    %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1678:0
    %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %2, %6), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1678:0
    return (%x.1)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.8 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.8)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.8)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1676:0
    %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1678:0
    %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %2, %6), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1678:0
    return (%x.3)

MultiHeadAttention._actual_script_module
  graph(%self.15 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.14 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.15)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.15)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.15)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.15)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.14, %5), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
    %bs.3 : Long() = prim::NumToTensor(%6), scope: __module.attentions.1
    %9 : int = aten::Int(%bs.3), scope: __module.attentions.1
    %10 : int = aten::Int(%bs.3), scope: __module.attentions.1
    %11 : int = aten::Int(%bs.3), scope: __module.attentions.1
    %12 : int = aten::Int(%bs.3), scope: __module.attentions.1
    %13 : int = aten::Int(%bs.3), scope: __module.attentions.1
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.14, %14), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
    %qlen.2 : Long() = prim::NumToTensor(%15), scope: __module.attentions.1
    %17 : int = aten::Int(%qlen.2), scope: __module.attentions.1
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.14)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.1
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.14)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.1
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.14)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.1
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.1 # transformers/modeling_xlm.py:188:0
    %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %48), scope: __module.attentions.1 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %50, %51), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
    %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %52), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.1 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.1 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.1
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
    %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.3), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.1 # transformers/modeling_xlm.py:191:0
    %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %62), scope: __module.attentions.1 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.1
    %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %64, %65, %66, %67), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.1
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %69, %70), scope: __module.attentions.1 # torch/nn/functional.py:1498:0
    %input.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.15), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.1 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.1 # torch/nn/functional.py:973:0
    %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.16, %73, %74), scope: __module.attentions.1 # torch/nn/functional.py:973:0
    %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.attentions.1 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %77, %78), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.1
    %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.17)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.17 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.17)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.17)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1676:0
    %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1678:0
    %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %2, %6), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1678:0
    return (%x.6)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.linear.Linear,
        %input.17 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.19)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.19)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1676:0
    %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %4), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1678:0
    %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %2, %6), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1678:0
    return (%input.18)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1676:0
    %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1678:0
    %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %2, %6), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1678:0
    return (%x.5)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.18)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.18)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1676:0
    %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1678:0
    %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %2, %6), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1678:0
    return (%x.7)

MultiHeadAttention._actual_script_module
  graph(%self.25 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.24 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.25)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.25)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.25)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.25)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.24, %5), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
    %bs.4 : Long() = prim::NumToTensor(%6), scope: __module.attentions.2
    %9 : int = aten::Int(%bs.4), scope: __module.attentions.2
    %10 : int = aten::Int(%bs.4), scope: __module.attentions.2
    %11 : int = aten::Int(%bs.4), scope: __module.attentions.2
    %12 : int = aten::Int(%bs.4), scope: __module.attentions.2
    %13 : int = aten::Int(%bs.4), scope: __module.attentions.2
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.24, %14), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
    %qlen.3 : Long() = prim::NumToTensor(%15), scope: __module.attentions.2
    %17 : int = aten::Int(%qlen.3), scope: __module.attentions.2
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.24)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.2
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.24)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.2
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.24)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.2
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.2 # transformers/modeling_xlm.py:188:0
    %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %48), scope: __module.attentions.2 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %50, %51), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
    %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %52), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.2 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.2 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.2
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
    %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.5), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.2 # transformers/modeling_xlm.py:191:0
    %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %62), scope: __module.attentions.2 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.2
    %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %64, %65, %66, %67), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.2
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %69, %70), scope: __module.attentions.2 # torch/nn/functional.py:1498:0
    %input.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.25), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.2 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.2 # torch/nn/functional.py:973:0
    %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.26, %73, %74), scope: __module.attentions.2 # torch/nn/functional.py:973:0
    %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.attentions.2 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %77, %78), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.2
    %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.27)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.27)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.27)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1676:0
    %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1678:0
    %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %2, %6), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1678:0
    return (%x.10)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %input.27 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1676:0
    %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %4), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1678:0
    %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %2, %6), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1678:0
    return (%input.28)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.26 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.26)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.26)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1676:0
    %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1678:0
    %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %2, %6), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1678:0
    return (%x.9)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.28 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.28)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.28)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1676:0
    %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1678:0
    %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %2, %6), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1678:0
    return (%x.11)

MultiHeadAttention._actual_script_module
  graph(%self.35 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.34 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.35)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.35)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.35)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.35)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.34, %5), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
    %bs.5 : Long() = prim::NumToTensor(%6), scope: __module.attentions.3
    %9 : int = aten::Int(%bs.5), scope: __module.attentions.3
    %10 : int = aten::Int(%bs.5), scope: __module.attentions.3
    %11 : int = aten::Int(%bs.5), scope: __module.attentions.3
    %12 : int = aten::Int(%bs.5), scope: __module.attentions.3
    %13 : int = aten::Int(%bs.5), scope: __module.attentions.3
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.34, %14), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
    %qlen.4 : Long() = prim::NumToTensor(%15), scope: __module.attentions.3
    %17 : int = aten::Int(%qlen.4), scope: __module.attentions.3
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.34)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.3
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.34)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.3
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.34)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.3
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.3 # transformers/modeling_xlm.py:188:0
    %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %48), scope: __module.attentions.3 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %50, %51), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
    %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %52), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.3 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.3 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.3
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
    %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.7), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.3 # transformers/modeling_xlm.py:191:0
    %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %62), scope: __module.attentions.3 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.3
    %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %64, %65, %66, %67), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.3
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %69, %70), scope: __module.attentions.3 # torch/nn/functional.py:1498:0
    %input.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.35), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.3 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.3 # torch/nn/functional.py:973:0
    %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.36, %73, %74), scope: __module.attentions.3 # torch/nn/functional.py:973:0
    %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.attentions.3 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %77, %78), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.3
    %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.37)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.37)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.37)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1676:0
    %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1678:0
    %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %2, %6), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1678:0
    return (%x.14)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.39 : __torch__.torch.nn.modules.linear.Linear,
        %input.37 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.39)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.39)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1676:0
    %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %4), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1678:0
    %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %2, %6), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1678:0
    return (%input.38)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.36)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1676:0
    %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1678:0
    %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %2, %6), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1678:0
    return (%x.13)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1676:0
    %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1678:0
    %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %2, %6), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1678:0
    return (%x.15)

MultiHeadAttention._actual_script_module
  graph(%self.45 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.44 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.45)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.45)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.45)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.45)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.44, %5), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
    %bs.6 : Long() = prim::NumToTensor(%6), scope: __module.attentions.4
    %9 : int = aten::Int(%bs.6), scope: __module.attentions.4
    %10 : int = aten::Int(%bs.6), scope: __module.attentions.4
    %11 : int = aten::Int(%bs.6), scope: __module.attentions.4
    %12 : int = aten::Int(%bs.6), scope: __module.attentions.4
    %13 : int = aten::Int(%bs.6), scope: __module.attentions.4
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.44, %14), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
    %qlen.5 : Long() = prim::NumToTensor(%15), scope: __module.attentions.4
    %17 : int = aten::Int(%qlen.5), scope: __module.attentions.4
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.44)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.4
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.44)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.4
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.44)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.4
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.4 # transformers/modeling_xlm.py:188:0
    %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %48), scope: __module.attentions.4 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %50, %51), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
    %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %52), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.4 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.4 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.4
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
    %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.9), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.4 # transformers/modeling_xlm.py:191:0
    %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %62), scope: __module.attentions.4 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.4
    %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %64, %65, %66, %67), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.4
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %69, %70), scope: __module.attentions.4 # torch/nn/functional.py:1498:0
    %input.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.45), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.4 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.4 # torch/nn/functional.py:973:0
    %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.46, %73, %74), scope: __module.attentions.4 # torch/nn/functional.py:973:0
    %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.attentions.4 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %77, %78), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.4
    %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.47)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.47 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.47)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.47)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1676:0
    %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1678:0
    %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %2, %6), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1678:0
    return (%x.18)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.linear.Linear,
        %input.47 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.49)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.49)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1676:0
    %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %4), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1678:0
    %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %2, %6), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1678:0
    return (%input.48)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.46)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.46)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1676:0
    %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1678:0
    %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %2, %6), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1678:0
    return (%x.17)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1676:0
    %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1678:0
    %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %2, %6), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1678:0
    return (%x.19)

MultiHeadAttention._actual_script_module
  graph(%self.55 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.54 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.55)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.55)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.55)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.55)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.54, %5), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
    %bs.7 : Long() = prim::NumToTensor(%6), scope: __module.attentions.5
    %9 : int = aten::Int(%bs.7), scope: __module.attentions.5
    %10 : int = aten::Int(%bs.7), scope: __module.attentions.5
    %11 : int = aten::Int(%bs.7), scope: __module.attentions.5
    %12 : int = aten::Int(%bs.7), scope: __module.attentions.5
    %13 : int = aten::Int(%bs.7), scope: __module.attentions.5
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.54, %14), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
    %qlen.6 : Long() = prim::NumToTensor(%15), scope: __module.attentions.5
    %17 : int = aten::Int(%qlen.6), scope: __module.attentions.5
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.54)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.5
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.54)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.5
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.54)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.5
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.5 # transformers/modeling_xlm.py:188:0
    %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %48), scope: __module.attentions.5 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %50, %51), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
    %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %52), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.5 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.5 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.5
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
    %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.11), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.5 # transformers/modeling_xlm.py:191:0
    %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %62), scope: __module.attentions.5 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.5
    %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %64, %65, %66, %67), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.5
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %69, %70), scope: __module.attentions.5 # torch/nn/functional.py:1498:0
    %input.56 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.55), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.5 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.5 # torch/nn/functional.py:973:0
    %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.56, %73, %74), scope: __module.attentions.5 # torch/nn/functional.py:973:0
    %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.attentions.5 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %77, %78), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.5
    %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.57)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.57)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1676:0
    %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1678:0
    %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %2, %6), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1678:0
    return (%x.22)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.linear.Linear,
        %input.57 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.59)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1676:0
    %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %4), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1678:0
    %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %2, %6), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1678:0
    return (%input.58)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.56 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.56)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.56)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1676:0
    %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1678:0
    %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %2, %6), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1678:0
    return (%x.21)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.58)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1676:0
    %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1678:0
    %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %2, %6), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1678:0
    return (%x.23)

MultiHeadAttention._actual_script_module
  graph(%self.65 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.64 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.65)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.65)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.65)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.65)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.64, %5), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
    %bs.8 : Long() = prim::NumToTensor(%6), scope: __module.attentions.6
    %9 : int = aten::Int(%bs.8), scope: __module.attentions.6
    %10 : int = aten::Int(%bs.8), scope: __module.attentions.6
    %11 : int = aten::Int(%bs.8), scope: __module.attentions.6
    %12 : int = aten::Int(%bs.8), scope: __module.attentions.6
    %13 : int = aten::Int(%bs.8), scope: __module.attentions.6
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.64, %14), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
    %qlen.7 : Long() = prim::NumToTensor(%15), scope: __module.attentions.6
    %17 : int = aten::Int(%qlen.7), scope: __module.attentions.6
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.64)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.6
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.64)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.6
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.64)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.6
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.6 # transformers/modeling_xlm.py:188:0
    %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %48), scope: __module.attentions.6 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %50, %51), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
    %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %52), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.6 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.6 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.6
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
    %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.13), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.6 # transformers/modeling_xlm.py:191:0
    %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %62), scope: __module.attentions.6 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.6
    %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %64, %65, %66, %67), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.6
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %69, %70), scope: __module.attentions.6 # torch/nn/functional.py:1498:0
    %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.65), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.6 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.6 # torch/nn/functional.py:973:0
    %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.66, %73, %74), scope: __module.attentions.6 # torch/nn/functional.py:973:0
    %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.attentions.6 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %77, %78), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.6
    %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.67)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.67)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.67)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1676:0
    %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1678:0
    %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %2, %6), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1678:0
    return (%x.26)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.linear.Linear,
        %input.67 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.69)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.69)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1676:0
    %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %4), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1678:0
    %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %2, %6), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1678:0
    return (%input.68)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.66)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.66)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1676:0
    %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1678:0
    %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %2, %6), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1678:0
    return (%x.25)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.68)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.68)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1676:0
    %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1678:0
    %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %2, %6), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1678:0
    return (%x.27)

MultiHeadAttention._actual_script_module
  graph(%self.75 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.74 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.75)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.75)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.75)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.75)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.74, %5), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
    %bs.9 : Long() = prim::NumToTensor(%6), scope: __module.attentions.7
    %9 : int = aten::Int(%bs.9), scope: __module.attentions.7
    %10 : int = aten::Int(%bs.9), scope: __module.attentions.7
    %11 : int = aten::Int(%bs.9), scope: __module.attentions.7
    %12 : int = aten::Int(%bs.9), scope: __module.attentions.7
    %13 : int = aten::Int(%bs.9), scope: __module.attentions.7
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.74, %14), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
    %qlen.8 : Long() = prim::NumToTensor(%15), scope: __module.attentions.7
    %17 : int = aten::Int(%qlen.8), scope: __module.attentions.7
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.74)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.7
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.74)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.7
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.74)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.7
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.7 # transformers/modeling_xlm.py:188:0
    %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %48), scope: __module.attentions.7 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %50, %51), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
    %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %52), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.7 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.7 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.7
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
    %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.15), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.7 # transformers/modeling_xlm.py:191:0
    %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %62), scope: __module.attentions.7 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.7
    %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %64, %65, %66, %67), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.7
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %69, %70), scope: __module.attentions.7 # torch/nn/functional.py:1498:0
    %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.75), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.7 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.7 # torch/nn/functional.py:973:0
    %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %73, %74), scope: __module.attentions.7 # torch/nn/functional.py:973:0
    %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.attentions.7 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %77, %78), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.7
    %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.77)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.77)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.77)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1676:0
    %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1678:0
    %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %2, %6), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1678:0
    return (%x.30)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.linear.Linear,
        %input.77 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.79)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.79)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1676:0
    %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %4), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1678:0
    %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %2, %6), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1678:0
    return (%input.78)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1676:0
    %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1678:0
    %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %2, %6), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1678:0
    return (%x.29)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1676:0
    %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1678:0
    %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %2, %6), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1678:0
    return (%x.31)

MultiHeadAttention._actual_script_module
  graph(%self.85 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.84 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.85)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.85)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.85)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.85)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.84, %5), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
    %bs.10 : Long() = prim::NumToTensor(%6), scope: __module.attentions.8
    %9 : int = aten::Int(%bs.10), scope: __module.attentions.8
    %10 : int = aten::Int(%bs.10), scope: __module.attentions.8
    %11 : int = aten::Int(%bs.10), scope: __module.attentions.8
    %12 : int = aten::Int(%bs.10), scope: __module.attentions.8
    %13 : int = aten::Int(%bs.10), scope: __module.attentions.8
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.84, %14), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
    %qlen.9 : Long() = prim::NumToTensor(%15), scope: __module.attentions.8
    %17 : int = aten::Int(%qlen.9), scope: __module.attentions.8
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.84)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.8
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.84)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.8
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.84)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.8
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.8 # transformers/modeling_xlm.py:188:0
    %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %48), scope: __module.attentions.8 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %50, %51), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
    %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %52), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.8 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.8 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.8
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
    %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.17), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.8 # transformers/modeling_xlm.py:191:0
    %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %62), scope: __module.attentions.8 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.8
    %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %64, %65, %66, %67), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.8
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %69, %70), scope: __module.attentions.8 # torch/nn/functional.py:1498:0
    %input.86 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.85), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.8 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.8 # torch/nn/functional.py:973:0
    %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.86, %73, %74), scope: __module.attentions.8 # torch/nn/functional.py:973:0
    %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.attentions.8 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %77, %78), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.8
    %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.87)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1676:0
    %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1678:0
    %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %2, %6), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1678:0
    return (%x.34)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.linear.Linear,
        %input.87 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1676:0
    %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %4), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1678:0
    %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %2, %6), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1678:0
    return (%input.88)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.86 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.86)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.86)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1676:0
    %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1678:0
    %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %2, %6), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1678:0
    return (%x.33)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.88 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.88)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.88)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1676:0
    %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1678:0
    %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %2, %6), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1678:0
    return (%x.35)

MultiHeadAttention._actual_script_module
  graph(%self.95 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.94 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.95)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.95)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.95)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.95)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.94, %5), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
    %bs.11 : Long() = prim::NumToTensor(%6), scope: __module.attentions.9
    %9 : int = aten::Int(%bs.11), scope: __module.attentions.9
    %10 : int = aten::Int(%bs.11), scope: __module.attentions.9
    %11 : int = aten::Int(%bs.11), scope: __module.attentions.9
    %12 : int = aten::Int(%bs.11), scope: __module.attentions.9
    %13 : int = aten::Int(%bs.11), scope: __module.attentions.9
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.94, %14), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
    %qlen.10 : Long() = prim::NumToTensor(%15), scope: __module.attentions.9
    %17 : int = aten::Int(%qlen.10), scope: __module.attentions.9
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.94)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.9
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.94)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.9
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.94)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.9
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.9 # transformers/modeling_xlm.py:188:0
    %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %48), scope: __module.attentions.9 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %50, %51), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
    %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %52), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.9 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.9 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.9
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
    %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.19), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.9 # transformers/modeling_xlm.py:191:0
    %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %62), scope: __module.attentions.9 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.9
    %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %64, %65, %66, %67), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.9
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %69, %70), scope: __module.attentions.9 # torch/nn/functional.py:1498:0
    %input.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.95), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.9 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.9 # torch/nn/functional.py:973:0
    %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.96, %73, %74), scope: __module.attentions.9 # torch/nn/functional.py:973:0
    %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.attentions.9 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %77, %78), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.9
    %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.97)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.97 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.97)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.97)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1676:0
    %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1678:0
    %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %2, %6), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1678:0
    return (%x.38)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.linear.Linear,
        %input.97 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.99)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.99)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1676:0
    %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %4), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1678:0
    %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %2, %6), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1678:0
    return (%input.98)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.96 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.96)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.96)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1676:0
    %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1678:0
    %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %2, %6), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1678:0
    return (%x.37)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.98)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.98)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1676:0
    %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1678:0
    %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %2, %6), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1678:0
    return (%x.39)

MultiHeadAttention._actual_script_module
  graph(%self.105 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.104 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.105)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.105)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.105)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.105)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.104, %5), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
    %bs.12 : Long() = prim::NumToTensor(%6), scope: __module.attentions.10
    %9 : int = aten::Int(%bs.12), scope: __module.attentions.10
    %10 : int = aten::Int(%bs.12), scope: __module.attentions.10
    %11 : int = aten::Int(%bs.12), scope: __module.attentions.10
    %12 : int = aten::Int(%bs.12), scope: __module.attentions.10
    %13 : int = aten::Int(%bs.12), scope: __module.attentions.10
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.104, %14), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
    %qlen.11 : Long() = prim::NumToTensor(%15), scope: __module.attentions.10
    %17 : int = aten::Int(%qlen.11), scope: __module.attentions.10
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.104)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.10
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.104)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.10
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.104)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.10
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.10 # transformers/modeling_xlm.py:188:0
    %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %48), scope: __module.attentions.10 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %50, %51), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
    %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %52), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.10 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.10 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.10
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
    %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.21), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.10 # transformers/modeling_xlm.py:191:0
    %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %62), scope: __module.attentions.10 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.10
    %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %64, %65, %66, %67), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.10
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %69, %70), scope: __module.attentions.10 # torch/nn/functional.py:1498:0
    %input.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.105), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.10 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.10 # torch/nn/functional.py:973:0
    %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.106, %73, %74), scope: __module.attentions.10 # torch/nn/functional.py:973:0
    %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.attentions.10 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %77, %78), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.10
    %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.107)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.107 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.107)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.107)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1676:0
    %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1678:0
    %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %2, %6), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1678:0
    return (%x.42)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.linear.Linear,
        %input.107 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.109)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1676:0
    %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %4), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1678:0
    %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %2, %6), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1678:0
    return (%input.108)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.106 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.106)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.106)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1676:0
    %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1678:0
    %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %2, %6), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1678:0
    return (%x.41)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1676:0
    %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1678:0
    %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %2, %6), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1678:0
    return (%x.43)

MultiHeadAttention._actual_script_module
  graph(%self.115 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.114 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.115)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.115)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.115)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.115)
    %5 : int = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
    %6 : int = aten::size(%input.114, %5), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
    %bs : Long() = prim::NumToTensor(%6), scope: __module.attentions.11
    %9 : int = aten::Int(%bs), scope: __module.attentions.11
    %10 : int = aten::Int(%bs), scope: __module.attentions.11
    %11 : int = aten::Int(%bs), scope: __module.attentions.11
    %12 : int = aten::Int(%bs), scope: __module.attentions.11
    %13 : int = aten::Int(%bs), scope: __module.attentions.11
    %14 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
    %15 : int = aten::size(%input.114, %14), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
    %qlen : Long() = prim::NumToTensor(%15), scope: __module.attentions.11
    %17 : int = aten::Int(%qlen), scope: __module.attentions.11
    %87 : Tensor = prim::CallMethod[name="forward"](%4, %input.114)
    %22 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %23 : int = prim::Constant[value=16](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=128](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %25 : int[] = prim::ListConstruct(%13, %22, %23, %24), scope: __module.attentions.11
    %26 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %25), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %27 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%26, %27, %28), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%3, %input.114)
    %31 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %32 : int = prim::Constant[value=16](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=128](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %34 : int[] = prim::ListConstruct(%12, %31, %32, %33), scope: __module.attentions.11
    %35 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %34), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %36 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%35, %36, %37), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%2, %input.114)
    %40 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %41 : int = prim::Constant[value=16](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=128](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %43 : int[] = prim::ListConstruct(%11, %40, %41, %42), scope: __module.attentions.11
    %44 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %43), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %45 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%44, %45, %46), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
    %48 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.11 # transformers/modeling_xlm.py:188:0
    %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %48), scope: __module.attentions.11 # transformers/modeling_xlm.py:188:0
    %50 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
    %51 : int = prim::Constant[value=3](), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
    %52 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %50, %51), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
    %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %52), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
    %54 : int = prim::Constant[value=0](), scope: __module.attentions.11 # torch/tensor.py:22:0
    %55 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %54), scope: __module.attentions.11 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%10, %57, %58, %17), scope: __module.attentions.11
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%55, %59), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
    %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.23), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.attentions.11 # transformers/modeling_xlm.py:191:0
    %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %62), scope: __module.attentions.11 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.attentions.11
    %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %64, %65, %66, %67), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.attentions.11
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %69, %70), scope: __module.attentions.11 # torch/nn/functional.py:1498:0
    %input.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.115), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.11 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.attentions.11 # torch/nn/functional.py:973:0
    %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.116, %73, %74), scope: __module.attentions.11 # torch/nn/functional.py:973:0
    %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.attentions.11 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %77, %78), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%9, %82, %83), scope: __module.attentions.11
    %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%1, %input.117)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.117 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.117)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.117)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1676:0
    %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1678:0
    %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %2, %6), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1678:0
    return (%x.46)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.linear.Linear,
        %input.117 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.119)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.119)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1676:0
    %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %4), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1678:0
    %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %2, %6), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1678:0
    return (%input.118)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.116 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.116)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.116)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1676:0
    %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1678:0
    %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %2, %6), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1678:0
    return (%x.45)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.118 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.118)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.118)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1676:0
    %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1678:0
    %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %2, %6), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1678:0
    return (%x.47)

LayerNorm._actual_script_module
  graph(%self.10 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.9 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.10)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.10)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.0
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %4, %2, %1, %5, %6), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

LayerNorm._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.19 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.20)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.1
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %4, %2, %1, %5, %6), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

LayerNorm._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.29 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.30)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.30)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.2
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %4, %2, %1, %5, %6), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

LayerNorm._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.39 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.40)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.40)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.3
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %4, %2, %1, %5, %6), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

LayerNorm._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.49 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.50)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.4
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %4, %2, %1, %5, %6), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

LayerNorm._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.59 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.5
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %4, %2, %1, %5, %6), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

LayerNorm._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.6
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
    %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %4, %2, %1, %5, %6), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
    return (%input_tensor.7)

LayerNorm._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.79 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.7
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
    %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %4, %2, %1, %5, %6), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
    return (%input_tensor.8)

LayerNorm._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.89 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.90)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.90)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.8
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %4, %2, %1, %5, %6), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

LayerNorm._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.99 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.100)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.9
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
    %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %4, %2, %1, %5, %6), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
    return (%input_tensor.10)

LayerNorm._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.109 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.10
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
    %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %4, %2, %1, %5, %6), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
    return (%input_tensor.11)

LayerNorm._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.119 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.120)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm1.11
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
    %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %4, %2, %1, %5, %6), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
    return (%input_tensor)

TransformerFFN._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.11)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.11)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.0 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.11)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.0 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.0 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.0 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.12)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1676:0
    %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1678:0
    %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %2, %6), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1678:0
    return (%input.10)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %input.11 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1676:0
    %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %4), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1678:0
    %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %2, %6), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1678:0
    return (%input.12)

TransformerFFN._actual_script_module
  graph(%self.21 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.21)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.21)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.1 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.21)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.1 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.1 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.1 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.22 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.22)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.22)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1676:0
    %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1678:0
    %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %2, %6), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1678:0
    return (%input.20)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.linear.Linear,
        %input.21 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.23)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.23)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1676:0
    %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %4), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1678:0
    %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %2, %6), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1678:0
    return (%input.22)

TransformerFFN._actual_script_module
  graph(%self.31 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.31)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.31)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.2 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.31)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.2 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.2 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.2 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1676:0
    %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1678:0
    %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %2, %6), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1678:0
    return (%input.30)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.linear.Linear,
        %input.31 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.33)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.33)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1676:0
    %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %4), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1678:0
    %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %2, %6), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1678:0
    return (%input.32)

TransformerFFN._actual_script_module
  graph(%self.41 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.41)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.41)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.3 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.41)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.3 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.3 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.3 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.42)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1676:0
    %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1678:0
    %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %2, %6), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1678:0
    return (%input.40)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.linear.Linear,
        %input.41 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.43)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1676:0
    %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %4), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1678:0
    %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %2, %6), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1678:0
    return (%input.42)

TransformerFFN._actual_script_module
  graph(%self.51 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.51)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.51)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.4 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.51)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.4 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.4 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.4 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.52)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.52)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1676:0
    %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1678:0
    %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %2, %6), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1678:0
    return (%input.50)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.linear.Linear,
        %input.51 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.53)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.53)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1676:0
    %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %4), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1678:0
    %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %2, %6), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1678:0
    return (%input.52)

TransformerFFN._actual_script_module
  graph(%self.61 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.61)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.61)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.5 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.61)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.5 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.5 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.5 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.62 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.62)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.62)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1676:0
    %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1678:0
    %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %2, %6), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1678:0
    return (%input.60)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.linear.Linear,
        %input.61 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.63)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.63)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1676:0
    %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %4), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1678:0
    %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %2, %6), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1678:0
    return (%input.62)

TransformerFFN._actual_script_module
  graph(%self.71 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.71)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.71)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.6 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.71)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.6 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.6 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.6 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.72 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.72)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.72)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1676:0
    %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1678:0
    %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %2, %6), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1678:0
    return (%input.70)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.73 : __torch__.torch.nn.modules.linear.Linear,
        %input.71 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.73)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.73)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1676:0
    %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %4), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1678:0
    %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %2, %6), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1678:0
    return (%input.72)

TransformerFFN._actual_script_module
  graph(%self.81 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.81)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.81)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.7 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.81)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.7 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.7 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.7 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1676:0
    %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1678:0
    %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %2, %6), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1678:0
    return (%input.80)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.linear.Linear,
        %input.81 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.83)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.83)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1676:0
    %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %4), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1678:0
    %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %2, %6), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1678:0
    return (%input.82)

TransformerFFN._actual_script_module
  graph(%self.91 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.91)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.91)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.8 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.91)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.8 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.8 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.8 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.92)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.92)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1676:0
    %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1678:0
    %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %2, %6), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1678:0
    return (%input.90)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.linear.Linear,
        %input.91 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1676:0
    %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %4), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1678:0
    %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %2, %6), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1678:0
    return (%input.92)

TransformerFFN._actual_script_module
  graph(%self.101 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.101)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.101)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.9 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.101)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.9 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.9 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.9 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.102)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1676:0
    %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1678:0
    %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %2, %6), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1678:0
    return (%input.100)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.103 : __torch__.torch.nn.modules.linear.Linear,
        %input.101 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.103)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.103)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1676:0
    %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %4), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1678:0
    %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %2, %6), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1678:0
    return (%input.102)

TransformerFFN._actual_script_module
  graph(%self.111 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.111)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.111)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.10 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.111)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.10 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.10 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.10 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.112)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.112)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1676:0
    %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1678:0
    %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %2, %6), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1678:0
    return (%input.110)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.linear.Linear,
        %input.111 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.113)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.113)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1676:0
    %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %4), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1678:0
    %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %2, %6), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1678:0
    return (%input.112)

TransformerFFN._actual_script_module
  graph(%self.121 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %5 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.121)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.121)
    %31 : Tensor = prim::CallMethod[name="forward"](%2, %5)
    %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.ffns.11 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%1, %input.121)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.11 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.ffns.11 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.ffns.11 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.122)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.122)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1676:0
    %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1678:0
    %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %2, %6), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1678:0
    return (%input.120)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.linear.Linear,
        %input.121 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1676:0
    %output : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %4), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1678:0
    %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output, %2, %6), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1678:0
    return (%input.122)

LayerNorm._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.13 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.0
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
    %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %4, %2, %1, %5, %6), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
    return (%tensor.2)

LayerNorm._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.23 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.24)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.1
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
    %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %4, %2, %1, %5, %6), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
    return (%tensor.3)

LayerNorm._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.33 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.2
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
    %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %4, %2, %1, %5, %6), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
    return (%tensor.4)

LayerNorm._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.43 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.3
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
    %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %4, %2, %1, %5, %6), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
    return (%tensor.5)

LayerNorm._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.53 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.4
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
    %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %4, %2, %1, %5, %6), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
    return (%tensor.6)

LayerNorm._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.63 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.5
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
    %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %4, %2, %1, %5, %6), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
    return (%tensor.7)

LayerNorm._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.73 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.74)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.74)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.6
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
    %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %4, %2, %1, %5, %6), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
    return (%tensor.8)

LayerNorm._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.83 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.84)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.7
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
    %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %4, %2, %1, %5, %6), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
    return (%tensor.9)

LayerNorm._actual_script_module
  graph(%self.94 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.93 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.94)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.94)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.8
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
    %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %4, %2, %1, %5, %6), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
    return (%tensor.10)

LayerNorm._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.103 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.9
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
    %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %4, %2, %1, %5, %6), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
    return (%tensor.11)

LayerNorm._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.113 : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.10
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
    %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %4, %2, %1, %5, %6), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
    return (%tensor.12)

LayerNorm._actual_script_module
  graph(%self : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input : Float(17:26624, 13:2048, 2048:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self)
    %2 : Tensor = prim::GetAttr[name="weight"](%self)
    %3 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
    %4 : int[] = prim::ListConstruct(%3), scope: __module.layer_norm2.11
    %5 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
    %6 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
    %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input, %4, %2, %1, %5, %6), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
    return (%tensor)

