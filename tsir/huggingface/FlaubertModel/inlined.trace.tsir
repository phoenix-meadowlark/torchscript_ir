graph(%self.1 : __torch__.transformers.modeling_flaubert.___torch_mangle_1173.FlaubertModel,
      %input_ids : Long(17:13, 13:1),
      %padding_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %4 : __torch__.torch.nn.modules.normalization.___torch_mangle_1171.LayerNorm = prim::GetAttr[name="11"](%3)
  %5 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %6 : __torch__.transformers.modeling_xlm.___torch_mangle_1158.TransformerFFN = prim::GetAttr[name="11"](%5)
  %7 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %8 : __torch__.torch.nn.modules.normalization.___torch_mangle_1121.LayerNorm = prim::GetAttr[name="11"](%7)
  %9 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %10 : __torch__.transformers.modeling_xlm.___torch_mangle_1108.MultiHeadAttention = prim::GetAttr[name="11"](%9)
  %11 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %12 : __torch__.torch.nn.modules.normalization.___torch_mangle_1170.LayerNorm = prim::GetAttr[name="10"](%11)
  %13 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %14 : __torch__.transformers.modeling_xlm.___torch_mangle_1155.TransformerFFN = prim::GetAttr[name="10"](%13)
  %15 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %16 : __torch__.torch.nn.modules.normalization.___torch_mangle_1120.LayerNorm = prim::GetAttr[name="10"](%15)
  %17 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %18 : __torch__.transformers.modeling_xlm.___torch_mangle_1103.MultiHeadAttention = prim::GetAttr[name="10"](%17)
  %19 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %20 : __torch__.torch.nn.modules.normalization.___torch_mangle_1169.LayerNorm = prim::GetAttr[name="9"](%19)
  %21 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %22 : __torch__.transformers.modeling_xlm.___torch_mangle_1152.TransformerFFN = prim::GetAttr[name="9"](%21)
  %23 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %24 : __torch__.torch.nn.modules.normalization.___torch_mangle_1119.LayerNorm = prim::GetAttr[name="9"](%23)
  %25 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %26 : __torch__.transformers.modeling_xlm.___torch_mangle_1098.MultiHeadAttention = prim::GetAttr[name="9"](%25)
  %27 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %28 : __torch__.torch.nn.modules.normalization.___torch_mangle_1168.LayerNorm = prim::GetAttr[name="8"](%27)
  %29 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %30 : __torch__.transformers.modeling_xlm.___torch_mangle_1149.TransformerFFN = prim::GetAttr[name="8"](%29)
  %31 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %32 : __torch__.torch.nn.modules.normalization.___torch_mangle_1118.LayerNorm = prim::GetAttr[name="8"](%31)
  %33 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %34 : __torch__.transformers.modeling_xlm.___torch_mangle_1093.MultiHeadAttention = prim::GetAttr[name="8"](%33)
  %35 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %36 : __torch__.torch.nn.modules.normalization.___torch_mangle_1167.LayerNorm = prim::GetAttr[name="7"](%35)
  %37 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %38 : __torch__.transformers.modeling_xlm.___torch_mangle_1146.TransformerFFN = prim::GetAttr[name="7"](%37)
  %39 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %40 : __torch__.torch.nn.modules.normalization.___torch_mangle_1117.LayerNorm = prim::GetAttr[name="7"](%39)
  %41 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %42 : __torch__.transformers.modeling_xlm.___torch_mangle_1088.MultiHeadAttention = prim::GetAttr[name="7"](%41)
  %43 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %44 : __torch__.torch.nn.modules.normalization.___torch_mangle_1166.LayerNorm = prim::GetAttr[name="6"](%43)
  %45 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %46 : __torch__.transformers.modeling_xlm.___torch_mangle_1143.TransformerFFN = prim::GetAttr[name="6"](%45)
  %47 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %48 : __torch__.torch.nn.modules.normalization.___torch_mangle_1116.LayerNorm = prim::GetAttr[name="6"](%47)
  %49 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %50 : __torch__.transformers.modeling_xlm.___torch_mangle_1083.MultiHeadAttention = prim::GetAttr[name="6"](%49)
  %51 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %52 : __torch__.torch.nn.modules.normalization.___torch_mangle_1165.LayerNorm = prim::GetAttr[name="5"](%51)
  %53 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %54 : __torch__.transformers.modeling_xlm.___torch_mangle_1140.TransformerFFN = prim::GetAttr[name="5"](%53)
  %55 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %56 : __torch__.torch.nn.modules.normalization.___torch_mangle_1115.LayerNorm = prim::GetAttr[name="5"](%55)
  %57 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %58 : __torch__.transformers.modeling_xlm.___torch_mangle_1078.MultiHeadAttention = prim::GetAttr[name="5"](%57)
  %59 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %60 : __torch__.torch.nn.modules.normalization.___torch_mangle_1164.LayerNorm = prim::GetAttr[name="4"](%59)
  %61 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %62 : __torch__.transformers.modeling_xlm.___torch_mangle_1137.TransformerFFN = prim::GetAttr[name="4"](%61)
  %63 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %64 : __torch__.torch.nn.modules.normalization.___torch_mangle_1114.LayerNorm = prim::GetAttr[name="4"](%63)
  %65 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %66 : __torch__.transformers.modeling_xlm.___torch_mangle_1073.MultiHeadAttention = prim::GetAttr[name="4"](%65)
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %68 : __torch__.torch.nn.modules.normalization.___torch_mangle_1163.LayerNorm = prim::GetAttr[name="3"](%67)
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %70 : __torch__.transformers.modeling_xlm.___torch_mangle_1134.TransformerFFN = prim::GetAttr[name="3"](%69)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %72 : __torch__.torch.nn.modules.normalization.___torch_mangle_1113.LayerNorm = prim::GetAttr[name="3"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %74 : __torch__.transformers.modeling_xlm.___torch_mangle_1068.MultiHeadAttention = prim::GetAttr[name="3"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %76 : __torch__.torch.nn.modules.normalization.___torch_mangle_1162.LayerNorm = prim::GetAttr[name="2"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %78 : __torch__.transformers.modeling_xlm.___torch_mangle_1131.TransformerFFN = prim::GetAttr[name="2"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %80 : __torch__.torch.nn.modules.normalization.___torch_mangle_1112.LayerNorm = prim::GetAttr[name="2"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %82 : __torch__.transformers.modeling_xlm.___torch_mangle_1063.MultiHeadAttention = prim::GetAttr[name="2"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %84 : __torch__.torch.nn.modules.normalization.___torch_mangle_1161.LayerNorm = prim::GetAttr[name="1"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %86 : __torch__.transformers.modeling_xlm.___torch_mangle_1128.TransformerFFN = prim::GetAttr[name="1"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %88 : __torch__.torch.nn.modules.normalization.___torch_mangle_1111.LayerNorm = prim::GetAttr[name="1"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %90 : __torch__.transformers.modeling_xlm.___torch_mangle_1058.MultiHeadAttention = prim::GetAttr[name="1"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_1172.ModuleList = prim::GetAttr[name="layer_norm2"](%self.1)
  %92 : __torch__.torch.nn.modules.normalization.___torch_mangle_1160.LayerNorm = prim::GetAttr[name="0"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_1159.ModuleList = prim::GetAttr[name="ffns"](%self.1)
  %94 : __torch__.transformers.modeling_xlm.___torch_mangle_1125.TransformerFFN = prim::GetAttr[name="0"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_1122.ModuleList = prim::GetAttr[name="layer_norm1"](%self.1)
  %96 : __torch__.torch.nn.modules.normalization.___torch_mangle_1110.LayerNorm = prim::GetAttr[name="0"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_1109.ModuleList = prim::GetAttr[name="attentions"](%self.1)
  %98 : __torch__.transformers.modeling_xlm.___torch_mangle_1053.MultiHeadAttention = prim::GetAttr[name="0"](%97)
  %99 : __torch__.torch.nn.modules.normalization.___torch_mangle_1048.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%self.1)
  %100 : __torch__.torch.nn.modules.sparse.___torch_mangle_1046.Embedding = prim::GetAttr[name="position_embeddings"](%self.1)
  %101 : __torch__.torch.nn.modules.sparse.___torch_mangle_1047.Embedding = prim::GetAttr[name="embeddings"](%self.1)
  %102 : int = prim::Constant[value=0]() # transformers/modeling_flaubert.py:174:0
  %103 : int = aten::size(%input_ids, %102) # transformers/modeling_flaubert.py:174:0
  %bs.1 : Long() = prim::NumToTensor(%103)
  %105 : int = aten::Int(%bs.1)
  %106 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:174:0
  %107 : int = aten::size(%input_ids, %106) # transformers/modeling_flaubert.py:174:0
  %slen : Long() = prim::NumToTensor(%107)
  %109 : int = aten::Int(%slen)
  %110 : Scalar = aten::ScalarImplicit(%slen)
  %111 : int = prim::Constant[value=4]() # transformers/modeling_flaubert.py:203:0
  %112 : int = prim::Constant[value=0]() # transformers/modeling_flaubert.py:203:0
  %113 : Device = prim::Constant[value="cpu"]() # transformers/modeling_flaubert.py:203:0
  %114 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:203:0
  %position_ids : Long(13:1) = aten::arange(%110, %111, %112, %113, %114) # transformers/modeling_flaubert.py:203:0
  %116 : int = prim::Constant[value=0]() # transformers/modeling_flaubert.py:204:0
  %117 : Long(1:13, 13:1) = aten::unsqueeze(%position_ids, %116) # transformers/modeling_flaubert.py:204:0
  %118 : int[] = prim::ListConstruct(%105, %109)
  %119 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:204:0
  %input.1 : Long(17:0, 13:1) = aten::expand(%117, %118, %119) # transformers/modeling_flaubert.py:204:0
  %367 : bool = prim::Constant[value=0](), scope: __module.embeddings # torch/nn/functional.py:1814:0
  %368 : int = prim::Constant[value=2](), scope: __module.embeddings # torch/nn/functional.py:1814:0
  %369 : Tensor = prim::GetAttr[name="weight"](%101)
  %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%369, %input_ids, %368, %367, %367), scope: __module.embeddings # torch/nn/functional.py:1814:0
  %371 : bool = prim::Constant[value=0](), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
  %372 : int = prim::Constant[value=-1](), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
  %373 : Tensor = prim::GetAttr[name="weight"](%100)
  %374 : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%373, %input.1, %372, %371, %371), scope: __module.position_embeddings # torch/nn/functional.py:1814:0
  %123 : Float(17:26624, 13:2048, 2048:1) = aten::expand_as(%374, %inputs_embeds) # transformers/modeling_flaubert.py:231:0
  %124 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:231:0
  %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%inputs_embeds, %123, %124) # transformers/modeling_flaubert.py:231:0
  %375 : bool = prim::Constant[value=1](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
  %376 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
  %377 : int = prim::Constant[value=2048](), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
  %378 : Tensor = prim::GetAttr[name="bias"](%99)
  %379 : Tensor = prim::GetAttr[name="weight"](%99)
  %380 : int[] = prim::ListConstruct(%377), scope: __module.layer_norm_emb
  %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %380, %379, %378, %376, %375), scope: __module.layer_norm_emb # torch/nn/functional.py:2048:0
  %127 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %128 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.3, %127, %128) # torch/nn/functional.py:973:0
  %130 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:238:0
  %131 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %130) # transformers/modeling_flaubert.py:238:0
  %132 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:238:0
  %133 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:238:0
  %134 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:238:0
  %135 : None = prim::Constant()
  %136 : Float(17:13, 13:1, 1:1) = aten::to(%131, %132, %133, %134, %135) # transformers/modeling_flaubert.py:238:0
  %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %136) # transformers/modeling_flaubert.py:238:0
  %382 : int = prim::Constant[value=2048](), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
  %383 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.0 # torch/nn/functional.py:973:0
  %384 : None = prim::Constant(), scope: __module.attentions.0
  %385 : bool = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
  %386 : int = prim::Constant[value=6](), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
  %387 : float = prim::Constant[value=-inf](), scope: __module.attentions.0 # transformers/modeling_xlm.py:191:0
  %388 : int = prim::Constant[value=3](), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
  %389 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.0 # transformers/modeling_xlm.py:188:0
  %390 : int = prim::Constant[value=2](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %391 : int = prim::Constant[value=128](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %392 : int = prim::Constant[value=16](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %393 : int = prim::Constant[value=-1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %394 : int = prim::Constant[value=1](), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
  %395 : int = prim::Constant[value=0](), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
  %396 : __torch__.torch.nn.modules.linear.___torch_mangle_1052.Linear = prim::GetAttr[name="out_lin"](%98)
  %397 : __torch__.torch.nn.modules.linear.___torch_mangle_1051.Linear = prim::GetAttr[name="v_lin"](%98)
  %398 : __torch__.torch.nn.modules.linear.___torch_mangle_1050.Linear = prim::GetAttr[name="k_lin"](%98)
  %399 : __torch__.torch.nn.modules.linear.___torch_mangle_1049.Linear = prim::GetAttr[name="q_lin"](%98)
  %400 : int = aten::size(%input.4, %395), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
  %401 : int = aten::size(%input.4, %394), scope: __module.attentions.0 # transformers/modeling_xlm.py:151:0
  %402 : Tensor = prim::GetAttr[name="bias"](%399)
  %403 : Tensor = prim::GetAttr[name="weight"](%399)
  %404 : Float(2048:1, 2048:2048) = aten::t(%403), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %404), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %402, %394), scope: __module.attentions.0/__module.attentions.0.q_lin # torch/nn/functional.py:1678:0
  %407 : int[] = prim::ListConstruct(%400, %393, %392, %391), scope: __module.attentions.0
  %408 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.1, %407), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%408, %394, %390), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %410 : Tensor = prim::GetAttr[name="bias"](%398)
  %411 : Tensor = prim::GetAttr[name="weight"](%398)
  %412 : Float(2048:1, 2048:2048) = aten::t(%411), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %412), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %410, %394), scope: __module.attentions.0/__module.attentions.0.k_lin # torch/nn/functional.py:1678:0
  %415 : int[] = prim::ListConstruct(%400, %393, %392, %391), scope: __module.attentions.0
  %416 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.2, %415), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%416, %394, %390), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %418 : Tensor = prim::GetAttr[name="bias"](%397)
  %419 : Tensor = prim::GetAttr[name="weight"](%397)
  %420 : Float(2048:1, 2048:2048) = aten::t(%419), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %420), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %418, %394), scope: __module.attentions.0/__module.attentions.0.v_lin # torch/nn/functional.py:1678:0
  %423 : int[] = prim::ListConstruct(%400, %393, %392, %391), scope: __module.attentions.0
  %424 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.3, %423), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%424, %394, %390), scope: __module.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %389), scope: __module.attentions.0 # transformers/modeling_xlm.py:188:0
  %427 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %390, %388), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
  %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %427), scope: __module.attentions.0 # transformers/modeling_xlm.py:189:0
  %429 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %395), scope: __module.attentions.0 # torch/tensor.py:22:0
  %430 : int[] = prim::ListConstruct(%400, %394, %394, %401), scope: __module.attentions.0
  %431 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%429, %430), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
  %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%431, %scores.1), scope: __module.attentions.0 # transformers/modeling_xlm.py:190:0
  %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %387), scope: __module.attentions.0 # transformers/modeling_xlm.py:191:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %386, %385, %385, %384), scope: __module.attentions.0 # transformers/modeling_xlm.py:193:0
  %435 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %393, %384), scope: __module.attentions.0 # torch/nn/functional.py:1498:0
  %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%435, %383, %385), scope: __module.attentions.0 # torch/nn/functional.py:973:0
  %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.attentions.0 # transformers/modeling_xlm.py:200:0
  %438 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %394, %390), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
  %439 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%438, %395), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
  %440 : int[] = prim::ListConstruct(%400, %393, %382), scope: __module.attentions.0
  %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%439, %440), scope: __module.attentions.0 # transformers/modeling_xlm.py:167:0
  %442 : Tensor = prim::GetAttr[name="bias"](%396)
  %443 : Tensor = prim::GetAttr[name="weight"](%396)
  %444 : Float(2048:1, 2048:2048) = aten::t(%443), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %444), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %442, %394), scope: __module.attentions.0/__module.attentions.0.out_lin # torch/nn/functional.py:1678:0
  %139 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %140 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.8, %139, %140) # torch/nn/functional.py:973:0
  %142 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %142) # transformers/modeling_flaubert.py:265:0
  %447 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
  %448 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
  %449 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
  %450 : Tensor = prim::GetAttr[name="bias"](%96)
  %451 : Tensor = prim::GetAttr[name="weight"](%96)
  %452 : int[] = prim::ListConstruct(%449), scope: __module.layer_norm1.0
  %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %452, %451, %450, %448, %447), scope: __module.layer_norm1.0 # torch/nn/functional.py:2048:0
  %454 : bool = prim::Constant[value=0](), scope: __module.ffns.0 # torch/nn/functional.py:973:0
  %455 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.0 # torch/nn/functional.py:973:0
  %456 : int = prim::Constant[value=1](), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1678:0
  %457 : __torch__.torch.nn.modules.linear.___torch_mangle_1124.Linear = prim::GetAttr[name="lin2"](%94)
  %458 : __torch__.torch.nn.modules.linear.___torch_mangle_1123.Linear = prim::GetAttr[name="lin1"](%94)
  %459 : Tensor = prim::GetAttr[name="bias"](%458)
  %460 : Tensor = prim::GetAttr[name="weight"](%458)
  %461 : Float(2048:1, 8192:2048) = aten::t(%460), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.1, %461), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %459, %456), scope: __module.ffns.0/__module.ffns.0.lin1 # torch/nn/functional.py:1678:0
  %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.10), scope: __module.ffns.0 # torch/nn/functional.py:1369:0
  %465 : Tensor = prim::GetAttr[name="bias"](%457)
  %466 : Tensor = prim::GetAttr[name="weight"](%457)
  %467 : Float(8192:1, 2048:8192) = aten::t(%466), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %467), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %465, %456), scope: __module.ffns.0/__module.ffns.0.lin2 # torch/nn/functional.py:1678:0
  %470 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.12, %455, %454), scope: __module.ffns.0 # torch/nn/functional.py:973:0
  %146 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.1, %470, %146) # transformers/modeling_flaubert.py:285:0
  %471 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
  %472 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
  %473 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
  %474 : Tensor = prim::GetAttr[name="bias"](%92)
  %475 : Tensor = prim::GetAttr[name="weight"](%92)
  %476 : int[] = prim::ListConstruct(%473), scope: __module.layer_norm2.0
  %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %476, %475, %474, %472, %471), scope: __module.layer_norm2.0 # torch/nn/functional.py:2048:0
  %149 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %150 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %149) # transformers/modeling_flaubert.py:291:0
  %151 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %152 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %153 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %154 : None = prim::Constant()
  %155 : Float(17:13, 13:1, 1:1) = aten::to(%150, %151, %152, %153, %154) # transformers/modeling_flaubert.py:291:0
  %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.2, %155) # transformers/modeling_flaubert.py:291:0
  %478 : int = prim::Constant[value=2048](), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
  %479 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.1 # torch/nn/functional.py:973:0
  %480 : None = prim::Constant(), scope: __module.attentions.1
  %481 : bool = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
  %482 : int = prim::Constant[value=6](), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
  %483 : float = prim::Constant[value=-inf](), scope: __module.attentions.1 # transformers/modeling_xlm.py:191:0
  %484 : int = prim::Constant[value=3](), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
  %485 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.1 # transformers/modeling_xlm.py:188:0
  %486 : int = prim::Constant[value=2](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %487 : int = prim::Constant[value=128](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %488 : int = prim::Constant[value=16](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %489 : int = prim::Constant[value=-1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %490 : int = prim::Constant[value=1](), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
  %491 : int = prim::Constant[value=0](), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
  %492 : __torch__.torch.nn.modules.linear.___torch_mangle_1057.Linear = prim::GetAttr[name="out_lin"](%90)
  %493 : __torch__.torch.nn.modules.linear.___torch_mangle_1056.Linear = prim::GetAttr[name="v_lin"](%90)
  %494 : __torch__.torch.nn.modules.linear.___torch_mangle_1055.Linear = prim::GetAttr[name="k_lin"](%90)
  %495 : __torch__.torch.nn.modules.linear.___torch_mangle_1054.Linear = prim::GetAttr[name="q_lin"](%90)
  %496 : int = aten::size(%input.14, %491), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
  %497 : int = aten::size(%input.14, %490), scope: __module.attentions.1 # transformers/modeling_xlm.py:151:0
  %498 : Tensor = prim::GetAttr[name="bias"](%495)
  %499 : Tensor = prim::GetAttr[name="weight"](%495)
  %500 : Float(2048:1, 2048:2048) = aten::t(%499), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %500), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %498, %490), scope: __module.attentions.1/__module.attentions.1.q_lin # torch/nn/functional.py:1678:0
  %503 : int[] = prim::ListConstruct(%496, %489, %488, %487), scope: __module.attentions.1
  %504 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.5, %503), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%504, %490, %486), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %506 : Tensor = prim::GetAttr[name="bias"](%494)
  %507 : Tensor = prim::GetAttr[name="weight"](%494)
  %508 : Float(2048:1, 2048:2048) = aten::t(%507), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %508), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %506, %490), scope: __module.attentions.1/__module.attentions.1.k_lin # torch/nn/functional.py:1678:0
  %511 : int[] = prim::ListConstruct(%496, %489, %488, %487), scope: __module.attentions.1
  %512 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.6, %511), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%512, %490, %486), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %514 : Tensor = prim::GetAttr[name="bias"](%493)
  %515 : Tensor = prim::GetAttr[name="weight"](%493)
  %516 : Float(2048:1, 2048:2048) = aten::t(%515), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %516), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %514, %490), scope: __module.attentions.1/__module.attentions.1.v_lin # torch/nn/functional.py:1678:0
  %519 : int[] = prim::ListConstruct(%496, %489, %488, %487), scope: __module.attentions.1
  %520 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.7, %519), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%520, %490, %486), scope: __module.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %485), scope: __module.attentions.1 # transformers/modeling_xlm.py:188:0
  %523 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %486, %484), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
  %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %523), scope: __module.attentions.1 # transformers/modeling_xlm.py:189:0
  %525 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %491), scope: __module.attentions.1 # torch/tensor.py:22:0
  %526 : int[] = prim::ListConstruct(%496, %490, %490, %497), scope: __module.attentions.1
  %527 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%525, %526), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
  %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%527, %scores.3), scope: __module.attentions.1 # transformers/modeling_xlm.py:190:0
  %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %483), scope: __module.attentions.1 # transformers/modeling_xlm.py:191:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %482, %481, %481, %480), scope: __module.attentions.1 # transformers/modeling_xlm.py:193:0
  %531 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %489, %480), scope: __module.attentions.1 # torch/nn/functional.py:1498:0
  %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%531, %479, %481), scope: __module.attentions.1 # torch/nn/functional.py:973:0
  %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.attentions.1 # transformers/modeling_xlm.py:200:0
  %534 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %490, %486), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
  %535 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%534, %491), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
  %536 : int[] = prim::ListConstruct(%496, %489, %478), scope: __module.attentions.1
  %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%535, %536), scope: __module.attentions.1 # transformers/modeling_xlm.py:167:0
  %538 : Tensor = prim::GetAttr[name="bias"](%492)
  %539 : Tensor = prim::GetAttr[name="weight"](%492)
  %540 : Float(2048:1, 2048:2048) = aten::t(%539), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %540), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %538, %490), scope: __module.attentions.1/__module.attentions.1.out_lin # torch/nn/functional.py:1678:0
  %158 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %159 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.18, %158, %159) # torch/nn/functional.py:973:0
  %161 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %161) # transformers/modeling_flaubert.py:265:0
  %543 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
  %544 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
  %545 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
  %546 : Tensor = prim::GetAttr[name="bias"](%88)
  %547 : Tensor = prim::GetAttr[name="weight"](%88)
  %548 : int[] = prim::ListConstruct(%545), scope: __module.layer_norm1.1
  %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %548, %547, %546, %544, %543), scope: __module.layer_norm1.1 # torch/nn/functional.py:2048:0
  %550 : bool = prim::Constant[value=0](), scope: __module.ffns.1 # torch/nn/functional.py:973:0
  %551 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.1 # torch/nn/functional.py:973:0
  %552 : int = prim::Constant[value=1](), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1678:0
  %553 : __torch__.torch.nn.modules.linear.___torch_mangle_1127.Linear = prim::GetAttr[name="lin2"](%86)
  %554 : __torch__.torch.nn.modules.linear.___torch_mangle_1126.Linear = prim::GetAttr[name="lin1"](%86)
  %555 : Tensor = prim::GetAttr[name="bias"](%554)
  %556 : Tensor = prim::GetAttr[name="weight"](%554)
  %557 : Float(2048:1, 8192:2048) = aten::t(%556), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.2, %557), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %555, %552), scope: __module.ffns.1/__module.ffns.1.lin1 # torch/nn/functional.py:1678:0
  %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.20), scope: __module.ffns.1 # torch/nn/functional.py:1369:0
  %561 : Tensor = prim::GetAttr[name="bias"](%553)
  %562 : Tensor = prim::GetAttr[name="weight"](%553)
  %563 : Float(8192:1, 2048:8192) = aten::t(%562), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %563), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %561, %552), scope: __module.ffns.1/__module.ffns.1.lin2 # torch/nn/functional.py:1678:0
  %566 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.22, %551, %550), scope: __module.ffns.1 # torch/nn/functional.py:973:0
  %165 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.2, %566, %165) # transformers/modeling_flaubert.py:285:0
  %567 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
  %568 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
  %569 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
  %570 : Tensor = prim::GetAttr[name="bias"](%84)
  %571 : Tensor = prim::GetAttr[name="weight"](%84)
  %572 : int[] = prim::ListConstruct(%569), scope: __module.layer_norm2.1
  %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %572, %571, %570, %568, %567), scope: __module.layer_norm2.1 # torch/nn/functional.py:2048:0
  %168 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %169 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %168) # transformers/modeling_flaubert.py:291:0
  %170 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %171 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %172 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %173 : None = prim::Constant()
  %174 : Float(17:13, 13:1, 1:1) = aten::to(%169, %170, %171, %172, %173) # transformers/modeling_flaubert.py:291:0
  %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.3, %174) # transformers/modeling_flaubert.py:291:0
  %574 : int = prim::Constant[value=2048](), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
  %575 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.2 # torch/nn/functional.py:973:0
  %576 : None = prim::Constant(), scope: __module.attentions.2
  %577 : bool = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
  %578 : int = prim::Constant[value=6](), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
  %579 : float = prim::Constant[value=-inf](), scope: __module.attentions.2 # transformers/modeling_xlm.py:191:0
  %580 : int = prim::Constant[value=3](), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
  %581 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.2 # transformers/modeling_xlm.py:188:0
  %582 : int = prim::Constant[value=2](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %583 : int = prim::Constant[value=128](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %584 : int = prim::Constant[value=16](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %585 : int = prim::Constant[value=-1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %586 : int = prim::Constant[value=1](), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
  %587 : int = prim::Constant[value=0](), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
  %588 : __torch__.torch.nn.modules.linear.___torch_mangle_1062.Linear = prim::GetAttr[name="out_lin"](%82)
  %589 : __torch__.torch.nn.modules.linear.___torch_mangle_1061.Linear = prim::GetAttr[name="v_lin"](%82)
  %590 : __torch__.torch.nn.modules.linear.___torch_mangle_1060.Linear = prim::GetAttr[name="k_lin"](%82)
  %591 : __torch__.torch.nn.modules.linear.___torch_mangle_1059.Linear = prim::GetAttr[name="q_lin"](%82)
  %592 : int = aten::size(%input.24, %587), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
  %593 : int = aten::size(%input.24, %586), scope: __module.attentions.2 # transformers/modeling_xlm.py:151:0
  %594 : Tensor = prim::GetAttr[name="bias"](%591)
  %595 : Tensor = prim::GetAttr[name="weight"](%591)
  %596 : Float(2048:1, 2048:2048) = aten::t(%595), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %596), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %594, %586), scope: __module.attentions.2/__module.attentions.2.q_lin # torch/nn/functional.py:1678:0
  %599 : int[] = prim::ListConstruct(%592, %585, %584, %583), scope: __module.attentions.2
  %600 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.9, %599), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%600, %586, %582), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %602 : Tensor = prim::GetAttr[name="bias"](%590)
  %603 : Tensor = prim::GetAttr[name="weight"](%590)
  %604 : Float(2048:1, 2048:2048) = aten::t(%603), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %604), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %602, %586), scope: __module.attentions.2/__module.attentions.2.k_lin # torch/nn/functional.py:1678:0
  %607 : int[] = prim::ListConstruct(%592, %585, %584, %583), scope: __module.attentions.2
  %608 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.10, %607), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%608, %586, %582), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %610 : Tensor = prim::GetAttr[name="bias"](%589)
  %611 : Tensor = prim::GetAttr[name="weight"](%589)
  %612 : Float(2048:1, 2048:2048) = aten::t(%611), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %612), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %610, %586), scope: __module.attentions.2/__module.attentions.2.v_lin # torch/nn/functional.py:1678:0
  %615 : int[] = prim::ListConstruct(%592, %585, %584, %583), scope: __module.attentions.2
  %616 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.11, %615), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%616, %586, %582), scope: __module.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %581), scope: __module.attentions.2 # transformers/modeling_xlm.py:188:0
  %619 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %582, %580), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
  %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %619), scope: __module.attentions.2 # transformers/modeling_xlm.py:189:0
  %621 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %587), scope: __module.attentions.2 # torch/tensor.py:22:0
  %622 : int[] = prim::ListConstruct(%592, %586, %586, %593), scope: __module.attentions.2
  %623 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%621, %622), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
  %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%623, %scores.5), scope: __module.attentions.2 # transformers/modeling_xlm.py:190:0
  %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %579), scope: __module.attentions.2 # transformers/modeling_xlm.py:191:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %578, %577, %577, %576), scope: __module.attentions.2 # transformers/modeling_xlm.py:193:0
  %627 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %585, %576), scope: __module.attentions.2 # torch/nn/functional.py:1498:0
  %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%627, %575, %577), scope: __module.attentions.2 # torch/nn/functional.py:973:0
  %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.attentions.2 # transformers/modeling_xlm.py:200:0
  %630 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %586, %582), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
  %631 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%630, %587), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
  %632 : int[] = prim::ListConstruct(%592, %585, %574), scope: __module.attentions.2
  %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%631, %632), scope: __module.attentions.2 # transformers/modeling_xlm.py:167:0
  %634 : Tensor = prim::GetAttr[name="bias"](%588)
  %635 : Tensor = prim::GetAttr[name="weight"](%588)
  %636 : Float(2048:1, 2048:2048) = aten::t(%635), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %636), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %634, %586), scope: __module.attentions.2/__module.attentions.2.out_lin # torch/nn/functional.py:1678:0
  %177 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %178 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.28, %177, %178) # torch/nn/functional.py:973:0
  %180 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %180) # transformers/modeling_flaubert.py:265:0
  %639 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
  %640 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
  %641 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
  %642 : Tensor = prim::GetAttr[name="bias"](%80)
  %643 : Tensor = prim::GetAttr[name="weight"](%80)
  %644 : int[] = prim::ListConstruct(%641), scope: __module.layer_norm1.2
  %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %644, %643, %642, %640, %639), scope: __module.layer_norm1.2 # torch/nn/functional.py:2048:0
  %646 : bool = prim::Constant[value=0](), scope: __module.ffns.2 # torch/nn/functional.py:973:0
  %647 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.2 # torch/nn/functional.py:973:0
  %648 : int = prim::Constant[value=1](), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1678:0
  %649 : __torch__.torch.nn.modules.linear.___torch_mangle_1130.Linear = prim::GetAttr[name="lin2"](%78)
  %650 : __torch__.torch.nn.modules.linear.___torch_mangle_1129.Linear = prim::GetAttr[name="lin1"](%78)
  %651 : Tensor = prim::GetAttr[name="bias"](%650)
  %652 : Tensor = prim::GetAttr[name="weight"](%650)
  %653 : Float(2048:1, 8192:2048) = aten::t(%652), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.3, %653), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %651, %648), scope: __module.ffns.2/__module.ffns.2.lin1 # torch/nn/functional.py:1678:0
  %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.30), scope: __module.ffns.2 # torch/nn/functional.py:1369:0
  %657 : Tensor = prim::GetAttr[name="bias"](%649)
  %658 : Tensor = prim::GetAttr[name="weight"](%649)
  %659 : Float(8192:1, 2048:8192) = aten::t(%658), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %659), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %657, %648), scope: __module.ffns.2/__module.ffns.2.lin2 # torch/nn/functional.py:1678:0
  %662 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.32, %647, %646), scope: __module.ffns.2 # torch/nn/functional.py:973:0
  %184 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.3, %662, %184) # transformers/modeling_flaubert.py:285:0
  %663 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
  %664 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
  %665 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
  %666 : Tensor = prim::GetAttr[name="bias"](%76)
  %667 : Tensor = prim::GetAttr[name="weight"](%76)
  %668 : int[] = prim::ListConstruct(%665), scope: __module.layer_norm2.2
  %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %668, %667, %666, %664, %663), scope: __module.layer_norm2.2 # torch/nn/functional.py:2048:0
  %187 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %188 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %187) # transformers/modeling_flaubert.py:291:0
  %189 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %190 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %191 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %192 : None = prim::Constant()
  %193 : Float(17:13, 13:1, 1:1) = aten::to(%188, %189, %190, %191, %192) # transformers/modeling_flaubert.py:291:0
  %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.4, %193) # transformers/modeling_flaubert.py:291:0
  %670 : int = prim::Constant[value=2048](), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
  %671 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.3 # torch/nn/functional.py:973:0
  %672 : None = prim::Constant(), scope: __module.attentions.3
  %673 : bool = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
  %674 : int = prim::Constant[value=6](), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
  %675 : float = prim::Constant[value=-inf](), scope: __module.attentions.3 # transformers/modeling_xlm.py:191:0
  %676 : int = prim::Constant[value=3](), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
  %677 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.3 # transformers/modeling_xlm.py:188:0
  %678 : int = prim::Constant[value=2](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %679 : int = prim::Constant[value=128](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %680 : int = prim::Constant[value=16](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %681 : int = prim::Constant[value=-1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %682 : int = prim::Constant[value=1](), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
  %683 : int = prim::Constant[value=0](), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
  %684 : __torch__.torch.nn.modules.linear.___torch_mangle_1067.Linear = prim::GetAttr[name="out_lin"](%74)
  %685 : __torch__.torch.nn.modules.linear.___torch_mangle_1066.Linear = prim::GetAttr[name="v_lin"](%74)
  %686 : __torch__.torch.nn.modules.linear.___torch_mangle_1065.Linear = prim::GetAttr[name="k_lin"](%74)
  %687 : __torch__.torch.nn.modules.linear.___torch_mangle_1064.Linear = prim::GetAttr[name="q_lin"](%74)
  %688 : int = aten::size(%input.34, %683), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
  %689 : int = aten::size(%input.34, %682), scope: __module.attentions.3 # transformers/modeling_xlm.py:151:0
  %690 : Tensor = prim::GetAttr[name="bias"](%687)
  %691 : Tensor = prim::GetAttr[name="weight"](%687)
  %692 : Float(2048:1, 2048:2048) = aten::t(%691), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %692), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %690, %682), scope: __module.attentions.3/__module.attentions.3.q_lin # torch/nn/functional.py:1678:0
  %695 : int[] = prim::ListConstruct(%688, %681, %680, %679), scope: __module.attentions.3
  %696 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.13, %695), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%696, %682, %678), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %698 : Tensor = prim::GetAttr[name="bias"](%686)
  %699 : Tensor = prim::GetAttr[name="weight"](%686)
  %700 : Float(2048:1, 2048:2048) = aten::t(%699), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %700), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %698, %682), scope: __module.attentions.3/__module.attentions.3.k_lin # torch/nn/functional.py:1678:0
  %703 : int[] = prim::ListConstruct(%688, %681, %680, %679), scope: __module.attentions.3
  %704 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.14, %703), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%704, %682, %678), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %706 : Tensor = prim::GetAttr[name="bias"](%685)
  %707 : Tensor = prim::GetAttr[name="weight"](%685)
  %708 : Float(2048:1, 2048:2048) = aten::t(%707), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %708), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %706, %682), scope: __module.attentions.3/__module.attentions.3.v_lin # torch/nn/functional.py:1678:0
  %711 : int[] = prim::ListConstruct(%688, %681, %680, %679), scope: __module.attentions.3
  %712 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.15, %711), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%712, %682, %678), scope: __module.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %677), scope: __module.attentions.3 # transformers/modeling_xlm.py:188:0
  %715 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %678, %676), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
  %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %715), scope: __module.attentions.3 # transformers/modeling_xlm.py:189:0
  %717 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %683), scope: __module.attentions.3 # torch/tensor.py:22:0
  %718 : int[] = prim::ListConstruct(%688, %682, %682, %689), scope: __module.attentions.3
  %719 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%717, %718), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
  %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%719, %scores.7), scope: __module.attentions.3 # transformers/modeling_xlm.py:190:0
  %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %675), scope: __module.attentions.3 # transformers/modeling_xlm.py:191:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %674, %673, %673, %672), scope: __module.attentions.3 # transformers/modeling_xlm.py:193:0
  %723 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %681, %672), scope: __module.attentions.3 # torch/nn/functional.py:1498:0
  %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%723, %671, %673), scope: __module.attentions.3 # torch/nn/functional.py:973:0
  %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.attentions.3 # transformers/modeling_xlm.py:200:0
  %726 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %682, %678), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
  %727 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%726, %683), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
  %728 : int[] = prim::ListConstruct(%688, %681, %670), scope: __module.attentions.3
  %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%727, %728), scope: __module.attentions.3 # transformers/modeling_xlm.py:167:0
  %730 : Tensor = prim::GetAttr[name="bias"](%684)
  %731 : Tensor = prim::GetAttr[name="weight"](%684)
  %732 : Float(2048:1, 2048:2048) = aten::t(%731), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %732), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %730, %682), scope: __module.attentions.3/__module.attentions.3.out_lin # torch/nn/functional.py:1678:0
  %196 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %197 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.38, %196, %197) # torch/nn/functional.py:973:0
  %199 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %199) # transformers/modeling_flaubert.py:265:0
  %735 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
  %736 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
  %737 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
  %738 : Tensor = prim::GetAttr[name="bias"](%72)
  %739 : Tensor = prim::GetAttr[name="weight"](%72)
  %740 : int[] = prim::ListConstruct(%737), scope: __module.layer_norm1.3
  %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %740, %739, %738, %736, %735), scope: __module.layer_norm1.3 # torch/nn/functional.py:2048:0
  %742 : bool = prim::Constant[value=0](), scope: __module.ffns.3 # torch/nn/functional.py:973:0
  %743 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.3 # torch/nn/functional.py:973:0
  %744 : int = prim::Constant[value=1](), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1678:0
  %745 : __torch__.torch.nn.modules.linear.___torch_mangle_1133.Linear = prim::GetAttr[name="lin2"](%70)
  %746 : __torch__.torch.nn.modules.linear.___torch_mangle_1132.Linear = prim::GetAttr[name="lin1"](%70)
  %747 : Tensor = prim::GetAttr[name="bias"](%746)
  %748 : Tensor = prim::GetAttr[name="weight"](%746)
  %749 : Float(2048:1, 8192:2048) = aten::t(%748), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.4, %749), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %747, %744), scope: __module.ffns.3/__module.ffns.3.lin1 # torch/nn/functional.py:1678:0
  %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.40), scope: __module.ffns.3 # torch/nn/functional.py:1369:0
  %753 : Tensor = prim::GetAttr[name="bias"](%745)
  %754 : Tensor = prim::GetAttr[name="weight"](%745)
  %755 : Float(8192:1, 2048:8192) = aten::t(%754), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %755), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %753, %744), scope: __module.ffns.3/__module.ffns.3.lin2 # torch/nn/functional.py:1678:0
  %758 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.42, %743, %742), scope: __module.ffns.3 # torch/nn/functional.py:973:0
  %203 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.4, %758, %203) # transformers/modeling_flaubert.py:285:0
  %759 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
  %760 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
  %761 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
  %762 : Tensor = prim::GetAttr[name="bias"](%68)
  %763 : Tensor = prim::GetAttr[name="weight"](%68)
  %764 : int[] = prim::ListConstruct(%761), scope: __module.layer_norm2.3
  %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %764, %763, %762, %760, %759), scope: __module.layer_norm2.3 # torch/nn/functional.py:2048:0
  %206 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %207 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %206) # transformers/modeling_flaubert.py:291:0
  %208 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %209 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %210 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %211 : None = prim::Constant()
  %212 : Float(17:13, 13:1, 1:1) = aten::to(%207, %208, %209, %210, %211) # transformers/modeling_flaubert.py:291:0
  %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.5, %212) # transformers/modeling_flaubert.py:291:0
  %766 : int = prim::Constant[value=2048](), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
  %767 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.4 # torch/nn/functional.py:973:0
  %768 : None = prim::Constant(), scope: __module.attentions.4
  %769 : bool = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
  %770 : int = prim::Constant[value=6](), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
  %771 : float = prim::Constant[value=-inf](), scope: __module.attentions.4 # transformers/modeling_xlm.py:191:0
  %772 : int = prim::Constant[value=3](), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
  %773 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.4 # transformers/modeling_xlm.py:188:0
  %774 : int = prim::Constant[value=2](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %775 : int = prim::Constant[value=128](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %776 : int = prim::Constant[value=16](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %777 : int = prim::Constant[value=-1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %778 : int = prim::Constant[value=1](), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
  %779 : int = prim::Constant[value=0](), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
  %780 : __torch__.torch.nn.modules.linear.___torch_mangle_1072.Linear = prim::GetAttr[name="out_lin"](%66)
  %781 : __torch__.torch.nn.modules.linear.___torch_mangle_1071.Linear = prim::GetAttr[name="v_lin"](%66)
  %782 : __torch__.torch.nn.modules.linear.___torch_mangle_1070.Linear = prim::GetAttr[name="k_lin"](%66)
  %783 : __torch__.torch.nn.modules.linear.___torch_mangle_1069.Linear = prim::GetAttr[name="q_lin"](%66)
  %784 : int = aten::size(%input.44, %779), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
  %785 : int = aten::size(%input.44, %778), scope: __module.attentions.4 # transformers/modeling_xlm.py:151:0
  %786 : Tensor = prim::GetAttr[name="bias"](%783)
  %787 : Tensor = prim::GetAttr[name="weight"](%783)
  %788 : Float(2048:1, 2048:2048) = aten::t(%787), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %788), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %786, %778), scope: __module.attentions.4/__module.attentions.4.q_lin # torch/nn/functional.py:1678:0
  %791 : int[] = prim::ListConstruct(%784, %777, %776, %775), scope: __module.attentions.4
  %792 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.17, %791), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%792, %778, %774), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %794 : Tensor = prim::GetAttr[name="bias"](%782)
  %795 : Tensor = prim::GetAttr[name="weight"](%782)
  %796 : Float(2048:1, 2048:2048) = aten::t(%795), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %796), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %794, %778), scope: __module.attentions.4/__module.attentions.4.k_lin # torch/nn/functional.py:1678:0
  %799 : int[] = prim::ListConstruct(%784, %777, %776, %775), scope: __module.attentions.4
  %800 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.18, %799), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%800, %778, %774), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %802 : Tensor = prim::GetAttr[name="bias"](%781)
  %803 : Tensor = prim::GetAttr[name="weight"](%781)
  %804 : Float(2048:1, 2048:2048) = aten::t(%803), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %804), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %802, %778), scope: __module.attentions.4/__module.attentions.4.v_lin # torch/nn/functional.py:1678:0
  %807 : int[] = prim::ListConstruct(%784, %777, %776, %775), scope: __module.attentions.4
  %808 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.19, %807), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%808, %778, %774), scope: __module.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %773), scope: __module.attentions.4 # transformers/modeling_xlm.py:188:0
  %811 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %774, %772), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
  %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %811), scope: __module.attentions.4 # transformers/modeling_xlm.py:189:0
  %813 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %779), scope: __module.attentions.4 # torch/tensor.py:22:0
  %814 : int[] = prim::ListConstruct(%784, %778, %778, %785), scope: __module.attentions.4
  %815 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%813, %814), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
  %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%815, %scores.9), scope: __module.attentions.4 # transformers/modeling_xlm.py:190:0
  %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %771), scope: __module.attentions.4 # transformers/modeling_xlm.py:191:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %770, %769, %769, %768), scope: __module.attentions.4 # transformers/modeling_xlm.py:193:0
  %819 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %777, %768), scope: __module.attentions.4 # torch/nn/functional.py:1498:0
  %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%819, %767, %769), scope: __module.attentions.4 # torch/nn/functional.py:973:0
  %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.attentions.4 # transformers/modeling_xlm.py:200:0
  %822 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %778, %774), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
  %823 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%822, %779), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
  %824 : int[] = prim::ListConstruct(%784, %777, %766), scope: __module.attentions.4
  %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%823, %824), scope: __module.attentions.4 # transformers/modeling_xlm.py:167:0
  %826 : Tensor = prim::GetAttr[name="bias"](%780)
  %827 : Tensor = prim::GetAttr[name="weight"](%780)
  %828 : Float(2048:1, 2048:2048) = aten::t(%827), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %828), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %826, %778), scope: __module.attentions.4/__module.attentions.4.out_lin # torch/nn/functional.py:1678:0
  %215 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %216 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.48, %215, %216) # torch/nn/functional.py:973:0
  %218 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %218) # transformers/modeling_flaubert.py:265:0
  %831 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
  %832 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
  %833 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
  %834 : Tensor = prim::GetAttr[name="bias"](%64)
  %835 : Tensor = prim::GetAttr[name="weight"](%64)
  %836 : int[] = prim::ListConstruct(%833), scope: __module.layer_norm1.4
  %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %836, %835, %834, %832, %831), scope: __module.layer_norm1.4 # torch/nn/functional.py:2048:0
  %838 : bool = prim::Constant[value=0](), scope: __module.ffns.4 # torch/nn/functional.py:973:0
  %839 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.4 # torch/nn/functional.py:973:0
  %840 : int = prim::Constant[value=1](), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1678:0
  %841 : __torch__.torch.nn.modules.linear.___torch_mangle_1136.Linear = prim::GetAttr[name="lin2"](%62)
  %842 : __torch__.torch.nn.modules.linear.___torch_mangle_1135.Linear = prim::GetAttr[name="lin1"](%62)
  %843 : Tensor = prim::GetAttr[name="bias"](%842)
  %844 : Tensor = prim::GetAttr[name="weight"](%842)
  %845 : Float(2048:1, 8192:2048) = aten::t(%844), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.5, %845), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %843, %840), scope: __module.ffns.4/__module.ffns.4.lin1 # torch/nn/functional.py:1678:0
  %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.50), scope: __module.ffns.4 # torch/nn/functional.py:1369:0
  %849 : Tensor = prim::GetAttr[name="bias"](%841)
  %850 : Tensor = prim::GetAttr[name="weight"](%841)
  %851 : Float(8192:1, 2048:8192) = aten::t(%850), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %851), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %849, %840), scope: __module.ffns.4/__module.ffns.4.lin2 # torch/nn/functional.py:1678:0
  %854 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.52, %839, %838), scope: __module.ffns.4 # torch/nn/functional.py:973:0
  %222 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.5, %854, %222) # transformers/modeling_flaubert.py:285:0
  %855 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
  %856 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
  %857 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
  %858 : Tensor = prim::GetAttr[name="bias"](%60)
  %859 : Tensor = prim::GetAttr[name="weight"](%60)
  %860 : int[] = prim::ListConstruct(%857), scope: __module.layer_norm2.4
  %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %860, %859, %858, %856, %855), scope: __module.layer_norm2.4 # torch/nn/functional.py:2048:0
  %225 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %226 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %225) # transformers/modeling_flaubert.py:291:0
  %227 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %228 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %229 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %230 : None = prim::Constant()
  %231 : Float(17:13, 13:1, 1:1) = aten::to(%226, %227, %228, %229, %230) # transformers/modeling_flaubert.py:291:0
  %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.6, %231) # transformers/modeling_flaubert.py:291:0
  %862 : int = prim::Constant[value=2048](), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
  %863 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.5 # torch/nn/functional.py:973:0
  %864 : None = prim::Constant(), scope: __module.attentions.5
  %865 : bool = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
  %866 : int = prim::Constant[value=6](), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
  %867 : float = prim::Constant[value=-inf](), scope: __module.attentions.5 # transformers/modeling_xlm.py:191:0
  %868 : int = prim::Constant[value=3](), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
  %869 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.5 # transformers/modeling_xlm.py:188:0
  %870 : int = prim::Constant[value=2](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %871 : int = prim::Constant[value=128](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %872 : int = prim::Constant[value=16](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %873 : int = prim::Constant[value=-1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %874 : int = prim::Constant[value=1](), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
  %875 : int = prim::Constant[value=0](), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
  %876 : __torch__.torch.nn.modules.linear.___torch_mangle_1077.Linear = prim::GetAttr[name="out_lin"](%58)
  %877 : __torch__.torch.nn.modules.linear.___torch_mangle_1076.Linear = prim::GetAttr[name="v_lin"](%58)
  %878 : __torch__.torch.nn.modules.linear.___torch_mangle_1075.Linear = prim::GetAttr[name="k_lin"](%58)
  %879 : __torch__.torch.nn.modules.linear.___torch_mangle_1074.Linear = prim::GetAttr[name="q_lin"](%58)
  %880 : int = aten::size(%input.54, %875), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
  %881 : int = aten::size(%input.54, %874), scope: __module.attentions.5 # transformers/modeling_xlm.py:151:0
  %882 : Tensor = prim::GetAttr[name="bias"](%879)
  %883 : Tensor = prim::GetAttr[name="weight"](%879)
  %884 : Float(2048:1, 2048:2048) = aten::t(%883), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %884), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %882, %874), scope: __module.attentions.5/__module.attentions.5.q_lin # torch/nn/functional.py:1678:0
  %887 : int[] = prim::ListConstruct(%880, %873, %872, %871), scope: __module.attentions.5
  %888 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.21, %887), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%888, %874, %870), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %890 : Tensor = prim::GetAttr[name="bias"](%878)
  %891 : Tensor = prim::GetAttr[name="weight"](%878)
  %892 : Float(2048:1, 2048:2048) = aten::t(%891), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %892), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %890, %874), scope: __module.attentions.5/__module.attentions.5.k_lin # torch/nn/functional.py:1678:0
  %895 : int[] = prim::ListConstruct(%880, %873, %872, %871), scope: __module.attentions.5
  %896 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.22, %895), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%896, %874, %870), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %898 : Tensor = prim::GetAttr[name="bias"](%877)
  %899 : Tensor = prim::GetAttr[name="weight"](%877)
  %900 : Float(2048:1, 2048:2048) = aten::t(%899), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %900), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %898, %874), scope: __module.attentions.5/__module.attentions.5.v_lin # torch/nn/functional.py:1678:0
  %903 : int[] = prim::ListConstruct(%880, %873, %872, %871), scope: __module.attentions.5
  %904 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.23, %903), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%904, %874, %870), scope: __module.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %869), scope: __module.attentions.5 # transformers/modeling_xlm.py:188:0
  %907 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %870, %868), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
  %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %907), scope: __module.attentions.5 # transformers/modeling_xlm.py:189:0
  %909 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %875), scope: __module.attentions.5 # torch/tensor.py:22:0
  %910 : int[] = prim::ListConstruct(%880, %874, %874, %881), scope: __module.attentions.5
  %911 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%909, %910), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
  %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%911, %scores.11), scope: __module.attentions.5 # transformers/modeling_xlm.py:190:0
  %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %867), scope: __module.attentions.5 # transformers/modeling_xlm.py:191:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %866, %865, %865, %864), scope: __module.attentions.5 # transformers/modeling_xlm.py:193:0
  %915 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %873, %864), scope: __module.attentions.5 # torch/nn/functional.py:1498:0
  %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%915, %863, %865), scope: __module.attentions.5 # torch/nn/functional.py:973:0
  %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.attentions.5 # transformers/modeling_xlm.py:200:0
  %918 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %874, %870), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
  %919 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%918, %875), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
  %920 : int[] = prim::ListConstruct(%880, %873, %862), scope: __module.attentions.5
  %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%919, %920), scope: __module.attentions.5 # transformers/modeling_xlm.py:167:0
  %922 : Tensor = prim::GetAttr[name="bias"](%876)
  %923 : Tensor = prim::GetAttr[name="weight"](%876)
  %924 : Float(2048:1, 2048:2048) = aten::t(%923), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %924), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %922, %874), scope: __module.attentions.5/__module.attentions.5.out_lin # torch/nn/functional.py:1678:0
  %234 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %235 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.58, %234, %235) # torch/nn/functional.py:973:0
  %237 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %237) # transformers/modeling_flaubert.py:265:0
  %927 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
  %928 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
  %929 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
  %930 : Tensor = prim::GetAttr[name="bias"](%56)
  %931 : Tensor = prim::GetAttr[name="weight"](%56)
  %932 : int[] = prim::ListConstruct(%929), scope: __module.layer_norm1.5
  %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %932, %931, %930, %928, %927), scope: __module.layer_norm1.5 # torch/nn/functional.py:2048:0
  %934 : bool = prim::Constant[value=0](), scope: __module.ffns.5 # torch/nn/functional.py:973:0
  %935 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.5 # torch/nn/functional.py:973:0
  %936 : int = prim::Constant[value=1](), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1678:0
  %937 : __torch__.torch.nn.modules.linear.___torch_mangle_1139.Linear = prim::GetAttr[name="lin2"](%54)
  %938 : __torch__.torch.nn.modules.linear.___torch_mangle_1138.Linear = prim::GetAttr[name="lin1"](%54)
  %939 : Tensor = prim::GetAttr[name="bias"](%938)
  %940 : Tensor = prim::GetAttr[name="weight"](%938)
  %941 : Float(2048:1, 8192:2048) = aten::t(%940), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.6, %941), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %939, %936), scope: __module.ffns.5/__module.ffns.5.lin1 # torch/nn/functional.py:1678:0
  %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.60), scope: __module.ffns.5 # torch/nn/functional.py:1369:0
  %945 : Tensor = prim::GetAttr[name="bias"](%937)
  %946 : Tensor = prim::GetAttr[name="weight"](%937)
  %947 : Float(8192:1, 2048:8192) = aten::t(%946), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %947), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %945, %936), scope: __module.ffns.5/__module.ffns.5.lin2 # torch/nn/functional.py:1678:0
  %950 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.62, %935, %934), scope: __module.ffns.5 # torch/nn/functional.py:973:0
  %241 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.6, %950, %241) # transformers/modeling_flaubert.py:285:0
  %951 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
  %952 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
  %953 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
  %954 : Tensor = prim::GetAttr[name="bias"](%52)
  %955 : Tensor = prim::GetAttr[name="weight"](%52)
  %956 : int[] = prim::ListConstruct(%953), scope: __module.layer_norm2.5
  %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %956, %955, %954, %952, %951), scope: __module.layer_norm2.5 # torch/nn/functional.py:2048:0
  %244 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %245 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %244) # transformers/modeling_flaubert.py:291:0
  %246 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %247 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %248 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %249 : None = prim::Constant()
  %250 : Float(17:13, 13:1, 1:1) = aten::to(%245, %246, %247, %248, %249) # transformers/modeling_flaubert.py:291:0
  %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.7, %250) # transformers/modeling_flaubert.py:291:0
  %958 : int = prim::Constant[value=2048](), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
  %959 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.6 # torch/nn/functional.py:973:0
  %960 : None = prim::Constant(), scope: __module.attentions.6
  %961 : bool = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
  %962 : int = prim::Constant[value=6](), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
  %963 : float = prim::Constant[value=-inf](), scope: __module.attentions.6 # transformers/modeling_xlm.py:191:0
  %964 : int = prim::Constant[value=3](), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
  %965 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.6 # transformers/modeling_xlm.py:188:0
  %966 : int = prim::Constant[value=2](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %967 : int = prim::Constant[value=128](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %968 : int = prim::Constant[value=16](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %969 : int = prim::Constant[value=-1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %970 : int = prim::Constant[value=1](), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
  %971 : int = prim::Constant[value=0](), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
  %972 : __torch__.torch.nn.modules.linear.___torch_mangle_1082.Linear = prim::GetAttr[name="out_lin"](%50)
  %973 : __torch__.torch.nn.modules.linear.___torch_mangle_1081.Linear = prim::GetAttr[name="v_lin"](%50)
  %974 : __torch__.torch.nn.modules.linear.___torch_mangle_1080.Linear = prim::GetAttr[name="k_lin"](%50)
  %975 : __torch__.torch.nn.modules.linear.___torch_mangle_1079.Linear = prim::GetAttr[name="q_lin"](%50)
  %976 : int = aten::size(%input.64, %971), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
  %977 : int = aten::size(%input.64, %970), scope: __module.attentions.6 # transformers/modeling_xlm.py:151:0
  %978 : Tensor = prim::GetAttr[name="bias"](%975)
  %979 : Tensor = prim::GetAttr[name="weight"](%975)
  %980 : Float(2048:1, 2048:2048) = aten::t(%979), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %980), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %978, %970), scope: __module.attentions.6/__module.attentions.6.q_lin # torch/nn/functional.py:1678:0
  %983 : int[] = prim::ListConstruct(%976, %969, %968, %967), scope: __module.attentions.6
  %984 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.25, %983), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%984, %970, %966), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %986 : Tensor = prim::GetAttr[name="bias"](%974)
  %987 : Tensor = prim::GetAttr[name="weight"](%974)
  %988 : Float(2048:1, 2048:2048) = aten::t(%987), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %988), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %986, %970), scope: __module.attentions.6/__module.attentions.6.k_lin # torch/nn/functional.py:1678:0
  %991 : int[] = prim::ListConstruct(%976, %969, %968, %967), scope: __module.attentions.6
  %992 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.26, %991), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%992, %970, %966), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %994 : Tensor = prim::GetAttr[name="bias"](%973)
  %995 : Tensor = prim::GetAttr[name="weight"](%973)
  %996 : Float(2048:1, 2048:2048) = aten::t(%995), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %996), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %994, %970), scope: __module.attentions.6/__module.attentions.6.v_lin # torch/nn/functional.py:1678:0
  %999 : int[] = prim::ListConstruct(%976, %969, %968, %967), scope: __module.attentions.6
  %1000 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.27, %999), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1000, %970, %966), scope: __module.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %965), scope: __module.attentions.6 # transformers/modeling_xlm.py:188:0
  %1003 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %966, %964), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
  %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %1003), scope: __module.attentions.6 # transformers/modeling_xlm.py:189:0
  %1005 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %971), scope: __module.attentions.6 # torch/tensor.py:22:0
  %1006 : int[] = prim::ListConstruct(%976, %970, %970, %977), scope: __module.attentions.6
  %1007 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1005, %1006), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
  %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1007, %scores.13), scope: __module.attentions.6 # transformers/modeling_xlm.py:190:0
  %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %963), scope: __module.attentions.6 # transformers/modeling_xlm.py:191:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %962, %961, %961, %960), scope: __module.attentions.6 # transformers/modeling_xlm.py:193:0
  %1011 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %969, %960), scope: __module.attentions.6 # torch/nn/functional.py:1498:0
  %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1011, %959, %961), scope: __module.attentions.6 # torch/nn/functional.py:973:0
  %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.attentions.6 # transformers/modeling_xlm.py:200:0
  %1014 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %970, %966), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
  %1015 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1014, %971), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
  %1016 : int[] = prim::ListConstruct(%976, %969, %958), scope: __module.attentions.6
  %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1015, %1016), scope: __module.attentions.6 # transformers/modeling_xlm.py:167:0
  %1018 : Tensor = prim::GetAttr[name="bias"](%972)
  %1019 : Tensor = prim::GetAttr[name="weight"](%972)
  %1020 : Float(2048:1, 2048:2048) = aten::t(%1019), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %1020), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %1018, %970), scope: __module.attentions.6/__module.attentions.6.out_lin # torch/nn/functional.py:1678:0
  %253 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %254 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.68, %253, %254) # torch/nn/functional.py:973:0
  %256 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %256) # transformers/modeling_flaubert.py:265:0
  %1023 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
  %1024 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
  %1025 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
  %1026 : Tensor = prim::GetAttr[name="bias"](%48)
  %1027 : Tensor = prim::GetAttr[name="weight"](%48)
  %1028 : int[] = prim::ListConstruct(%1025), scope: __module.layer_norm1.6
  %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %1028, %1027, %1026, %1024, %1023), scope: __module.layer_norm1.6 # torch/nn/functional.py:2048:0
  %1030 : bool = prim::Constant[value=0](), scope: __module.ffns.6 # torch/nn/functional.py:973:0
  %1031 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.6 # torch/nn/functional.py:973:0
  %1032 : int = prim::Constant[value=1](), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1678:0
  %1033 : __torch__.torch.nn.modules.linear.___torch_mangle_1142.Linear = prim::GetAttr[name="lin2"](%46)
  %1034 : __torch__.torch.nn.modules.linear.___torch_mangle_1141.Linear = prim::GetAttr[name="lin1"](%46)
  %1035 : Tensor = prim::GetAttr[name="bias"](%1034)
  %1036 : Tensor = prim::GetAttr[name="weight"](%1034)
  %1037 : Float(2048:1, 8192:2048) = aten::t(%1036), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.7, %1037), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %1035, %1032), scope: __module.ffns.6/__module.ffns.6.lin1 # torch/nn/functional.py:1678:0
  %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.70), scope: __module.ffns.6 # torch/nn/functional.py:1369:0
  %1041 : Tensor = prim::GetAttr[name="bias"](%1033)
  %1042 : Tensor = prim::GetAttr[name="weight"](%1033)
  %1043 : Float(8192:1, 2048:8192) = aten::t(%1042), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %1043), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %1041, %1032), scope: __module.ffns.6/__module.ffns.6.lin2 # torch/nn/functional.py:1678:0
  %1046 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.72, %1031, %1030), scope: __module.ffns.6 # torch/nn/functional.py:973:0
  %260 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.7, %1046, %260) # transformers/modeling_flaubert.py:285:0
  %1047 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
  %1048 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
  %1049 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
  %1050 : Tensor = prim::GetAttr[name="bias"](%44)
  %1051 : Tensor = prim::GetAttr[name="weight"](%44)
  %1052 : int[] = prim::ListConstruct(%1049), scope: __module.layer_norm2.6
  %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %1052, %1051, %1050, %1048, %1047), scope: __module.layer_norm2.6 # torch/nn/functional.py:2048:0
  %263 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %264 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %263) # transformers/modeling_flaubert.py:291:0
  %265 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %266 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %267 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %268 : None = prim::Constant()
  %269 : Float(17:13, 13:1, 1:1) = aten::to(%264, %265, %266, %267, %268) # transformers/modeling_flaubert.py:291:0
  %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.8, %269) # transformers/modeling_flaubert.py:291:0
  %1054 : int = prim::Constant[value=2048](), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
  %1055 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.7 # torch/nn/functional.py:973:0
  %1056 : None = prim::Constant(), scope: __module.attentions.7
  %1057 : bool = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
  %1058 : int = prim::Constant[value=6](), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
  %1059 : float = prim::Constant[value=-inf](), scope: __module.attentions.7 # transformers/modeling_xlm.py:191:0
  %1060 : int = prim::Constant[value=3](), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
  %1061 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.7 # transformers/modeling_xlm.py:188:0
  %1062 : int = prim::Constant[value=2](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %1063 : int = prim::Constant[value=128](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %1064 : int = prim::Constant[value=16](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %1065 : int = prim::Constant[value=-1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %1066 : int = prim::Constant[value=1](), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
  %1067 : int = prim::Constant[value=0](), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
  %1068 : __torch__.torch.nn.modules.linear.___torch_mangle_1087.Linear = prim::GetAttr[name="out_lin"](%42)
  %1069 : __torch__.torch.nn.modules.linear.___torch_mangle_1086.Linear = prim::GetAttr[name="v_lin"](%42)
  %1070 : __torch__.torch.nn.modules.linear.___torch_mangle_1085.Linear = prim::GetAttr[name="k_lin"](%42)
  %1071 : __torch__.torch.nn.modules.linear.___torch_mangle_1084.Linear = prim::GetAttr[name="q_lin"](%42)
  %1072 : int = aten::size(%input.74, %1067), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
  %1073 : int = aten::size(%input.74, %1066), scope: __module.attentions.7 # transformers/modeling_xlm.py:151:0
  %1074 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1075 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1076 : Float(2048:1, 2048:2048) = aten::t(%1075), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %1076), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %1074, %1066), scope: __module.attentions.7/__module.attentions.7.q_lin # torch/nn/functional.py:1678:0
  %1079 : int[] = prim::ListConstruct(%1072, %1065, %1064, %1063), scope: __module.attentions.7
  %1080 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.29, %1079), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1080, %1066, %1062), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %1082 : Tensor = prim::GetAttr[name="bias"](%1070)
  %1083 : Tensor = prim::GetAttr[name="weight"](%1070)
  %1084 : Float(2048:1, 2048:2048) = aten::t(%1083), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %1084), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %1082, %1066), scope: __module.attentions.7/__module.attentions.7.k_lin # torch/nn/functional.py:1678:0
  %1087 : int[] = prim::ListConstruct(%1072, %1065, %1064, %1063), scope: __module.attentions.7
  %1088 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.30, %1087), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1088, %1066, %1062), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %1090 : Tensor = prim::GetAttr[name="bias"](%1069)
  %1091 : Tensor = prim::GetAttr[name="weight"](%1069)
  %1092 : Float(2048:1, 2048:2048) = aten::t(%1091), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %1092), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %1090, %1066), scope: __module.attentions.7/__module.attentions.7.v_lin # torch/nn/functional.py:1678:0
  %1095 : int[] = prim::ListConstruct(%1072, %1065, %1064, %1063), scope: __module.attentions.7
  %1096 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.31, %1095), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1096, %1066, %1062), scope: __module.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %1061), scope: __module.attentions.7 # transformers/modeling_xlm.py:188:0
  %1099 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %1062, %1060), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
  %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %1099), scope: __module.attentions.7 # transformers/modeling_xlm.py:189:0
  %1101 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %1067), scope: __module.attentions.7 # torch/tensor.py:22:0
  %1102 : int[] = prim::ListConstruct(%1072, %1066, %1066, %1073), scope: __module.attentions.7
  %1103 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1101, %1102), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
  %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1103, %scores.15), scope: __module.attentions.7 # transformers/modeling_xlm.py:190:0
  %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %1059), scope: __module.attentions.7 # transformers/modeling_xlm.py:191:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %1058, %1057, %1057, %1056), scope: __module.attentions.7 # transformers/modeling_xlm.py:193:0
  %1107 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %1065, %1056), scope: __module.attentions.7 # torch/nn/functional.py:1498:0
  %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1107, %1055, %1057), scope: __module.attentions.7 # torch/nn/functional.py:973:0
  %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.attentions.7 # transformers/modeling_xlm.py:200:0
  %1110 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %1066, %1062), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
  %1111 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1110, %1067), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
  %1112 : int[] = prim::ListConstruct(%1072, %1065, %1054), scope: __module.attentions.7
  %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1111, %1112), scope: __module.attentions.7 # transformers/modeling_xlm.py:167:0
  %1114 : Tensor = prim::GetAttr[name="bias"](%1068)
  %1115 : Tensor = prim::GetAttr[name="weight"](%1068)
  %1116 : Float(2048:1, 2048:2048) = aten::t(%1115), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %1116), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %1114, %1066), scope: __module.attentions.7/__module.attentions.7.out_lin # torch/nn/functional.py:1678:0
  %272 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %273 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.78, %272, %273) # torch/nn/functional.py:973:0
  %275 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %275) # transformers/modeling_flaubert.py:265:0
  %1119 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
  %1120 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
  %1121 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
  %1122 : Tensor = prim::GetAttr[name="bias"](%40)
  %1123 : Tensor = prim::GetAttr[name="weight"](%40)
  %1124 : int[] = prim::ListConstruct(%1121), scope: __module.layer_norm1.7
  %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %1124, %1123, %1122, %1120, %1119), scope: __module.layer_norm1.7 # torch/nn/functional.py:2048:0
  %1126 : bool = prim::Constant[value=0](), scope: __module.ffns.7 # torch/nn/functional.py:973:0
  %1127 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.7 # torch/nn/functional.py:973:0
  %1128 : int = prim::Constant[value=1](), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1678:0
  %1129 : __torch__.torch.nn.modules.linear.___torch_mangle_1145.Linear = prim::GetAttr[name="lin2"](%38)
  %1130 : __torch__.torch.nn.modules.linear.___torch_mangle_1144.Linear = prim::GetAttr[name="lin1"](%38)
  %1131 : Tensor = prim::GetAttr[name="bias"](%1130)
  %1132 : Tensor = prim::GetAttr[name="weight"](%1130)
  %1133 : Float(2048:1, 8192:2048) = aten::t(%1132), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.8, %1133), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %1131, %1128), scope: __module.ffns.7/__module.ffns.7.lin1 # torch/nn/functional.py:1678:0
  %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.80), scope: __module.ffns.7 # torch/nn/functional.py:1369:0
  %1137 : Tensor = prim::GetAttr[name="bias"](%1129)
  %1138 : Tensor = prim::GetAttr[name="weight"](%1129)
  %1139 : Float(8192:1, 2048:8192) = aten::t(%1138), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %1139), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %1137, %1128), scope: __module.ffns.7/__module.ffns.7.lin2 # torch/nn/functional.py:1678:0
  %1142 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.82, %1127, %1126), scope: __module.ffns.7 # torch/nn/functional.py:973:0
  %279 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.8, %1142, %279) # transformers/modeling_flaubert.py:285:0
  %1143 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
  %1144 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
  %1145 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
  %1146 : Tensor = prim::GetAttr[name="bias"](%36)
  %1147 : Tensor = prim::GetAttr[name="weight"](%36)
  %1148 : int[] = prim::ListConstruct(%1145), scope: __module.layer_norm2.7
  %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %1148, %1147, %1146, %1144, %1143), scope: __module.layer_norm2.7 # torch/nn/functional.py:2048:0
  %282 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %283 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %282) # transformers/modeling_flaubert.py:291:0
  %284 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %285 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %286 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %287 : None = prim::Constant()
  %288 : Float(17:13, 13:1, 1:1) = aten::to(%283, %284, %285, %286, %287) # transformers/modeling_flaubert.py:291:0
  %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.9, %288) # transformers/modeling_flaubert.py:291:0
  %1150 : int = prim::Constant[value=2048](), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
  %1151 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.8 # torch/nn/functional.py:973:0
  %1152 : None = prim::Constant(), scope: __module.attentions.8
  %1153 : bool = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
  %1154 : int = prim::Constant[value=6](), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
  %1155 : float = prim::Constant[value=-inf](), scope: __module.attentions.8 # transformers/modeling_xlm.py:191:0
  %1156 : int = prim::Constant[value=3](), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
  %1157 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.8 # transformers/modeling_xlm.py:188:0
  %1158 : int = prim::Constant[value=2](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %1159 : int = prim::Constant[value=128](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %1160 : int = prim::Constant[value=16](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %1161 : int = prim::Constant[value=-1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %1162 : int = prim::Constant[value=1](), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
  %1163 : int = prim::Constant[value=0](), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
  %1164 : __torch__.torch.nn.modules.linear.___torch_mangle_1092.Linear = prim::GetAttr[name="out_lin"](%34)
  %1165 : __torch__.torch.nn.modules.linear.___torch_mangle_1091.Linear = prim::GetAttr[name="v_lin"](%34)
  %1166 : __torch__.torch.nn.modules.linear.___torch_mangle_1090.Linear = prim::GetAttr[name="k_lin"](%34)
  %1167 : __torch__.torch.nn.modules.linear.___torch_mangle_1089.Linear = prim::GetAttr[name="q_lin"](%34)
  %1168 : int = aten::size(%input.84, %1163), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
  %1169 : int = aten::size(%input.84, %1162), scope: __module.attentions.8 # transformers/modeling_xlm.py:151:0
  %1170 : Tensor = prim::GetAttr[name="bias"](%1167)
  %1171 : Tensor = prim::GetAttr[name="weight"](%1167)
  %1172 : Float(2048:1, 2048:2048) = aten::t(%1171), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %1172), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %1170, %1162), scope: __module.attentions.8/__module.attentions.8.q_lin # torch/nn/functional.py:1678:0
  %1175 : int[] = prim::ListConstruct(%1168, %1161, %1160, %1159), scope: __module.attentions.8
  %1176 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.33, %1175), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1176, %1162, %1158), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %1178 : Tensor = prim::GetAttr[name="bias"](%1166)
  %1179 : Tensor = prim::GetAttr[name="weight"](%1166)
  %1180 : Float(2048:1, 2048:2048) = aten::t(%1179), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %1180), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %1178, %1162), scope: __module.attentions.8/__module.attentions.8.k_lin # torch/nn/functional.py:1678:0
  %1183 : int[] = prim::ListConstruct(%1168, %1161, %1160, %1159), scope: __module.attentions.8
  %1184 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.34, %1183), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1184, %1162, %1158), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %1186 : Tensor = prim::GetAttr[name="bias"](%1165)
  %1187 : Tensor = prim::GetAttr[name="weight"](%1165)
  %1188 : Float(2048:1, 2048:2048) = aten::t(%1187), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %1188), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %1186, %1162), scope: __module.attentions.8/__module.attentions.8.v_lin # torch/nn/functional.py:1678:0
  %1191 : int[] = prim::ListConstruct(%1168, %1161, %1160, %1159), scope: __module.attentions.8
  %1192 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.35, %1191), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1192, %1162, %1158), scope: __module.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %1157), scope: __module.attentions.8 # transformers/modeling_xlm.py:188:0
  %1195 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %1158, %1156), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
  %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %1195), scope: __module.attentions.8 # transformers/modeling_xlm.py:189:0
  %1197 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %1163), scope: __module.attentions.8 # torch/tensor.py:22:0
  %1198 : int[] = prim::ListConstruct(%1168, %1162, %1162, %1169), scope: __module.attentions.8
  %1199 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1197, %1198), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
  %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1199, %scores.17), scope: __module.attentions.8 # transformers/modeling_xlm.py:190:0
  %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %1155), scope: __module.attentions.8 # transformers/modeling_xlm.py:191:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %1154, %1153, %1153, %1152), scope: __module.attentions.8 # transformers/modeling_xlm.py:193:0
  %1203 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %1161, %1152), scope: __module.attentions.8 # torch/nn/functional.py:1498:0
  %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1203, %1151, %1153), scope: __module.attentions.8 # torch/nn/functional.py:973:0
  %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.attentions.8 # transformers/modeling_xlm.py:200:0
  %1206 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %1162, %1158), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
  %1207 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1206, %1163), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
  %1208 : int[] = prim::ListConstruct(%1168, %1161, %1150), scope: __module.attentions.8
  %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1207, %1208), scope: __module.attentions.8 # transformers/modeling_xlm.py:167:0
  %1210 : Tensor = prim::GetAttr[name="bias"](%1164)
  %1211 : Tensor = prim::GetAttr[name="weight"](%1164)
  %1212 : Float(2048:1, 2048:2048) = aten::t(%1211), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %1212), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %1210, %1162), scope: __module.attentions.8/__module.attentions.8.out_lin # torch/nn/functional.py:1678:0
  %291 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %292 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.88, %291, %292) # torch/nn/functional.py:973:0
  %294 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %294) # transformers/modeling_flaubert.py:265:0
  %1215 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
  %1216 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
  %1217 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
  %1218 : Tensor = prim::GetAttr[name="bias"](%32)
  %1219 : Tensor = prim::GetAttr[name="weight"](%32)
  %1220 : int[] = prim::ListConstruct(%1217), scope: __module.layer_norm1.8
  %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %1220, %1219, %1218, %1216, %1215), scope: __module.layer_norm1.8 # torch/nn/functional.py:2048:0
  %1222 : bool = prim::Constant[value=0](), scope: __module.ffns.8 # torch/nn/functional.py:973:0
  %1223 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.8 # torch/nn/functional.py:973:0
  %1224 : int = prim::Constant[value=1](), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1678:0
  %1225 : __torch__.torch.nn.modules.linear.___torch_mangle_1148.Linear = prim::GetAttr[name="lin2"](%30)
  %1226 : __torch__.torch.nn.modules.linear.___torch_mangle_1147.Linear = prim::GetAttr[name="lin1"](%30)
  %1227 : Tensor = prim::GetAttr[name="bias"](%1226)
  %1228 : Tensor = prim::GetAttr[name="weight"](%1226)
  %1229 : Float(2048:1, 8192:2048) = aten::t(%1228), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.9, %1229), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %1227, %1224), scope: __module.ffns.8/__module.ffns.8.lin1 # torch/nn/functional.py:1678:0
  %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.90), scope: __module.ffns.8 # torch/nn/functional.py:1369:0
  %1233 : Tensor = prim::GetAttr[name="bias"](%1225)
  %1234 : Tensor = prim::GetAttr[name="weight"](%1225)
  %1235 : Float(8192:1, 2048:8192) = aten::t(%1234), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %1235), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %1233, %1224), scope: __module.ffns.8/__module.ffns.8.lin2 # torch/nn/functional.py:1678:0
  %1238 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.92, %1223, %1222), scope: __module.ffns.8 # torch/nn/functional.py:973:0
  %298 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.9, %1238, %298) # transformers/modeling_flaubert.py:285:0
  %1239 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
  %1240 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
  %1241 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
  %1242 : Tensor = prim::GetAttr[name="bias"](%28)
  %1243 : Tensor = prim::GetAttr[name="weight"](%28)
  %1244 : int[] = prim::ListConstruct(%1241), scope: __module.layer_norm2.8
  %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %1244, %1243, %1242, %1240, %1239), scope: __module.layer_norm2.8 # torch/nn/functional.py:2048:0
  %301 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %302 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %301) # transformers/modeling_flaubert.py:291:0
  %303 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %304 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %305 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %306 : None = prim::Constant()
  %307 : Float(17:13, 13:1, 1:1) = aten::to(%302, %303, %304, %305, %306) # transformers/modeling_flaubert.py:291:0
  %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.10, %307) # transformers/modeling_flaubert.py:291:0
  %1246 : int = prim::Constant[value=2048](), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
  %1247 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.9 # torch/nn/functional.py:973:0
  %1248 : None = prim::Constant(), scope: __module.attentions.9
  %1249 : bool = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
  %1250 : int = prim::Constant[value=6](), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
  %1251 : float = prim::Constant[value=-inf](), scope: __module.attentions.9 # transformers/modeling_xlm.py:191:0
  %1252 : int = prim::Constant[value=3](), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
  %1253 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.9 # transformers/modeling_xlm.py:188:0
  %1254 : int = prim::Constant[value=2](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %1255 : int = prim::Constant[value=128](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %1256 : int = prim::Constant[value=16](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %1257 : int = prim::Constant[value=-1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %1258 : int = prim::Constant[value=1](), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
  %1259 : int = prim::Constant[value=0](), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
  %1260 : __torch__.torch.nn.modules.linear.___torch_mangle_1097.Linear = prim::GetAttr[name="out_lin"](%26)
  %1261 : __torch__.torch.nn.modules.linear.___torch_mangle_1096.Linear = prim::GetAttr[name="v_lin"](%26)
  %1262 : __torch__.torch.nn.modules.linear.___torch_mangle_1095.Linear = prim::GetAttr[name="k_lin"](%26)
  %1263 : __torch__.torch.nn.modules.linear.___torch_mangle_1094.Linear = prim::GetAttr[name="q_lin"](%26)
  %1264 : int = aten::size(%input.94, %1259), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
  %1265 : int = aten::size(%input.94, %1258), scope: __module.attentions.9 # transformers/modeling_xlm.py:151:0
  %1266 : Tensor = prim::GetAttr[name="bias"](%1263)
  %1267 : Tensor = prim::GetAttr[name="weight"](%1263)
  %1268 : Float(2048:1, 2048:2048) = aten::t(%1267), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %1268), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %1266, %1258), scope: __module.attentions.9/__module.attentions.9.q_lin # torch/nn/functional.py:1678:0
  %1271 : int[] = prim::ListConstruct(%1264, %1257, %1256, %1255), scope: __module.attentions.9
  %1272 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.37, %1271), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1272, %1258, %1254), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %1274 : Tensor = prim::GetAttr[name="bias"](%1262)
  %1275 : Tensor = prim::GetAttr[name="weight"](%1262)
  %1276 : Float(2048:1, 2048:2048) = aten::t(%1275), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %1276), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %1274, %1258), scope: __module.attentions.9/__module.attentions.9.k_lin # torch/nn/functional.py:1678:0
  %1279 : int[] = prim::ListConstruct(%1264, %1257, %1256, %1255), scope: __module.attentions.9
  %1280 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.38, %1279), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1280, %1258, %1254), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %1282 : Tensor = prim::GetAttr[name="bias"](%1261)
  %1283 : Tensor = prim::GetAttr[name="weight"](%1261)
  %1284 : Float(2048:1, 2048:2048) = aten::t(%1283), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %1284), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %1282, %1258), scope: __module.attentions.9/__module.attentions.9.v_lin # torch/nn/functional.py:1678:0
  %1287 : int[] = prim::ListConstruct(%1264, %1257, %1256, %1255), scope: __module.attentions.9
  %1288 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.39, %1287), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1288, %1258, %1254), scope: __module.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %1253), scope: __module.attentions.9 # transformers/modeling_xlm.py:188:0
  %1291 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %1254, %1252), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
  %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %1291), scope: __module.attentions.9 # transformers/modeling_xlm.py:189:0
  %1293 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %1259), scope: __module.attentions.9 # torch/tensor.py:22:0
  %1294 : int[] = prim::ListConstruct(%1264, %1258, %1258, %1265), scope: __module.attentions.9
  %1295 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1293, %1294), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
  %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1295, %scores.19), scope: __module.attentions.9 # transformers/modeling_xlm.py:190:0
  %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %1251), scope: __module.attentions.9 # transformers/modeling_xlm.py:191:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %1250, %1249, %1249, %1248), scope: __module.attentions.9 # transformers/modeling_xlm.py:193:0
  %1299 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %1257, %1248), scope: __module.attentions.9 # torch/nn/functional.py:1498:0
  %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1299, %1247, %1249), scope: __module.attentions.9 # torch/nn/functional.py:973:0
  %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.attentions.9 # transformers/modeling_xlm.py:200:0
  %1302 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %1258, %1254), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
  %1303 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1302, %1259), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
  %1304 : int[] = prim::ListConstruct(%1264, %1257, %1246), scope: __module.attentions.9
  %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1303, %1304), scope: __module.attentions.9 # transformers/modeling_xlm.py:167:0
  %1306 : Tensor = prim::GetAttr[name="bias"](%1260)
  %1307 : Tensor = prim::GetAttr[name="weight"](%1260)
  %1308 : Float(2048:1, 2048:2048) = aten::t(%1307), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %1308), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %1306, %1258), scope: __module.attentions.9/__module.attentions.9.out_lin # torch/nn/functional.py:1678:0
  %310 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %311 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.98, %310, %311) # torch/nn/functional.py:973:0
  %313 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %313) # transformers/modeling_flaubert.py:265:0
  %1311 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
  %1312 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
  %1313 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
  %1314 : Tensor = prim::GetAttr[name="bias"](%24)
  %1315 : Tensor = prim::GetAttr[name="weight"](%24)
  %1316 : int[] = prim::ListConstruct(%1313), scope: __module.layer_norm1.9
  %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %1316, %1315, %1314, %1312, %1311), scope: __module.layer_norm1.9 # torch/nn/functional.py:2048:0
  %1318 : bool = prim::Constant[value=0](), scope: __module.ffns.9 # torch/nn/functional.py:973:0
  %1319 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.9 # torch/nn/functional.py:973:0
  %1320 : int = prim::Constant[value=1](), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1678:0
  %1321 : __torch__.torch.nn.modules.linear.___torch_mangle_1151.Linear = prim::GetAttr[name="lin2"](%22)
  %1322 : __torch__.torch.nn.modules.linear.___torch_mangle_1150.Linear = prim::GetAttr[name="lin1"](%22)
  %1323 : Tensor = prim::GetAttr[name="bias"](%1322)
  %1324 : Tensor = prim::GetAttr[name="weight"](%1322)
  %1325 : Float(2048:1, 8192:2048) = aten::t(%1324), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.10, %1325), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %1323, %1320), scope: __module.ffns.9/__module.ffns.9.lin1 # torch/nn/functional.py:1678:0
  %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.100), scope: __module.ffns.9 # torch/nn/functional.py:1369:0
  %1329 : Tensor = prim::GetAttr[name="bias"](%1321)
  %1330 : Tensor = prim::GetAttr[name="weight"](%1321)
  %1331 : Float(8192:1, 2048:8192) = aten::t(%1330), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %1331), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %1329, %1320), scope: __module.ffns.9/__module.ffns.9.lin2 # torch/nn/functional.py:1678:0
  %1334 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.102, %1319, %1318), scope: __module.ffns.9 # torch/nn/functional.py:973:0
  %317 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.10, %1334, %317) # transformers/modeling_flaubert.py:285:0
  %1335 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
  %1336 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
  %1337 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
  %1338 : Tensor = prim::GetAttr[name="bias"](%20)
  %1339 : Tensor = prim::GetAttr[name="weight"](%20)
  %1340 : int[] = prim::ListConstruct(%1337), scope: __module.layer_norm2.9
  %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %1340, %1339, %1338, %1336, %1335), scope: __module.layer_norm2.9 # torch/nn/functional.py:2048:0
  %320 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %321 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %320) # transformers/modeling_flaubert.py:291:0
  %322 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %323 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %324 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %325 : None = prim::Constant()
  %326 : Float(17:13, 13:1, 1:1) = aten::to(%321, %322, %323, %324, %325) # transformers/modeling_flaubert.py:291:0
  %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.11, %326) # transformers/modeling_flaubert.py:291:0
  %1342 : int = prim::Constant[value=2048](), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
  %1343 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.10 # torch/nn/functional.py:973:0
  %1344 : None = prim::Constant(), scope: __module.attentions.10
  %1345 : bool = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
  %1346 : int = prim::Constant[value=6](), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
  %1347 : float = prim::Constant[value=-inf](), scope: __module.attentions.10 # transformers/modeling_xlm.py:191:0
  %1348 : int = prim::Constant[value=3](), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
  %1349 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.10 # transformers/modeling_xlm.py:188:0
  %1350 : int = prim::Constant[value=2](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %1351 : int = prim::Constant[value=128](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %1352 : int = prim::Constant[value=16](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %1353 : int = prim::Constant[value=-1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %1354 : int = prim::Constant[value=1](), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
  %1355 : int = prim::Constant[value=0](), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
  %1356 : __torch__.torch.nn.modules.linear.___torch_mangle_1102.Linear = prim::GetAttr[name="out_lin"](%18)
  %1357 : __torch__.torch.nn.modules.linear.___torch_mangle_1101.Linear = prim::GetAttr[name="v_lin"](%18)
  %1358 : __torch__.torch.nn.modules.linear.___torch_mangle_1100.Linear = prim::GetAttr[name="k_lin"](%18)
  %1359 : __torch__.torch.nn.modules.linear.___torch_mangle_1099.Linear = prim::GetAttr[name="q_lin"](%18)
  %1360 : int = aten::size(%input.104, %1355), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
  %1361 : int = aten::size(%input.104, %1354), scope: __module.attentions.10 # transformers/modeling_xlm.py:151:0
  %1362 : Tensor = prim::GetAttr[name="bias"](%1359)
  %1363 : Tensor = prim::GetAttr[name="weight"](%1359)
  %1364 : Float(2048:1, 2048:2048) = aten::t(%1363), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %1364), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %1362, %1354), scope: __module.attentions.10/__module.attentions.10.q_lin # torch/nn/functional.py:1678:0
  %1367 : int[] = prim::ListConstruct(%1360, %1353, %1352, %1351), scope: __module.attentions.10
  %1368 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.41, %1367), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1368, %1354, %1350), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %1370 : Tensor = prim::GetAttr[name="bias"](%1358)
  %1371 : Tensor = prim::GetAttr[name="weight"](%1358)
  %1372 : Float(2048:1, 2048:2048) = aten::t(%1371), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %1372), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %1370, %1354), scope: __module.attentions.10/__module.attentions.10.k_lin # torch/nn/functional.py:1678:0
  %1375 : int[] = prim::ListConstruct(%1360, %1353, %1352, %1351), scope: __module.attentions.10
  %1376 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.42, %1375), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1376, %1354, %1350), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %1378 : Tensor = prim::GetAttr[name="bias"](%1357)
  %1379 : Tensor = prim::GetAttr[name="weight"](%1357)
  %1380 : Float(2048:1, 2048:2048) = aten::t(%1379), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %1380), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %1378, %1354), scope: __module.attentions.10/__module.attentions.10.v_lin # torch/nn/functional.py:1678:0
  %1383 : int[] = prim::ListConstruct(%1360, %1353, %1352, %1351), scope: __module.attentions.10
  %1384 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.43, %1383), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1384, %1354, %1350), scope: __module.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %1349), scope: __module.attentions.10 # transformers/modeling_xlm.py:188:0
  %1387 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %1350, %1348), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
  %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %1387), scope: __module.attentions.10 # transformers/modeling_xlm.py:189:0
  %1389 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %1355), scope: __module.attentions.10 # torch/tensor.py:22:0
  %1390 : int[] = prim::ListConstruct(%1360, %1354, %1354, %1361), scope: __module.attentions.10
  %1391 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1389, %1390), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
  %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1391, %scores.21), scope: __module.attentions.10 # transformers/modeling_xlm.py:190:0
  %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %1347), scope: __module.attentions.10 # transformers/modeling_xlm.py:191:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %1346, %1345, %1345, %1344), scope: __module.attentions.10 # transformers/modeling_xlm.py:193:0
  %1395 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %1353, %1344), scope: __module.attentions.10 # torch/nn/functional.py:1498:0
  %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1395, %1343, %1345), scope: __module.attentions.10 # torch/nn/functional.py:973:0
  %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.attentions.10 # transformers/modeling_xlm.py:200:0
  %1398 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %1354, %1350), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
  %1399 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1398, %1355), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
  %1400 : int[] = prim::ListConstruct(%1360, %1353, %1342), scope: __module.attentions.10
  %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1399, %1400), scope: __module.attentions.10 # transformers/modeling_xlm.py:167:0
  %1402 : Tensor = prim::GetAttr[name="bias"](%1356)
  %1403 : Tensor = prim::GetAttr[name="weight"](%1356)
  %1404 : Float(2048:1, 2048:2048) = aten::t(%1403), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %1404), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %1402, %1354), scope: __module.attentions.10/__module.attentions.10.out_lin # torch/nn/functional.py:1678:0
  %329 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %330 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.108, %329, %330) # torch/nn/functional.py:973:0
  %332 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %332) # transformers/modeling_flaubert.py:265:0
  %1407 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
  %1408 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
  %1409 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
  %1410 : Tensor = prim::GetAttr[name="bias"](%16)
  %1411 : Tensor = prim::GetAttr[name="weight"](%16)
  %1412 : int[] = prim::ListConstruct(%1409), scope: __module.layer_norm1.10
  %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %1412, %1411, %1410, %1408, %1407), scope: __module.layer_norm1.10 # torch/nn/functional.py:2048:0
  %1414 : bool = prim::Constant[value=0](), scope: __module.ffns.10 # torch/nn/functional.py:973:0
  %1415 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.10 # torch/nn/functional.py:973:0
  %1416 : int = prim::Constant[value=1](), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1678:0
  %1417 : __torch__.torch.nn.modules.linear.___torch_mangle_1154.Linear = prim::GetAttr[name="lin2"](%14)
  %1418 : __torch__.torch.nn.modules.linear.___torch_mangle_1153.Linear = prim::GetAttr[name="lin1"](%14)
  %1419 : Tensor = prim::GetAttr[name="bias"](%1418)
  %1420 : Tensor = prim::GetAttr[name="weight"](%1418)
  %1421 : Float(2048:1, 8192:2048) = aten::t(%1420), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.11, %1421), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %1419, %1416), scope: __module.ffns.10/__module.ffns.10.lin1 # torch/nn/functional.py:1678:0
  %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.110), scope: __module.ffns.10 # torch/nn/functional.py:1369:0
  %1425 : Tensor = prim::GetAttr[name="bias"](%1417)
  %1426 : Tensor = prim::GetAttr[name="weight"](%1417)
  %1427 : Float(8192:1, 2048:8192) = aten::t(%1426), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %1427), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %1425, %1416), scope: __module.ffns.10/__module.ffns.10.lin2 # torch/nn/functional.py:1678:0
  %1430 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.112, %1415, %1414), scope: __module.ffns.10 # torch/nn/functional.py:973:0
  %336 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.11, %1430, %336) # transformers/modeling_flaubert.py:285:0
  %1431 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1432 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1433 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1434 : Tensor = prim::GetAttr[name="bias"](%12)
  %1435 : Tensor = prim::GetAttr[name="weight"](%12)
  %1436 : int[] = prim::ListConstruct(%1433), scope: __module.layer_norm2.10
  %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %1436, %1435, %1434, %1432, %1431), scope: __module.layer_norm2.10 # torch/nn/functional.py:2048:0
  %339 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %340 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %339) # transformers/modeling_flaubert.py:291:0
  %341 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %342 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %343 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %344 : None = prim::Constant()
  %345 : Float(17:13, 13:1, 1:1) = aten::to(%340, %341, %342, %343, %344) # transformers/modeling_flaubert.py:291:0
  %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.12, %345) # transformers/modeling_flaubert.py:291:0
  %1438 : int = prim::Constant[value=2048](), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
  %1439 : float = prim::Constant[value=0.10000000000000001](), scope: __module.attentions.11 # torch/nn/functional.py:973:0
  %1440 : None = prim::Constant(), scope: __module.attentions.11
  %1441 : bool = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
  %1442 : int = prim::Constant[value=6](), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
  %1443 : float = prim::Constant[value=-inf](), scope: __module.attentions.11 # transformers/modeling_xlm.py:191:0
  %1444 : int = prim::Constant[value=3](), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
  %1445 : Double() = prim::Constant[value={11.3137}](), scope: __module.attentions.11 # transformers/modeling_xlm.py:188:0
  %1446 : int = prim::Constant[value=2](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %1447 : int = prim::Constant[value=128](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %1448 : int = prim::Constant[value=16](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %1449 : int = prim::Constant[value=-1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %1450 : int = prim::Constant[value=1](), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
  %1451 : int = prim::Constant[value=0](), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
  %1452 : __torch__.torch.nn.modules.linear.___torch_mangle_1107.Linear = prim::GetAttr[name="out_lin"](%10)
  %1453 : __torch__.torch.nn.modules.linear.___torch_mangle_1106.Linear = prim::GetAttr[name="v_lin"](%10)
  %1454 : __torch__.torch.nn.modules.linear.___torch_mangle_1105.Linear = prim::GetAttr[name="k_lin"](%10)
  %1455 : __torch__.torch.nn.modules.linear.___torch_mangle_1104.Linear = prim::GetAttr[name="q_lin"](%10)
  %1456 : int = aten::size(%input.114, %1451), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
  %1457 : int = aten::size(%input.114, %1450), scope: __module.attentions.11 # transformers/modeling_xlm.py:151:0
  %1458 : Tensor = prim::GetAttr[name="bias"](%1455)
  %1459 : Tensor = prim::GetAttr[name="weight"](%1455)
  %1460 : Float(2048:1, 2048:2048) = aten::t(%1459), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1460), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %1458, %1450), scope: __module.attentions.11/__module.attentions.11.q_lin # torch/nn/functional.py:1678:0
  %1463 : int[] = prim::ListConstruct(%1456, %1449, %1448, %1447), scope: __module.attentions.11
  %1464 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.45, %1463), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1464, %1450, %1446), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %1466 : Tensor = prim::GetAttr[name="bias"](%1454)
  %1467 : Tensor = prim::GetAttr[name="weight"](%1454)
  %1468 : Float(2048:1, 2048:2048) = aten::t(%1467), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1468), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %1466, %1450), scope: __module.attentions.11/__module.attentions.11.k_lin # torch/nn/functional.py:1678:0
  %1471 : int[] = prim::ListConstruct(%1456, %1449, %1448, %1447), scope: __module.attentions.11
  %1472 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.46, %1471), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1472, %1450, %1446), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %1474 : Tensor = prim::GetAttr[name="bias"](%1453)
  %1475 : Tensor = prim::GetAttr[name="weight"](%1453)
  %1476 : Float(2048:1, 2048:2048) = aten::t(%1475), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1476), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %1474, %1450), scope: __module.attentions.11/__module.attentions.11.v_lin # torch/nn/functional.py:1678:0
  %1479 : int[] = prim::ListConstruct(%1456, %1449, %1448, %1447), scope: __module.attentions.11
  %1480 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.47, %1479), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1480, %1450, %1446), scope: __module.attentions.11 # transformers/modeling_xlm.py:163:0
  %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %1445), scope: __module.attentions.11 # transformers/modeling_xlm.py:188:0
  %1483 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %1446, %1444), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
  %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %1483), scope: __module.attentions.11 # transformers/modeling_xlm.py:189:0
  %1485 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %1451), scope: __module.attentions.11 # torch/tensor.py:22:0
  %1486 : int[] = prim::ListConstruct(%1456, %1450, %1450, %1457), scope: __module.attentions.11
  %1487 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1485, %1486), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
  %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1487, %scores.23), scope: __module.attentions.11 # transformers/modeling_xlm.py:190:0
  %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %1443), scope: __module.attentions.11 # transformers/modeling_xlm.py:191:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %1442, %1441, %1441, %1440), scope: __module.attentions.11 # transformers/modeling_xlm.py:193:0
  %1491 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %1449, %1440), scope: __module.attentions.11 # torch/nn/functional.py:1498:0
  %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1491, %1439, %1441), scope: __module.attentions.11 # torch/nn/functional.py:973:0
  %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.attentions.11 # transformers/modeling_xlm.py:200:0
  %1494 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %1450, %1446), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
  %1495 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1494, %1451), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
  %1496 : int[] = prim::ListConstruct(%1456, %1449, %1438), scope: __module.attentions.11
  %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1495, %1496), scope: __module.attentions.11 # transformers/modeling_xlm.py:167:0
  %1498 : Tensor = prim::GetAttr[name="bias"](%1452)
  %1499 : Tensor = prim::GetAttr[name="weight"](%1452)
  %1500 : Float(2048:1, 2048:2048) = aten::t(%1499), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %1500), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %1498, %1450), scope: __module.attentions.11/__module.attentions.11.out_lin # torch/nn/functional.py:1678:0
  %348 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/functional.py:973:0
  %349 : bool = prim::Constant[value=0]() # torch/nn/functional.py:973:0
  %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.118, %348, %349) # torch/nn/functional.py:973:0
  %351 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:265:0
  %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %351) # transformers/modeling_flaubert.py:265:0
  %1503 : bool = prim::Constant[value=1](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1504 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1505 : int = prim::Constant[value=2048](), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1506 : Tensor = prim::GetAttr[name="bias"](%8)
  %1507 : Tensor = prim::GetAttr[name="weight"](%8)
  %1508 : int[] = prim::ListConstruct(%1505), scope: __module.layer_norm1.11
  %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %1508, %1507, %1506, %1504, %1503), scope: __module.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1510 : bool = prim::Constant[value=0](), scope: __module.ffns.11 # torch/nn/functional.py:973:0
  %1511 : float = prim::Constant[value=0.10000000000000001](), scope: __module.ffns.11 # torch/nn/functional.py:973:0
  %1512 : int = prim::Constant[value=1](), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1678:0
  %1513 : __torch__.torch.nn.modules.linear.___torch_mangle_1157.Linear = prim::GetAttr[name="lin2"](%6)
  %1514 : __torch__.torch.nn.modules.linear.___torch_mangle_1156.Linear = prim::GetAttr[name="lin1"](%6)
  %1515 : Tensor = prim::GetAttr[name="bias"](%1514)
  %1516 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1517 : Float(2048:1, 8192:2048) = aten::t(%1516), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor, %1517), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %1515, %1512), scope: __module.ffns.11/__module.ffns.11.lin1 # torch/nn/functional.py:1678:0
  %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.120), scope: __module.ffns.11 # torch/nn/functional.py:1369:0
  %1521 : Tensor = prim::GetAttr[name="bias"](%1513)
  %1522 : Tensor = prim::GetAttr[name="weight"](%1513)
  %1523 : Float(8192:1, 2048:8192) = aten::t(%1522), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %output : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %1523), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output, %1521, %1512), scope: __module.ffns.11/__module.ffns.11.lin2 # torch/nn/functional.py:1678:0
  %1526 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.122, %1511, %1510), scope: __module.ffns.11 # torch/nn/functional.py:973:0
  %355 : int = prim::Constant[value=1]() # transformers/modeling_flaubert.py:285:0
  %input : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor, %1526, %355) # transformers/modeling_flaubert.py:285:0
  %1527 : bool = prim::Constant[value=1](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1528 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1529 : int = prim::Constant[value=2048](), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1530 : Tensor = prim::GetAttr[name="bias"](%4)
  %1531 : Tensor = prim::GetAttr[name="weight"](%4)
  %1532 : int[] = prim::ListConstruct(%1529), scope: __module.layer_norm2.11
  %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input, %1532, %1531, %1530, %1528, %1527), scope: __module.layer_norm2.11 # torch/nn/functional.py:2048:0
  %358 : int = prim::Constant[value=-1]() # transformers/modeling_flaubert.py:291:0
  %359 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %358) # transformers/modeling_flaubert.py:291:0
  %360 : int = prim::Constant[value=6]() # transformers/modeling_flaubert.py:291:0
  %361 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %362 : bool = prim::Constant[value=0]() # transformers/modeling_flaubert.py:291:0
  %363 : None = prim::Constant()
  %364 : Float(17:13, 13:1, 1:1) = aten::to(%359, %360, %361, %362, %363) # transformers/modeling_flaubert.py:291:0
  %365 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor, %364) # transformers/modeling_flaubert.py:291:0
  %366 : (Float(17:26624, 13:2048, 2048:1)) = prim::TupleConstruct(%365)
  return (%366)
