graph(%self.1 : __torch__.transformers.modeling_flaubert.FlaubertForTokenClassification,
      %input_ids : Long(17:13, 13:1),
      %padding_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_914.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_913.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_flaubert.___torch_mangle_912.FlaubertModel = prim::GetAttr[name="transformer"](%self.1)
  %10 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %11 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %12 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %13 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %14 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %15 : None = prim::Constant(), scope: __module.transformer
  %16 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %17 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
  %18 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %19 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %20 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %21 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %22 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %23 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %24 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %25 : int = prim::Constant[value=4](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %26 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %27 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %28 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %29 : __torch__.torch.nn.modules.normalization.___torch_mangle_910.LayerNorm = prim::GetAttr[name="11"](%28)
  %30 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %31 : __torch__.transformers.modeling_xlm.___torch_mangle_897.TransformerFFN = prim::GetAttr[name="11"](%30)
  %32 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %33 : __torch__.torch.nn.modules.normalization.___torch_mangle_860.LayerNorm = prim::GetAttr[name="11"](%32)
  %34 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %35 : __torch__.transformers.modeling_xlm.___torch_mangle_847.MultiHeadAttention = prim::GetAttr[name="11"](%34)
  %36 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %37 : __torch__.torch.nn.modules.normalization.___torch_mangle_909.LayerNorm = prim::GetAttr[name="10"](%36)
  %38 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %39 : __torch__.transformers.modeling_xlm.___torch_mangle_894.TransformerFFN = prim::GetAttr[name="10"](%38)
  %40 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %41 : __torch__.torch.nn.modules.normalization.___torch_mangle_859.LayerNorm = prim::GetAttr[name="10"](%40)
  %42 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %43 : __torch__.transformers.modeling_xlm.___torch_mangle_842.MultiHeadAttention = prim::GetAttr[name="10"](%42)
  %44 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %45 : __torch__.torch.nn.modules.normalization.___torch_mangle_908.LayerNorm = prim::GetAttr[name="9"](%44)
  %46 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %47 : __torch__.transformers.modeling_xlm.___torch_mangle_891.TransformerFFN = prim::GetAttr[name="9"](%46)
  %48 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %49 : __torch__.torch.nn.modules.normalization.___torch_mangle_858.LayerNorm = prim::GetAttr[name="9"](%48)
  %50 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %51 : __torch__.transformers.modeling_xlm.___torch_mangle_837.MultiHeadAttention = prim::GetAttr[name="9"](%50)
  %52 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %53 : __torch__.torch.nn.modules.normalization.___torch_mangle_907.LayerNorm = prim::GetAttr[name="8"](%52)
  %54 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %55 : __torch__.transformers.modeling_xlm.___torch_mangle_888.TransformerFFN = prim::GetAttr[name="8"](%54)
  %56 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %57 : __torch__.torch.nn.modules.normalization.___torch_mangle_857.LayerNorm = prim::GetAttr[name="8"](%56)
  %58 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %59 : __torch__.transformers.modeling_xlm.___torch_mangle_832.MultiHeadAttention = prim::GetAttr[name="8"](%58)
  %60 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %61 : __torch__.torch.nn.modules.normalization.___torch_mangle_906.LayerNorm = prim::GetAttr[name="7"](%60)
  %62 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %63 : __torch__.transformers.modeling_xlm.___torch_mangle_885.TransformerFFN = prim::GetAttr[name="7"](%62)
  %64 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %65 : __torch__.torch.nn.modules.normalization.___torch_mangle_856.LayerNorm = prim::GetAttr[name="7"](%64)
  %66 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %67 : __torch__.transformers.modeling_xlm.___torch_mangle_827.MultiHeadAttention = prim::GetAttr[name="7"](%66)
  %68 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %69 : __torch__.torch.nn.modules.normalization.___torch_mangle_905.LayerNorm = prim::GetAttr[name="6"](%68)
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %71 : __torch__.transformers.modeling_xlm.___torch_mangle_882.TransformerFFN = prim::GetAttr[name="6"](%70)
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %73 : __torch__.torch.nn.modules.normalization.___torch_mangle_855.LayerNorm = prim::GetAttr[name="6"](%72)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %75 : __torch__.transformers.modeling_xlm.___torch_mangle_822.MultiHeadAttention = prim::GetAttr[name="6"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %77 : __torch__.torch.nn.modules.normalization.___torch_mangle_904.LayerNorm = prim::GetAttr[name="5"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %79 : __torch__.transformers.modeling_xlm.___torch_mangle_879.TransformerFFN = prim::GetAttr[name="5"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %81 : __torch__.torch.nn.modules.normalization.___torch_mangle_854.LayerNorm = prim::GetAttr[name="5"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %83 : __torch__.transformers.modeling_xlm.___torch_mangle_817.MultiHeadAttention = prim::GetAttr[name="5"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %85 : __torch__.torch.nn.modules.normalization.___torch_mangle_903.LayerNorm = prim::GetAttr[name="4"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %87 : __torch__.transformers.modeling_xlm.___torch_mangle_876.TransformerFFN = prim::GetAttr[name="4"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %89 : __torch__.torch.nn.modules.normalization.___torch_mangle_853.LayerNorm = prim::GetAttr[name="4"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %91 : __torch__.transformers.modeling_xlm.___torch_mangle_812.MultiHeadAttention = prim::GetAttr[name="4"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %93 : __torch__.torch.nn.modules.normalization.___torch_mangle_902.LayerNorm = prim::GetAttr[name="3"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %95 : __torch__.transformers.modeling_xlm.___torch_mangle_873.TransformerFFN = prim::GetAttr[name="3"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %97 : __torch__.torch.nn.modules.normalization.___torch_mangle_852.LayerNorm = prim::GetAttr[name="3"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %99 : __torch__.transformers.modeling_xlm.___torch_mangle_807.MultiHeadAttention = prim::GetAttr[name="3"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %101 : __torch__.torch.nn.modules.normalization.___torch_mangle_901.LayerNorm = prim::GetAttr[name="2"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %103 : __torch__.transformers.modeling_xlm.___torch_mangle_870.TransformerFFN = prim::GetAttr[name="2"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %105 : __torch__.torch.nn.modules.normalization.___torch_mangle_851.LayerNorm = prim::GetAttr[name="2"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %107 : __torch__.transformers.modeling_xlm.___torch_mangle_802.MultiHeadAttention = prim::GetAttr[name="2"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %109 : __torch__.torch.nn.modules.normalization.___torch_mangle_900.LayerNorm = prim::GetAttr[name="1"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %111 : __torch__.transformers.modeling_xlm.___torch_mangle_867.TransformerFFN = prim::GetAttr[name="1"](%110)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %113 : __torch__.torch.nn.modules.normalization.___torch_mangle_850.LayerNorm = prim::GetAttr[name="1"](%112)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %115 : __torch__.transformers.modeling_xlm.___torch_mangle_797.MultiHeadAttention = prim::GetAttr[name="1"](%114)
  %116 : __torch__.torch.nn.modules.container.___torch_mangle_911.ModuleList = prim::GetAttr[name="layer_norm2"](%5)
  %117 : __torch__.torch.nn.modules.normalization.___torch_mangle_899.LayerNorm = prim::GetAttr[name="0"](%116)
  %118 : __torch__.torch.nn.modules.container.___torch_mangle_898.ModuleList = prim::GetAttr[name="ffns"](%5)
  %119 : __torch__.transformers.modeling_xlm.___torch_mangle_864.TransformerFFN = prim::GetAttr[name="0"](%118)
  %120 : __torch__.torch.nn.modules.container.___torch_mangle_861.ModuleList = prim::GetAttr[name="layer_norm1"](%5)
  %121 : __torch__.torch.nn.modules.normalization.___torch_mangle_849.LayerNorm = prim::GetAttr[name="0"](%120)
  %122 : __torch__.torch.nn.modules.container.___torch_mangle_848.ModuleList = prim::GetAttr[name="attentions"](%5)
  %123 : __torch__.transformers.modeling_xlm.___torch_mangle_792.MultiHeadAttention = prim::GetAttr[name="0"](%122)
  %124 : __torch__.torch.nn.modules.normalization.___torch_mangle_787.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%5)
  %125 : __torch__.torch.nn.modules.sparse.___torch_mangle_785.Embedding = prim::GetAttr[name="position_embeddings"](%5)
  %126 : __torch__.torch.nn.modules.sparse.___torch_mangle_786.Embedding = prim::GetAttr[name="embeddings"](%5)
  %127 : int = aten::size(%input_ids, %27), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %128 : int = aten::size(%input_ids, %26), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %position_ids : Long(13:1) = aten::arange(%128, %25, %27, %24, %23), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %130 : Long(1:13, 13:1) = aten::unsqueeze(%position_ids, %27), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
  %131 : int[] = prim::ListConstruct(%127, %128), scope: __module.transformer
  %input.1 : Long(17:0, 13:1) = aten::expand(%130, %131, %23), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
  %133 : Tensor = prim::GetAttr[name="weight"](%126)
  %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%133, %input_ids, %22, %23, %23), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %135 : Tensor = prim::GetAttr[name="weight"](%125)
  %136 : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%135, %input.1, %21, %23, %23), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %137 : Float(17:26624, 13:2048, 2048:1) = aten::expand_as(%136, %inputs_embeds), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
  %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%inputs_embeds, %137, %26), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
  %139 : Tensor = prim::GetAttr[name="bias"](%124)
  %140 : Tensor = prim::GetAttr[name="weight"](%124)
  %141 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm_emb
  %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %141, %140, %139, %19, %20), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.3, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %144 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %145 : Float(17:13, 13:1, 1:1) = aten::to(%144, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %145), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %147 : __torch__.torch.nn.modules.linear.___torch_mangle_791.Linear = prim::GetAttr[name="out_lin"](%123)
  %148 : __torch__.torch.nn.modules.linear.___torch_mangle_790.Linear = prim::GetAttr[name="v_lin"](%123)
  %149 : __torch__.torch.nn.modules.linear.___torch_mangle_789.Linear = prim::GetAttr[name="k_lin"](%123)
  %150 : __torch__.torch.nn.modules.linear.___torch_mangle_788.Linear = prim::GetAttr[name="q_lin"](%123)
  %151 : int = aten::size(%input.4, %27), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %152 : int = aten::size(%input.4, %26), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %153 : Tensor = prim::GetAttr[name="bias"](%150)
  %154 : Tensor = prim::GetAttr[name="weight"](%150)
  %155 : Float(2048:1, 2048:2048) = aten::t(%154), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %155), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %153, %26), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1678:0
  %158 : int[] = prim::ListConstruct(%151, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.0
  %159 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.1, %158), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%159, %26, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %161 : Tensor = prim::GetAttr[name="bias"](%149)
  %162 : Tensor = prim::GetAttr[name="weight"](%149)
  %163 : Float(2048:1, 2048:2048) = aten::t(%162), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %163), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %161, %26), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1678:0
  %166 : int[] = prim::ListConstruct(%151, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.0
  %167 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.2, %166), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%167, %26, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %169 : Tensor = prim::GetAttr[name="bias"](%148)
  %170 : Tensor = prim::GetAttr[name="weight"](%148)
  %171 : Float(2048:1, 2048:2048) = aten::t(%170), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %171), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %169, %26), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1678:0
  %174 : int[] = prim::ListConstruct(%151, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.0
  %175 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.3, %174), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%175, %26, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %12), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %178 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %22, %13), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %178), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %180 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.0 # torch/tensor.py:22:0
  %181 : int[] = prim::ListConstruct(%151, %26, %26, %152), scope: __module.transformer/__module.transformer.attentions.0
  %182 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%180, %181), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%182, %scores.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %14), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
  %186 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %21, %15), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:1498:0
  %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%186, %17, %23), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
  %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:200:0
  %189 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %26, %22), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %190 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%189, %27), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %191 : int[] = prim::ListConstruct(%151, %21, %18), scope: __module.transformer/__module.transformer.attentions.0
  %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%190, %191), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %193 : Tensor = prim::GetAttr[name="bias"](%147)
  %194 : Tensor = prim::GetAttr[name="weight"](%147)
  %195 : Float(2048:1, 2048:2048) = aten::t(%194), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %195), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %193, %26), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1678:0
  %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.8, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %200 : Tensor = prim::GetAttr[name="bias"](%121)
  %201 : Tensor = prim::GetAttr[name="weight"](%121)
  %202 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.0
  %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %202, %201, %200, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
  %204 : __torch__.torch.nn.modules.linear.___torch_mangle_863.Linear = prim::GetAttr[name="lin2"](%119)
  %205 : __torch__.torch.nn.modules.linear.___torch_mangle_862.Linear = prim::GetAttr[name="lin1"](%119)
  %206 : Tensor = prim::GetAttr[name="bias"](%205)
  %207 : Tensor = prim::GetAttr[name="weight"](%205)
  %208 : Float(2048:1, 8192:2048) = aten::t(%207), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.1, %208), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %206, %26), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1678:0
  %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.10), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:1369:0
  %212 : Tensor = prim::GetAttr[name="bias"](%204)
  %213 : Tensor = prim::GetAttr[name="weight"](%204)
  %214 : Float(8192:1, 2048:8192) = aten::t(%213), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %214), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %212, %26), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1678:0
  %217 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.12, %17, %23), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
  %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.1, %217, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %219 : Tensor = prim::GetAttr[name="bias"](%117)
  %220 : Tensor = prim::GetAttr[name="weight"](%117)
  %221 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.0
  %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %221, %220, %219, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
  %223 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %224 : Float(17:13, 13:1, 1:1) = aten::to(%223, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.2, %224), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %226 : __torch__.torch.nn.modules.linear.___torch_mangle_796.Linear = prim::GetAttr[name="out_lin"](%115)
  %227 : __torch__.torch.nn.modules.linear.___torch_mangle_795.Linear = prim::GetAttr[name="v_lin"](%115)
  %228 : __torch__.torch.nn.modules.linear.___torch_mangle_794.Linear = prim::GetAttr[name="k_lin"](%115)
  %229 : __torch__.torch.nn.modules.linear.___torch_mangle_793.Linear = prim::GetAttr[name="q_lin"](%115)
  %230 : int = aten::size(%input.14, %27), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %231 : int = aten::size(%input.14, %26), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %232 : Tensor = prim::GetAttr[name="bias"](%229)
  %233 : Tensor = prim::GetAttr[name="weight"](%229)
  %234 : Float(2048:1, 2048:2048) = aten::t(%233), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %234), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %232, %26), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1678:0
  %237 : int[] = prim::ListConstruct(%230, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.1
  %238 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.5, %237), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%238, %26, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %240 : Tensor = prim::GetAttr[name="bias"](%228)
  %241 : Tensor = prim::GetAttr[name="weight"](%228)
  %242 : Float(2048:1, 2048:2048) = aten::t(%241), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %242), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %240, %26), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1678:0
  %245 : int[] = prim::ListConstruct(%230, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.1
  %246 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.6, %245), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%246, %26, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %248 : Tensor = prim::GetAttr[name="bias"](%227)
  %249 : Tensor = prim::GetAttr[name="weight"](%227)
  %250 : Float(2048:1, 2048:2048) = aten::t(%249), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %250), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %248, %26), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1678:0
  %253 : int[] = prim::ListConstruct(%230, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.1
  %254 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.7, %253), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%254, %26, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %12), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:188:0
  %257 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %22, %13), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %257), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %259 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.1 # torch/tensor.py:22:0
  %260 : int[] = prim::ListConstruct(%230, %26, %26, %231), scope: __module.transformer/__module.transformer.attentions.1
  %261 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%259, %260), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%261, %scores.3), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %14), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:191:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
  %265 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %21, %15), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:1498:0
  %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%265, %17, %23), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
  %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:200:0
  %268 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %26, %22), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %269 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%268, %27), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %270 : int[] = prim::ListConstruct(%230, %21, %18), scope: __module.transformer/__module.transformer.attentions.1
  %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%269, %270), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %272 : Tensor = prim::GetAttr[name="bias"](%226)
  %273 : Tensor = prim::GetAttr[name="weight"](%226)
  %274 : Float(2048:1, 2048:2048) = aten::t(%273), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %274), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %272, %26), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1678:0
  %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.18, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %279 : Tensor = prim::GetAttr[name="bias"](%113)
  %280 : Tensor = prim::GetAttr[name="weight"](%113)
  %281 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.1
  %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %281, %280, %279, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
  %283 : __torch__.torch.nn.modules.linear.___torch_mangle_866.Linear = prim::GetAttr[name="lin2"](%111)
  %284 : __torch__.torch.nn.modules.linear.___torch_mangle_865.Linear = prim::GetAttr[name="lin1"](%111)
  %285 : Tensor = prim::GetAttr[name="bias"](%284)
  %286 : Tensor = prim::GetAttr[name="weight"](%284)
  %287 : Float(2048:1, 8192:2048) = aten::t(%286), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.2, %287), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %285, %26), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1678:0
  %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.20), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:1369:0
  %291 : Tensor = prim::GetAttr[name="bias"](%283)
  %292 : Tensor = prim::GetAttr[name="weight"](%283)
  %293 : Float(8192:1, 2048:8192) = aten::t(%292), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %293), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %291, %26), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1678:0
  %296 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.22, %17, %23), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
  %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.2, %296, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %298 : Tensor = prim::GetAttr[name="bias"](%109)
  %299 : Tensor = prim::GetAttr[name="weight"](%109)
  %300 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.1
  %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %300, %299, %298, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
  %302 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %303 : Float(17:13, 13:1, 1:1) = aten::to(%302, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.3, %303), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %305 : __torch__.torch.nn.modules.linear.___torch_mangle_801.Linear = prim::GetAttr[name="out_lin"](%107)
  %306 : __torch__.torch.nn.modules.linear.___torch_mangle_800.Linear = prim::GetAttr[name="v_lin"](%107)
  %307 : __torch__.torch.nn.modules.linear.___torch_mangle_799.Linear = prim::GetAttr[name="k_lin"](%107)
  %308 : __torch__.torch.nn.modules.linear.___torch_mangle_798.Linear = prim::GetAttr[name="q_lin"](%107)
  %309 : int = aten::size(%input.24, %27), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %310 : int = aten::size(%input.24, %26), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %311 : Tensor = prim::GetAttr[name="bias"](%308)
  %312 : Tensor = prim::GetAttr[name="weight"](%308)
  %313 : Float(2048:1, 2048:2048) = aten::t(%312), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %313), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %311, %26), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1678:0
  %316 : int[] = prim::ListConstruct(%309, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.2
  %317 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.9, %316), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%317, %26, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %319 : Tensor = prim::GetAttr[name="bias"](%307)
  %320 : Tensor = prim::GetAttr[name="weight"](%307)
  %321 : Float(2048:1, 2048:2048) = aten::t(%320), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %321), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %319, %26), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1678:0
  %324 : int[] = prim::ListConstruct(%309, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.2
  %325 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.10, %324), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%325, %26, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %327 : Tensor = prim::GetAttr[name="bias"](%306)
  %328 : Tensor = prim::GetAttr[name="weight"](%306)
  %329 : Float(2048:1, 2048:2048) = aten::t(%328), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %329), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %327, %26), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1678:0
  %332 : int[] = prim::ListConstruct(%309, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.2
  %333 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.11, %332), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%333, %26, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %12), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:188:0
  %336 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %22, %13), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %336), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %338 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.2 # torch/tensor.py:22:0
  %339 : int[] = prim::ListConstruct(%309, %26, %26, %310), scope: __module.transformer/__module.transformer.attentions.2
  %340 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%338, %339), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%340, %scores.5), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %14), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:191:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
  %344 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %21, %15), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:1498:0
  %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%344, %17, %23), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
  %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:200:0
  %347 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %26, %22), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %348 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%347, %27), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %349 : int[] = prim::ListConstruct(%309, %21, %18), scope: __module.transformer/__module.transformer.attentions.2
  %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%348, %349), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %351 : Tensor = prim::GetAttr[name="bias"](%305)
  %352 : Tensor = prim::GetAttr[name="weight"](%305)
  %353 : Float(2048:1, 2048:2048) = aten::t(%352), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %353), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %351, %26), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1678:0
  %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.28, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %358 : Tensor = prim::GetAttr[name="bias"](%105)
  %359 : Tensor = prim::GetAttr[name="weight"](%105)
  %360 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.2
  %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %360, %359, %358, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
  %362 : __torch__.torch.nn.modules.linear.___torch_mangle_869.Linear = prim::GetAttr[name="lin2"](%103)
  %363 : __torch__.torch.nn.modules.linear.___torch_mangle_868.Linear = prim::GetAttr[name="lin1"](%103)
  %364 : Tensor = prim::GetAttr[name="bias"](%363)
  %365 : Tensor = prim::GetAttr[name="weight"](%363)
  %366 : Float(2048:1, 8192:2048) = aten::t(%365), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.3, %366), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %364, %26), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1678:0
  %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.30), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:1369:0
  %370 : Tensor = prim::GetAttr[name="bias"](%362)
  %371 : Tensor = prim::GetAttr[name="weight"](%362)
  %372 : Float(8192:1, 2048:8192) = aten::t(%371), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %372), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %370, %26), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1678:0
  %375 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.32, %17, %23), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
  %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.3, %375, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %377 : Tensor = prim::GetAttr[name="bias"](%101)
  %378 : Tensor = prim::GetAttr[name="weight"](%101)
  %379 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.2
  %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %379, %378, %377, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
  %381 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %382 : Float(17:13, 13:1, 1:1) = aten::to(%381, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.4, %382), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %384 : __torch__.torch.nn.modules.linear.___torch_mangle_806.Linear = prim::GetAttr[name="out_lin"](%99)
  %385 : __torch__.torch.nn.modules.linear.___torch_mangle_805.Linear = prim::GetAttr[name="v_lin"](%99)
  %386 : __torch__.torch.nn.modules.linear.___torch_mangle_804.Linear = prim::GetAttr[name="k_lin"](%99)
  %387 : __torch__.torch.nn.modules.linear.___torch_mangle_803.Linear = prim::GetAttr[name="q_lin"](%99)
  %388 : int = aten::size(%input.34, %27), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %389 : int = aten::size(%input.34, %26), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %390 : Tensor = prim::GetAttr[name="bias"](%387)
  %391 : Tensor = prim::GetAttr[name="weight"](%387)
  %392 : Float(2048:1, 2048:2048) = aten::t(%391), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %392), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %390, %26), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1678:0
  %395 : int[] = prim::ListConstruct(%388, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.3
  %396 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.13, %395), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%396, %26, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %398 : Tensor = prim::GetAttr[name="bias"](%386)
  %399 : Tensor = prim::GetAttr[name="weight"](%386)
  %400 : Float(2048:1, 2048:2048) = aten::t(%399), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %400), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %398, %26), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1678:0
  %403 : int[] = prim::ListConstruct(%388, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.3
  %404 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.14, %403), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%404, %26, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %406 : Tensor = prim::GetAttr[name="bias"](%385)
  %407 : Tensor = prim::GetAttr[name="weight"](%385)
  %408 : Float(2048:1, 2048:2048) = aten::t(%407), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %408), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %406, %26), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1678:0
  %411 : int[] = prim::ListConstruct(%388, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.3
  %412 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.15, %411), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%412, %26, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %12), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:188:0
  %415 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %22, %13), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %415), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %417 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.3 # torch/tensor.py:22:0
  %418 : int[] = prim::ListConstruct(%388, %26, %26, %389), scope: __module.transformer/__module.transformer.attentions.3
  %419 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%417, %418), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%419, %scores.7), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %14), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:191:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
  %423 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %21, %15), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:1498:0
  %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%423, %17, %23), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
  %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:200:0
  %426 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %26, %22), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %427 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%426, %27), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %428 : int[] = prim::ListConstruct(%388, %21, %18), scope: __module.transformer/__module.transformer.attentions.3
  %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%427, %428), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %430 : Tensor = prim::GetAttr[name="bias"](%384)
  %431 : Tensor = prim::GetAttr[name="weight"](%384)
  %432 : Float(2048:1, 2048:2048) = aten::t(%431), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %432), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %430, %26), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1678:0
  %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.38, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %437 : Tensor = prim::GetAttr[name="bias"](%97)
  %438 : Tensor = prim::GetAttr[name="weight"](%97)
  %439 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.3
  %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %439, %438, %437, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
  %441 : __torch__.torch.nn.modules.linear.___torch_mangle_872.Linear = prim::GetAttr[name="lin2"](%95)
  %442 : __torch__.torch.nn.modules.linear.___torch_mangle_871.Linear = prim::GetAttr[name="lin1"](%95)
  %443 : Tensor = prim::GetAttr[name="bias"](%442)
  %444 : Tensor = prim::GetAttr[name="weight"](%442)
  %445 : Float(2048:1, 8192:2048) = aten::t(%444), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.4, %445), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %443, %26), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1678:0
  %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.40), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:1369:0
  %449 : Tensor = prim::GetAttr[name="bias"](%441)
  %450 : Tensor = prim::GetAttr[name="weight"](%441)
  %451 : Float(8192:1, 2048:8192) = aten::t(%450), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %451), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %449, %26), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1678:0
  %454 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.42, %17, %23), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
  %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.4, %454, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %456 : Tensor = prim::GetAttr[name="bias"](%93)
  %457 : Tensor = prim::GetAttr[name="weight"](%93)
  %458 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.3
  %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %458, %457, %456, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
  %460 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %461 : Float(17:13, 13:1, 1:1) = aten::to(%460, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.5, %461), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %463 : __torch__.torch.nn.modules.linear.___torch_mangle_811.Linear = prim::GetAttr[name="out_lin"](%91)
  %464 : __torch__.torch.nn.modules.linear.___torch_mangle_810.Linear = prim::GetAttr[name="v_lin"](%91)
  %465 : __torch__.torch.nn.modules.linear.___torch_mangle_809.Linear = prim::GetAttr[name="k_lin"](%91)
  %466 : __torch__.torch.nn.modules.linear.___torch_mangle_808.Linear = prim::GetAttr[name="q_lin"](%91)
  %467 : int = aten::size(%input.44, %27), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %468 : int = aten::size(%input.44, %26), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %469 : Tensor = prim::GetAttr[name="bias"](%466)
  %470 : Tensor = prim::GetAttr[name="weight"](%466)
  %471 : Float(2048:1, 2048:2048) = aten::t(%470), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %471), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %469, %26), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1678:0
  %474 : int[] = prim::ListConstruct(%467, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.4
  %475 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.17, %474), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%475, %26, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %477 : Tensor = prim::GetAttr[name="bias"](%465)
  %478 : Tensor = prim::GetAttr[name="weight"](%465)
  %479 : Float(2048:1, 2048:2048) = aten::t(%478), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %479), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %477, %26), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1678:0
  %482 : int[] = prim::ListConstruct(%467, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.4
  %483 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.18, %482), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%483, %26, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %485 : Tensor = prim::GetAttr[name="bias"](%464)
  %486 : Tensor = prim::GetAttr[name="weight"](%464)
  %487 : Float(2048:1, 2048:2048) = aten::t(%486), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %487), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %485, %26), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1678:0
  %490 : int[] = prim::ListConstruct(%467, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.4
  %491 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.19, %490), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%491, %26, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %12), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:188:0
  %494 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %22, %13), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %494), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %496 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.4 # torch/tensor.py:22:0
  %497 : int[] = prim::ListConstruct(%467, %26, %26, %468), scope: __module.transformer/__module.transformer.attentions.4
  %498 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%496, %497), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%498, %scores.9), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %14), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:191:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
  %502 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %21, %15), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:1498:0
  %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%502, %17, %23), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
  %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:200:0
  %505 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %26, %22), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %506 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%505, %27), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %507 : int[] = prim::ListConstruct(%467, %21, %18), scope: __module.transformer/__module.transformer.attentions.4
  %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%506, %507), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %509 : Tensor = prim::GetAttr[name="bias"](%463)
  %510 : Tensor = prim::GetAttr[name="weight"](%463)
  %511 : Float(2048:1, 2048:2048) = aten::t(%510), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %511), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %509, %26), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1678:0
  %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.48, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %516 : Tensor = prim::GetAttr[name="bias"](%89)
  %517 : Tensor = prim::GetAttr[name="weight"](%89)
  %518 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.4
  %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %518, %517, %516, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
  %520 : __torch__.torch.nn.modules.linear.___torch_mangle_875.Linear = prim::GetAttr[name="lin2"](%87)
  %521 : __torch__.torch.nn.modules.linear.___torch_mangle_874.Linear = prim::GetAttr[name="lin1"](%87)
  %522 : Tensor = prim::GetAttr[name="bias"](%521)
  %523 : Tensor = prim::GetAttr[name="weight"](%521)
  %524 : Float(2048:1, 8192:2048) = aten::t(%523), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.5, %524), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %522, %26), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1678:0
  %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.50), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:1369:0
  %528 : Tensor = prim::GetAttr[name="bias"](%520)
  %529 : Tensor = prim::GetAttr[name="weight"](%520)
  %530 : Float(8192:1, 2048:8192) = aten::t(%529), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %530), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %528, %26), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1678:0
  %533 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.52, %17, %23), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
  %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.5, %533, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %535 : Tensor = prim::GetAttr[name="bias"](%85)
  %536 : Tensor = prim::GetAttr[name="weight"](%85)
  %537 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.4
  %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %537, %536, %535, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
  %539 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %540 : Float(17:13, 13:1, 1:1) = aten::to(%539, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.6, %540), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %542 : __torch__.torch.nn.modules.linear.___torch_mangle_816.Linear = prim::GetAttr[name="out_lin"](%83)
  %543 : __torch__.torch.nn.modules.linear.___torch_mangle_815.Linear = prim::GetAttr[name="v_lin"](%83)
  %544 : __torch__.torch.nn.modules.linear.___torch_mangle_814.Linear = prim::GetAttr[name="k_lin"](%83)
  %545 : __torch__.torch.nn.modules.linear.___torch_mangle_813.Linear = prim::GetAttr[name="q_lin"](%83)
  %546 : int = aten::size(%input.54, %27), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %547 : int = aten::size(%input.54, %26), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %548 : Tensor = prim::GetAttr[name="bias"](%545)
  %549 : Tensor = prim::GetAttr[name="weight"](%545)
  %550 : Float(2048:1, 2048:2048) = aten::t(%549), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %550), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %548, %26), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1678:0
  %553 : int[] = prim::ListConstruct(%546, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.5
  %554 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.21, %553), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%554, %26, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %556 : Tensor = prim::GetAttr[name="bias"](%544)
  %557 : Tensor = prim::GetAttr[name="weight"](%544)
  %558 : Float(2048:1, 2048:2048) = aten::t(%557), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %558), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %556, %26), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1678:0
  %561 : int[] = prim::ListConstruct(%546, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.5
  %562 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.22, %561), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%562, %26, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %564 : Tensor = prim::GetAttr[name="bias"](%543)
  %565 : Tensor = prim::GetAttr[name="weight"](%543)
  %566 : Float(2048:1, 2048:2048) = aten::t(%565), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %566), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %564, %26), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1678:0
  %569 : int[] = prim::ListConstruct(%546, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.5
  %570 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.23, %569), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%570, %26, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %12), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:188:0
  %573 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %22, %13), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %573), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %575 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.5 # torch/tensor.py:22:0
  %576 : int[] = prim::ListConstruct(%546, %26, %26, %547), scope: __module.transformer/__module.transformer.attentions.5
  %577 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%575, %576), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%577, %scores.11), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %14), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:191:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
  %581 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %21, %15), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:1498:0
  %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%581, %17, %23), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
  %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:200:0
  %584 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %26, %22), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %585 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%584, %27), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %586 : int[] = prim::ListConstruct(%546, %21, %18), scope: __module.transformer/__module.transformer.attentions.5
  %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%585, %586), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %588 : Tensor = prim::GetAttr[name="bias"](%542)
  %589 : Tensor = prim::GetAttr[name="weight"](%542)
  %590 : Float(2048:1, 2048:2048) = aten::t(%589), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %590), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %588, %26), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1678:0
  %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.58, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %595 : Tensor = prim::GetAttr[name="bias"](%81)
  %596 : Tensor = prim::GetAttr[name="weight"](%81)
  %597 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.5
  %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %597, %596, %595, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
  %599 : __torch__.torch.nn.modules.linear.___torch_mangle_878.Linear = prim::GetAttr[name="lin2"](%79)
  %600 : __torch__.torch.nn.modules.linear.___torch_mangle_877.Linear = prim::GetAttr[name="lin1"](%79)
  %601 : Tensor = prim::GetAttr[name="bias"](%600)
  %602 : Tensor = prim::GetAttr[name="weight"](%600)
  %603 : Float(2048:1, 8192:2048) = aten::t(%602), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.6, %603), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %601, %26), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1678:0
  %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.60), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:1369:0
  %607 : Tensor = prim::GetAttr[name="bias"](%599)
  %608 : Tensor = prim::GetAttr[name="weight"](%599)
  %609 : Float(8192:1, 2048:8192) = aten::t(%608), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %609), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %607, %26), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1678:0
  %612 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.62, %17, %23), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
  %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.6, %612, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %614 : Tensor = prim::GetAttr[name="bias"](%77)
  %615 : Tensor = prim::GetAttr[name="weight"](%77)
  %616 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.5
  %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %616, %615, %614, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
  %618 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %619 : Float(17:13, 13:1, 1:1) = aten::to(%618, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.7, %619), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %621 : __torch__.torch.nn.modules.linear.___torch_mangle_821.Linear = prim::GetAttr[name="out_lin"](%75)
  %622 : __torch__.torch.nn.modules.linear.___torch_mangle_820.Linear = prim::GetAttr[name="v_lin"](%75)
  %623 : __torch__.torch.nn.modules.linear.___torch_mangle_819.Linear = prim::GetAttr[name="k_lin"](%75)
  %624 : __torch__.torch.nn.modules.linear.___torch_mangle_818.Linear = prim::GetAttr[name="q_lin"](%75)
  %625 : int = aten::size(%input.64, %27), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %626 : int = aten::size(%input.64, %26), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %627 : Tensor = prim::GetAttr[name="bias"](%624)
  %628 : Tensor = prim::GetAttr[name="weight"](%624)
  %629 : Float(2048:1, 2048:2048) = aten::t(%628), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %629), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %627, %26), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1678:0
  %632 : int[] = prim::ListConstruct(%625, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.6
  %633 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.25, %632), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%633, %26, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %635 : Tensor = prim::GetAttr[name="bias"](%623)
  %636 : Tensor = prim::GetAttr[name="weight"](%623)
  %637 : Float(2048:1, 2048:2048) = aten::t(%636), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %637), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %635, %26), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1678:0
  %640 : int[] = prim::ListConstruct(%625, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.6
  %641 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.26, %640), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%641, %26, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %643 : Tensor = prim::GetAttr[name="bias"](%622)
  %644 : Tensor = prim::GetAttr[name="weight"](%622)
  %645 : Float(2048:1, 2048:2048) = aten::t(%644), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %645), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %643, %26), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1678:0
  %648 : int[] = prim::ListConstruct(%625, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.6
  %649 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.27, %648), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%649, %26, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %12), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:188:0
  %652 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %22, %13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %652), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %654 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.6 # torch/tensor.py:22:0
  %655 : int[] = prim::ListConstruct(%625, %26, %26, %626), scope: __module.transformer/__module.transformer.attentions.6
  %656 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%654, %655), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%656, %scores.13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %14), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:191:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
  %660 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %21, %15), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:1498:0
  %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%660, %17, %23), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
  %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:200:0
  %663 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %26, %22), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %664 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%663, %27), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %665 : int[] = prim::ListConstruct(%625, %21, %18), scope: __module.transformer/__module.transformer.attentions.6
  %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%664, %665), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %667 : Tensor = prim::GetAttr[name="bias"](%621)
  %668 : Tensor = prim::GetAttr[name="weight"](%621)
  %669 : Float(2048:1, 2048:2048) = aten::t(%668), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %669), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %667, %26), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1678:0
  %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.68, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %674 : Tensor = prim::GetAttr[name="bias"](%73)
  %675 : Tensor = prim::GetAttr[name="weight"](%73)
  %676 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.6
  %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %676, %675, %674, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
  %678 : __torch__.torch.nn.modules.linear.___torch_mangle_881.Linear = prim::GetAttr[name="lin2"](%71)
  %679 : __torch__.torch.nn.modules.linear.___torch_mangle_880.Linear = prim::GetAttr[name="lin1"](%71)
  %680 : Tensor = prim::GetAttr[name="bias"](%679)
  %681 : Tensor = prim::GetAttr[name="weight"](%679)
  %682 : Float(2048:1, 8192:2048) = aten::t(%681), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.7, %682), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %680, %26), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1678:0
  %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:1369:0
  %686 : Tensor = prim::GetAttr[name="bias"](%678)
  %687 : Tensor = prim::GetAttr[name="weight"](%678)
  %688 : Float(8192:1, 2048:8192) = aten::t(%687), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %688), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %686, %26), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1678:0
  %691 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.72, %17, %23), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
  %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.7, %691, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %693 : Tensor = prim::GetAttr[name="bias"](%69)
  %694 : Tensor = prim::GetAttr[name="weight"](%69)
  %695 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.6
  %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %695, %694, %693, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
  %697 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %698 : Float(17:13, 13:1, 1:1) = aten::to(%697, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.8, %698), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %700 : __torch__.torch.nn.modules.linear.___torch_mangle_826.Linear = prim::GetAttr[name="out_lin"](%67)
  %701 : __torch__.torch.nn.modules.linear.___torch_mangle_825.Linear = prim::GetAttr[name="v_lin"](%67)
  %702 : __torch__.torch.nn.modules.linear.___torch_mangle_824.Linear = prim::GetAttr[name="k_lin"](%67)
  %703 : __torch__.torch.nn.modules.linear.___torch_mangle_823.Linear = prim::GetAttr[name="q_lin"](%67)
  %704 : int = aten::size(%input.74, %27), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %705 : int = aten::size(%input.74, %26), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %706 : Tensor = prim::GetAttr[name="bias"](%703)
  %707 : Tensor = prim::GetAttr[name="weight"](%703)
  %708 : Float(2048:1, 2048:2048) = aten::t(%707), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %708), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %706, %26), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1678:0
  %711 : int[] = prim::ListConstruct(%704, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.7
  %712 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.29, %711), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%712, %26, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %714 : Tensor = prim::GetAttr[name="bias"](%702)
  %715 : Tensor = prim::GetAttr[name="weight"](%702)
  %716 : Float(2048:1, 2048:2048) = aten::t(%715), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %716), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %714, %26), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1678:0
  %719 : int[] = prim::ListConstruct(%704, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.7
  %720 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.30, %719), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%720, %26, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %722 : Tensor = prim::GetAttr[name="bias"](%701)
  %723 : Tensor = prim::GetAttr[name="weight"](%701)
  %724 : Float(2048:1, 2048:2048) = aten::t(%723), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %724), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %722, %26), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1678:0
  %727 : int[] = prim::ListConstruct(%704, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.7
  %728 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.31, %727), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%728, %26, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %12), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:188:0
  %731 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %22, %13), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %731), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %733 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.7 # torch/tensor.py:22:0
  %734 : int[] = prim::ListConstruct(%704, %26, %26, %705), scope: __module.transformer/__module.transformer.attentions.7
  %735 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%733, %734), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%735, %scores.15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %14), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:191:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
  %739 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %21, %15), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:1498:0
  %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%739, %17, %23), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
  %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:200:0
  %742 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %26, %22), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %743 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%742, %27), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %744 : int[] = prim::ListConstruct(%704, %21, %18), scope: __module.transformer/__module.transformer.attentions.7
  %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%743, %744), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %746 : Tensor = prim::GetAttr[name="bias"](%700)
  %747 : Tensor = prim::GetAttr[name="weight"](%700)
  %748 : Float(2048:1, 2048:2048) = aten::t(%747), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %748), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %746, %26), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1678:0
  %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.78, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %753 : Tensor = prim::GetAttr[name="bias"](%65)
  %754 : Tensor = prim::GetAttr[name="weight"](%65)
  %755 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.7
  %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %755, %754, %753, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
  %757 : __torch__.torch.nn.modules.linear.___torch_mangle_884.Linear = prim::GetAttr[name="lin2"](%63)
  %758 : __torch__.torch.nn.modules.linear.___torch_mangle_883.Linear = prim::GetAttr[name="lin1"](%63)
  %759 : Tensor = prim::GetAttr[name="bias"](%758)
  %760 : Tensor = prim::GetAttr[name="weight"](%758)
  %761 : Float(2048:1, 8192:2048) = aten::t(%760), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.8, %761), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %759, %26), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1678:0
  %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.80), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:1369:0
  %765 : Tensor = prim::GetAttr[name="bias"](%757)
  %766 : Tensor = prim::GetAttr[name="weight"](%757)
  %767 : Float(8192:1, 2048:8192) = aten::t(%766), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %767), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %765, %26), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1678:0
  %770 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.82, %17, %23), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
  %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.8, %770, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %772 : Tensor = prim::GetAttr[name="bias"](%61)
  %773 : Tensor = prim::GetAttr[name="weight"](%61)
  %774 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.7
  %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %774, %773, %772, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
  %776 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %777 : Float(17:13, 13:1, 1:1) = aten::to(%776, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.9, %777), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %779 : __torch__.torch.nn.modules.linear.___torch_mangle_831.Linear = prim::GetAttr[name="out_lin"](%59)
  %780 : __torch__.torch.nn.modules.linear.___torch_mangle_830.Linear = prim::GetAttr[name="v_lin"](%59)
  %781 : __torch__.torch.nn.modules.linear.___torch_mangle_829.Linear = prim::GetAttr[name="k_lin"](%59)
  %782 : __torch__.torch.nn.modules.linear.___torch_mangle_828.Linear = prim::GetAttr[name="q_lin"](%59)
  %783 : int = aten::size(%input.84, %27), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %784 : int = aten::size(%input.84, %26), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %785 : Tensor = prim::GetAttr[name="bias"](%782)
  %786 : Tensor = prim::GetAttr[name="weight"](%782)
  %787 : Float(2048:1, 2048:2048) = aten::t(%786), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %787), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %785, %26), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1678:0
  %790 : int[] = prim::ListConstruct(%783, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.8
  %791 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.33, %790), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%791, %26, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %793 : Tensor = prim::GetAttr[name="bias"](%781)
  %794 : Tensor = prim::GetAttr[name="weight"](%781)
  %795 : Float(2048:1, 2048:2048) = aten::t(%794), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %795), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %793, %26), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1678:0
  %798 : int[] = prim::ListConstruct(%783, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.8
  %799 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.34, %798), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%799, %26, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %801 : Tensor = prim::GetAttr[name="bias"](%780)
  %802 : Tensor = prim::GetAttr[name="weight"](%780)
  %803 : Float(2048:1, 2048:2048) = aten::t(%802), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %803), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %801, %26), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1678:0
  %806 : int[] = prim::ListConstruct(%783, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.8
  %807 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.35, %806), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%807, %26, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %12), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:188:0
  %810 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %22, %13), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %810), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %812 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.8 # torch/tensor.py:22:0
  %813 : int[] = prim::ListConstruct(%783, %26, %26, %784), scope: __module.transformer/__module.transformer.attentions.8
  %814 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%812, %813), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%814, %scores.17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %14), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:191:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
  %818 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %21, %15), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:1498:0
  %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%818, %17, %23), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
  %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:200:0
  %821 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %26, %22), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %822 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%821, %27), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %823 : int[] = prim::ListConstruct(%783, %21, %18), scope: __module.transformer/__module.transformer.attentions.8
  %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%822, %823), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %825 : Tensor = prim::GetAttr[name="bias"](%779)
  %826 : Tensor = prim::GetAttr[name="weight"](%779)
  %827 : Float(2048:1, 2048:2048) = aten::t(%826), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %827), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %825, %26), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1678:0
  %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.88, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %832 : Tensor = prim::GetAttr[name="bias"](%57)
  %833 : Tensor = prim::GetAttr[name="weight"](%57)
  %834 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.8
  %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %834, %833, %832, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
  %836 : __torch__.torch.nn.modules.linear.___torch_mangle_887.Linear = prim::GetAttr[name="lin2"](%55)
  %837 : __torch__.torch.nn.modules.linear.___torch_mangle_886.Linear = prim::GetAttr[name="lin1"](%55)
  %838 : Tensor = prim::GetAttr[name="bias"](%837)
  %839 : Tensor = prim::GetAttr[name="weight"](%837)
  %840 : Float(2048:1, 8192:2048) = aten::t(%839), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.9, %840), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %838, %26), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1678:0
  %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.90), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:1369:0
  %844 : Tensor = prim::GetAttr[name="bias"](%836)
  %845 : Tensor = prim::GetAttr[name="weight"](%836)
  %846 : Float(8192:1, 2048:8192) = aten::t(%845), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %846), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %844, %26), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1678:0
  %849 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.92, %17, %23), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
  %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.9, %849, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %851 : Tensor = prim::GetAttr[name="bias"](%53)
  %852 : Tensor = prim::GetAttr[name="weight"](%53)
  %853 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.8
  %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %853, %852, %851, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
  %855 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %856 : Float(17:13, 13:1, 1:1) = aten::to(%855, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.10, %856), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %858 : __torch__.torch.nn.modules.linear.___torch_mangle_836.Linear = prim::GetAttr[name="out_lin"](%51)
  %859 : __torch__.torch.nn.modules.linear.___torch_mangle_835.Linear = prim::GetAttr[name="v_lin"](%51)
  %860 : __torch__.torch.nn.modules.linear.___torch_mangle_834.Linear = prim::GetAttr[name="k_lin"](%51)
  %861 : __torch__.torch.nn.modules.linear.___torch_mangle_833.Linear = prim::GetAttr[name="q_lin"](%51)
  %862 : int = aten::size(%input.94, %27), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %863 : int = aten::size(%input.94, %26), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %864 : Tensor = prim::GetAttr[name="bias"](%861)
  %865 : Tensor = prim::GetAttr[name="weight"](%861)
  %866 : Float(2048:1, 2048:2048) = aten::t(%865), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %866), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %864, %26), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1678:0
  %869 : int[] = prim::ListConstruct(%862, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.9
  %870 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.37, %869), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%870, %26, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %872 : Tensor = prim::GetAttr[name="bias"](%860)
  %873 : Tensor = prim::GetAttr[name="weight"](%860)
  %874 : Float(2048:1, 2048:2048) = aten::t(%873), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %874), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %872, %26), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1678:0
  %877 : int[] = prim::ListConstruct(%862, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.9
  %878 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.38, %877), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%878, %26, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %880 : Tensor = prim::GetAttr[name="bias"](%859)
  %881 : Tensor = prim::GetAttr[name="weight"](%859)
  %882 : Float(2048:1, 2048:2048) = aten::t(%881), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %882), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %880, %26), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1678:0
  %885 : int[] = prim::ListConstruct(%862, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.9
  %886 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.39, %885), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%886, %26, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %12), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:188:0
  %889 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %22, %13), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %889), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %891 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.9 # torch/tensor.py:22:0
  %892 : int[] = prim::ListConstruct(%862, %26, %26, %863), scope: __module.transformer/__module.transformer.attentions.9
  %893 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%891, %892), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%893, %scores.19), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %14), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:191:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
  %897 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %21, %15), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:1498:0
  %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%897, %17, %23), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
  %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:200:0
  %900 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %26, %22), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %901 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%900, %27), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %902 : int[] = prim::ListConstruct(%862, %21, %18), scope: __module.transformer/__module.transformer.attentions.9
  %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%901, %902), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %904 : Tensor = prim::GetAttr[name="bias"](%858)
  %905 : Tensor = prim::GetAttr[name="weight"](%858)
  %906 : Float(2048:1, 2048:2048) = aten::t(%905), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %906), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %904, %26), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1678:0
  %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.98, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %911 : Tensor = prim::GetAttr[name="bias"](%49)
  %912 : Tensor = prim::GetAttr[name="weight"](%49)
  %913 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.9
  %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %913, %912, %911, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
  %915 : __torch__.torch.nn.modules.linear.___torch_mangle_890.Linear = prim::GetAttr[name="lin2"](%47)
  %916 : __torch__.torch.nn.modules.linear.___torch_mangle_889.Linear = prim::GetAttr[name="lin1"](%47)
  %917 : Tensor = prim::GetAttr[name="bias"](%916)
  %918 : Tensor = prim::GetAttr[name="weight"](%916)
  %919 : Float(2048:1, 8192:2048) = aten::t(%918), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.10, %919), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %917, %26), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1678:0
  %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.100), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:1369:0
  %923 : Tensor = prim::GetAttr[name="bias"](%915)
  %924 : Tensor = prim::GetAttr[name="weight"](%915)
  %925 : Float(8192:1, 2048:8192) = aten::t(%924), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %925), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %923, %26), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1678:0
  %928 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.102, %17, %23), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
  %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.10, %928, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %930 : Tensor = prim::GetAttr[name="bias"](%45)
  %931 : Tensor = prim::GetAttr[name="weight"](%45)
  %932 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.9
  %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %932, %931, %930, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
  %934 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %935 : Float(17:13, 13:1, 1:1) = aten::to(%934, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.11, %935), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %937 : __torch__.torch.nn.modules.linear.___torch_mangle_841.Linear = prim::GetAttr[name="out_lin"](%43)
  %938 : __torch__.torch.nn.modules.linear.___torch_mangle_840.Linear = prim::GetAttr[name="v_lin"](%43)
  %939 : __torch__.torch.nn.modules.linear.___torch_mangle_839.Linear = prim::GetAttr[name="k_lin"](%43)
  %940 : __torch__.torch.nn.modules.linear.___torch_mangle_838.Linear = prim::GetAttr[name="q_lin"](%43)
  %941 : int = aten::size(%input.104, %27), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %942 : int = aten::size(%input.104, %26), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %943 : Tensor = prim::GetAttr[name="bias"](%940)
  %944 : Tensor = prim::GetAttr[name="weight"](%940)
  %945 : Float(2048:1, 2048:2048) = aten::t(%944), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %945), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %943, %26), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1678:0
  %948 : int[] = prim::ListConstruct(%941, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.10
  %949 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.41, %948), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%949, %26, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %951 : Tensor = prim::GetAttr[name="bias"](%939)
  %952 : Tensor = prim::GetAttr[name="weight"](%939)
  %953 : Float(2048:1, 2048:2048) = aten::t(%952), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %953), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %951, %26), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1678:0
  %956 : int[] = prim::ListConstruct(%941, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.10
  %957 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.42, %956), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%957, %26, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %959 : Tensor = prim::GetAttr[name="bias"](%938)
  %960 : Tensor = prim::GetAttr[name="weight"](%938)
  %961 : Float(2048:1, 2048:2048) = aten::t(%960), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %961), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %959, %26), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1678:0
  %964 : int[] = prim::ListConstruct(%941, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.10
  %965 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.43, %964), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%965, %26, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %12), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:188:0
  %968 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %22, %13), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %968), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %970 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.10 # torch/tensor.py:22:0
  %971 : int[] = prim::ListConstruct(%941, %26, %26, %942), scope: __module.transformer/__module.transformer.attentions.10
  %972 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%970, %971), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%972, %scores.21), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %14), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:191:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
  %976 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %21, %15), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:1498:0
  %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%976, %17, %23), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
  %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:200:0
  %979 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %26, %22), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %980 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%979, %27), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %981 : int[] = prim::ListConstruct(%941, %21, %18), scope: __module.transformer/__module.transformer.attentions.10
  %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%980, %981), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %983 : Tensor = prim::GetAttr[name="bias"](%937)
  %984 : Tensor = prim::GetAttr[name="weight"](%937)
  %985 : Float(2048:1, 2048:2048) = aten::t(%984), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %985), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %983, %26), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1678:0
  %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.108, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %990 : Tensor = prim::GetAttr[name="bias"](%41)
  %991 : Tensor = prim::GetAttr[name="weight"](%41)
  %992 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.10
  %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %992, %991, %990, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
  %994 : __torch__.torch.nn.modules.linear.___torch_mangle_893.Linear = prim::GetAttr[name="lin2"](%39)
  %995 : __torch__.torch.nn.modules.linear.___torch_mangle_892.Linear = prim::GetAttr[name="lin1"](%39)
  %996 : Tensor = prim::GetAttr[name="bias"](%995)
  %997 : Tensor = prim::GetAttr[name="weight"](%995)
  %998 : Float(2048:1, 8192:2048) = aten::t(%997), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.11, %998), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %996, %26), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1678:0
  %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.110), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:1369:0
  %1002 : Tensor = prim::GetAttr[name="bias"](%994)
  %1003 : Tensor = prim::GetAttr[name="weight"](%994)
  %1004 : Float(8192:1, 2048:8192) = aten::t(%1003), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %1004), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %1002, %26), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1678:0
  %1007 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.112, %17, %23), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
  %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.11, %1007, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %1009 : Tensor = prim::GetAttr[name="bias"](%37)
  %1010 : Tensor = prim::GetAttr[name="weight"](%37)
  %1011 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.10
  %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %1011, %1010, %1009, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1013 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1014 : Float(17:13, 13:1, 1:1) = aten::to(%1013, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.12, %1014), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1016 : __torch__.torch.nn.modules.linear.___torch_mangle_846.Linear = prim::GetAttr[name="out_lin"](%35)
  %1017 : __torch__.torch.nn.modules.linear.___torch_mangle_845.Linear = prim::GetAttr[name="v_lin"](%35)
  %1018 : __torch__.torch.nn.modules.linear.___torch_mangle_844.Linear = prim::GetAttr[name="k_lin"](%35)
  %1019 : __torch__.torch.nn.modules.linear.___torch_mangle_843.Linear = prim::GetAttr[name="q_lin"](%35)
  %1020 : int = aten::size(%input.114, %27), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1021 : int = aten::size(%input.114, %26), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1022 : Tensor = prim::GetAttr[name="bias"](%1019)
  %1023 : Tensor = prim::GetAttr[name="weight"](%1019)
  %1024 : Float(2048:1, 2048:2048) = aten::t(%1023), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1024), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %1022, %26), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1678:0
  %1027 : int[] = prim::ListConstruct(%1020, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.11
  %1028 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.45, %1027), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1028, %26, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1030 : Tensor = prim::GetAttr[name="bias"](%1018)
  %1031 : Tensor = prim::GetAttr[name="weight"](%1018)
  %1032 : Float(2048:1, 2048:2048) = aten::t(%1031), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1032), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %1030, %26), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1678:0
  %1035 : int[] = prim::ListConstruct(%1020, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.11
  %1036 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.46, %1035), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1036, %26, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1038 : Tensor = prim::GetAttr[name="bias"](%1017)
  %1039 : Tensor = prim::GetAttr[name="weight"](%1017)
  %1040 : Float(2048:1, 2048:2048) = aten::t(%1039), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1040), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %1038, %26), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1678:0
  %1043 : int[] = prim::ListConstruct(%1020, %21, %10, %11), scope: __module.transformer/__module.transformer.attentions.11
  %1044 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.47, %1043), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1044, %26, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %12), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:188:0
  %1047 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %22, %13), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %1047), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %1049 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %27), scope: __module.transformer/__module.transformer.attentions.11 # torch/tensor.py:22:0
  %1050 : int[] = prim::ListConstruct(%1020, %26, %26, %1021), scope: __module.transformer/__module.transformer.attentions.11
  %1051 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1049, %1050), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1051, %scores.23), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %14), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:191:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %16, %23, %23, %15), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
  %1055 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %21, %15), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:1498:0
  %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1055, %17, %23), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
  %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:200:0
  %1058 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %26, %22), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1059 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1058, %27), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1060 : int[] = prim::ListConstruct(%1020, %21, %18), scope: __module.transformer/__module.transformer.attentions.11
  %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1059, %1060), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1062 : Tensor = prim::GetAttr[name="bias"](%1016)
  %1063 : Tensor = prim::GetAttr[name="weight"](%1016)
  %1064 : Float(2048:1, 2048:2048) = aten::t(%1063), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %1064), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %1062, %26), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1678:0
  %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.118, %17, %23), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %26), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %1069 : Tensor = prim::GetAttr[name="bias"](%33)
  %1070 : Tensor = prim::GetAttr[name="weight"](%33)
  %1071 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm1.11
  %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %1071, %1070, %1069, %19, %20), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1073 : __torch__.torch.nn.modules.linear.___torch_mangle_896.Linear = prim::GetAttr[name="lin2"](%31)
  %1074 : __torch__.torch.nn.modules.linear.___torch_mangle_895.Linear = prim::GetAttr[name="lin1"](%31)
  %1075 : Tensor = prim::GetAttr[name="bias"](%1074)
  %1076 : Tensor = prim::GetAttr[name="weight"](%1074)
  %1077 : Float(2048:1, 8192:2048) = aten::t(%1076), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor, %1077), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %1075, %26), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1678:0
  %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.120), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:1369:0
  %1081 : Tensor = prim::GetAttr[name="bias"](%1073)
  %1082 : Tensor = prim::GetAttr[name="weight"](%1073)
  %1083 : Float(8192:1, 2048:8192) = aten::t(%1082), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %output.72 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %1083), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.72, %1081, %26), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1678:0
  %1086 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.122, %17, %23), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
  %input.123 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor, %1086, %26), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %1088 : Tensor = prim::GetAttr[name="bias"](%29)
  %1089 : Tensor = prim::GetAttr[name="weight"](%29)
  %1090 : int[] = prim::ListConstruct(%18), scope: __module.transformer/__module.transformer.layer_norm2.11
  %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.123, %1090, %1089, %1088, %19, %20), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1092 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %21), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1093 : Float(17:13, 13:1, 1:1) = aten::to(%1092, %16, %23, %23, %15), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.124 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor, %1093), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1095 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1096 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.124, %1096, %1095), scope: __module.dropout # torch/nn/functional.py:973:0
  %1098 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
  %1099 : Tensor = prim::GetAttr[name="bias"](%3)
  %1100 : Tensor = prim::GetAttr[name="weight"](%3)
  %1101 : Float(2048:1, 2:2048) = aten::t(%1100), scope: __module.classifier # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1101), scope: __module.classifier # torch/nn/functional.py:1676:0
  %1103 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1099, %1098), scope: __module.classifier # torch/nn/functional.py:1678:0
  %9 : (Float(17:26, 13:2, 2:1)) = prim::TupleConstruct(%1103)
  return (%9)
