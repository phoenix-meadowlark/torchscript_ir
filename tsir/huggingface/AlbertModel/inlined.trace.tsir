graph(%self.1 : __torch__.transformers.modeling_albert.___torch_mangle_366.AlbertModel,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.activation.___torch_mangle_365.Tanh = prim::GetAttr[name="pooler_activation"](%self.1)
  %4 : __torch__.torch.nn.modules.linear.___torch_mangle_364.Linear = prim::GetAttr[name="pooler"](%self.1)
  %5 : __torch__.transformers.modeling_albert.___torch_mangle_363.AlbertTransformer = prim::GetAttr[name="encoder"](%self.1)
  %6 : __torch__.transformers.modeling_albert.___torch_mangle_345.AlbertEmbeddings = prim::GetAttr[name="embeddings"](%self.1)
  %7 : int = prim::Constant[value=0]() # transformers/modeling_albert.py:663:0
  %8 : int = aten::size(%input_ids, %7) # transformers/modeling_albert.py:663:0
  %9 : Long() = prim::NumToTensor(%8)
  %10 : int = aten::Int(%9)
  %11 : int = prim::Constant[value=1]() # transformers/modeling_albert.py:663:0
  %12 : int = aten::size(%input_ids, %11) # transformers/modeling_albert.py:663:0
  %13 : Long() = prim::NumToTensor(%12)
  %14 : int = aten::Int(%13)
  %15 : int[] = prim::ListConstruct(%10, %14)
  %16 : int = prim::Constant[value=4]() # transformers/modeling_albert.py:674:0
  %17 : int = prim::Constant[value=0]() # transformers/modeling_albert.py:674:0
  %18 : Device = prim::Constant[value="cpu"]() # transformers/modeling_albert.py:674:0
  %19 : bool = prim::Constant[value=0]() # transformers/modeling_albert.py:674:0
  %input.2 : Long(17:13, 13:1) = aten::zeros(%15, %16, %17, %18, %19) # transformers/modeling_albert.py:674:0
  %21 : int = prim::Constant[value=1]() # transformers/modeling_albert.py:676:0
  %22 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%attention_mask.1, %21) # transformers/modeling_albert.py:676:0
  %23 : int = prim::Constant[value=2]() # transformers/modeling_albert.py:676:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%22, %23) # transformers/modeling_albert.py:676:0
  %25 : int = prim::Constant[value=6]() # transformers/modeling_albert.py:677:0
  %26 : bool = prim::Constant[value=0]() # transformers/modeling_albert.py:677:0
  %27 : bool = prim::Constant[value=0]() # transformers/modeling_albert.py:677:0
  %28 : None = prim::Constant()
  %29 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %25, %26, %27, %28) # transformers/modeling_albert.py:677:0
  %30 : float = prim::Constant[value=1.]() # torch/tensor.py:396:0
  %31 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
  %32 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%29, %30, %31) # torch/tensor.py:396:0
  %33 : Double() = prim::Constant[value={-10000}]() # transformers/modeling_albert.py:678:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%32, %33) # transformers/modeling_albert.py:678:0
  %48 : float = prim::Constant[value=0.](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %49 : int = prim::Constant[value=128](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %50 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %51 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %52 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %53 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %54 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # transformers/modeling_albert.py:229:0
  %55 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_albert.py:229:0
  %56 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_albert.py:222:0
  %57 : __torch__.torch.nn.modules.normalization.___torch_mangle_343.LayerNorm = prim::GetAttr[name="LayerNorm"](%6)
  %58 : __torch__.torch.nn.modules.sparse.___torch_mangle_342.Embedding = prim::GetAttr[name="token_type_embeddings"](%6)
  %59 : __torch__.torch.nn.modules.sparse.___torch_mangle_341.Embedding = prim::GetAttr[name="position_embeddings"](%6)
  %60 : __torch__.torch.nn.modules.sparse.___torch_mangle_340.Embedding = prim::GetAttr[name="word_embeddings"](%6)
  %61 : Tensor = prim::GetAttr[name="position_ids"](%6)
  %62 : int = aten::size(%input_ids, %56), scope: __module.embeddings # transformers/modeling_albert.py:222:0
  %63 : Long(1:512, 512:1) = aten::slice(%61, %55, %55, %54, %56), scope: __module.embeddings # transformers/modeling_albert.py:229:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%63, %56, %55, %62, %56), scope: __module.embeddings # transformers/modeling_albert.py:229:0
  %65 : Tensor = prim::GetAttr[name="weight"](%60)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%65, %input_ids, %55, %53, %53), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %67 : Tensor = prim::GetAttr[name="weight"](%59)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%67, %input.1, %52, %53, %53), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %69 : Tensor = prim::GetAttr[name="weight"](%58)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%69, %input.2, %52, %53, %53), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %71 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %56), scope: __module.embeddings # transformers/modeling_albert.py:239:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%71, %token_type_embeddings, %56), scope: __module.embeddings # transformers/modeling_albert.py:239:0
  %73 : Tensor = prim::GetAttr[name="bias"](%57)
  %74 : Tensor = prim::GetAttr[name="weight"](%57)
  %75 : int[] = prim::ListConstruct(%49), scope: __module.embeddings/__module.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %75, %74, %73, %50, %51), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %48, %53), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %78 : Double() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %79 : Double() = prim::Constant[value={0.797885}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %80 : Double() = prim::Constant[value={0.044715}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %81 : float = prim::Constant[value=3.](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %82 : Double() = prim::Constant[value={0.5}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %83 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %84 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %85 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %86 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %87 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %88 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %89 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %90 : None = prim::Constant(), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %91 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %92 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %93 : int = prim::Constant[value=4096](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %94 : int = prim::Constant[value=6](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %95 : str = prim::Constant[value="bfnd,ndh->bfh"](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %96 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %97 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %98 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1678:0
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_362.ModuleList = prim::GetAttr[name="albert_layer_groups"](%5)
  %100 : __torch__.transformers.modeling_albert.___torch_mangle_361.AlbertLayerGroup = prim::GetAttr[name="0"](%99)
  %101 : __torch__.torch.nn.modules.linear.___torch_mangle_346.Linear = prim::GetAttr[name="embedding_hidden_mapping_in"](%5)
  %102 : Tensor = prim::GetAttr[name="bias"](%101)
  %103 : Tensor = prim::GetAttr[name="weight"](%101)
  %104 : Float(128:1, 4096:128) = aten::t(%103), scope: __module.encoder/__module.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %output.1 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.5, %104), scope: __module.encoder/__module.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %input.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.1, %102, %98), scope: __module.encoder/__module.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1678:0
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %108 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%107)
  %109 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%108)
  %110 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%108)
  %111 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%108)
  %112 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%108)
  %113 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%112)
  %114 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%112)
  %115 : Tensor = prim::GetAttr[name="bias"](%114)
  %116 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%112)
  %117 : Tensor = prim::GetAttr[name="weight"](%116)
  %118 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%112)
  %119 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%112)
  %120 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%112)
  %121 : Tensor = prim::GetAttr[name="bias"](%120)
  %122 : Tensor = prim::GetAttr[name="weight"](%120)
  %123 : Float(4096:1, 4096:4096) = aten::t(%122), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %123), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.2, %121, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %126 : Tensor = prim::GetAttr[name="bias"](%119)
  %127 : Tensor = prim::GetAttr[name="weight"](%119)
  %128 : Float(4096:1, 4096:4096) = aten::t(%127), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %128), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.3, %126, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %131 : Tensor = prim::GetAttr[name="bias"](%118)
  %132 : Tensor = prim::GetAttr[name="weight"](%118)
  %133 : Float(4096:1, 4096:4096) = aten::t(%132), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %133), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.4, %131, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %136 : int = aten::size(%x.1, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %137 : int = aten::size(%x.1, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %138 : int[] = prim::ListConstruct(%136, %137, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.2 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.1, %138), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %140 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.2, %140), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %142 : int = aten::size(%x.3, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %143 : int = aten::size(%x.3, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %144 : int[] = prim::ListConstruct(%142, %143, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.4 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.3, %144), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %146 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.4, %146), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %148 : int = aten::size(%x.5, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %149 : int = aten::size(%x.5, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %150 : int[] = prim::ListConstruct(%148, %149, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.6 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.5, %150), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %152 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.6, %152), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %154 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.1, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %154), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.1, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.7, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.8, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %161 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %162 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.1, %161), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %163 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%162, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %164 : Float(4096:1, 4096:4096) = aten::t(%117), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %165 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %166 : Float(64:64, 64:1, 4096:4096) = aten::view(%164, %165), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %167 : Float(64:64, 64:1, 4096:4096) = aten::to(%166, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.1 : Float(4096:1) = aten::to(%115, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %169 : Tensor[] = prim::ListConstruct(%163, %167), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %170 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %169), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.9 : Float(17:53248, 13:4096, 4096:1) = aten::add(%170, %b.1, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.1 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.9, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:53248, 13:4096, 4096:1) = aten::add(%input.6, %projected_context_layer.1, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %174 : Tensor = prim::GetAttr[name="bias"](%113)
  %175 : Tensor = prim::GetAttr[name="weight"](%113)
  %176 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.1 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.10, %176, %175, %174, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %178 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.1, %b.1)
  %179 : Float(17:53248, 13:4096, 4096:1), %180 : Float(4096:1) = prim::TupleUnpack(%178)
  %181 : Tensor = prim::GetAttr[name="bias"](%111)
  %182 : Tensor = prim::GetAttr[name="weight"](%111)
  %183 : Float(4096:1, 16384:4096) = aten::t(%182), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.5 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%179, %183), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.7 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.5, %181, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %186 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.7, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %187 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.7, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %188 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%187, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %189 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.7, %188, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %190 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%189, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %191 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%190), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %192 : Float(17:212992, 13:16384, 16384:1) = aten::add(%191, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.11 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%186, %192), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %194 : Tensor = prim::GetAttr[name="bias"](%110)
  %195 : Tensor = prim::GetAttr[name="weight"](%110)
  %196 : Float(16384:1, 4096:16384) = aten::t(%195), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.6 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.11, %196), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.6, %194, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.12 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.1, %179, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %200 : Tensor = prim::GetAttr[name="bias"](%109)
  %201 : Tensor = prim::GetAttr[name="weight"](%109)
  %202 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.13 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.12, %202, %201, %200, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %204 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.13, %180)
  %205 : Float(17:53248, 13:4096, 4096:1), %206 : Float(4096:1) = prim::TupleUnpack(%204)
  %207 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%205, %206)
  %208 : Float(17:53248, 13:4096, 4096:1), %209 : Float(4096:1) = prim::TupleUnpack(%207)
  %210 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %211 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%210)
  %212 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%211)
  %213 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%211)
  %214 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%211)
  %215 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%211)
  %216 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%215)
  %217 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%215)
  %218 : Tensor = prim::GetAttr[name="weight"](%217)
  %219 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%215)
  %220 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%215)
  %221 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%215)
  %222 : Tensor = prim::GetAttr[name="bias"](%221)
  %223 : Tensor = prim::GetAttr[name="weight"](%221)
  %224 : Float(4096:1, 4096:4096) = aten::t(%223), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%208, %224), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.7, %222, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %227 : Tensor = prim::GetAttr[name="bias"](%220)
  %228 : Tensor = prim::GetAttr[name="weight"](%220)
  %229 : Float(4096:1, 4096:4096) = aten::t(%228), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%208, %229), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.8, %227, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %232 : Tensor = prim::GetAttr[name="bias"](%219)
  %233 : Tensor = prim::GetAttr[name="weight"](%219)
  %234 : Float(4096:1, 4096:4096) = aten::t(%233), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%208, %234), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.12 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.9, %232, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %237 : int = aten::size(%x.8, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %238 : int = aten::size(%x.8, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %239 : int[] = prim::ListConstruct(%237, %238, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.9 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.8, %239), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %241 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.9, %241), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %243 : int = aten::size(%x.10, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %244 : int = aten::size(%x.10, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %245 : int[] = prim::ListConstruct(%243, %244, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.11 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.10, %245), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %247 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.11, %247), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %249 : int = aten::size(%x.12, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %250 : int = aten::size(%x.12, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %251 : int[] = prim::ListConstruct(%249, %250, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.13 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.12, %251), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %253 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.13, %253), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %255 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.2, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %255), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.3, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.14, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.15, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.2 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %262 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %263 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.2, %262), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %264 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%263, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %265 : Float(4096:1, 4096:4096) = aten::t(%218), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %266 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %267 : Float(64:64, 64:1, 4096:4096) = aten::view(%265, %266), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %268 : Float(64:64, 64:1, 4096:4096) = aten::to(%267, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.2 : Float(4096:1) = aten::to(%209, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %270 : Tensor[] = prim::ListConstruct(%264, %268), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %271 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %270), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.16 : Float(17:53248, 13:4096, 4096:1) = aten::add(%271, %b.2, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.2 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.16, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.17 : Float(17:53248, 13:4096, 4096:1) = aten::add(%208, %projected_context_layer.2, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %275 : Tensor = prim::GetAttr[name="bias"](%216)
  %276 : Tensor = prim::GetAttr[name="weight"](%216)
  %277 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.2 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.17, %277, %276, %275, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %279 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.2, %b.2)
  %280 : Float(17:53248, 13:4096, 4096:1), %281 : Float(4096:1) = prim::TupleUnpack(%279)
  %282 : Tensor = prim::GetAttr[name="bias"](%214)
  %283 : Tensor = prim::GetAttr[name="weight"](%214)
  %284 : Float(4096:1, 16384:4096) = aten::t(%283), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.10 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%280, %284), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.14 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.10, %282, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %287 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.14, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %288 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.14, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %289 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%288, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %290 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.14, %289, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %291 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%290, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %292 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%291), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %293 : Float(17:212992, 13:16384, 16384:1) = aten::add(%292, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.18 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%287, %293), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %295 : Tensor = prim::GetAttr[name="bias"](%213)
  %296 : Tensor = prim::GetAttr[name="weight"](%213)
  %297 : Float(16384:1, 4096:16384) = aten::t(%296), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.11 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.18, %297), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.2 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.11, %295, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.19 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.2, %280, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %301 : Tensor = prim::GetAttr[name="bias"](%212)
  %302 : Tensor = prim::GetAttr[name="weight"](%212)
  %303 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.20 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.19, %303, %302, %301, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %305 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.20, %281)
  %306 : Float(17:53248, 13:4096, 4096:1), %307 : Float(4096:1) = prim::TupleUnpack(%305)
  %308 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%306, %307)
  %309 : Float(17:53248, 13:4096, 4096:1), %310 : Float(4096:1) = prim::TupleUnpack(%308)
  %311 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %312 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%311)
  %313 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%312)
  %314 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%312)
  %315 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%312)
  %316 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%312)
  %317 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%316)
  %318 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%316)
  %319 : Tensor = prim::GetAttr[name="weight"](%318)
  %320 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%316)
  %321 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%316)
  %322 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%316)
  %323 : Tensor = prim::GetAttr[name="bias"](%322)
  %324 : Tensor = prim::GetAttr[name="weight"](%322)
  %325 : Float(4096:1, 4096:4096) = aten::t(%324), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.12 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%309, %325), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.15 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.12, %323, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %328 : Tensor = prim::GetAttr[name="bias"](%321)
  %329 : Tensor = prim::GetAttr[name="weight"](%321)
  %330 : Float(4096:1, 4096:4096) = aten::t(%329), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.13 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%309, %330), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.17 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.13, %328, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %333 : Tensor = prim::GetAttr[name="bias"](%320)
  %334 : Tensor = prim::GetAttr[name="weight"](%320)
  %335 : Float(4096:1, 4096:4096) = aten::t(%334), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.14 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%309, %335), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.19 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.14, %333, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %338 : int = aten::size(%x.15, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %339 : int = aten::size(%x.15, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %340 : int[] = prim::ListConstruct(%338, %339, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.16 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.15, %340), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %342 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.16, %342), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %344 : int = aten::size(%x.17, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %345 : int = aten::size(%x.17, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %346 : int[] = prim::ListConstruct(%344, %345, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.18 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.17, %346), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %348 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.18, %348), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %350 : int = aten::size(%x.19, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %351 : int = aten::size(%x.19, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %352 : int[] = prim::ListConstruct(%350, %351, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.20 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.19, %352), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %354 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.20, %354), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %356 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.3, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %356), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.5, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.21, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.22, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %363 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %364 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.3, %363), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %365 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%364, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %366 : Float(4096:1, 4096:4096) = aten::t(%319), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %367 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %368 : Float(64:64, 64:1, 4096:4096) = aten::view(%366, %367), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %369 : Float(64:64, 64:1, 4096:4096) = aten::to(%368, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.3 : Float(4096:1) = aten::to(%310, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %371 : Tensor[] = prim::ListConstruct(%365, %369), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %372 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %371), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.23 : Float(17:53248, 13:4096, 4096:1) = aten::add(%372, %b.3, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.3 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.23, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:53248, 13:4096, 4096:1) = aten::add(%309, %projected_context_layer.3, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %376 : Tensor = prim::GetAttr[name="bias"](%317)
  %377 : Tensor = prim::GetAttr[name="weight"](%317)
  %378 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.3 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.24, %378, %377, %376, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %380 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.3, %b.3)
  %381 : Float(17:53248, 13:4096, 4096:1), %382 : Float(4096:1) = prim::TupleUnpack(%380)
  %383 : Tensor = prim::GetAttr[name="bias"](%315)
  %384 : Tensor = prim::GetAttr[name="weight"](%315)
  %385 : Float(4096:1, 16384:4096) = aten::t(%384), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.15 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%381, %385), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.21 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.15, %383, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %388 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.21, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %389 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.21, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %390 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%389, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %391 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.21, %390, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %392 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%391, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %393 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%392), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %394 : Float(17:212992, 13:16384, 16384:1) = aten::add(%393, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.25 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%388, %394), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %396 : Tensor = prim::GetAttr[name="bias"](%314)
  %397 : Tensor = prim::GetAttr[name="weight"](%314)
  %398 : Float(16384:1, 4096:16384) = aten::t(%397), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.16 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.25, %398), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.16, %396, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.26 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.3, %381, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %402 : Tensor = prim::GetAttr[name="bias"](%313)
  %403 : Tensor = prim::GetAttr[name="weight"](%313)
  %404 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.27 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.26, %404, %403, %402, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %406 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.27, %382)
  %407 : Float(17:53248, 13:4096, 4096:1), %408 : Float(4096:1) = prim::TupleUnpack(%406)
  %409 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%407, %408)
  %410 : Float(17:53248, 13:4096, 4096:1), %411 : Float(4096:1) = prim::TupleUnpack(%409)
  %412 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %413 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%412)
  %414 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%413)
  %415 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%413)
  %416 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%413)
  %417 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%413)
  %418 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%417)
  %419 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%417)
  %420 : Tensor = prim::GetAttr[name="weight"](%419)
  %421 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%417)
  %422 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%417)
  %423 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%417)
  %424 : Tensor = prim::GetAttr[name="bias"](%423)
  %425 : Tensor = prim::GetAttr[name="weight"](%423)
  %426 : Float(4096:1, 4096:4096) = aten::t(%425), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.17 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%410, %426), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.22 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.17, %424, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %429 : Tensor = prim::GetAttr[name="bias"](%422)
  %430 : Tensor = prim::GetAttr[name="weight"](%422)
  %431 : Float(4096:1, 4096:4096) = aten::t(%430), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.18 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%410, %431), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.24 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.18, %429, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %434 : Tensor = prim::GetAttr[name="bias"](%421)
  %435 : Tensor = prim::GetAttr[name="weight"](%421)
  %436 : Float(4096:1, 4096:4096) = aten::t(%435), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.19 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%410, %436), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.26 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.19, %434, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %439 : int = aten::size(%x.22, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %440 : int = aten::size(%x.22, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %441 : int[] = prim::ListConstruct(%439, %440, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.23 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.22, %441), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %443 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.23, %443), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %445 : int = aten::size(%x.24, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %446 : int = aten::size(%x.24, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %447 : int[] = prim::ListConstruct(%445, %446, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.25 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.24, %447), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %449 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.25, %449), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %451 : int = aten::size(%x.26, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %452 : int = aten::size(%x.26, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %453 : int[] = prim::ListConstruct(%451, %452, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.27 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.26, %453), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %455 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.27, %455), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %457 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.4, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %457), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.7, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.28 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.29 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.28, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.29, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.4 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %464 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %465 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.4, %464), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %466 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%465, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %467 : Float(4096:1, 4096:4096) = aten::t(%420), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %468 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %469 : Float(64:64, 64:1, 4096:4096) = aten::view(%467, %468), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %470 : Float(64:64, 64:1, 4096:4096) = aten::to(%469, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.4 : Float(4096:1) = aten::to(%411, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %472 : Tensor[] = prim::ListConstruct(%466, %470), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %473 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %472), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.30 : Float(17:53248, 13:4096, 4096:1) = aten::add(%473, %b.4, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.4 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.30, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:53248, 13:4096, 4096:1) = aten::add(%410, %projected_context_layer.4, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %477 : Tensor = prim::GetAttr[name="bias"](%418)
  %478 : Tensor = prim::GetAttr[name="weight"](%418)
  %479 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.4 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.31, %479, %478, %477, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %481 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.4, %b.4)
  %482 : Float(17:53248, 13:4096, 4096:1), %483 : Float(4096:1) = prim::TupleUnpack(%481)
  %484 : Tensor = prim::GetAttr[name="bias"](%416)
  %485 : Tensor = prim::GetAttr[name="weight"](%416)
  %486 : Float(4096:1, 16384:4096) = aten::t(%485), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.20 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%482, %486), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.28 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.20, %484, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %489 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.28, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %490 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.28, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %491 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%490, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %492 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.28, %491, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %493 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%492, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %494 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%493), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %495 : Float(17:212992, 13:16384, 16384:1) = aten::add(%494, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.32 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%489, %495), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %497 : Tensor = prim::GetAttr[name="bias"](%415)
  %498 : Tensor = prim::GetAttr[name="weight"](%415)
  %499 : Float(16384:1, 4096:16384) = aten::t(%498), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.21 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.32, %499), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.4 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.21, %497, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.33 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.4, %482, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %503 : Tensor = prim::GetAttr[name="bias"](%414)
  %504 : Tensor = prim::GetAttr[name="weight"](%414)
  %505 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.34 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.33, %505, %504, %503, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %507 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.34, %483)
  %508 : Float(17:53248, 13:4096, 4096:1), %509 : Float(4096:1) = prim::TupleUnpack(%507)
  %510 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%508, %509)
  %511 : Float(17:53248, 13:4096, 4096:1), %512 : Float(4096:1) = prim::TupleUnpack(%510)
  %513 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %514 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%513)
  %515 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%514)
  %516 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%514)
  %517 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%514)
  %518 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%514)
  %519 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%518)
  %520 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%518)
  %521 : Tensor = prim::GetAttr[name="weight"](%520)
  %522 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%518)
  %523 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%518)
  %524 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%518)
  %525 : Tensor = prim::GetAttr[name="bias"](%524)
  %526 : Tensor = prim::GetAttr[name="weight"](%524)
  %527 : Float(4096:1, 4096:4096) = aten::t(%526), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.22 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%511, %527), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.29 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.22, %525, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %530 : Tensor = prim::GetAttr[name="bias"](%523)
  %531 : Tensor = prim::GetAttr[name="weight"](%523)
  %532 : Float(4096:1, 4096:4096) = aten::t(%531), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.23 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%511, %532), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.31 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.23, %530, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %535 : Tensor = prim::GetAttr[name="bias"](%522)
  %536 : Tensor = prim::GetAttr[name="weight"](%522)
  %537 : Float(4096:1, 4096:4096) = aten::t(%536), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.24 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%511, %537), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.33 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.24, %535, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %540 : int = aten::size(%x.29, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %541 : int = aten::size(%x.29, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %542 : int[] = prim::ListConstruct(%540, %541, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.30 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.29, %542), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %544 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.30, %544), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %546 : int = aten::size(%x.31, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %547 : int = aten::size(%x.31, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %548 : int[] = prim::ListConstruct(%546, %547, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.32 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.31, %548), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %550 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.32, %550), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %552 : int = aten::size(%x.33, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %553 : int = aten::size(%x.33, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %554 : int[] = prim::ListConstruct(%552, %553, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.34 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.33, %554), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %556 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.34, %556), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %558 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.5, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %558), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.9, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.35 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.36 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.35, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.36, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %565 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %566 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.5, %565), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %567 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%566, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %568 : Float(4096:1, 4096:4096) = aten::t(%521), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %569 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %570 : Float(64:64, 64:1, 4096:4096) = aten::view(%568, %569), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %571 : Float(64:64, 64:1, 4096:4096) = aten::to(%570, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.5 : Float(4096:1) = aten::to(%512, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %573 : Tensor[] = prim::ListConstruct(%567, %571), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %574 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %573), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.37 : Float(17:53248, 13:4096, 4096:1) = aten::add(%574, %b.5, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.5 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.37, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.38 : Float(17:53248, 13:4096, 4096:1) = aten::add(%511, %projected_context_layer.5, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %578 : Tensor = prim::GetAttr[name="bias"](%519)
  %579 : Tensor = prim::GetAttr[name="weight"](%519)
  %580 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.5 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.38, %580, %579, %578, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %582 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.5, %b.5)
  %583 : Float(17:53248, 13:4096, 4096:1), %584 : Float(4096:1) = prim::TupleUnpack(%582)
  %585 : Tensor = prim::GetAttr[name="bias"](%517)
  %586 : Tensor = prim::GetAttr[name="weight"](%517)
  %587 : Float(4096:1, 16384:4096) = aten::t(%586), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.25 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%583, %587), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.35 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.25, %585, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %590 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.35, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %591 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.35, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %592 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%591, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %593 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.35, %592, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %594 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%593, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %595 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%594), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %596 : Float(17:212992, 13:16384, 16384:1) = aten::add(%595, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.39 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%590, %596), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %598 : Tensor = prim::GetAttr[name="bias"](%516)
  %599 : Tensor = prim::GetAttr[name="weight"](%516)
  %600 : Float(16384:1, 4096:16384) = aten::t(%599), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.26 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.39, %600), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.26, %598, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.40 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.5, %583, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %604 : Tensor = prim::GetAttr[name="bias"](%515)
  %605 : Tensor = prim::GetAttr[name="weight"](%515)
  %606 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.41 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.40, %606, %605, %604, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %608 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.41, %584)
  %609 : Float(17:53248, 13:4096, 4096:1), %610 : Float(4096:1) = prim::TupleUnpack(%608)
  %611 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%609, %610)
  %612 : Float(17:53248, 13:4096, 4096:1), %613 : Float(4096:1) = prim::TupleUnpack(%611)
  %614 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %615 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%614)
  %616 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%615)
  %617 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%615)
  %618 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%615)
  %619 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%615)
  %620 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%619)
  %621 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%619)
  %622 : Tensor = prim::GetAttr[name="weight"](%621)
  %623 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%619)
  %624 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%619)
  %625 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%619)
  %626 : Tensor = prim::GetAttr[name="bias"](%625)
  %627 : Tensor = prim::GetAttr[name="weight"](%625)
  %628 : Float(4096:1, 4096:4096) = aten::t(%627), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.27 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%612, %628), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.36 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.27, %626, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %631 : Tensor = prim::GetAttr[name="bias"](%624)
  %632 : Tensor = prim::GetAttr[name="weight"](%624)
  %633 : Float(4096:1, 4096:4096) = aten::t(%632), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.28 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%612, %633), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.38 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.28, %631, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %636 : Tensor = prim::GetAttr[name="bias"](%623)
  %637 : Tensor = prim::GetAttr[name="weight"](%623)
  %638 : Float(4096:1, 4096:4096) = aten::t(%637), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.29 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%612, %638), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.40 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.29, %636, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %641 : int = aten::size(%x.36, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %642 : int = aten::size(%x.36, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %643 : int[] = prim::ListConstruct(%641, %642, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.37 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.36, %643), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %645 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.37, %645), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %647 : int = aten::size(%x.38, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %648 : int = aten::size(%x.38, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %649 : int[] = prim::ListConstruct(%647, %648, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.39 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.38, %649), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %651 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.39, %651), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %653 : int = aten::size(%x.40, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %654 : int = aten::size(%x.40, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %655 : int[] = prim::ListConstruct(%653, %654, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.41 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.40, %655), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %657 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.41, %657), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %659 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.6, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %659), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.12 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.11, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.42 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.43 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.42, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.43, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.6 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %666 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %667 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.6, %666), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %668 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%667, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %669 : Float(4096:1, 4096:4096) = aten::t(%622), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %670 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %671 : Float(64:64, 64:1, 4096:4096) = aten::view(%669, %670), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %672 : Float(64:64, 64:1, 4096:4096) = aten::to(%671, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.6 : Float(4096:1) = aten::to(%613, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %674 : Tensor[] = prim::ListConstruct(%668, %672), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %675 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %674), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.44 : Float(17:53248, 13:4096, 4096:1) = aten::add(%675, %b.6, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.6 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.44, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:53248, 13:4096, 4096:1) = aten::add(%612, %projected_context_layer.6, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %679 : Tensor = prim::GetAttr[name="bias"](%620)
  %680 : Tensor = prim::GetAttr[name="weight"](%620)
  %681 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.6 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.45, %681, %680, %679, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %683 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.6, %b.6)
  %684 : Float(17:53248, 13:4096, 4096:1), %685 : Float(4096:1) = prim::TupleUnpack(%683)
  %686 : Tensor = prim::GetAttr[name="bias"](%618)
  %687 : Tensor = prim::GetAttr[name="weight"](%618)
  %688 : Float(4096:1, 16384:4096) = aten::t(%687), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.30 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%684, %688), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.42 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.30, %686, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %691 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.42, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %692 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.42, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %693 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%692, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %694 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.42, %693, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %695 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%694, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %696 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%695), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %697 : Float(17:212992, 13:16384, 16384:1) = aten::add(%696, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.46 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%691, %697), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %699 : Tensor = prim::GetAttr[name="bias"](%617)
  %700 : Tensor = prim::GetAttr[name="weight"](%617)
  %701 : Float(16384:1, 4096:16384) = aten::t(%700), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.31 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.46, %701), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.31, %699, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.47 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.6, %684, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %705 : Tensor = prim::GetAttr[name="bias"](%616)
  %706 : Tensor = prim::GetAttr[name="weight"](%616)
  %707 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.48 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.47, %707, %706, %705, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %709 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.48, %685)
  %710 : Float(17:53248, 13:4096, 4096:1), %711 : Float(4096:1) = prim::TupleUnpack(%709)
  %712 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%710, %711)
  %713 : Float(17:53248, 13:4096, 4096:1), %714 : Float(4096:1) = prim::TupleUnpack(%712)
  %715 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %716 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%715)
  %717 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%716)
  %718 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%716)
  %719 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%716)
  %720 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%716)
  %721 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%720)
  %722 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%720)
  %723 : Tensor = prim::GetAttr[name="weight"](%722)
  %724 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%720)
  %725 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%720)
  %726 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%720)
  %727 : Tensor = prim::GetAttr[name="bias"](%726)
  %728 : Tensor = prim::GetAttr[name="weight"](%726)
  %729 : Float(4096:1, 4096:4096) = aten::t(%728), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%713, %729), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.32, %727, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %732 : Tensor = prim::GetAttr[name="bias"](%725)
  %733 : Tensor = prim::GetAttr[name="weight"](%725)
  %734 : Float(4096:1, 4096:4096) = aten::t(%733), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%713, %734), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.33, %732, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %737 : Tensor = prim::GetAttr[name="bias"](%724)
  %738 : Tensor = prim::GetAttr[name="weight"](%724)
  %739 : Float(4096:1, 4096:4096) = aten::t(%738), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%713, %739), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.34, %737, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %742 : int = aten::size(%x.43, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %743 : int = aten::size(%x.43, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %744 : int[] = prim::ListConstruct(%742, %743, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.44 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.43, %744), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %746 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.44, %746), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %748 : int = aten::size(%x.45, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %749 : int = aten::size(%x.45, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %750 : int[] = prim::ListConstruct(%748, %749, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.46 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.45, %750), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %752 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.46, %752), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %754 : int = aten::size(%x.47, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %755 : int = aten::size(%x.47, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %756 : int[] = prim::ListConstruct(%754, %755, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.48 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.47, %756), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %758 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.48, %758), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %760 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.7, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.13 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %760), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.13, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.49 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.50 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.49, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.50, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %767 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %768 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.7, %767), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %769 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%768, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %770 : Float(4096:1, 4096:4096) = aten::t(%723), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %771 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %772 : Float(64:64, 64:1, 4096:4096) = aten::view(%770, %771), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %773 : Float(64:64, 64:1, 4096:4096) = aten::to(%772, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.7 : Float(4096:1) = aten::to(%714, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %775 : Tensor[] = prim::ListConstruct(%769, %773), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %776 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %775), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.51 : Float(17:53248, 13:4096, 4096:1) = aten::add(%776, %b.7, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.7 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.51, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.52 : Float(17:53248, 13:4096, 4096:1) = aten::add(%713, %projected_context_layer.7, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %780 : Tensor = prim::GetAttr[name="bias"](%721)
  %781 : Tensor = prim::GetAttr[name="weight"](%721)
  %782 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.7 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.52, %782, %781, %780, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %784 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.7, %b.7)
  %785 : Float(17:53248, 13:4096, 4096:1), %786 : Float(4096:1) = prim::TupleUnpack(%784)
  %787 : Tensor = prim::GetAttr[name="bias"](%719)
  %788 : Tensor = prim::GetAttr[name="weight"](%719)
  %789 : Float(4096:1, 16384:4096) = aten::t(%788), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.35 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%785, %789), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.49 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.35, %787, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %792 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.49, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %793 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.49, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %794 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%793, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %795 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.49, %794, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %796 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%795, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %797 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%796), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %798 : Float(17:212992, 13:16384, 16384:1) = aten::add(%797, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.53 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%792, %798), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %800 : Tensor = prim::GetAttr[name="bias"](%718)
  %801 : Tensor = prim::GetAttr[name="weight"](%718)
  %802 : Float(16384:1, 4096:16384) = aten::t(%801), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.36 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.53, %802), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.7 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.36, %800, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.54 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.7, %785, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %806 : Tensor = prim::GetAttr[name="bias"](%717)
  %807 : Tensor = prim::GetAttr[name="weight"](%717)
  %808 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.55 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.54, %808, %807, %806, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %810 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.55, %786)
  %811 : Float(17:53248, 13:4096, 4096:1), %812 : Float(4096:1) = prim::TupleUnpack(%810)
  %813 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%811, %812)
  %814 : Float(17:53248, 13:4096, 4096:1), %815 : Float(4096:1) = prim::TupleUnpack(%813)
  %816 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %817 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%816)
  %818 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%817)
  %819 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%817)
  %820 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%817)
  %821 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%817)
  %822 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%821)
  %823 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%821)
  %824 : Tensor = prim::GetAttr[name="weight"](%823)
  %825 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%821)
  %826 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%821)
  %827 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%821)
  %828 : Tensor = prim::GetAttr[name="bias"](%827)
  %829 : Tensor = prim::GetAttr[name="weight"](%827)
  %830 : Float(4096:1, 4096:4096) = aten::t(%829), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%814, %830), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.50 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.37, %828, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %833 : Tensor = prim::GetAttr[name="bias"](%826)
  %834 : Tensor = prim::GetAttr[name="weight"](%826)
  %835 : Float(4096:1, 4096:4096) = aten::t(%834), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%814, %835), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.52 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.38, %833, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %838 : Tensor = prim::GetAttr[name="bias"](%825)
  %839 : Tensor = prim::GetAttr[name="weight"](%825)
  %840 : Float(4096:1, 4096:4096) = aten::t(%839), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%814, %840), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.54 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.39, %838, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %843 : int = aten::size(%x.50, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %844 : int = aten::size(%x.50, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %845 : int[] = prim::ListConstruct(%843, %844, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.51 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.50, %845), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %847 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.51, %847), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %849 : int = aten::size(%x.52, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %850 : int = aten::size(%x.52, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %851 : int[] = prim::ListConstruct(%849, %850, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.53 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.52, %851), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %853 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.53, %853), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %855 : int = aten::size(%x.54, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %856 : int = aten::size(%x.54, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %857 : int[] = prim::ListConstruct(%855, %856, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.55 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.54, %857), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %859 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.55, %859), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %861 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.8, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %861), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.16 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.15, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.56 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.57 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.56, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.57, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.8 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %868 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %869 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.8, %868), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %870 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%869, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %871 : Float(4096:1, 4096:4096) = aten::t(%824), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %872 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %873 : Float(64:64, 64:1, 4096:4096) = aten::view(%871, %872), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %874 : Float(64:64, 64:1, 4096:4096) = aten::to(%873, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.8 : Float(4096:1) = aten::to(%815, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %876 : Tensor[] = prim::ListConstruct(%870, %874), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %877 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %876), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.58 : Float(17:53248, 13:4096, 4096:1) = aten::add(%877, %b.8, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.8 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.58, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.59 : Float(17:53248, 13:4096, 4096:1) = aten::add(%814, %projected_context_layer.8, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %881 : Tensor = prim::GetAttr[name="bias"](%822)
  %882 : Tensor = prim::GetAttr[name="weight"](%822)
  %883 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.8 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.59, %883, %882, %881, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %885 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.8, %b.8)
  %886 : Float(17:53248, 13:4096, 4096:1), %887 : Float(4096:1) = prim::TupleUnpack(%885)
  %888 : Tensor = prim::GetAttr[name="bias"](%820)
  %889 : Tensor = prim::GetAttr[name="weight"](%820)
  %890 : Float(4096:1, 16384:4096) = aten::t(%889), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.40 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%886, %890), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.56 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.40, %888, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %893 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.56, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %894 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.56, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %895 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%894, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %896 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.56, %895, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %897 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%896, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %898 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%897), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %899 : Float(17:212992, 13:16384, 16384:1) = aten::add(%898, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.60 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%893, %899), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %901 : Tensor = prim::GetAttr[name="bias"](%819)
  %902 : Tensor = prim::GetAttr[name="weight"](%819)
  %903 : Float(16384:1, 4096:16384) = aten::t(%902), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.41 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.60, %903), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.41, %901, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.61 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.8, %886, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %907 : Tensor = prim::GetAttr[name="bias"](%818)
  %908 : Tensor = prim::GetAttr[name="weight"](%818)
  %909 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.62 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.61, %909, %908, %907, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %911 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.62, %887)
  %912 : Float(17:53248, 13:4096, 4096:1), %913 : Float(4096:1) = prim::TupleUnpack(%911)
  %914 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%912, %913)
  %915 : Float(17:53248, 13:4096, 4096:1), %916 : Float(4096:1) = prim::TupleUnpack(%914)
  %917 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %918 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%917)
  %919 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%918)
  %920 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%918)
  %921 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%918)
  %922 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%918)
  %923 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%922)
  %924 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%922)
  %925 : Tensor = prim::GetAttr[name="weight"](%924)
  %926 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%922)
  %927 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%922)
  %928 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%922)
  %929 : Tensor = prim::GetAttr[name="bias"](%928)
  %930 : Tensor = prim::GetAttr[name="weight"](%928)
  %931 : Float(4096:1, 4096:4096) = aten::t(%930), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.42 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%915, %931), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.57 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.42, %929, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %934 : Tensor = prim::GetAttr[name="bias"](%927)
  %935 : Tensor = prim::GetAttr[name="weight"](%927)
  %936 : Float(4096:1, 4096:4096) = aten::t(%935), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.43 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%915, %936), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.59 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.43, %934, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %939 : Tensor = prim::GetAttr[name="bias"](%926)
  %940 : Tensor = prim::GetAttr[name="weight"](%926)
  %941 : Float(4096:1, 4096:4096) = aten::t(%940), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.44 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%915, %941), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.61 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.44, %939, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %944 : int = aten::size(%x.57, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %945 : int = aten::size(%x.57, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %946 : int[] = prim::ListConstruct(%944, %945, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.58 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.57, %946), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %948 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.58, %948), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %950 : int = aten::size(%x.59, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %951 : int = aten::size(%x.59, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %952 : int[] = prim::ListConstruct(%950, %951, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.60 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.59, %952), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %954 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.60, %954), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %956 : int = aten::size(%x.61, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %957 : int = aten::size(%x.61, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %958 : int[] = prim::ListConstruct(%956, %957, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.62 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.61, %958), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %960 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.62, %960), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %962 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.9, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.17 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %962), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.18 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.17, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.63 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.64 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.63, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.64, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %969 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %970 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.9, %969), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %971 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%970, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %972 : Float(4096:1, 4096:4096) = aten::t(%925), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %973 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %974 : Float(64:64, 64:1, 4096:4096) = aten::view(%972, %973), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %975 : Float(64:64, 64:1, 4096:4096) = aten::to(%974, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.9 : Float(4096:1) = aten::to(%916, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %977 : Tensor[] = prim::ListConstruct(%971, %975), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %978 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %977), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.65 : Float(17:53248, 13:4096, 4096:1) = aten::add(%978, %b.9, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.9 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.65, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.66 : Float(17:53248, 13:4096, 4096:1) = aten::add(%915, %projected_context_layer.9, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %982 : Tensor = prim::GetAttr[name="bias"](%923)
  %983 : Tensor = prim::GetAttr[name="weight"](%923)
  %984 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.9 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.66, %984, %983, %982, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %986 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.9, %b.9)
  %987 : Float(17:53248, 13:4096, 4096:1), %988 : Float(4096:1) = prim::TupleUnpack(%986)
  %989 : Tensor = prim::GetAttr[name="bias"](%921)
  %990 : Tensor = prim::GetAttr[name="weight"](%921)
  %991 : Float(4096:1, 16384:4096) = aten::t(%990), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.45 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%987, %991), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.63 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.45, %989, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %994 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.63, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %995 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.63, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %996 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%995, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %997 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.63, %996, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %998 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%997, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %999 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%998), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1000 : Float(17:212992, 13:16384, 16384:1) = aten::add(%999, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.67 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%994, %1000), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1002 : Tensor = prim::GetAttr[name="bias"](%920)
  %1003 : Tensor = prim::GetAttr[name="weight"](%920)
  %1004 : Float(16384:1, 4096:16384) = aten::t(%1003), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.46 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.67, %1004), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.9 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.46, %1002, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.68 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.9, %987, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1008 : Tensor = prim::GetAttr[name="bias"](%919)
  %1009 : Tensor = prim::GetAttr[name="weight"](%919)
  %1010 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.69 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.68, %1010, %1009, %1008, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1012 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.69, %988)
  %1013 : Float(17:53248, 13:4096, 4096:1), %1014 : Float(4096:1) = prim::TupleUnpack(%1012)
  %1015 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1013, %1014)
  %1016 : Float(17:53248, 13:4096, 4096:1), %1017 : Float(4096:1) = prim::TupleUnpack(%1015)
  %1018 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %1019 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%1018)
  %1020 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1019)
  %1021 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%1019)
  %1022 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%1019)
  %1023 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%1019)
  %1024 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%1023)
  %1025 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%1023)
  %1026 : Tensor = prim::GetAttr[name="weight"](%1025)
  %1027 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%1023)
  %1028 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%1023)
  %1029 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%1023)
  %1030 : Tensor = prim::GetAttr[name="bias"](%1029)
  %1031 : Tensor = prim::GetAttr[name="weight"](%1029)
  %1032 : Float(4096:1, 4096:4096) = aten::t(%1031), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.47 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1016, %1032), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.64 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.47, %1030, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1035 : Tensor = prim::GetAttr[name="bias"](%1028)
  %1036 : Tensor = prim::GetAttr[name="weight"](%1028)
  %1037 : Float(4096:1, 4096:4096) = aten::t(%1036), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.48 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1016, %1037), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.66 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.48, %1035, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1040 : Tensor = prim::GetAttr[name="bias"](%1027)
  %1041 : Tensor = prim::GetAttr[name="weight"](%1027)
  %1042 : Float(4096:1, 4096:4096) = aten::t(%1041), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.49 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1016, %1042), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.68 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.49, %1040, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1045 : int = aten::size(%x.64, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1046 : int = aten::size(%x.64, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1047 : int[] = prim::ListConstruct(%1045, %1046, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.65 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.64, %1047), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1049 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.65, %1049), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1051 : int = aten::size(%x.66, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1052 : int = aten::size(%x.66, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1053 : int[] = prim::ListConstruct(%1051, %1052, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.67 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.66, %1053), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1055 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.67, %1055), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1057 : int = aten::size(%x.68, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1058 : int = aten::size(%x.68, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1059 : int[] = prim::ListConstruct(%1057, %1058, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.69 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.68, %1059), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1061 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.69, %1061), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1063 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.10, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.19 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %1063), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.20 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.19, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.70 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.71 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.70, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.71, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.10 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1070 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1071 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.10, %1070), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1072 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1071, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1073 : Float(4096:1, 4096:4096) = aten::t(%1026), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1074 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1075 : Float(64:64, 64:1, 4096:4096) = aten::view(%1073, %1074), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1076 : Float(64:64, 64:1, 4096:4096) = aten::to(%1075, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.10 : Float(4096:1) = aten::to(%1017, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1078 : Tensor[] = prim::ListConstruct(%1072, %1076), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1079 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %1078), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.72 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1079, %b.10, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.10 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.72, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1016, %projected_context_layer.10, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1083 : Tensor = prim::GetAttr[name="bias"](%1024)
  %1084 : Tensor = prim::GetAttr[name="weight"](%1024)
  %1085 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.10 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.73, %1085, %1084, %1083, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1087 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.10, %b.10)
  %1088 : Float(17:53248, 13:4096, 4096:1), %1089 : Float(4096:1) = prim::TupleUnpack(%1087)
  %1090 : Tensor = prim::GetAttr[name="bias"](%1022)
  %1091 : Tensor = prim::GetAttr[name="weight"](%1022)
  %1092 : Float(4096:1, 16384:4096) = aten::t(%1091), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.50 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1088, %1092), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.70 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.50, %1090, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1095 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.70, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1096 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.70, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1097 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1096, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1098 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.70, %1097, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1099 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1098, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1100 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1099), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1101 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1100, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.74 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1095, %1101), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1103 : Tensor = prim::GetAttr[name="bias"](%1021)
  %1104 : Tensor = prim::GetAttr[name="weight"](%1021)
  %1105 : Float(16384:1, 4096:16384) = aten::t(%1104), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.51 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.74, %1105), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.51, %1103, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.75 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.10, %1088, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1109 : Tensor = prim::GetAttr[name="bias"](%1020)
  %1110 : Tensor = prim::GetAttr[name="weight"](%1020)
  %1111 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.76 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.75, %1111, %1110, %1109, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1113 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.76, %1089)
  %1114 : Float(17:53248, 13:4096, 4096:1), %1115 : Float(4096:1) = prim::TupleUnpack(%1113)
  %1116 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1114, %1115)
  %1117 : Float(17:53248, 13:4096, 4096:1), %1118 : Float(4096:1) = prim::TupleUnpack(%1116)
  %1119 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %1120 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%1119)
  %1121 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1120)
  %1122 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%1120)
  %1123 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%1120)
  %1124 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%1120)
  %1125 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%1124)
  %1126 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%1124)
  %1127 : Tensor = prim::GetAttr[name="weight"](%1126)
  %1128 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%1124)
  %1129 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%1124)
  %1130 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%1124)
  %1131 : Tensor = prim::GetAttr[name="bias"](%1130)
  %1132 : Tensor = prim::GetAttr[name="weight"](%1130)
  %1133 : Float(4096:1, 4096:4096) = aten::t(%1132), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.52 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1117, %1133), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.71 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.52, %1131, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1136 : Tensor = prim::GetAttr[name="bias"](%1129)
  %1137 : Tensor = prim::GetAttr[name="weight"](%1129)
  %1138 : Float(4096:1, 4096:4096) = aten::t(%1137), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.53 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1117, %1138), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.73 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.53, %1136, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1141 : Tensor = prim::GetAttr[name="bias"](%1128)
  %1142 : Tensor = prim::GetAttr[name="weight"](%1128)
  %1143 : Float(4096:1, 4096:4096) = aten::t(%1142), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.54 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1117, %1143), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.75 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.54, %1141, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1146 : int = aten::size(%x.71, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1147 : int = aten::size(%x.71, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1148 : int[] = prim::ListConstruct(%1146, %1147, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.72 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.71, %1148), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1150 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.72, %1150), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1152 : int = aten::size(%x.73, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1153 : int = aten::size(%x.73, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1154 : int[] = prim::ListConstruct(%1152, %1153, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.74 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.73, %1154), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1156 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.74, %1156), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1158 : int = aten::size(%x.75, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1159 : int = aten::size(%x.75, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1160 : int[] = prim::ListConstruct(%1158, %1159, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.76 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.75, %1160), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1162 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.76, %1162), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1164 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.11, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1164), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.21, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.77 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.78 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.77, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.78, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1171 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1172 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.11, %1171), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1173 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1172, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1174 : Float(4096:1, 4096:4096) = aten::t(%1127), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1175 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1176 : Float(64:64, 64:1, 4096:4096) = aten::view(%1174, %1175), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1177 : Float(64:64, 64:1, 4096:4096) = aten::to(%1176, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.11 : Float(4096:1) = aten::to(%1118, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1179 : Tensor[] = prim::ListConstruct(%1173, %1177), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1180 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %1179), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.79 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1180, %b.11, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.11 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.79, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1117, %projected_context_layer.11, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1184 : Tensor = prim::GetAttr[name="bias"](%1125)
  %1185 : Tensor = prim::GetAttr[name="weight"](%1125)
  %1186 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.11 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.80, %1186, %1185, %1184, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1188 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.11, %b.11)
  %1189 : Float(17:53248, 13:4096, 4096:1), %1190 : Float(4096:1) = prim::TupleUnpack(%1188)
  %1191 : Tensor = prim::GetAttr[name="bias"](%1123)
  %1192 : Tensor = prim::GetAttr[name="weight"](%1123)
  %1193 : Float(4096:1, 16384:4096) = aten::t(%1192), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.55 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1189, %1193), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.77 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.55, %1191, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1196 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.77, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1197 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.77, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1198 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1197, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1199 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.77, %1198, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1200 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1199, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1201 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1200), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1202 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1201, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.81 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1196, %1202), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1204 : Tensor = prim::GetAttr[name="bias"](%1122)
  %1205 : Tensor = prim::GetAttr[name="weight"](%1122)
  %1206 : Float(16384:1, 4096:16384) = aten::t(%1205), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.56 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.81, %1206), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.11 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.56, %1204, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.82 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.11, %1189, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1210 : Tensor = prim::GetAttr[name="bias"](%1121)
  %1211 : Tensor = prim::GetAttr[name="weight"](%1121)
  %1212 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.83 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.82, %1212, %1211, %1210, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1214 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.83, %1190)
  %1215 : Float(17:53248, 13:4096, 4096:1), %1216 : Float(4096:1) = prim::TupleUnpack(%1214)
  %1217 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1215, %1216)
  %1218 : Float(17:53248, 13:4096, 4096:1), %1219 : Float(4096:1) = prim::TupleUnpack(%1217)
  %1220 : __torch__.torch.nn.modules.container.___torch_mangle_360.ModuleList = prim::GetAttr[name="albert_layers"](%100)
  %1221 : __torch__.transformers.modeling_albert.___torch_mangle_359.AlbertLayer = prim::GetAttr[name="0"](%1220)
  %1222 : __torch__.torch.nn.modules.normalization.___torch_mangle_347.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1221)
  %1223 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="ffn_output"](%1221)
  %1224 : __torch__.torch.nn.modules.linear.___torch_mangle_356.Linear = prim::GetAttr[name="ffn"](%1221)
  %1225 : __torch__.transformers.modeling_albert.___torch_mangle_355.AlbertAttention = prim::GetAttr[name="attention"](%1221)
  %1226 : __torch__.torch.nn.modules.normalization.___torch_mangle_354.LayerNorm = prim::GetAttr[name="LayerNorm"](%1225)
  %1227 : __torch__.torch.nn.modules.linear.___torch_mangle_353.Linear = prim::GetAttr[name="dense"](%1225)
  %1228 : Tensor = prim::GetAttr[name="weight"](%1227)
  %1229 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name="value"](%1225)
  %1230 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name="key"](%1225)
  %1231 : __torch__.torch.nn.modules.linear.___torch_mangle_348.Linear = prim::GetAttr[name="query"](%1225)
  %1232 : Tensor = prim::GetAttr[name="bias"](%1231)
  %1233 : Tensor = prim::GetAttr[name="weight"](%1231)
  %1234 : Float(4096:1, 4096:4096) = aten::t(%1233), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.57 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1218, %1234), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.78 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.57, %1232, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1237 : Tensor = prim::GetAttr[name="bias"](%1230)
  %1238 : Tensor = prim::GetAttr[name="weight"](%1230)
  %1239 : Float(4096:1, 4096:4096) = aten::t(%1238), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.58 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1218, %1239), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.80 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.58, %1237, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1242 : Tensor = prim::GetAttr[name="bias"](%1229)
  %1243 : Tensor = prim::GetAttr[name="weight"](%1229)
  %1244 : Float(4096:1, 4096:4096) = aten::t(%1243), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.59 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1218, %1244), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.82 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.59, %1242, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1247 : int = aten::size(%x.78, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1248 : int = aten::size(%x.78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1249 : int[] = prim::ListConstruct(%1247, %1248, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.79 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.78, %1249), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1251 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.79, %1251), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1253 : int = aten::size(%x.80, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1254 : int = aten::size(%x.80, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1255 : int[] = prim::ListConstruct(%1253, %1254, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.81 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.80, %1255), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1257 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.81, %1257), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1259 : int = aten::size(%x.82, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1260 : int = aten::size(%x.82, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1261 : int[] = prim::ListConstruct(%1259, %1260, %84, %84), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.83 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.82, %1261), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1263 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.83, %1263), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1265 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer, %87, %88), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.23 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer, %1265), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.23, %89), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.84 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.85 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.84, %87, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.85, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1272 : int[] = prim::ListConstruct(%83, %85, %98, %86), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1273 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer, %1272), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1274 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1273, %83), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1275 : Float(4096:1, 4096:4096) = aten::t(%1228), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1276 : int[] = prim::ListConstruct(%84, %84, %93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1277 : Float(64:64, 64:1, 4096:4096) = aten::view(%1275, %1276), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1278 : Float(64:64, 64:1, 4096:4096) = aten::to(%1277, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b : Float(4096:1) = aten::to(%1219, %94, %91, %91, %90), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1280 : Tensor[] = prim::ListConstruct(%1274, %1278), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1281 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%95, %1280), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.86 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1281, %b, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.86, %92, %91), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1218, %projected_context_layer, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1285 : Tensor = prim::GetAttr[name="bias"](%1226)
  %1286 : Tensor = prim::GetAttr[name="weight"](%1226)
  %1287 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.87, %1287, %1286, %1285, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1289 : Tensor = prim::GetAttr[name="bias"](%1224)
  %1290 : Tensor = prim::GetAttr[name="weight"](%1224)
  %1291 : Float(4096:1, 16384:4096) = aten::t(%1290), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.60 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%input_tensor, %1291), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.60, %1289, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1294 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x, %82), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1295 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x, %81), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1296 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1295, %80), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1297 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x, %1296, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1298 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1297, %79), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1299 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1298), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1300 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1299, %78, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.88 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1294, %1300), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1302 : Tensor = prim::GetAttr[name="bias"](%1223)
  %1303 : Tensor = prim::GetAttr[name="weight"](%1223)
  %1304 : Float(16384:1, 4096:16384) = aten::t(%1303), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.88, %1304), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output, %1302, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.89 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output, %input_tensor, %98), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1308 : Tensor = prim::GetAttr[name="bias"](%1222)
  %1309 : Tensor = prim::GetAttr[name="weight"](%1222)
  %1310 : int[] = prim::ListConstruct(%93), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %sequence_output : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.89, %1310, %1309, %1308, %97, %96), scope: __module.encoder/__module.encoder.albert_layer_groups.0/__module.encoder.albert_layer_groups.0.albert_layers.0/__module.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %37 : int = prim::Constant[value=0]() # transformers/modeling_albert.py:695:0
  %38 : int = prim::Constant[value=0]() # transformers/modeling_albert.py:695:0
  %39 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_albert.py:695:0
  %40 : int = prim::Constant[value=1]() # transformers/modeling_albert.py:695:0
  %41 : Float(17:53248, 13:4096, 4096:1) = aten::slice(%sequence_output, %37, %38, %39, %40) # transformers/modeling_albert.py:695:0
  %42 : int = prim::Constant[value=1]() # transformers/modeling_albert.py:695:0
  %43 : int = prim::Constant[value=0]() # transformers/modeling_albert.py:695:0
  %input.90 : Float(17:53248, 4096:1) = aten::select(%41, %42, %43) # transformers/modeling_albert.py:695:0
  %1312 : int = prim::Constant[value=1](), scope: __module.pooler # torch/nn/functional.py:1674:0
  %1313 : Tensor = prim::GetAttr[name="bias"](%4)
  %1314 : Tensor = prim::GetAttr[name="weight"](%4)
  %1315 : Float(4096:1, 4096:4096) = aten::t(%1314), scope: __module.pooler # torch/nn/functional.py:1674:0
  %input : Float(17:4096, 4096:1) = aten::addmm(%1313, %input.90, %1315, %1312, %1312), scope: __module.pooler # torch/nn/functional.py:1674:0
  %1317 : Float(17:4096, 4096:1) = aten::tanh(%input), scope: __module.pooler_activation # torch/nn/modules/activation.py:350:0
  %47 : (Float(17:53248, 13:4096, 4096:1), Float(17:4096, 4096:1)) = prim::TupleConstruct(%sequence_output, %1317)
  return (%47)
