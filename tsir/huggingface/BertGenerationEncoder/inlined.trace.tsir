graph(%self.1 : __torch__.transformers.modeling_bert_generation.___torch_mangle_7824.BertGenerationEncoder,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_bert.___torch_mangle_7823.BertEncoder = prim::GetAttr[name="encoder"](%self.1)
  %4 : __torch__.transformers.modeling_bert_generation.___torch_mangle_7413.BertGenerationEmbeddings = prim::GetAttr[name="embeddings"](%self.1)
  %5 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %6 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %7 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
  %8 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %9 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %5, %6, %7, %8) # transformers/modeling_utils.py:244:0
  %10 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %11 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%9, %10) # transformers/modeling_utils.py:244:0
  %12 : int = prim::Constant[value=2]() # transformers/modeling_utils.py:244:0
  %13 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%11, %12) # transformers/modeling_utils.py:244:0
  %14 : int = prim::Constant[value=3]() # transformers/modeling_utils.py:244:0
  %15 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %16 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
  %17 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%13, %14, %15, %16, %17) # transformers/modeling_utils.py:244:0
  %19 : int = prim::Constant[value=6]() # transformers/modeling_utils.py:257:0
  %20 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
  %21 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
  %22 : None = prim::Constant()
  %23 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %19, %20, %21, %22) # transformers/modeling_utils.py:257:0
  %24 : float = prim::Constant[value=1.]() # torch/tensor.py:396:0
  %25 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
  %26 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%23, %24, %25) # torch/tensor.py:396:0
  %27 : Double() = prim::Constant[value={-10000}]() # transformers/modeling_utils.py:258:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%26, %27) # transformers/modeling_utils.py:258:0
  %32 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %33 : int = prim::Constant[value=1024](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %34 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %35 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %36 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %37 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %38 : int = prim::Constant[value=9223372036854775807](), scope: __module.embeddings # transformers/modeling_bert_generation.py:156:0
  %39 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_bert_generation.py:156:0
  %40 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_bert_generation.py:149:0
  %41 : __torch__.torch.nn.modules.normalization.___torch_mangle_7411.LayerNorm = prim::GetAttr[name="LayerNorm"](%4)
  %42 : __torch__.torch.nn.modules.sparse.___torch_mangle_7410.Embedding = prim::GetAttr[name="position_embeddings"](%4)
  %43 : __torch__.torch.nn.modules.sparse.___torch_mangle_7409.Embedding = prim::GetAttr[name="word_embeddings"](%4)
  %44 : Tensor = prim::GetAttr[name="position_ids"](%4)
  %45 : int = aten::size(%input_ids, %40), scope: __module.embeddings # transformers/modeling_bert_generation.py:149:0
  %46 : Long(1:512, 512:1) = aten::slice(%44, %39, %39, %38, %40), scope: __module.embeddings # transformers/modeling_bert_generation.py:156:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%46, %40, %39, %45, %40), scope: __module.embeddings # transformers/modeling_bert_generation.py:156:0
  %48 : Tensor = prim::GetAttr[name="weight"](%43)
  %inputs_embeds : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%48, %input_ids, %39, %37, %37), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %50 : Tensor = prim::GetAttr[name="weight"](%42)
  %position_embeddings : Float(1:13312, 13:1024, 1024:1) = aten::embedding(%50, %input.1, %36, %37, %37), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %input.2 : Float(17:13312, 13:1024, 1024:1) = aten::add(%inputs_embeds, %position_embeddings, %40), scope: __module.embeddings # transformers/modeling_bert_generation.py:162:0
  %53 : Tensor = prim::GetAttr[name="bias"](%41)
  %54 : Tensor = prim::GetAttr[name="weight"](%41)
  %55 : int[] = prim::ListConstruct(%33), scope: __module.embeddings/__module.embeddings.LayerNorm
  %input.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.2, %55, %54, %53, %34, %35), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.4 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.3, %32, %37), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %58 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %59 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %60 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %61 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %62 : int = prim::Constant[value=16](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %63 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %64 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %65 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %66 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %67 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %68 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %69 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %70 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %71 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %72 : int = prim::Constant[value=1024](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %74 : __torch__.transformers.modeling_bert.___torch_mangle_7821.BertLayer = prim::GetAttr[name="23"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %76 : __torch__.transformers.modeling_bert.___torch_mangle_7804.BertLayer = prim::GetAttr[name="22"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %78 : __torch__.transformers.modeling_bert.___torch_mangle_7787.BertLayer = prim::GetAttr[name="21"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %80 : __torch__.transformers.modeling_bert.___torch_mangle_7770.BertLayer = prim::GetAttr[name="20"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %82 : __torch__.transformers.modeling_bert.___torch_mangle_7753.BertLayer = prim::GetAttr[name="19"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %84 : __torch__.transformers.modeling_bert.___torch_mangle_7736.BertLayer = prim::GetAttr[name="18"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %86 : __torch__.transformers.modeling_bert.___torch_mangle_7719.BertLayer = prim::GetAttr[name="17"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %88 : __torch__.transformers.modeling_bert.___torch_mangle_7702.BertLayer = prim::GetAttr[name="16"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %90 : __torch__.transformers.modeling_bert.___torch_mangle_7685.BertLayer = prim::GetAttr[name="15"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %92 : __torch__.transformers.modeling_bert.___torch_mangle_7668.BertLayer = prim::GetAttr[name="14"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %94 : __torch__.transformers.modeling_bert.___torch_mangle_7651.BertLayer = prim::GetAttr[name="13"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %96 : __torch__.transformers.modeling_bert.___torch_mangle_7634.BertLayer = prim::GetAttr[name="12"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %98 : __torch__.transformers.modeling_bert.___torch_mangle_7617.BertLayer = prim::GetAttr[name="11"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %100 : __torch__.transformers.modeling_bert.___torch_mangle_7600.BertLayer = prim::GetAttr[name="10"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %102 : __torch__.transformers.modeling_bert.___torch_mangle_7583.BertLayer = prim::GetAttr[name="9"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %104 : __torch__.transformers.modeling_bert.___torch_mangle_7566.BertLayer = prim::GetAttr[name="8"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %106 : __torch__.transformers.modeling_bert.___torch_mangle_7549.BertLayer = prim::GetAttr[name="7"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %108 : __torch__.transformers.modeling_bert.___torch_mangle_7532.BertLayer = prim::GetAttr[name="6"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %110 : __torch__.transformers.modeling_bert.___torch_mangle_7515.BertLayer = prim::GetAttr[name="5"](%109)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %112 : __torch__.transformers.modeling_bert.___torch_mangle_7498.BertLayer = prim::GetAttr[name="4"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %114 : __torch__.transformers.modeling_bert.___torch_mangle_7481.BertLayer = prim::GetAttr[name="3"](%113)
  %115 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %116 : __torch__.transformers.modeling_bert.___torch_mangle_7464.BertLayer = prim::GetAttr[name="2"](%115)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %118 : __torch__.transformers.modeling_bert.___torch_mangle_7447.BertLayer = prim::GetAttr[name="1"](%117)
  %119 : __torch__.torch.nn.modules.container.___torch_mangle_7822.ModuleList = prim::GetAttr[name="layer"](%3)
  %120 : __torch__.transformers.modeling_bert.___torch_mangle_7430.BertLayer = prim::GetAttr[name="0"](%119)
  %121 : __torch__.transformers.modeling_bert.___torch_mangle_7429.BertOutput = prim::GetAttr[name="output"](%120)
  %122 : __torch__.transformers.modeling_bert.___torch_mangle_7425.BertIntermediate = prim::GetAttr[name="intermediate"](%120)
  %123 : __torch__.transformers.modeling_bert.___torch_mangle_7423.BertAttention = prim::GetAttr[name="attention"](%120)
  %124 : __torch__.transformers.modeling_bert.___torch_mangle_7422.BertSelfOutput = prim::GetAttr[name="output"](%123)
  %125 : __torch__.transformers.modeling_bert.___torch_mangle_7418.BertSelfAttention = prim::GetAttr[name="self"](%123)
  %126 : __torch__.torch.nn.modules.linear.___torch_mangle_7416.Linear = prim::GetAttr[name="value"](%125)
  %127 : __torch__.torch.nn.modules.linear.___torch_mangle_7415.Linear = prim::GetAttr[name="key"](%125)
  %128 : __torch__.torch.nn.modules.linear.___torch_mangle_7414.Linear = prim::GetAttr[name="query"](%125)
  %129 : Tensor = prim::GetAttr[name="bias"](%128)
  %130 : Tensor = prim::GetAttr[name="weight"](%128)
  %131 : Float(1024:1, 1024:1024) = aten::t(%130), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.4, %131), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.1, %129, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %134 : Tensor = prim::GetAttr[name="bias"](%127)
  %135 : Tensor = prim::GetAttr[name="weight"](%127)
  %136 : Float(1024:1, 1024:1024) = aten::t(%135), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.4, %136), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.2, %134, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %139 : Tensor = prim::GetAttr[name="bias"](%126)
  %140 : Tensor = prim::GetAttr[name="weight"](%126)
  %141 : Float(1024:1, 1024:1024) = aten::t(%140), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.4, %141), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.3, %139, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %144 : int = aten::size(%x.1, %61), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %145 : int = aten::size(%x.1, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %146 : int[] = prim::ListConstruct(%144, %145, %62, %63), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.2 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.1, %146), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %148 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.2, %148), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %150 : int = aten::size(%x.3, %61), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %151 : int = aten::size(%x.3, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %152 : int[] = prim::ListConstruct(%150, %151, %62, %63), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.4 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.3, %152), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %154 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.4, %154), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %156 : int = aten::size(%x.5, %61), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %157 : int = aten::size(%x.5, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %158 : int[] = prim::ListConstruct(%156, %157, %62, %63), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.6 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.5, %158), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %160 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.6, %160), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %162 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.1, %66, %67), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %162), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.1, %68), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:262:0
  %input.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %66, %69), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.6, %71, %70), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:275:0
  %169 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %170 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.1, %169), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.2 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%170, %61), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %172 : int = aten::size(%context_layer.2, %61), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %173 : int = aten::size(%context_layer.2, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %174 : int[] = prim::ListConstruct(%172, %173, %72), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %input.7 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.2, %174), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
  %176 : __torch__.torch.nn.modules.normalization.___torch_mangle_7420.LayerNorm = prim::GetAttr[name="LayerNorm"](%124)
  %177 : __torch__.torch.nn.modules.linear.___torch_mangle_7419.Linear = prim::GetAttr[name="dense"](%124)
  %178 : Tensor = prim::GetAttr[name="bias"](%177)
  %179 : Tensor = prim::GetAttr[name="weight"](%177)
  %180 : Float(1024:1, 1024:1024) = aten::t(%179), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.7, %180), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.8 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.4, %178, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.8, %71, %70), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.1, %input.4, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # transformers/modeling_bert.py:295:0
  %185 : Tensor = prim::GetAttr[name="bias"](%176)
  %186 : Tensor = prim::GetAttr[name="weight"](%176)
  %187 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.9, %187, %186, %185, %59, %58), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %189 : __torch__.torch.nn.modules.linear.___torch_mangle_7424.Linear = prim::GetAttr[name="dense"](%122)
  %190 : Tensor = prim::GetAttr[name="bias"](%189)
  %191 : Tensor = prim::GetAttr[name="weight"](%189)
  %192 : Float(1024:1, 4096:1024) = aten::t(%191), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.1, %192), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.5, %190, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.11 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.10), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %196 : __torch__.torch.nn.modules.normalization.___torch_mangle_7427.LayerNorm = prim::GetAttr[name="LayerNorm"](%121)
  %197 : __torch__.torch.nn.modules.linear.___torch_mangle_7426.Linear = prim::GetAttr[name="dense"](%121)
  %198 : Tensor = prim::GetAttr[name="bias"](%197)
  %199 : Tensor = prim::GetAttr[name="weight"](%197)
  %200 : Float(4096:1, 1024:4096) = aten::t(%199), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.11, %200), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.12 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.6, %198, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.12, %71, %70), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.13 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.2, %input_tensor.1, %60), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # transformers/modeling_bert.py:371:0
  %205 : Tensor = prim::GetAttr[name="bias"](%196)
  %206 : Tensor = prim::GetAttr[name="weight"](%196)
  %207 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm
  %input.14 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.13, %207, %206, %205, %59, %58), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %209 : __torch__.transformers.modeling_bert.___torch_mangle_7446.BertOutput = prim::GetAttr[name="output"](%118)
  %210 : __torch__.transformers.modeling_bert.___torch_mangle_7442.BertIntermediate = prim::GetAttr[name="intermediate"](%118)
  %211 : __torch__.transformers.modeling_bert.___torch_mangle_7440.BertAttention = prim::GetAttr[name="attention"](%118)
  %212 : __torch__.transformers.modeling_bert.___torch_mangle_7439.BertSelfOutput = prim::GetAttr[name="output"](%211)
  %213 : __torch__.transformers.modeling_bert.___torch_mangle_7435.BertSelfAttention = prim::GetAttr[name="self"](%211)
  %214 : __torch__.torch.nn.modules.linear.___torch_mangle_7433.Linear = prim::GetAttr[name="value"](%213)
  %215 : __torch__.torch.nn.modules.linear.___torch_mangle_7432.Linear = prim::GetAttr[name="key"](%213)
  %216 : __torch__.torch.nn.modules.linear.___torch_mangle_7431.Linear = prim::GetAttr[name="query"](%213)
  %217 : Tensor = prim::GetAttr[name="bias"](%216)
  %218 : Tensor = prim::GetAttr[name="weight"](%216)
  %219 : Float(1024:1, 1024:1024) = aten::t(%218), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.14, %219), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.7, %217, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %222 : Tensor = prim::GetAttr[name="bias"](%215)
  %223 : Tensor = prim::GetAttr[name="weight"](%215)
  %224 : Float(1024:1, 1024:1024) = aten::t(%223), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.14, %224), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.8, %222, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %227 : Tensor = prim::GetAttr[name="bias"](%214)
  %228 : Tensor = prim::GetAttr[name="weight"](%214)
  %229 : Float(1024:1, 1024:1024) = aten::t(%228), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.14, %229), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.9, %227, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %232 : int = aten::size(%x.7, %61), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %233 : int = aten::size(%x.7, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %234 : int[] = prim::ListConstruct(%232, %233, %62, %63), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.8 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.7, %234), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %236 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.8, %236), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %238 : int = aten::size(%x.9, %61), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %239 : int = aten::size(%x.9, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %240 : int[] = prim::ListConstruct(%238, %239, %62, %63), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.10 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.9, %240), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %242 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.10, %242), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %244 : int = aten::size(%x.11, %61), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %245 : int = aten::size(%x.11, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %246 : int[] = prim::ListConstruct(%244, %245, %62, %63), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.12 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.11, %246), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %248 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.12, %248), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %250 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.2, %66, %67), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %250), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.3, %68), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:259:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:262:0
  %input.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %66, %69), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.16, %71, %70), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:275:0
  %257 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %258 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.3, %257), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.4 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%258, %61), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %260 : int = aten::size(%context_layer.4, %61), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %261 : int = aten::size(%context_layer.4, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %262 : int[] = prim::ListConstruct(%260, %261, %72), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %input.17 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.4, %262), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_bert.py:279:0
  %264 : __torch__.torch.nn.modules.normalization.___torch_mangle_7437.LayerNorm = prim::GetAttr[name="LayerNorm"](%212)
  %265 : __torch__.torch.nn.modules.linear.___torch_mangle_7436.Linear = prim::GetAttr[name="dense"](%212)
  %266 : Tensor = prim::GetAttr[name="bias"](%265)
  %267 : Tensor = prim::GetAttr[name="weight"](%265)
  %268 : Float(1024:1, 1024:1024) = aten::t(%267), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.17, %268), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.18 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.10, %266, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.18, %71, %70), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.19 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.3, %input.14, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # transformers/modeling_bert.py:295:0
  %273 : Tensor = prim::GetAttr[name="bias"](%264)
  %274 : Tensor = prim::GetAttr[name="weight"](%264)
  %275 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.19, %275, %274, %273, %59, %58), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %277 : __torch__.torch.nn.modules.linear.___torch_mangle_7441.Linear = prim::GetAttr[name="dense"](%210)
  %278 : Tensor = prim::GetAttr[name="bias"](%277)
  %279 : Tensor = prim::GetAttr[name="weight"](%277)
  %280 : Float(1024:1, 4096:1024) = aten::t(%279), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.2, %280), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.20 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.11, %278, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.21 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.20), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %284 : __torch__.torch.nn.modules.normalization.___torch_mangle_7444.LayerNorm = prim::GetAttr[name="LayerNorm"](%209)
  %285 : __torch__.torch.nn.modules.linear.___torch_mangle_7443.Linear = prim::GetAttr[name="dense"](%209)
  %286 : Tensor = prim::GetAttr[name="bias"](%285)
  %287 : Tensor = prim::GetAttr[name="weight"](%285)
  %288 : Float(4096:1, 1024:4096) = aten::t(%287), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.21, %288), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.22 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.12, %286, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.22, %71, %70), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.4, %input_tensor.2, %60), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # transformers/modeling_bert.py:371:0
  %293 : Tensor = prim::GetAttr[name="bias"](%284)
  %294 : Tensor = prim::GetAttr[name="weight"](%284)
  %295 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm
  %input.24 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.23, %295, %294, %293, %59, %58), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %297 : __torch__.transformers.modeling_bert.___torch_mangle_7463.BertOutput = prim::GetAttr[name="output"](%116)
  %298 : __torch__.transformers.modeling_bert.___torch_mangle_7459.BertIntermediate = prim::GetAttr[name="intermediate"](%116)
  %299 : __torch__.transformers.modeling_bert.___torch_mangle_7457.BertAttention = prim::GetAttr[name="attention"](%116)
  %300 : __torch__.transformers.modeling_bert.___torch_mangle_7456.BertSelfOutput = prim::GetAttr[name="output"](%299)
  %301 : __torch__.transformers.modeling_bert.___torch_mangle_7452.BertSelfAttention = prim::GetAttr[name="self"](%299)
  %302 : __torch__.torch.nn.modules.linear.___torch_mangle_7450.Linear = prim::GetAttr[name="value"](%301)
  %303 : __torch__.torch.nn.modules.linear.___torch_mangle_7449.Linear = prim::GetAttr[name="key"](%301)
  %304 : __torch__.torch.nn.modules.linear.___torch_mangle_7448.Linear = prim::GetAttr[name="query"](%301)
  %305 : Tensor = prim::GetAttr[name="bias"](%304)
  %306 : Tensor = prim::GetAttr[name="weight"](%304)
  %307 : Float(1024:1, 1024:1024) = aten::t(%306), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.24, %307), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.13, %305, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %310 : Tensor = prim::GetAttr[name="bias"](%303)
  %311 : Tensor = prim::GetAttr[name="weight"](%303)
  %312 : Float(1024:1, 1024:1024) = aten::t(%311), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.24, %312), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.14, %310, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %315 : Tensor = prim::GetAttr[name="bias"](%302)
  %316 : Tensor = prim::GetAttr[name="weight"](%302)
  %317 : Float(1024:1, 1024:1024) = aten::t(%316), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.24, %317), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.15, %315, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %320 : int = aten::size(%x.13, %61), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %321 : int = aten::size(%x.13, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %322 : int[] = prim::ListConstruct(%320, %321, %62, %63), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.14 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.13, %322), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %324 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.14, %324), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %326 : int = aten::size(%x.15, %61), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %327 : int = aten::size(%x.15, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %328 : int[] = prim::ListConstruct(%326, %327, %62, %63), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.16 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.15, %328), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %330 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.16, %330), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %332 : int = aten::size(%x.17, %61), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %333 : int = aten::size(%x.17, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %334 : int[] = prim::ListConstruct(%332, %333, %62, %63), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.18 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.17, %334), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %336 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.18, %336), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %338 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.3, %66, %67), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %338), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.5, %68), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:259:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:262:0
  %input.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %66, %69), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.26, %71, %70), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:275:0
  %345 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %346 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.5, %345), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.6 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%346, %61), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %348 : int = aten::size(%context_layer.6, %61), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %349 : int = aten::size(%context_layer.6, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %350 : int[] = prim::ListConstruct(%348, %349, %72), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %input.27 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.6, %350), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_bert.py:279:0
  %352 : __torch__.torch.nn.modules.normalization.___torch_mangle_7454.LayerNorm = prim::GetAttr[name="LayerNorm"](%300)
  %353 : __torch__.torch.nn.modules.linear.___torch_mangle_7453.Linear = prim::GetAttr[name="dense"](%300)
  %354 : Tensor = prim::GetAttr[name="bias"](%353)
  %355 : Tensor = prim::GetAttr[name="weight"](%353)
  %356 : Float(1024:1, 1024:1024) = aten::t(%355), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.27, %356), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.28 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.16, %354, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.28, %71, %70), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.29 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.5, %input.24, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # transformers/modeling_bert.py:295:0
  %361 : Tensor = prim::GetAttr[name="bias"](%352)
  %362 : Tensor = prim::GetAttr[name="weight"](%352)
  %363 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.29, %363, %362, %361, %59, %58), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %365 : __torch__.torch.nn.modules.linear.___torch_mangle_7458.Linear = prim::GetAttr[name="dense"](%298)
  %366 : Tensor = prim::GetAttr[name="bias"](%365)
  %367 : Tensor = prim::GetAttr[name="weight"](%365)
  %368 : Float(1024:1, 4096:1024) = aten::t(%367), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.3, %368), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.30 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.17, %366, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.31 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.30), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %372 : __torch__.torch.nn.modules.normalization.___torch_mangle_7461.LayerNorm = prim::GetAttr[name="LayerNorm"](%297)
  %373 : __torch__.torch.nn.modules.linear.___torch_mangle_7460.Linear = prim::GetAttr[name="dense"](%297)
  %374 : Tensor = prim::GetAttr[name="bias"](%373)
  %375 : Tensor = prim::GetAttr[name="weight"](%373)
  %376 : Float(4096:1, 1024:4096) = aten::t(%375), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.31, %376), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.32 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.18, %374, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.32, %71, %70), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.6, %input_tensor.3, %60), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # transformers/modeling_bert.py:371:0
  %381 : Tensor = prim::GetAttr[name="bias"](%372)
  %382 : Tensor = prim::GetAttr[name="weight"](%372)
  %383 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm
  %input.34 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.33, %383, %382, %381, %59, %58), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %385 : __torch__.transformers.modeling_bert.___torch_mangle_7480.BertOutput = prim::GetAttr[name="output"](%114)
  %386 : __torch__.transformers.modeling_bert.___torch_mangle_7476.BertIntermediate = prim::GetAttr[name="intermediate"](%114)
  %387 : __torch__.transformers.modeling_bert.___torch_mangle_7474.BertAttention = prim::GetAttr[name="attention"](%114)
  %388 : __torch__.transformers.modeling_bert.___torch_mangle_7473.BertSelfOutput = prim::GetAttr[name="output"](%387)
  %389 : __torch__.transformers.modeling_bert.___torch_mangle_7469.BertSelfAttention = prim::GetAttr[name="self"](%387)
  %390 : __torch__.torch.nn.modules.linear.___torch_mangle_7467.Linear = prim::GetAttr[name="value"](%389)
  %391 : __torch__.torch.nn.modules.linear.___torch_mangle_7466.Linear = prim::GetAttr[name="key"](%389)
  %392 : __torch__.torch.nn.modules.linear.___torch_mangle_7465.Linear = prim::GetAttr[name="query"](%389)
  %393 : Tensor = prim::GetAttr[name="bias"](%392)
  %394 : Tensor = prim::GetAttr[name="weight"](%392)
  %395 : Float(1024:1, 1024:1024) = aten::t(%394), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.34, %395), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.19, %393, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %398 : Tensor = prim::GetAttr[name="bias"](%391)
  %399 : Tensor = prim::GetAttr[name="weight"](%391)
  %400 : Float(1024:1, 1024:1024) = aten::t(%399), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.34, %400), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.20, %398, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %403 : Tensor = prim::GetAttr[name="bias"](%390)
  %404 : Tensor = prim::GetAttr[name="weight"](%390)
  %405 : Float(1024:1, 1024:1024) = aten::t(%404), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.34, %405), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.21, %403, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %408 : int = aten::size(%x.19, %61), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %409 : int = aten::size(%x.19, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %410 : int[] = prim::ListConstruct(%408, %409, %62, %63), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.20 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.19, %410), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %412 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.20, %412), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %414 : int = aten::size(%x.21, %61), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %415 : int = aten::size(%x.21, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %416 : int[] = prim::ListConstruct(%414, %415, %62, %63), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.22 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.21, %416), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %418 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.22, %418), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %420 : int = aten::size(%x.23, %61), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %421 : int = aten::size(%x.23, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %422 : int[] = prim::ListConstruct(%420, %421, %62, %63), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.24 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.23, %422), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %424 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.24, %424), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %426 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.4, %66, %67), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %426), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.7, %68), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:259:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:262:0
  %input.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %66, %69), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.36, %71, %70), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:275:0
  %433 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %434 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.7, %433), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.8 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%434, %61), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %436 : int = aten::size(%context_layer.8, %61), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %437 : int = aten::size(%context_layer.8, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %438 : int[] = prim::ListConstruct(%436, %437, %72), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %input.37 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.8, %438), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_bert.py:279:0
  %440 : __torch__.torch.nn.modules.normalization.___torch_mangle_7471.LayerNorm = prim::GetAttr[name="LayerNorm"](%388)
  %441 : __torch__.torch.nn.modules.linear.___torch_mangle_7470.Linear = prim::GetAttr[name="dense"](%388)
  %442 : Tensor = prim::GetAttr[name="bias"](%441)
  %443 : Tensor = prim::GetAttr[name="weight"](%441)
  %444 : Float(1024:1, 1024:1024) = aten::t(%443), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.37, %444), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.38 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.22, %442, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.38, %71, %70), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.39 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.7, %input.34, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # transformers/modeling_bert.py:295:0
  %449 : Tensor = prim::GetAttr[name="bias"](%440)
  %450 : Tensor = prim::GetAttr[name="weight"](%440)
  %451 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.39, %451, %450, %449, %59, %58), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %453 : __torch__.torch.nn.modules.linear.___torch_mangle_7475.Linear = prim::GetAttr[name="dense"](%386)
  %454 : Tensor = prim::GetAttr[name="bias"](%453)
  %455 : Tensor = prim::GetAttr[name="weight"](%453)
  %456 : Float(1024:1, 4096:1024) = aten::t(%455), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.4, %456), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.40 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.23, %454, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.41 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.40), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %460 : __torch__.torch.nn.modules.normalization.___torch_mangle_7478.LayerNorm = prim::GetAttr[name="LayerNorm"](%385)
  %461 : __torch__.torch.nn.modules.linear.___torch_mangle_7477.Linear = prim::GetAttr[name="dense"](%385)
  %462 : Tensor = prim::GetAttr[name="bias"](%461)
  %463 : Tensor = prim::GetAttr[name="weight"](%461)
  %464 : Float(4096:1, 1024:4096) = aten::t(%463), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.41, %464), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.42 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.24, %462, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.42, %71, %70), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.43 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.8, %input_tensor.4, %60), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # transformers/modeling_bert.py:371:0
  %469 : Tensor = prim::GetAttr[name="bias"](%460)
  %470 : Tensor = prim::GetAttr[name="weight"](%460)
  %471 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm
  %input.44 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.43, %471, %470, %469, %59, %58), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %473 : __torch__.transformers.modeling_bert.___torch_mangle_7497.BertOutput = prim::GetAttr[name="output"](%112)
  %474 : __torch__.transformers.modeling_bert.___torch_mangle_7493.BertIntermediate = prim::GetAttr[name="intermediate"](%112)
  %475 : __torch__.transformers.modeling_bert.___torch_mangle_7491.BertAttention = prim::GetAttr[name="attention"](%112)
  %476 : __torch__.transformers.modeling_bert.___torch_mangle_7490.BertSelfOutput = prim::GetAttr[name="output"](%475)
  %477 : __torch__.transformers.modeling_bert.___torch_mangle_7486.BertSelfAttention = prim::GetAttr[name="self"](%475)
  %478 : __torch__.torch.nn.modules.linear.___torch_mangle_7484.Linear = prim::GetAttr[name="value"](%477)
  %479 : __torch__.torch.nn.modules.linear.___torch_mangle_7483.Linear = prim::GetAttr[name="key"](%477)
  %480 : __torch__.torch.nn.modules.linear.___torch_mangle_7482.Linear = prim::GetAttr[name="query"](%477)
  %481 : Tensor = prim::GetAttr[name="bias"](%480)
  %482 : Tensor = prim::GetAttr[name="weight"](%480)
  %483 : Float(1024:1, 1024:1024) = aten::t(%482), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.44, %483), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.25, %481, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %486 : Tensor = prim::GetAttr[name="bias"](%479)
  %487 : Tensor = prim::GetAttr[name="weight"](%479)
  %488 : Float(1024:1, 1024:1024) = aten::t(%487), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.44, %488), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.26, %486, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %491 : Tensor = prim::GetAttr[name="bias"](%478)
  %492 : Tensor = prim::GetAttr[name="weight"](%478)
  %493 : Float(1024:1, 1024:1024) = aten::t(%492), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.44, %493), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.27, %491, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %496 : int = aten::size(%x.25, %61), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %497 : int = aten::size(%x.25, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %498 : int[] = prim::ListConstruct(%496, %497, %62, %63), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.26 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.25, %498), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %500 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.26, %500), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %502 : int = aten::size(%x.27, %61), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %503 : int = aten::size(%x.27, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %504 : int[] = prim::ListConstruct(%502, %503, %62, %63), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.28 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.27, %504), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %506 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.28, %506), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %508 : int = aten::size(%x.29, %61), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %509 : int = aten::size(%x.29, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %510 : int[] = prim::ListConstruct(%508, %509, %62, %63), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.30 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.29, %510), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %512 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.30, %512), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %514 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.5, %66, %67), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %514), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.9, %68), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:259:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:262:0
  %input.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %66, %69), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.46, %71, %70), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:275:0
  %521 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %522 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.9, %521), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.10 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%522, %61), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %524 : int = aten::size(%context_layer.10, %61), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %525 : int = aten::size(%context_layer.10, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %526 : int[] = prim::ListConstruct(%524, %525, %72), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %input.47 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.10, %526), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_bert.py:279:0
  %528 : __torch__.torch.nn.modules.normalization.___torch_mangle_7488.LayerNorm = prim::GetAttr[name="LayerNorm"](%476)
  %529 : __torch__.torch.nn.modules.linear.___torch_mangle_7487.Linear = prim::GetAttr[name="dense"](%476)
  %530 : Tensor = prim::GetAttr[name="bias"](%529)
  %531 : Tensor = prim::GetAttr[name="weight"](%529)
  %532 : Float(1024:1, 1024:1024) = aten::t(%531), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.47, %532), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.48 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.28, %530, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.48, %71, %70), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.49 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.9, %input.44, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # transformers/modeling_bert.py:295:0
  %537 : Tensor = prim::GetAttr[name="bias"](%528)
  %538 : Tensor = prim::GetAttr[name="weight"](%528)
  %539 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.49, %539, %538, %537, %59, %58), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %541 : __torch__.torch.nn.modules.linear.___torch_mangle_7492.Linear = prim::GetAttr[name="dense"](%474)
  %542 : Tensor = prim::GetAttr[name="bias"](%541)
  %543 : Tensor = prim::GetAttr[name="weight"](%541)
  %544 : Float(1024:1, 4096:1024) = aten::t(%543), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.5, %544), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.50 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.29, %542, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.51 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.50), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %548 : __torch__.torch.nn.modules.normalization.___torch_mangle_7495.LayerNorm = prim::GetAttr[name="LayerNorm"](%473)
  %549 : __torch__.torch.nn.modules.linear.___torch_mangle_7494.Linear = prim::GetAttr[name="dense"](%473)
  %550 : Tensor = prim::GetAttr[name="bias"](%549)
  %551 : Tensor = prim::GetAttr[name="weight"](%549)
  %552 : Float(4096:1, 1024:4096) = aten::t(%551), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.51, %552), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.52 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.30, %550, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.52, %71, %70), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.53 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.10, %input_tensor.5, %60), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # transformers/modeling_bert.py:371:0
  %557 : Tensor = prim::GetAttr[name="bias"](%548)
  %558 : Tensor = prim::GetAttr[name="weight"](%548)
  %559 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm
  %input.54 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.53, %559, %558, %557, %59, %58), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %561 : __torch__.transformers.modeling_bert.___torch_mangle_7514.BertOutput = prim::GetAttr[name="output"](%110)
  %562 : __torch__.transformers.modeling_bert.___torch_mangle_7510.BertIntermediate = prim::GetAttr[name="intermediate"](%110)
  %563 : __torch__.transformers.modeling_bert.___torch_mangle_7508.BertAttention = prim::GetAttr[name="attention"](%110)
  %564 : __torch__.transformers.modeling_bert.___torch_mangle_7507.BertSelfOutput = prim::GetAttr[name="output"](%563)
  %565 : __torch__.transformers.modeling_bert.___torch_mangle_7503.BertSelfAttention = prim::GetAttr[name="self"](%563)
  %566 : __torch__.torch.nn.modules.linear.___torch_mangle_7501.Linear = prim::GetAttr[name="value"](%565)
  %567 : __torch__.torch.nn.modules.linear.___torch_mangle_7500.Linear = prim::GetAttr[name="key"](%565)
  %568 : __torch__.torch.nn.modules.linear.___torch_mangle_7499.Linear = prim::GetAttr[name="query"](%565)
  %569 : Tensor = prim::GetAttr[name="bias"](%568)
  %570 : Tensor = prim::GetAttr[name="weight"](%568)
  %571 : Float(1024:1, 1024:1024) = aten::t(%570), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.54, %571), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.31, %569, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %574 : Tensor = prim::GetAttr[name="bias"](%567)
  %575 : Tensor = prim::GetAttr[name="weight"](%567)
  %576 : Float(1024:1, 1024:1024) = aten::t(%575), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.54, %576), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.32, %574, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %579 : Tensor = prim::GetAttr[name="bias"](%566)
  %580 : Tensor = prim::GetAttr[name="weight"](%566)
  %581 : Float(1024:1, 1024:1024) = aten::t(%580), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.54, %581), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.33, %579, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %584 : int = aten::size(%x.31, %61), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %585 : int = aten::size(%x.31, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %586 : int[] = prim::ListConstruct(%584, %585, %62, %63), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.32 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.31, %586), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %588 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.32, %588), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %590 : int = aten::size(%x.33, %61), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %591 : int = aten::size(%x.33, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %592 : int[] = prim::ListConstruct(%590, %591, %62, %63), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.34 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.33, %592), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %594 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.34, %594), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %596 : int = aten::size(%x.35, %61), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %597 : int = aten::size(%x.35, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %598 : int[] = prim::ListConstruct(%596, %597, %62, %63), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.36 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.35, %598), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %600 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.36, %600), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %602 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.6, %66, %67), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %602), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.11, %68), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:259:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:262:0
  %input.56 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %66, %69), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.56, %71, %70), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:275:0
  %609 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %610 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.11, %609), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.12 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%610, %61), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %612 : int = aten::size(%context_layer.12, %61), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %613 : int = aten::size(%context_layer.12, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %614 : int[] = prim::ListConstruct(%612, %613, %72), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %input.57 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.12, %614), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_bert.py:279:0
  %616 : __torch__.torch.nn.modules.normalization.___torch_mangle_7505.LayerNorm = prim::GetAttr[name="LayerNorm"](%564)
  %617 : __torch__.torch.nn.modules.linear.___torch_mangle_7504.Linear = prim::GetAttr[name="dense"](%564)
  %618 : Tensor = prim::GetAttr[name="bias"](%617)
  %619 : Tensor = prim::GetAttr[name="weight"](%617)
  %620 : Float(1024:1, 1024:1024) = aten::t(%619), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.57, %620), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.58 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.34, %618, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.58, %71, %70), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.59 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.11, %input.54, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # transformers/modeling_bert.py:295:0
  %625 : Tensor = prim::GetAttr[name="bias"](%616)
  %626 : Tensor = prim::GetAttr[name="weight"](%616)
  %627 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.59, %627, %626, %625, %59, %58), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %629 : __torch__.torch.nn.modules.linear.___torch_mangle_7509.Linear = prim::GetAttr[name="dense"](%562)
  %630 : Tensor = prim::GetAttr[name="bias"](%629)
  %631 : Tensor = prim::GetAttr[name="weight"](%629)
  %632 : Float(1024:1, 4096:1024) = aten::t(%631), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.6, %632), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.60 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.35, %630, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.61 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %636 : __torch__.torch.nn.modules.normalization.___torch_mangle_7512.LayerNorm = prim::GetAttr[name="LayerNorm"](%561)
  %637 : __torch__.torch.nn.modules.linear.___torch_mangle_7511.Linear = prim::GetAttr[name="dense"](%561)
  %638 : Tensor = prim::GetAttr[name="bias"](%637)
  %639 : Tensor = prim::GetAttr[name="weight"](%637)
  %640 : Float(4096:1, 1024:4096) = aten::t(%639), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.61, %640), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.62 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.36, %638, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.62, %71, %70), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.63 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.12, %input_tensor.6, %60), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # transformers/modeling_bert.py:371:0
  %645 : Tensor = prim::GetAttr[name="bias"](%636)
  %646 : Tensor = prim::GetAttr[name="weight"](%636)
  %647 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm
  %input.64 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.63, %647, %646, %645, %59, %58), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %649 : __torch__.transformers.modeling_bert.___torch_mangle_7531.BertOutput = prim::GetAttr[name="output"](%108)
  %650 : __torch__.transformers.modeling_bert.___torch_mangle_7527.BertIntermediate = prim::GetAttr[name="intermediate"](%108)
  %651 : __torch__.transformers.modeling_bert.___torch_mangle_7525.BertAttention = prim::GetAttr[name="attention"](%108)
  %652 : __torch__.transformers.modeling_bert.___torch_mangle_7524.BertSelfOutput = prim::GetAttr[name="output"](%651)
  %653 : __torch__.transformers.modeling_bert.___torch_mangle_7520.BertSelfAttention = prim::GetAttr[name="self"](%651)
  %654 : __torch__.torch.nn.modules.linear.___torch_mangle_7518.Linear = prim::GetAttr[name="value"](%653)
  %655 : __torch__.torch.nn.modules.linear.___torch_mangle_7517.Linear = prim::GetAttr[name="key"](%653)
  %656 : __torch__.torch.nn.modules.linear.___torch_mangle_7516.Linear = prim::GetAttr[name="query"](%653)
  %657 : Tensor = prim::GetAttr[name="bias"](%656)
  %658 : Tensor = prim::GetAttr[name="weight"](%656)
  %659 : Float(1024:1, 1024:1024) = aten::t(%658), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.64, %659), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.37, %657, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %662 : Tensor = prim::GetAttr[name="bias"](%655)
  %663 : Tensor = prim::GetAttr[name="weight"](%655)
  %664 : Float(1024:1, 1024:1024) = aten::t(%663), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.64, %664), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.38, %662, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %667 : Tensor = prim::GetAttr[name="bias"](%654)
  %668 : Tensor = prim::GetAttr[name="weight"](%654)
  %669 : Float(1024:1, 1024:1024) = aten::t(%668), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.64, %669), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.39, %667, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %672 : int = aten::size(%x.37, %61), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %673 : int = aten::size(%x.37, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %674 : int[] = prim::ListConstruct(%672, %673, %62, %63), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.38 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.37, %674), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %676 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.38, %676), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %678 : int = aten::size(%x.39, %61), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %679 : int = aten::size(%x.39, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %680 : int[] = prim::ListConstruct(%678, %679, %62, %63), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.40 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.39, %680), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %682 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.40, %682), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %684 : int = aten::size(%x.41, %61), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %685 : int = aten::size(%x.41, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %686 : int[] = prim::ListConstruct(%684, %685, %62, %63), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.42 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.41, %686), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %688 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.42, %688), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %690 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.7, %66, %67), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %690), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.13, %68), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:259:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:262:0
  %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %66, %69), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.66, %71, %70), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:275:0
  %697 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %698 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.13, %697), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.14 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%698, %61), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %700 : int = aten::size(%context_layer.14, %61), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %701 : int = aten::size(%context_layer.14, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %702 : int[] = prim::ListConstruct(%700, %701, %72), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %input.67 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.14, %702), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_bert.py:279:0
  %704 : __torch__.torch.nn.modules.normalization.___torch_mangle_7522.LayerNorm = prim::GetAttr[name="LayerNorm"](%652)
  %705 : __torch__.torch.nn.modules.linear.___torch_mangle_7521.Linear = prim::GetAttr[name="dense"](%652)
  %706 : Tensor = prim::GetAttr[name="bias"](%705)
  %707 : Tensor = prim::GetAttr[name="weight"](%705)
  %708 : Float(1024:1, 1024:1024) = aten::t(%707), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.67, %708), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.68 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.40, %706, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.68, %71, %70), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.13, %input.64, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # transformers/modeling_bert.py:295:0
  %713 : Tensor = prim::GetAttr[name="bias"](%704)
  %714 : Tensor = prim::GetAttr[name="weight"](%704)
  %715 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.69, %715, %714, %713, %59, %58), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %717 : __torch__.torch.nn.modules.linear.___torch_mangle_7526.Linear = prim::GetAttr[name="dense"](%650)
  %718 : Tensor = prim::GetAttr[name="bias"](%717)
  %719 : Tensor = prim::GetAttr[name="weight"](%717)
  %720 : Float(1024:1, 4096:1024) = aten::t(%719), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.7, %720), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.70 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.41, %718, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.71 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.70), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %724 : __torch__.torch.nn.modules.normalization.___torch_mangle_7529.LayerNorm = prim::GetAttr[name="LayerNorm"](%649)
  %725 : __torch__.torch.nn.modules.linear.___torch_mangle_7528.Linear = prim::GetAttr[name="dense"](%649)
  %726 : Tensor = prim::GetAttr[name="bias"](%725)
  %727 : Tensor = prim::GetAttr[name="weight"](%725)
  %728 : Float(4096:1, 1024:4096) = aten::t(%727), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.71, %728), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.72 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.42, %726, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.72, %71, %70), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.14, %input_tensor.7, %60), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # transformers/modeling_bert.py:371:0
  %733 : Tensor = prim::GetAttr[name="bias"](%724)
  %734 : Tensor = prim::GetAttr[name="weight"](%724)
  %735 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm
  %input.74 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.73, %735, %734, %733, %59, %58), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %737 : __torch__.transformers.modeling_bert.___torch_mangle_7548.BertOutput = prim::GetAttr[name="output"](%106)
  %738 : __torch__.transformers.modeling_bert.___torch_mangle_7544.BertIntermediate = prim::GetAttr[name="intermediate"](%106)
  %739 : __torch__.transformers.modeling_bert.___torch_mangle_7542.BertAttention = prim::GetAttr[name="attention"](%106)
  %740 : __torch__.transformers.modeling_bert.___torch_mangle_7541.BertSelfOutput = prim::GetAttr[name="output"](%739)
  %741 : __torch__.transformers.modeling_bert.___torch_mangle_7537.BertSelfAttention = prim::GetAttr[name="self"](%739)
  %742 : __torch__.torch.nn.modules.linear.___torch_mangle_7535.Linear = prim::GetAttr[name="value"](%741)
  %743 : __torch__.torch.nn.modules.linear.___torch_mangle_7534.Linear = prim::GetAttr[name="key"](%741)
  %744 : __torch__.torch.nn.modules.linear.___torch_mangle_7533.Linear = prim::GetAttr[name="query"](%741)
  %745 : Tensor = prim::GetAttr[name="bias"](%744)
  %746 : Tensor = prim::GetAttr[name="weight"](%744)
  %747 : Float(1024:1, 1024:1024) = aten::t(%746), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.74, %747), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.43, %745, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %750 : Tensor = prim::GetAttr[name="bias"](%743)
  %751 : Tensor = prim::GetAttr[name="weight"](%743)
  %752 : Float(1024:1, 1024:1024) = aten::t(%751), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.74, %752), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.44, %750, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %755 : Tensor = prim::GetAttr[name="bias"](%742)
  %756 : Tensor = prim::GetAttr[name="weight"](%742)
  %757 : Float(1024:1, 1024:1024) = aten::t(%756), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.74, %757), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.45, %755, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %760 : int = aten::size(%x.43, %61), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %761 : int = aten::size(%x.43, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %762 : int[] = prim::ListConstruct(%760, %761, %62, %63), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.44 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.43, %762), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %764 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.44, %764), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %766 : int = aten::size(%x.45, %61), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %767 : int = aten::size(%x.45, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %768 : int[] = prim::ListConstruct(%766, %767, %62, %63), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.46 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.45, %768), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %770 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.46, %770), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %772 : int = aten::size(%x.47, %61), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %773 : int = aten::size(%x.47, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %774 : int[] = prim::ListConstruct(%772, %773, %62, %63), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.48 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.47, %774), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %776 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.48, %776), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %778 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.8, %66, %67), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %778), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.15, %68), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:259:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:262:0
  %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %66, %69), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %71, %70), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:275:0
  %785 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %786 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.15, %785), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.16 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%786, %61), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %788 : int = aten::size(%context_layer.16, %61), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %789 : int = aten::size(%context_layer.16, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %790 : int[] = prim::ListConstruct(%788, %789, %72), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %input.77 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.16, %790), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_bert.py:279:0
  %792 : __torch__.torch.nn.modules.normalization.___torch_mangle_7539.LayerNorm = prim::GetAttr[name="LayerNorm"](%740)
  %793 : __torch__.torch.nn.modules.linear.___torch_mangle_7538.Linear = prim::GetAttr[name="dense"](%740)
  %794 : Tensor = prim::GetAttr[name="bias"](%793)
  %795 : Tensor = prim::GetAttr[name="weight"](%793)
  %796 : Float(1024:1, 1024:1024) = aten::t(%795), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.77, %796), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.78 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.46, %794, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.78, %71, %70), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.79 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.15, %input.74, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # transformers/modeling_bert.py:295:0
  %801 : Tensor = prim::GetAttr[name="bias"](%792)
  %802 : Tensor = prim::GetAttr[name="weight"](%792)
  %803 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.79, %803, %802, %801, %59, %58), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %805 : __torch__.torch.nn.modules.linear.___torch_mangle_7543.Linear = prim::GetAttr[name="dense"](%738)
  %806 : Tensor = prim::GetAttr[name="bias"](%805)
  %807 : Tensor = prim::GetAttr[name="weight"](%805)
  %808 : Float(1024:1, 4096:1024) = aten::t(%807), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.8, %808), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.80 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.47, %806, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.81 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.80), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %812 : __torch__.torch.nn.modules.normalization.___torch_mangle_7546.LayerNorm = prim::GetAttr[name="LayerNorm"](%737)
  %813 : __torch__.torch.nn.modules.linear.___torch_mangle_7545.Linear = prim::GetAttr[name="dense"](%737)
  %814 : Tensor = prim::GetAttr[name="bias"](%813)
  %815 : Tensor = prim::GetAttr[name="weight"](%813)
  %816 : Float(4096:1, 1024:4096) = aten::t(%815), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.81, %816), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.82 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.48, %814, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.82, %71, %70), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.83 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.16, %input_tensor.8, %60), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # transformers/modeling_bert.py:371:0
  %821 : Tensor = prim::GetAttr[name="bias"](%812)
  %822 : Tensor = prim::GetAttr[name="weight"](%812)
  %823 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm
  %input.84 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.83, %823, %822, %821, %59, %58), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %825 : __torch__.transformers.modeling_bert.___torch_mangle_7565.BertOutput = prim::GetAttr[name="output"](%104)
  %826 : __torch__.transformers.modeling_bert.___torch_mangle_7561.BertIntermediate = prim::GetAttr[name="intermediate"](%104)
  %827 : __torch__.transformers.modeling_bert.___torch_mangle_7559.BertAttention = prim::GetAttr[name="attention"](%104)
  %828 : __torch__.transformers.modeling_bert.___torch_mangle_7558.BertSelfOutput = prim::GetAttr[name="output"](%827)
  %829 : __torch__.transformers.modeling_bert.___torch_mangle_7554.BertSelfAttention = prim::GetAttr[name="self"](%827)
  %830 : __torch__.torch.nn.modules.linear.___torch_mangle_7552.Linear = prim::GetAttr[name="value"](%829)
  %831 : __torch__.torch.nn.modules.linear.___torch_mangle_7551.Linear = prim::GetAttr[name="key"](%829)
  %832 : __torch__.torch.nn.modules.linear.___torch_mangle_7550.Linear = prim::GetAttr[name="query"](%829)
  %833 : Tensor = prim::GetAttr[name="bias"](%832)
  %834 : Tensor = prim::GetAttr[name="weight"](%832)
  %835 : Float(1024:1, 1024:1024) = aten::t(%834), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.84, %835), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.49, %833, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %838 : Tensor = prim::GetAttr[name="bias"](%831)
  %839 : Tensor = prim::GetAttr[name="weight"](%831)
  %840 : Float(1024:1, 1024:1024) = aten::t(%839), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.84, %840), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.50, %838, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %843 : Tensor = prim::GetAttr[name="bias"](%830)
  %844 : Tensor = prim::GetAttr[name="weight"](%830)
  %845 : Float(1024:1, 1024:1024) = aten::t(%844), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.84, %845), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.51, %843, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %848 : int = aten::size(%x.49, %61), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %849 : int = aten::size(%x.49, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %850 : int[] = prim::ListConstruct(%848, %849, %62, %63), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.50 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.49, %850), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %852 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.50, %852), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %854 : int = aten::size(%x.51, %61), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %855 : int = aten::size(%x.51, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %856 : int[] = prim::ListConstruct(%854, %855, %62, %63), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.52 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.51, %856), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %858 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.52, %858), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %860 : int = aten::size(%x.53, %61), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %861 : int = aten::size(%x.53, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %862 : int[] = prim::ListConstruct(%860, %861, %62, %63), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.54 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.53, %862), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %864 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.54, %864), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %866 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.9, %66, %67), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %866), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.17, %68), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:259:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:262:0
  %input.86 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %66, %69), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.86, %71, %70), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:275:0
  %873 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %874 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.17, %873), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.18 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%874, %61), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %876 : int = aten::size(%context_layer.18, %61), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %877 : int = aten::size(%context_layer.18, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %878 : int[] = prim::ListConstruct(%876, %877, %72), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %input.87 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.18, %878), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_bert.py:279:0
  %880 : __torch__.torch.nn.modules.normalization.___torch_mangle_7556.LayerNorm = prim::GetAttr[name="LayerNorm"](%828)
  %881 : __torch__.torch.nn.modules.linear.___torch_mangle_7555.Linear = prim::GetAttr[name="dense"](%828)
  %882 : Tensor = prim::GetAttr[name="bias"](%881)
  %883 : Tensor = prim::GetAttr[name="weight"](%881)
  %884 : Float(1024:1, 1024:1024) = aten::t(%883), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.87, %884), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.88 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.52, %882, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.88, %71, %70), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.89 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.17, %input.84, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # transformers/modeling_bert.py:295:0
  %889 : Tensor = prim::GetAttr[name="bias"](%880)
  %890 : Tensor = prim::GetAttr[name="weight"](%880)
  %891 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.89, %891, %890, %889, %59, %58), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %893 : __torch__.torch.nn.modules.linear.___torch_mangle_7560.Linear = prim::GetAttr[name="dense"](%826)
  %894 : Tensor = prim::GetAttr[name="bias"](%893)
  %895 : Tensor = prim::GetAttr[name="weight"](%893)
  %896 : Float(1024:1, 4096:1024) = aten::t(%895), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.9, %896), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.90 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.53, %894, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.91 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.90), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %900 : __torch__.torch.nn.modules.normalization.___torch_mangle_7563.LayerNorm = prim::GetAttr[name="LayerNorm"](%825)
  %901 : __torch__.torch.nn.modules.linear.___torch_mangle_7562.Linear = prim::GetAttr[name="dense"](%825)
  %902 : Tensor = prim::GetAttr[name="bias"](%901)
  %903 : Tensor = prim::GetAttr[name="weight"](%901)
  %904 : Float(4096:1, 1024:4096) = aten::t(%903), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.91, %904), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.92 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.54, %902, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.92, %71, %70), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.93 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.18, %input_tensor.9, %60), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # transformers/modeling_bert.py:371:0
  %909 : Tensor = prim::GetAttr[name="bias"](%900)
  %910 : Tensor = prim::GetAttr[name="weight"](%900)
  %911 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm
  %input.94 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.93, %911, %910, %909, %59, %58), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %913 : __torch__.transformers.modeling_bert.___torch_mangle_7582.BertOutput = prim::GetAttr[name="output"](%102)
  %914 : __torch__.transformers.modeling_bert.___torch_mangle_7578.BertIntermediate = prim::GetAttr[name="intermediate"](%102)
  %915 : __torch__.transformers.modeling_bert.___torch_mangle_7576.BertAttention = prim::GetAttr[name="attention"](%102)
  %916 : __torch__.transformers.modeling_bert.___torch_mangle_7575.BertSelfOutput = prim::GetAttr[name="output"](%915)
  %917 : __torch__.transformers.modeling_bert.___torch_mangle_7571.BertSelfAttention = prim::GetAttr[name="self"](%915)
  %918 : __torch__.torch.nn.modules.linear.___torch_mangle_7569.Linear = prim::GetAttr[name="value"](%917)
  %919 : __torch__.torch.nn.modules.linear.___torch_mangle_7568.Linear = prim::GetAttr[name="key"](%917)
  %920 : __torch__.torch.nn.modules.linear.___torch_mangle_7567.Linear = prim::GetAttr[name="query"](%917)
  %921 : Tensor = prim::GetAttr[name="bias"](%920)
  %922 : Tensor = prim::GetAttr[name="weight"](%920)
  %923 : Float(1024:1, 1024:1024) = aten::t(%922), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.94, %923), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.55, %921, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %926 : Tensor = prim::GetAttr[name="bias"](%919)
  %927 : Tensor = prim::GetAttr[name="weight"](%919)
  %928 : Float(1024:1, 1024:1024) = aten::t(%927), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.94, %928), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.56, %926, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %931 : Tensor = prim::GetAttr[name="bias"](%918)
  %932 : Tensor = prim::GetAttr[name="weight"](%918)
  %933 : Float(1024:1, 1024:1024) = aten::t(%932), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.94, %933), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.57, %931, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %936 : int = aten::size(%x.55, %61), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %937 : int = aten::size(%x.55, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %938 : int[] = prim::ListConstruct(%936, %937, %62, %63), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.56 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.55, %938), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %940 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.56, %940), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %942 : int = aten::size(%x.57, %61), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %943 : int = aten::size(%x.57, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %944 : int[] = prim::ListConstruct(%942, %943, %62, %63), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.58 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.57, %944), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %946 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.58, %946), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %948 : int = aten::size(%x.59, %61), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %949 : int = aten::size(%x.59, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %950 : int[] = prim::ListConstruct(%948, %949, %62, %63), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.60 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.59, %950), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %952 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.60, %952), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %954 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.10, %66, %67), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %954), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.19, %68), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:259:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:262:0
  %input.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %66, %69), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.96, %71, %70), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:275:0
  %961 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %962 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.19, %961), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.20 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%962, %61), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %964 : int = aten::size(%context_layer.20, %61), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %965 : int = aten::size(%context_layer.20, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %966 : int[] = prim::ListConstruct(%964, %965, %72), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %input.97 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.20, %966), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_bert.py:279:0
  %968 : __torch__.torch.nn.modules.normalization.___torch_mangle_7573.LayerNorm = prim::GetAttr[name="LayerNorm"](%916)
  %969 : __torch__.torch.nn.modules.linear.___torch_mangle_7572.Linear = prim::GetAttr[name="dense"](%916)
  %970 : Tensor = prim::GetAttr[name="bias"](%969)
  %971 : Tensor = prim::GetAttr[name="weight"](%969)
  %972 : Float(1024:1, 1024:1024) = aten::t(%971), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.97, %972), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.98 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.58, %970, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.98, %71, %70), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.99 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.19, %input.94, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # transformers/modeling_bert.py:295:0
  %977 : Tensor = prim::GetAttr[name="bias"](%968)
  %978 : Tensor = prim::GetAttr[name="weight"](%968)
  %979 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.99, %979, %978, %977, %59, %58), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %981 : __torch__.torch.nn.modules.linear.___torch_mangle_7577.Linear = prim::GetAttr[name="dense"](%914)
  %982 : Tensor = prim::GetAttr[name="bias"](%981)
  %983 : Tensor = prim::GetAttr[name="weight"](%981)
  %984 : Float(1024:1, 4096:1024) = aten::t(%983), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.10, %984), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.100 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.59, %982, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.101 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.100), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %988 : __torch__.torch.nn.modules.normalization.___torch_mangle_7580.LayerNorm = prim::GetAttr[name="LayerNorm"](%913)
  %989 : __torch__.torch.nn.modules.linear.___torch_mangle_7579.Linear = prim::GetAttr[name="dense"](%913)
  %990 : Tensor = prim::GetAttr[name="bias"](%989)
  %991 : Tensor = prim::GetAttr[name="weight"](%989)
  %992 : Float(4096:1, 1024:4096) = aten::t(%991), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.101, %992), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.102 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.60, %990, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.102, %71, %70), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.103 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.20, %input_tensor.10, %60), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # transformers/modeling_bert.py:371:0
  %997 : Tensor = prim::GetAttr[name="bias"](%988)
  %998 : Tensor = prim::GetAttr[name="weight"](%988)
  %999 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm
  %input.104 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.103, %999, %998, %997, %59, %58), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %1001 : __torch__.transformers.modeling_bert.___torch_mangle_7599.BertOutput = prim::GetAttr[name="output"](%100)
  %1002 : __torch__.transformers.modeling_bert.___torch_mangle_7595.BertIntermediate = prim::GetAttr[name="intermediate"](%100)
  %1003 : __torch__.transformers.modeling_bert.___torch_mangle_7593.BertAttention = prim::GetAttr[name="attention"](%100)
  %1004 : __torch__.transformers.modeling_bert.___torch_mangle_7592.BertSelfOutput = prim::GetAttr[name="output"](%1003)
  %1005 : __torch__.transformers.modeling_bert.___torch_mangle_7588.BertSelfAttention = prim::GetAttr[name="self"](%1003)
  %1006 : __torch__.torch.nn.modules.linear.___torch_mangle_7586.Linear = prim::GetAttr[name="value"](%1005)
  %1007 : __torch__.torch.nn.modules.linear.___torch_mangle_7585.Linear = prim::GetAttr[name="key"](%1005)
  %1008 : __torch__.torch.nn.modules.linear.___torch_mangle_7584.Linear = prim::GetAttr[name="query"](%1005)
  %1009 : Tensor = prim::GetAttr[name="bias"](%1008)
  %1010 : Tensor = prim::GetAttr[name="weight"](%1008)
  %1011 : Float(1024:1, 1024:1024) = aten::t(%1010), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.104, %1011), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.61, %1009, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %1014 : Tensor = prim::GetAttr[name="bias"](%1007)
  %1015 : Tensor = prim::GetAttr[name="weight"](%1007)
  %1016 : Float(1024:1, 1024:1024) = aten::t(%1015), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.104, %1016), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.62, %1014, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %1019 : Tensor = prim::GetAttr[name="bias"](%1006)
  %1020 : Tensor = prim::GetAttr[name="weight"](%1006)
  %1021 : Float(1024:1, 1024:1024) = aten::t(%1020), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.104, %1021), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.63, %1019, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %1024 : int = aten::size(%x.61, %61), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1025 : int = aten::size(%x.61, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1026 : int[] = prim::ListConstruct(%1024, %1025, %62, %63), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.62 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.61, %1026), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1028 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.62, %1028), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1030 : int = aten::size(%x.63, %61), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1031 : int = aten::size(%x.63, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1032 : int[] = prim::ListConstruct(%1030, %1031, %62, %63), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.64 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.63, %1032), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1034 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.64, %1034), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1036 : int = aten::size(%x.65, %61), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1037 : int = aten::size(%x.65, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1038 : int[] = prim::ListConstruct(%1036, %1037, %62, %63), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.66 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.65, %1038), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1040 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.66, %1040), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1042 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.11, %66, %67), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1042), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.21, %68), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:259:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:262:0
  %input.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %66, %69), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.106, %71, %70), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:275:0
  %1049 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %1050 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.21, %1049), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.22 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1050, %61), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %1052 : int = aten::size(%context_layer.22, %61), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1053 : int = aten::size(%context_layer.22, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1054 : int[] = prim::ListConstruct(%1052, %1053, %72), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %input.107 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.22, %1054), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_bert.py:279:0
  %1056 : __torch__.torch.nn.modules.normalization.___torch_mangle_7590.LayerNorm = prim::GetAttr[name="LayerNorm"](%1004)
  %1057 : __torch__.torch.nn.modules.linear.___torch_mangle_7589.Linear = prim::GetAttr[name="dense"](%1004)
  %1058 : Tensor = prim::GetAttr[name="bias"](%1057)
  %1059 : Tensor = prim::GetAttr[name="weight"](%1057)
  %1060 : Float(1024:1, 1024:1024) = aten::t(%1059), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.107, %1060), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.108 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.64, %1058, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.108, %71, %70), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.109 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.21, %input.104, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # transformers/modeling_bert.py:295:0
  %1065 : Tensor = prim::GetAttr[name="bias"](%1056)
  %1066 : Tensor = prim::GetAttr[name="weight"](%1056)
  %1067 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.109, %1067, %1066, %1065, %59, %58), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1069 : __torch__.torch.nn.modules.linear.___torch_mangle_7594.Linear = prim::GetAttr[name="dense"](%1002)
  %1070 : Tensor = prim::GetAttr[name="bias"](%1069)
  %1071 : Tensor = prim::GetAttr[name="weight"](%1069)
  %1072 : Float(1024:1, 4096:1024) = aten::t(%1071), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.11, %1072), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.110 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.65, %1070, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.111 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.110), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1076 : __torch__.torch.nn.modules.normalization.___torch_mangle_7597.LayerNorm = prim::GetAttr[name="LayerNorm"](%1001)
  %1077 : __torch__.torch.nn.modules.linear.___torch_mangle_7596.Linear = prim::GetAttr[name="dense"](%1001)
  %1078 : Tensor = prim::GetAttr[name="bias"](%1077)
  %1079 : Tensor = prim::GetAttr[name="weight"](%1077)
  %1080 : Float(4096:1, 1024:4096) = aten::t(%1079), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.111, %1080), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.112 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.66, %1078, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.112, %71, %70), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.113 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.22, %input_tensor.11, %60), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # transformers/modeling_bert.py:371:0
  %1085 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1086 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1087 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm
  %input.114 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.113, %1087, %1086, %1085, %59, %58), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1089 : __torch__.transformers.modeling_bert.___torch_mangle_7616.BertOutput = prim::GetAttr[name="output"](%98)
  %1090 : __torch__.transformers.modeling_bert.___torch_mangle_7612.BertIntermediate = prim::GetAttr[name="intermediate"](%98)
  %1091 : __torch__.transformers.modeling_bert.___torch_mangle_7610.BertAttention = prim::GetAttr[name="attention"](%98)
  %1092 : __torch__.transformers.modeling_bert.___torch_mangle_7609.BertSelfOutput = prim::GetAttr[name="output"](%1091)
  %1093 : __torch__.transformers.modeling_bert.___torch_mangle_7605.BertSelfAttention = prim::GetAttr[name="self"](%1091)
  %1094 : __torch__.torch.nn.modules.linear.___torch_mangle_7603.Linear = prim::GetAttr[name="value"](%1093)
  %1095 : __torch__.torch.nn.modules.linear.___torch_mangle_7602.Linear = prim::GetAttr[name="key"](%1093)
  %1096 : __torch__.torch.nn.modules.linear.___torch_mangle_7601.Linear = prim::GetAttr[name="query"](%1093)
  %1097 : Tensor = prim::GetAttr[name="bias"](%1096)
  %1098 : Tensor = prim::GetAttr[name="weight"](%1096)
  %1099 : Float(1024:1, 1024:1024) = aten::t(%1098), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.114, %1099), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.67, %1097, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1102 : Tensor = prim::GetAttr[name="bias"](%1095)
  %1103 : Tensor = prim::GetAttr[name="weight"](%1095)
  %1104 : Float(1024:1, 1024:1024) = aten::t(%1103), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.114, %1104), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.68, %1102, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1107 : Tensor = prim::GetAttr[name="bias"](%1094)
  %1108 : Tensor = prim::GetAttr[name="weight"](%1094)
  %1109 : Float(1024:1, 1024:1024) = aten::t(%1108), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.114, %1109), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.69, %1107, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1112 : int = aten::size(%x.67, %61), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1113 : int = aten::size(%x.67, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1114 : int[] = prim::ListConstruct(%1112, %1113, %62, %63), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x.68 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.67, %1114), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1116 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %query_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.68, %1116), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1118 : int = aten::size(%x.69, %61), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1119 : int = aten::size(%x.69, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1120 : int[] = prim::ListConstruct(%1118, %1119, %62, %63), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x.70 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.69, %1120), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1122 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %key_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.70, %1122), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1124 : int = aten::size(%x.71, %61), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1125 : int = aten::size(%x.71, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1126 : int[] = prim::ListConstruct(%1124, %1125, %62, %63), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x.72 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.71, %1126), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1128 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %value_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.72, %1128), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1130 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.12, %66, %67), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.12, %1130), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.24 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.23, %68), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:259:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.24, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:262:0
  %input.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %66, %69), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.116, %71, %70), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.12, %value_layer.12), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:275:0
  %1137 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %1138 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.23, %1137), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.24 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1138, %61), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %1140 : int = aten::size(%context_layer.24, %61), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1141 : int = aten::size(%context_layer.24, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1142 : int[] = prim::ListConstruct(%1140, %1141, %72), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %input.117 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.24, %1142), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_bert.py:279:0
  %1144 : __torch__.torch.nn.modules.normalization.___torch_mangle_7607.LayerNorm = prim::GetAttr[name="LayerNorm"](%1092)
  %1145 : __torch__.torch.nn.modules.linear.___torch_mangle_7606.Linear = prim::GetAttr[name="dense"](%1092)
  %1146 : Tensor = prim::GetAttr[name="bias"](%1145)
  %1147 : Tensor = prim::GetAttr[name="weight"](%1145)
  %1148 : Float(1024:1, 1024:1024) = aten::t(%1147), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.117, %1148), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.118 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.70, %1146, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.118, %71, %70), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.119 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.23, %input.114, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # transformers/modeling_bert.py:295:0
  %1153 : Tensor = prim::GetAttr[name="bias"](%1144)
  %1154 : Tensor = prim::GetAttr[name="weight"](%1144)
  %1155 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm
  %input_tensor.12 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.119, %1155, %1154, %1153, %59, %58), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1157 : __torch__.torch.nn.modules.linear.___torch_mangle_7611.Linear = prim::GetAttr[name="dense"](%1090)
  %1158 : Tensor = prim::GetAttr[name="bias"](%1157)
  %1159 : Tensor = prim::GetAttr[name="weight"](%1157)
  %1160 : Float(1024:1, 4096:1024) = aten::t(%1159), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.12, %1160), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.120 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.71, %1158, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.121 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.120), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1164 : __torch__.torch.nn.modules.normalization.___torch_mangle_7614.LayerNorm = prim::GetAttr[name="LayerNorm"](%1089)
  %1165 : __torch__.torch.nn.modules.linear.___torch_mangle_7613.Linear = prim::GetAttr[name="dense"](%1089)
  %1166 : Tensor = prim::GetAttr[name="bias"](%1165)
  %1167 : Tensor = prim::GetAttr[name="weight"](%1165)
  %1168 : Float(4096:1, 1024:4096) = aten::t(%1167), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.121, %1168), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.122 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.72, %1166, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.24 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.122, %71, %70), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.123 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.24, %input_tensor.12, %60), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # transformers/modeling_bert.py:371:0
  %1173 : Tensor = prim::GetAttr[name="bias"](%1164)
  %1174 : Tensor = prim::GetAttr[name="weight"](%1164)
  %1175 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm
  %input.124 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.123, %1175, %1174, %1173, %59, %58), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1177 : __torch__.transformers.modeling_bert.___torch_mangle_7633.BertOutput = prim::GetAttr[name="output"](%96)
  %1178 : __torch__.transformers.modeling_bert.___torch_mangle_7629.BertIntermediate = prim::GetAttr[name="intermediate"](%96)
  %1179 : __torch__.transformers.modeling_bert.___torch_mangle_7627.BertAttention = prim::GetAttr[name="attention"](%96)
  %1180 : __torch__.transformers.modeling_bert.___torch_mangle_7626.BertSelfOutput = prim::GetAttr[name="output"](%1179)
  %1181 : __torch__.transformers.modeling_bert.___torch_mangle_7622.BertSelfAttention = prim::GetAttr[name="self"](%1179)
  %1182 : __torch__.torch.nn.modules.linear.___torch_mangle_7620.Linear = prim::GetAttr[name="value"](%1181)
  %1183 : __torch__.torch.nn.modules.linear.___torch_mangle_7619.Linear = prim::GetAttr[name="key"](%1181)
  %1184 : __torch__.torch.nn.modules.linear.___torch_mangle_7618.Linear = prim::GetAttr[name="query"](%1181)
  %1185 : Tensor = prim::GetAttr[name="bias"](%1184)
  %1186 : Tensor = prim::GetAttr[name="weight"](%1184)
  %1187 : Float(1024:1, 1024:1024) = aten::t(%1186), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.query # torch/nn/functional.py:1676:0
  %output.73 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.124, %1187), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.query # torch/nn/functional.py:1676:0
  %x.73 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.73, %1185, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.query # torch/nn/functional.py:1678:0
  %1190 : Tensor = prim::GetAttr[name="bias"](%1183)
  %1191 : Tensor = prim::GetAttr[name="weight"](%1183)
  %1192 : Float(1024:1, 1024:1024) = aten::t(%1191), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.key # torch/nn/functional.py:1676:0
  %output.74 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.124, %1192), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.key # torch/nn/functional.py:1676:0
  %x.75 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.74, %1190, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.key # torch/nn/functional.py:1678:0
  %1195 : Tensor = prim::GetAttr[name="bias"](%1182)
  %1196 : Tensor = prim::GetAttr[name="weight"](%1182)
  %1197 : Float(1024:1, 1024:1024) = aten::t(%1196), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.value # torch/nn/functional.py:1676:0
  %output.75 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.124, %1197), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.value # torch/nn/functional.py:1676:0
  %x.77 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.75, %1195, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.value # torch/nn/functional.py:1678:0
  %1200 : int = aten::size(%x.73, %61), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1201 : int = aten::size(%x.73, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1202 : int[] = prim::ListConstruct(%1200, %1201, %62, %63), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
  %x.74 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.73, %1202), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
  %1204 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
  %query_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.74, %1204), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
  %1206 : int = aten::size(%x.75, %61), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1207 : int = aten::size(%x.75, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1208 : int[] = prim::ListConstruct(%1206, %1207, %62, %63), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
  %x.76 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.75, %1208), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
  %1210 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
  %key_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.76, %1210), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
  %1212 : int = aten::size(%x.77, %61), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1213 : int = aten::size(%x.77, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1214 : int[] = prim::ListConstruct(%1212, %1213, %62, %63), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
  %x.78 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.77, %1214), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
  %1216 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
  %value_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.78, %1216), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
  %1218 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.13, %66, %67), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.13, %1218), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.25, %68), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:259:0
  %input.125 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.26, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:262:0
  %input.126 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.125, %66, %69), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.126, %71, %70), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self/__module.encoder.layer.12.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.25 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.13, %value_layer.13), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:275:0
  %1225 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
  %1226 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.25, %1225), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.26 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1226, %61), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
  %1228 : int = aten::size(%context_layer.26, %61), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:278:0
  %1229 : int = aten::size(%context_layer.26, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:278:0
  %1230 : int[] = prim::ListConstruct(%1228, %1229, %72), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self
  %input.127 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.26, %1230), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.self # transformers/modeling_bert.py:279:0
  %1232 : __torch__.torch.nn.modules.normalization.___torch_mangle_7624.LayerNorm = prim::GetAttr[name="LayerNorm"](%1180)
  %1233 : __torch__.torch.nn.modules.linear.___torch_mangle_7623.Linear = prim::GetAttr[name="dense"](%1180)
  %1234 : Tensor = prim::GetAttr[name="bias"](%1233)
  %1235 : Tensor = prim::GetAttr[name="weight"](%1233)
  %1236 : Float(1024:1, 1024:1024) = aten::t(%1235), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1676:0
  %output.76 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.127, %1236), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1676:0
  %input.128 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.76, %1234, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.25 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.128, %71, %70), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.dropout # torch/nn/functional.py:973:0
  %input.129 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.25, %input.124, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output # transformers/modeling_bert.py:295:0
  %1241 : Tensor = prim::GetAttr[name="bias"](%1232)
  %1242 : Tensor = prim::GetAttr[name="weight"](%1232)
  %1243 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.LayerNorm
  %input_tensor.13 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.129, %1243, %1242, %1241, %59, %58), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.attention/__module.encoder.layer.12.attention.output/__module.encoder.layer.12.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1245 : __torch__.torch.nn.modules.linear.___torch_mangle_7628.Linear = prim::GetAttr[name="dense"](%1178)
  %1246 : Tensor = prim::GetAttr[name="bias"](%1245)
  %1247 : Tensor = prim::GetAttr[name="weight"](%1245)
  %1248 : Float(1024:1, 4096:1024) = aten::t(%1247), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate/__module.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1676:0
  %output.77 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.13, %1248), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate/__module.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1676:0
  %input.130 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.77, %1246, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate/__module.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1678:0
  %input.131 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.130), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.intermediate # torch/nn/functional.py:1369:0
  %1252 : __torch__.torch.nn.modules.normalization.___torch_mangle_7631.LayerNorm = prim::GetAttr[name="LayerNorm"](%1177)
  %1253 : __torch__.torch.nn.modules.linear.___torch_mangle_7630.Linear = prim::GetAttr[name="dense"](%1177)
  %1254 : Tensor = prim::GetAttr[name="bias"](%1253)
  %1255 : Tensor = prim::GetAttr[name="weight"](%1253)
  %1256 : Float(4096:1, 1024:4096) = aten::t(%1255), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.dense # torch/nn/functional.py:1676:0
  %output.78 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.131, %1256), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.dense # torch/nn/functional.py:1676:0
  %input.132 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.78, %1254, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.26 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.132, %71, %70), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.dropout # torch/nn/functional.py:973:0
  %input.133 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.26, %input_tensor.13, %60), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output # transformers/modeling_bert.py:371:0
  %1261 : Tensor = prim::GetAttr[name="bias"](%1252)
  %1262 : Tensor = prim::GetAttr[name="weight"](%1252)
  %1263 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.LayerNorm
  %input.134 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.133, %1263, %1262, %1261, %59, %58), scope: __module.encoder/__module.encoder.layer.12/__module.encoder.layer.12.output/__module.encoder.layer.12.output.LayerNorm # torch/nn/functional.py:2048:0
  %1265 : __torch__.transformers.modeling_bert.___torch_mangle_7650.BertOutput = prim::GetAttr[name="output"](%94)
  %1266 : __torch__.transformers.modeling_bert.___torch_mangle_7646.BertIntermediate = prim::GetAttr[name="intermediate"](%94)
  %1267 : __torch__.transformers.modeling_bert.___torch_mangle_7644.BertAttention = prim::GetAttr[name="attention"](%94)
  %1268 : __torch__.transformers.modeling_bert.___torch_mangle_7643.BertSelfOutput = prim::GetAttr[name="output"](%1267)
  %1269 : __torch__.transformers.modeling_bert.___torch_mangle_7639.BertSelfAttention = prim::GetAttr[name="self"](%1267)
  %1270 : __torch__.torch.nn.modules.linear.___torch_mangle_7637.Linear = prim::GetAttr[name="value"](%1269)
  %1271 : __torch__.torch.nn.modules.linear.___torch_mangle_7636.Linear = prim::GetAttr[name="key"](%1269)
  %1272 : __torch__.torch.nn.modules.linear.___torch_mangle_7635.Linear = prim::GetAttr[name="query"](%1269)
  %1273 : Tensor = prim::GetAttr[name="bias"](%1272)
  %1274 : Tensor = prim::GetAttr[name="weight"](%1272)
  %1275 : Float(1024:1, 1024:1024) = aten::t(%1274), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.query # torch/nn/functional.py:1676:0
  %output.79 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.134, %1275), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.query # torch/nn/functional.py:1676:0
  %x.79 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.79, %1273, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.query # torch/nn/functional.py:1678:0
  %1278 : Tensor = prim::GetAttr[name="bias"](%1271)
  %1279 : Tensor = prim::GetAttr[name="weight"](%1271)
  %1280 : Float(1024:1, 1024:1024) = aten::t(%1279), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.key # torch/nn/functional.py:1676:0
  %output.80 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.134, %1280), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.key # torch/nn/functional.py:1676:0
  %x.81 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.80, %1278, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.key # torch/nn/functional.py:1678:0
  %1283 : Tensor = prim::GetAttr[name="bias"](%1270)
  %1284 : Tensor = prim::GetAttr[name="weight"](%1270)
  %1285 : Float(1024:1, 1024:1024) = aten::t(%1284), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.value # torch/nn/functional.py:1676:0
  %output.81 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.134, %1285), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.value # torch/nn/functional.py:1676:0
  %x.83 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.81, %1283, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.value # torch/nn/functional.py:1678:0
  %1288 : int = aten::size(%x.79, %61), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1289 : int = aten::size(%x.79, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1290 : int[] = prim::ListConstruct(%1288, %1289, %62, %63), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
  %x.80 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.79, %1290), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
  %1292 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
  %query_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.80, %1292), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
  %1294 : int = aten::size(%x.81, %61), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1295 : int = aten::size(%x.81, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1296 : int[] = prim::ListConstruct(%1294, %1295, %62, %63), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
  %x.82 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.81, %1296), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
  %1298 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
  %key_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.82, %1298), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
  %1300 : int = aten::size(%x.83, %61), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1301 : int = aten::size(%x.83, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1302 : int[] = prim::ListConstruct(%1300, %1301, %62, %63), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
  %x.84 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.83, %1302), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
  %1304 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
  %value_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.84, %1304), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
  %1306 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.14, %66, %67), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.27 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.14, %1306), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.28 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.27, %68), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:259:0
  %input.135 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.28, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:262:0
  %input.136 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.135, %66, %69), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.136, %71, %70), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self/__module.encoder.layer.13.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.27 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.14, %value_layer.14), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:275:0
  %1313 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
  %1314 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.27, %1313), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.28 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1314, %61), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
  %1316 : int = aten::size(%context_layer.28, %61), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:278:0
  %1317 : int = aten::size(%context_layer.28, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:278:0
  %1318 : int[] = prim::ListConstruct(%1316, %1317, %72), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self
  %input.137 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.28, %1318), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.self # transformers/modeling_bert.py:279:0
  %1320 : __torch__.torch.nn.modules.normalization.___torch_mangle_7641.LayerNorm = prim::GetAttr[name="LayerNorm"](%1268)
  %1321 : __torch__.torch.nn.modules.linear.___torch_mangle_7640.Linear = prim::GetAttr[name="dense"](%1268)
  %1322 : Tensor = prim::GetAttr[name="bias"](%1321)
  %1323 : Tensor = prim::GetAttr[name="weight"](%1321)
  %1324 : Float(1024:1, 1024:1024) = aten::t(%1323), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1676:0
  %output.82 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.137, %1324), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1676:0
  %input.138 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.82, %1322, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.27 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.138, %71, %70), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.dropout # torch/nn/functional.py:973:0
  %input.139 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.27, %input.134, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output # transformers/modeling_bert.py:295:0
  %1329 : Tensor = prim::GetAttr[name="bias"](%1320)
  %1330 : Tensor = prim::GetAttr[name="weight"](%1320)
  %1331 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.LayerNorm
  %input_tensor.14 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.139, %1331, %1330, %1329, %59, %58), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.attention/__module.encoder.layer.13.attention.output/__module.encoder.layer.13.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1333 : __torch__.torch.nn.modules.linear.___torch_mangle_7645.Linear = prim::GetAttr[name="dense"](%1266)
  %1334 : Tensor = prim::GetAttr[name="bias"](%1333)
  %1335 : Tensor = prim::GetAttr[name="weight"](%1333)
  %1336 : Float(1024:1, 4096:1024) = aten::t(%1335), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate/__module.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1676:0
  %output.83 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.14, %1336), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate/__module.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1676:0
  %input.140 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.83, %1334, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate/__module.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1678:0
  %input.141 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.140), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.intermediate # torch/nn/functional.py:1369:0
  %1340 : __torch__.torch.nn.modules.normalization.___torch_mangle_7648.LayerNorm = prim::GetAttr[name="LayerNorm"](%1265)
  %1341 : __torch__.torch.nn.modules.linear.___torch_mangle_7647.Linear = prim::GetAttr[name="dense"](%1265)
  %1342 : Tensor = prim::GetAttr[name="bias"](%1341)
  %1343 : Tensor = prim::GetAttr[name="weight"](%1341)
  %1344 : Float(4096:1, 1024:4096) = aten::t(%1343), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.dense # torch/nn/functional.py:1676:0
  %output.84 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.141, %1344), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.dense # torch/nn/functional.py:1676:0
  %input.142 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.84, %1342, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.28 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.142, %71, %70), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.dropout # torch/nn/functional.py:973:0
  %input.143 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.28, %input_tensor.14, %60), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output # transformers/modeling_bert.py:371:0
  %1349 : Tensor = prim::GetAttr[name="bias"](%1340)
  %1350 : Tensor = prim::GetAttr[name="weight"](%1340)
  %1351 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.LayerNorm
  %input.144 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.143, %1351, %1350, %1349, %59, %58), scope: __module.encoder/__module.encoder.layer.13/__module.encoder.layer.13.output/__module.encoder.layer.13.output.LayerNorm # torch/nn/functional.py:2048:0
  %1353 : __torch__.transformers.modeling_bert.___torch_mangle_7667.BertOutput = prim::GetAttr[name="output"](%92)
  %1354 : __torch__.transformers.modeling_bert.___torch_mangle_7663.BertIntermediate = prim::GetAttr[name="intermediate"](%92)
  %1355 : __torch__.transformers.modeling_bert.___torch_mangle_7661.BertAttention = prim::GetAttr[name="attention"](%92)
  %1356 : __torch__.transformers.modeling_bert.___torch_mangle_7660.BertSelfOutput = prim::GetAttr[name="output"](%1355)
  %1357 : __torch__.transformers.modeling_bert.___torch_mangle_7656.BertSelfAttention = prim::GetAttr[name="self"](%1355)
  %1358 : __torch__.torch.nn.modules.linear.___torch_mangle_7654.Linear = prim::GetAttr[name="value"](%1357)
  %1359 : __torch__.torch.nn.modules.linear.___torch_mangle_7653.Linear = prim::GetAttr[name="key"](%1357)
  %1360 : __torch__.torch.nn.modules.linear.___torch_mangle_7652.Linear = prim::GetAttr[name="query"](%1357)
  %1361 : Tensor = prim::GetAttr[name="bias"](%1360)
  %1362 : Tensor = prim::GetAttr[name="weight"](%1360)
  %1363 : Float(1024:1, 1024:1024) = aten::t(%1362), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.query # torch/nn/functional.py:1676:0
  %output.85 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.144, %1363), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.query # torch/nn/functional.py:1676:0
  %x.85 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.85, %1361, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.query # torch/nn/functional.py:1678:0
  %1366 : Tensor = prim::GetAttr[name="bias"](%1359)
  %1367 : Tensor = prim::GetAttr[name="weight"](%1359)
  %1368 : Float(1024:1, 1024:1024) = aten::t(%1367), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.key # torch/nn/functional.py:1676:0
  %output.86 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.144, %1368), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.key # torch/nn/functional.py:1676:0
  %x.87 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.86, %1366, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.key # torch/nn/functional.py:1678:0
  %1371 : Tensor = prim::GetAttr[name="bias"](%1358)
  %1372 : Tensor = prim::GetAttr[name="weight"](%1358)
  %1373 : Float(1024:1, 1024:1024) = aten::t(%1372), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.value # torch/nn/functional.py:1676:0
  %output.87 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.144, %1373), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.value # torch/nn/functional.py:1676:0
  %x.89 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.87, %1371, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.value # torch/nn/functional.py:1678:0
  %1376 : int = aten::size(%x.85, %61), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1377 : int = aten::size(%x.85, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1378 : int[] = prim::ListConstruct(%1376, %1377, %62, %63), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
  %x.86 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.85, %1378), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
  %1380 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
  %query_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.86, %1380), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
  %1382 : int = aten::size(%x.87, %61), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1383 : int = aten::size(%x.87, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1384 : int[] = prim::ListConstruct(%1382, %1383, %62, %63), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
  %x.88 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.87, %1384), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
  %1386 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
  %key_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.88, %1386), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
  %1388 : int = aten::size(%x.89, %61), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1389 : int = aten::size(%x.89, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1390 : int[] = prim::ListConstruct(%1388, %1389, %62, %63), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
  %x.90 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.89, %1390), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
  %1392 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
  %value_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.90, %1392), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
  %1394 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.15, %66, %67), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.29 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.15, %1394), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.29, %68), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:259:0
  %input.145 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.30, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:262:0
  %input.146 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.145, %66, %69), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.146, %71, %70), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self/__module.encoder.layer.14.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.29 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.15, %value_layer.15), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:275:0
  %1401 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
  %1402 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.29, %1401), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.30 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1402, %61), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
  %1404 : int = aten::size(%context_layer.30, %61), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:278:0
  %1405 : int = aten::size(%context_layer.30, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:278:0
  %1406 : int[] = prim::ListConstruct(%1404, %1405, %72), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self
  %input.147 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.30, %1406), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.self # transformers/modeling_bert.py:279:0
  %1408 : __torch__.torch.nn.modules.normalization.___torch_mangle_7658.LayerNorm = prim::GetAttr[name="LayerNorm"](%1356)
  %1409 : __torch__.torch.nn.modules.linear.___torch_mangle_7657.Linear = prim::GetAttr[name="dense"](%1356)
  %1410 : Tensor = prim::GetAttr[name="bias"](%1409)
  %1411 : Tensor = prim::GetAttr[name="weight"](%1409)
  %1412 : Float(1024:1, 1024:1024) = aten::t(%1411), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1676:0
  %output.88 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.147, %1412), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1676:0
  %input.148 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.88, %1410, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.29 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.148, %71, %70), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.dropout # torch/nn/functional.py:973:0
  %input.149 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.29, %input.144, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output # transformers/modeling_bert.py:295:0
  %1417 : Tensor = prim::GetAttr[name="bias"](%1408)
  %1418 : Tensor = prim::GetAttr[name="weight"](%1408)
  %1419 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.LayerNorm
  %input_tensor.15 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.149, %1419, %1418, %1417, %59, %58), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.attention/__module.encoder.layer.14.attention.output/__module.encoder.layer.14.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1421 : __torch__.torch.nn.modules.linear.___torch_mangle_7662.Linear = prim::GetAttr[name="dense"](%1354)
  %1422 : Tensor = prim::GetAttr[name="bias"](%1421)
  %1423 : Tensor = prim::GetAttr[name="weight"](%1421)
  %1424 : Float(1024:1, 4096:1024) = aten::t(%1423), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate/__module.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1676:0
  %output.89 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.15, %1424), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate/__module.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1676:0
  %input.150 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.89, %1422, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate/__module.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1678:0
  %input.151 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.150), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.intermediate # torch/nn/functional.py:1369:0
  %1428 : __torch__.torch.nn.modules.normalization.___torch_mangle_7665.LayerNorm = prim::GetAttr[name="LayerNorm"](%1353)
  %1429 : __torch__.torch.nn.modules.linear.___torch_mangle_7664.Linear = prim::GetAttr[name="dense"](%1353)
  %1430 : Tensor = prim::GetAttr[name="bias"](%1429)
  %1431 : Tensor = prim::GetAttr[name="weight"](%1429)
  %1432 : Float(4096:1, 1024:4096) = aten::t(%1431), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.dense # torch/nn/functional.py:1676:0
  %output.90 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.151, %1432), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.dense # torch/nn/functional.py:1676:0
  %input.152 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.90, %1430, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.30 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.152, %71, %70), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.dropout # torch/nn/functional.py:973:0
  %input.153 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.30, %input_tensor.15, %60), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output # transformers/modeling_bert.py:371:0
  %1437 : Tensor = prim::GetAttr[name="bias"](%1428)
  %1438 : Tensor = prim::GetAttr[name="weight"](%1428)
  %1439 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.LayerNorm
  %input.154 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.153, %1439, %1438, %1437, %59, %58), scope: __module.encoder/__module.encoder.layer.14/__module.encoder.layer.14.output/__module.encoder.layer.14.output.LayerNorm # torch/nn/functional.py:2048:0
  %1441 : __torch__.transformers.modeling_bert.___torch_mangle_7684.BertOutput = prim::GetAttr[name="output"](%90)
  %1442 : __torch__.transformers.modeling_bert.___torch_mangle_7680.BertIntermediate = prim::GetAttr[name="intermediate"](%90)
  %1443 : __torch__.transformers.modeling_bert.___torch_mangle_7678.BertAttention = prim::GetAttr[name="attention"](%90)
  %1444 : __torch__.transformers.modeling_bert.___torch_mangle_7677.BertSelfOutput = prim::GetAttr[name="output"](%1443)
  %1445 : __torch__.transformers.modeling_bert.___torch_mangle_7673.BertSelfAttention = prim::GetAttr[name="self"](%1443)
  %1446 : __torch__.torch.nn.modules.linear.___torch_mangle_7671.Linear = prim::GetAttr[name="value"](%1445)
  %1447 : __torch__.torch.nn.modules.linear.___torch_mangle_7670.Linear = prim::GetAttr[name="key"](%1445)
  %1448 : __torch__.torch.nn.modules.linear.___torch_mangle_7669.Linear = prim::GetAttr[name="query"](%1445)
  %1449 : Tensor = prim::GetAttr[name="bias"](%1448)
  %1450 : Tensor = prim::GetAttr[name="weight"](%1448)
  %1451 : Float(1024:1, 1024:1024) = aten::t(%1450), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.query # torch/nn/functional.py:1676:0
  %output.91 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.154, %1451), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.query # torch/nn/functional.py:1676:0
  %x.91 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.91, %1449, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.query # torch/nn/functional.py:1678:0
  %1454 : Tensor = prim::GetAttr[name="bias"](%1447)
  %1455 : Tensor = prim::GetAttr[name="weight"](%1447)
  %1456 : Float(1024:1, 1024:1024) = aten::t(%1455), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.key # torch/nn/functional.py:1676:0
  %output.92 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.154, %1456), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.key # torch/nn/functional.py:1676:0
  %x.93 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.92, %1454, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.key # torch/nn/functional.py:1678:0
  %1459 : Tensor = prim::GetAttr[name="bias"](%1446)
  %1460 : Tensor = prim::GetAttr[name="weight"](%1446)
  %1461 : Float(1024:1, 1024:1024) = aten::t(%1460), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.value # torch/nn/functional.py:1676:0
  %output.93 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.154, %1461), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.value # torch/nn/functional.py:1676:0
  %x.95 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.93, %1459, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.value # torch/nn/functional.py:1678:0
  %1464 : int = aten::size(%x.91, %61), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1465 : int = aten::size(%x.91, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1466 : int[] = prim::ListConstruct(%1464, %1465, %62, %63), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
  %x.92 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.91, %1466), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
  %1468 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
  %query_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.92, %1468), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
  %1470 : int = aten::size(%x.93, %61), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1471 : int = aten::size(%x.93, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1472 : int[] = prim::ListConstruct(%1470, %1471, %62, %63), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
  %x.94 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.93, %1472), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
  %1474 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
  %key_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.94, %1474), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
  %1476 : int = aten::size(%x.95, %61), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1477 : int = aten::size(%x.95, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1478 : int[] = prim::ListConstruct(%1476, %1477, %62, %63), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
  %x.96 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.95, %1478), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
  %1480 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
  %value_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.96, %1480), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
  %1482 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.16, %66, %67), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.16, %1482), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.32 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.31, %68), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:259:0
  %input.155 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.32, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:262:0
  %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.155, %66, %69), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.156, %71, %70), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self/__module.encoder.layer.15.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.31 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.16, %value_layer.16), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:275:0
  %1489 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
  %1490 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.31, %1489), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.32 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1490, %61), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
  %1492 : int = aten::size(%context_layer.32, %61), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:278:0
  %1493 : int = aten::size(%context_layer.32, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:278:0
  %1494 : int[] = prim::ListConstruct(%1492, %1493, %72), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self
  %input.157 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.32, %1494), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.self # transformers/modeling_bert.py:279:0
  %1496 : __torch__.torch.nn.modules.normalization.___torch_mangle_7675.LayerNorm = prim::GetAttr[name="LayerNorm"](%1444)
  %1497 : __torch__.torch.nn.modules.linear.___torch_mangle_7674.Linear = prim::GetAttr[name="dense"](%1444)
  %1498 : Tensor = prim::GetAttr[name="bias"](%1497)
  %1499 : Tensor = prim::GetAttr[name="weight"](%1497)
  %1500 : Float(1024:1, 1024:1024) = aten::t(%1499), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1676:0
  %output.94 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.157, %1500), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1676:0
  %input.158 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.94, %1498, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.31 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.158, %71, %70), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.dropout # torch/nn/functional.py:973:0
  %input.159 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.31, %input.154, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output # transformers/modeling_bert.py:295:0
  %1505 : Tensor = prim::GetAttr[name="bias"](%1496)
  %1506 : Tensor = prim::GetAttr[name="weight"](%1496)
  %1507 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.LayerNorm
  %input_tensor.16 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.159, %1507, %1506, %1505, %59, %58), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.attention/__module.encoder.layer.15.attention.output/__module.encoder.layer.15.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1509 : __torch__.torch.nn.modules.linear.___torch_mangle_7679.Linear = prim::GetAttr[name="dense"](%1442)
  %1510 : Tensor = prim::GetAttr[name="bias"](%1509)
  %1511 : Tensor = prim::GetAttr[name="weight"](%1509)
  %1512 : Float(1024:1, 4096:1024) = aten::t(%1511), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate/__module.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1676:0
  %output.95 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.16, %1512), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate/__module.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1676:0
  %input.160 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.95, %1510, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate/__module.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1678:0
  %input.161 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.160), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.intermediate # torch/nn/functional.py:1369:0
  %1516 : __torch__.torch.nn.modules.normalization.___torch_mangle_7682.LayerNorm = prim::GetAttr[name="LayerNorm"](%1441)
  %1517 : __torch__.torch.nn.modules.linear.___torch_mangle_7681.Linear = prim::GetAttr[name="dense"](%1441)
  %1518 : Tensor = prim::GetAttr[name="bias"](%1517)
  %1519 : Tensor = prim::GetAttr[name="weight"](%1517)
  %1520 : Float(4096:1, 1024:4096) = aten::t(%1519), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.dense # torch/nn/functional.py:1676:0
  %output.96 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.161, %1520), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.dense # torch/nn/functional.py:1676:0
  %input.162 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.96, %1518, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.32 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.162, %71, %70), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.dropout # torch/nn/functional.py:973:0
  %input.163 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.32, %input_tensor.16, %60), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output # transformers/modeling_bert.py:371:0
  %1525 : Tensor = prim::GetAttr[name="bias"](%1516)
  %1526 : Tensor = prim::GetAttr[name="weight"](%1516)
  %1527 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.LayerNorm
  %input.164 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.163, %1527, %1526, %1525, %59, %58), scope: __module.encoder/__module.encoder.layer.15/__module.encoder.layer.15.output/__module.encoder.layer.15.output.LayerNorm # torch/nn/functional.py:2048:0
  %1529 : __torch__.transformers.modeling_bert.___torch_mangle_7701.BertOutput = prim::GetAttr[name="output"](%88)
  %1530 : __torch__.transformers.modeling_bert.___torch_mangle_7697.BertIntermediate = prim::GetAttr[name="intermediate"](%88)
  %1531 : __torch__.transformers.modeling_bert.___torch_mangle_7695.BertAttention = prim::GetAttr[name="attention"](%88)
  %1532 : __torch__.transformers.modeling_bert.___torch_mangle_7694.BertSelfOutput = prim::GetAttr[name="output"](%1531)
  %1533 : __torch__.transformers.modeling_bert.___torch_mangle_7690.BertSelfAttention = prim::GetAttr[name="self"](%1531)
  %1534 : __torch__.torch.nn.modules.linear.___torch_mangle_7688.Linear = prim::GetAttr[name="value"](%1533)
  %1535 : __torch__.torch.nn.modules.linear.___torch_mangle_7687.Linear = prim::GetAttr[name="key"](%1533)
  %1536 : __torch__.torch.nn.modules.linear.___torch_mangle_7686.Linear = prim::GetAttr[name="query"](%1533)
  %1537 : Tensor = prim::GetAttr[name="bias"](%1536)
  %1538 : Tensor = prim::GetAttr[name="weight"](%1536)
  %1539 : Float(1024:1, 1024:1024) = aten::t(%1538), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.query # torch/nn/functional.py:1676:0
  %output.97 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.164, %1539), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.query # torch/nn/functional.py:1676:0
  %x.97 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.97, %1537, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.query # torch/nn/functional.py:1678:0
  %1542 : Tensor = prim::GetAttr[name="bias"](%1535)
  %1543 : Tensor = prim::GetAttr[name="weight"](%1535)
  %1544 : Float(1024:1, 1024:1024) = aten::t(%1543), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.key # torch/nn/functional.py:1676:0
  %output.98 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.164, %1544), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.key # torch/nn/functional.py:1676:0
  %x.99 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.98, %1542, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.key # torch/nn/functional.py:1678:0
  %1547 : Tensor = prim::GetAttr[name="bias"](%1534)
  %1548 : Tensor = prim::GetAttr[name="weight"](%1534)
  %1549 : Float(1024:1, 1024:1024) = aten::t(%1548), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.value # torch/nn/functional.py:1676:0
  %output.99 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.164, %1549), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.value # torch/nn/functional.py:1676:0
  %x.101 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.99, %1547, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.value # torch/nn/functional.py:1678:0
  %1552 : int = aten::size(%x.97, %61), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1553 : int = aten::size(%x.97, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1554 : int[] = prim::ListConstruct(%1552, %1553, %62, %63), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
  %x.98 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.97, %1554), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
  %1556 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
  %query_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.98, %1556), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
  %1558 : int = aten::size(%x.99, %61), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1559 : int = aten::size(%x.99, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1560 : int[] = prim::ListConstruct(%1558, %1559, %62, %63), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
  %x.100 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.99, %1560), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
  %1562 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
  %key_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.100, %1562), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
  %1564 : int = aten::size(%x.101, %61), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1565 : int = aten::size(%x.101, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1566 : int[] = prim::ListConstruct(%1564, %1565, %62, %63), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
  %x.102 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.101, %1566), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
  %1568 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
  %value_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.102, %1568), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
  %1570 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.17, %66, %67), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.33 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.17, %1570), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.34 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.33, %68), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:259:0
  %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.34, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:262:0
  %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %66, %69), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %71, %70), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self/__module.encoder.layer.16.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.33 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.17, %value_layer.17), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:275:0
  %1577 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
  %1578 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.33, %1577), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.34 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1578, %61), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
  %1580 : int = aten::size(%context_layer.34, %61), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:278:0
  %1581 : int = aten::size(%context_layer.34, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:278:0
  %1582 : int[] = prim::ListConstruct(%1580, %1581, %72), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self
  %input.167 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.34, %1582), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.self # transformers/modeling_bert.py:279:0
  %1584 : __torch__.torch.nn.modules.normalization.___torch_mangle_7692.LayerNorm = prim::GetAttr[name="LayerNorm"](%1532)
  %1585 : __torch__.torch.nn.modules.linear.___torch_mangle_7691.Linear = prim::GetAttr[name="dense"](%1532)
  %1586 : Tensor = prim::GetAttr[name="bias"](%1585)
  %1587 : Tensor = prim::GetAttr[name="weight"](%1585)
  %1588 : Float(1024:1, 1024:1024) = aten::t(%1587), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1676:0
  %output.100 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.167, %1588), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1676:0
  %input.168 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.100, %1586, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.33 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.168, %71, %70), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.dropout # torch/nn/functional.py:973:0
  %input.169 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.33, %input.164, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output # transformers/modeling_bert.py:295:0
  %1593 : Tensor = prim::GetAttr[name="bias"](%1584)
  %1594 : Tensor = prim::GetAttr[name="weight"](%1584)
  %1595 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.LayerNorm
  %input_tensor.17 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.169, %1595, %1594, %1593, %59, %58), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.attention/__module.encoder.layer.16.attention.output/__module.encoder.layer.16.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1597 : __torch__.torch.nn.modules.linear.___torch_mangle_7696.Linear = prim::GetAttr[name="dense"](%1530)
  %1598 : Tensor = prim::GetAttr[name="bias"](%1597)
  %1599 : Tensor = prim::GetAttr[name="weight"](%1597)
  %1600 : Float(1024:1, 4096:1024) = aten::t(%1599), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate/__module.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1676:0
  %output.101 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.17, %1600), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate/__module.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1676:0
  %input.170 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.101, %1598, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate/__module.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1678:0
  %input.171 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.170), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.intermediate # torch/nn/functional.py:1369:0
  %1604 : __torch__.torch.nn.modules.normalization.___torch_mangle_7699.LayerNorm = prim::GetAttr[name="LayerNorm"](%1529)
  %1605 : __torch__.torch.nn.modules.linear.___torch_mangle_7698.Linear = prim::GetAttr[name="dense"](%1529)
  %1606 : Tensor = prim::GetAttr[name="bias"](%1605)
  %1607 : Tensor = prim::GetAttr[name="weight"](%1605)
  %1608 : Float(4096:1, 1024:4096) = aten::t(%1607), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.dense # torch/nn/functional.py:1676:0
  %output.102 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.171, %1608), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.dense # torch/nn/functional.py:1676:0
  %input.172 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.102, %1606, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.34 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.172, %71, %70), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.dropout # torch/nn/functional.py:973:0
  %input.173 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.34, %input_tensor.17, %60), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output # transformers/modeling_bert.py:371:0
  %1613 : Tensor = prim::GetAttr[name="bias"](%1604)
  %1614 : Tensor = prim::GetAttr[name="weight"](%1604)
  %1615 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.LayerNorm
  %input.174 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.173, %1615, %1614, %1613, %59, %58), scope: __module.encoder/__module.encoder.layer.16/__module.encoder.layer.16.output/__module.encoder.layer.16.output.LayerNorm # torch/nn/functional.py:2048:0
  %1617 : __torch__.transformers.modeling_bert.___torch_mangle_7718.BertOutput = prim::GetAttr[name="output"](%86)
  %1618 : __torch__.transformers.modeling_bert.___torch_mangle_7714.BertIntermediate = prim::GetAttr[name="intermediate"](%86)
  %1619 : __torch__.transformers.modeling_bert.___torch_mangle_7712.BertAttention = prim::GetAttr[name="attention"](%86)
  %1620 : __torch__.transformers.modeling_bert.___torch_mangle_7711.BertSelfOutput = prim::GetAttr[name="output"](%1619)
  %1621 : __torch__.transformers.modeling_bert.___torch_mangle_7707.BertSelfAttention = prim::GetAttr[name="self"](%1619)
  %1622 : __torch__.torch.nn.modules.linear.___torch_mangle_7705.Linear = prim::GetAttr[name="value"](%1621)
  %1623 : __torch__.torch.nn.modules.linear.___torch_mangle_7704.Linear = prim::GetAttr[name="key"](%1621)
  %1624 : __torch__.torch.nn.modules.linear.___torch_mangle_7703.Linear = prim::GetAttr[name="query"](%1621)
  %1625 : Tensor = prim::GetAttr[name="bias"](%1624)
  %1626 : Tensor = prim::GetAttr[name="weight"](%1624)
  %1627 : Float(1024:1, 1024:1024) = aten::t(%1626), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.query # torch/nn/functional.py:1676:0
  %output.103 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.174, %1627), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.query # torch/nn/functional.py:1676:0
  %x.103 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.103, %1625, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.query # torch/nn/functional.py:1678:0
  %1630 : Tensor = prim::GetAttr[name="bias"](%1623)
  %1631 : Tensor = prim::GetAttr[name="weight"](%1623)
  %1632 : Float(1024:1, 1024:1024) = aten::t(%1631), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.key # torch/nn/functional.py:1676:0
  %output.104 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.174, %1632), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.key # torch/nn/functional.py:1676:0
  %x.105 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.104, %1630, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.key # torch/nn/functional.py:1678:0
  %1635 : Tensor = prim::GetAttr[name="bias"](%1622)
  %1636 : Tensor = prim::GetAttr[name="weight"](%1622)
  %1637 : Float(1024:1, 1024:1024) = aten::t(%1636), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.value # torch/nn/functional.py:1676:0
  %output.105 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.174, %1637), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.value # torch/nn/functional.py:1676:0
  %x.107 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.105, %1635, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.value # torch/nn/functional.py:1678:0
  %1640 : int = aten::size(%x.103, %61), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1641 : int = aten::size(%x.103, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1642 : int[] = prim::ListConstruct(%1640, %1641, %62, %63), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
  %x.104 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.103, %1642), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
  %1644 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
  %query_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.104, %1644), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
  %1646 : int = aten::size(%x.105, %61), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1647 : int = aten::size(%x.105, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1648 : int[] = prim::ListConstruct(%1646, %1647, %62, %63), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
  %x.106 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.105, %1648), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
  %1650 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
  %key_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.106, %1650), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
  %1652 : int = aten::size(%x.107, %61), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1653 : int = aten::size(%x.107, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1654 : int[] = prim::ListConstruct(%1652, %1653, %62, %63), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
  %x.108 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.107, %1654), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
  %1656 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
  %value_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.108, %1656), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
  %1658 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.18, %66, %67), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.18, %1658), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.35, %68), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:259:0
  %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.36, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:262:0
  %input.176 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.175, %66, %69), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.176, %71, %70), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self/__module.encoder.layer.17.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.35 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.18, %value_layer.18), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:275:0
  %1665 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
  %1666 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.35, %1665), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.36 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1666, %61), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
  %1668 : int = aten::size(%context_layer.36, %61), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:278:0
  %1669 : int = aten::size(%context_layer.36, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:278:0
  %1670 : int[] = prim::ListConstruct(%1668, %1669, %72), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self
  %input.177 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.36, %1670), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.self # transformers/modeling_bert.py:279:0
  %1672 : __torch__.torch.nn.modules.normalization.___torch_mangle_7709.LayerNorm = prim::GetAttr[name="LayerNorm"](%1620)
  %1673 : __torch__.torch.nn.modules.linear.___torch_mangle_7708.Linear = prim::GetAttr[name="dense"](%1620)
  %1674 : Tensor = prim::GetAttr[name="bias"](%1673)
  %1675 : Tensor = prim::GetAttr[name="weight"](%1673)
  %1676 : Float(1024:1, 1024:1024) = aten::t(%1675), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1676:0
  %output.106 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.177, %1676), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1676:0
  %input.178 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.106, %1674, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.35 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.178, %71, %70), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.dropout # torch/nn/functional.py:973:0
  %input.179 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.35, %input.174, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output # transformers/modeling_bert.py:295:0
  %1681 : Tensor = prim::GetAttr[name="bias"](%1672)
  %1682 : Tensor = prim::GetAttr[name="weight"](%1672)
  %1683 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.LayerNorm
  %input_tensor.18 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.179, %1683, %1682, %1681, %59, %58), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.attention/__module.encoder.layer.17.attention.output/__module.encoder.layer.17.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1685 : __torch__.torch.nn.modules.linear.___torch_mangle_7713.Linear = prim::GetAttr[name="dense"](%1618)
  %1686 : Tensor = prim::GetAttr[name="bias"](%1685)
  %1687 : Tensor = prim::GetAttr[name="weight"](%1685)
  %1688 : Float(1024:1, 4096:1024) = aten::t(%1687), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate/__module.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1676:0
  %output.107 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.18, %1688), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate/__module.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1676:0
  %input.180 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.107, %1686, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate/__module.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1678:0
  %input.181 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.180), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.intermediate # torch/nn/functional.py:1369:0
  %1692 : __torch__.torch.nn.modules.normalization.___torch_mangle_7716.LayerNorm = prim::GetAttr[name="LayerNorm"](%1617)
  %1693 : __torch__.torch.nn.modules.linear.___torch_mangle_7715.Linear = prim::GetAttr[name="dense"](%1617)
  %1694 : Tensor = prim::GetAttr[name="bias"](%1693)
  %1695 : Tensor = prim::GetAttr[name="weight"](%1693)
  %1696 : Float(4096:1, 1024:4096) = aten::t(%1695), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.dense # torch/nn/functional.py:1676:0
  %output.108 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.181, %1696), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.dense # torch/nn/functional.py:1676:0
  %input.182 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.108, %1694, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.36 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.182, %71, %70), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.dropout # torch/nn/functional.py:973:0
  %input.183 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.36, %input_tensor.18, %60), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output # transformers/modeling_bert.py:371:0
  %1701 : Tensor = prim::GetAttr[name="bias"](%1692)
  %1702 : Tensor = prim::GetAttr[name="weight"](%1692)
  %1703 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.LayerNorm
  %input.184 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.183, %1703, %1702, %1701, %59, %58), scope: __module.encoder/__module.encoder.layer.17/__module.encoder.layer.17.output/__module.encoder.layer.17.output.LayerNorm # torch/nn/functional.py:2048:0
  %1705 : __torch__.transformers.modeling_bert.___torch_mangle_7735.BertOutput = prim::GetAttr[name="output"](%84)
  %1706 : __torch__.transformers.modeling_bert.___torch_mangle_7731.BertIntermediate = prim::GetAttr[name="intermediate"](%84)
  %1707 : __torch__.transformers.modeling_bert.___torch_mangle_7729.BertAttention = prim::GetAttr[name="attention"](%84)
  %1708 : __torch__.transformers.modeling_bert.___torch_mangle_7728.BertSelfOutput = prim::GetAttr[name="output"](%1707)
  %1709 : __torch__.transformers.modeling_bert.___torch_mangle_7724.BertSelfAttention = prim::GetAttr[name="self"](%1707)
  %1710 : __torch__.torch.nn.modules.linear.___torch_mangle_7722.Linear = prim::GetAttr[name="value"](%1709)
  %1711 : __torch__.torch.nn.modules.linear.___torch_mangle_7721.Linear = prim::GetAttr[name="key"](%1709)
  %1712 : __torch__.torch.nn.modules.linear.___torch_mangle_7720.Linear = prim::GetAttr[name="query"](%1709)
  %1713 : Tensor = prim::GetAttr[name="bias"](%1712)
  %1714 : Tensor = prim::GetAttr[name="weight"](%1712)
  %1715 : Float(1024:1, 1024:1024) = aten::t(%1714), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.query # torch/nn/functional.py:1676:0
  %output.109 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.184, %1715), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.query # torch/nn/functional.py:1676:0
  %x.109 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.109, %1713, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.query # torch/nn/functional.py:1678:0
  %1718 : Tensor = prim::GetAttr[name="bias"](%1711)
  %1719 : Tensor = prim::GetAttr[name="weight"](%1711)
  %1720 : Float(1024:1, 1024:1024) = aten::t(%1719), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.key # torch/nn/functional.py:1676:0
  %output.110 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.184, %1720), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.key # torch/nn/functional.py:1676:0
  %x.111 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.110, %1718, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.key # torch/nn/functional.py:1678:0
  %1723 : Tensor = prim::GetAttr[name="bias"](%1710)
  %1724 : Tensor = prim::GetAttr[name="weight"](%1710)
  %1725 : Float(1024:1, 1024:1024) = aten::t(%1724), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.value # torch/nn/functional.py:1676:0
  %output.111 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.184, %1725), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.value # torch/nn/functional.py:1676:0
  %x.113 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.111, %1723, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.value # torch/nn/functional.py:1678:0
  %1728 : int = aten::size(%x.109, %61), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1729 : int = aten::size(%x.109, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1730 : int[] = prim::ListConstruct(%1728, %1729, %62, %63), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
  %x.110 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.109, %1730), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
  %1732 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
  %query_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.110, %1732), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
  %1734 : int = aten::size(%x.111, %61), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1735 : int = aten::size(%x.111, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1736 : int[] = prim::ListConstruct(%1734, %1735, %62, %63), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
  %x.112 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.111, %1736), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
  %1738 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
  %key_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.112, %1738), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
  %1740 : int = aten::size(%x.113, %61), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1741 : int = aten::size(%x.113, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1742 : int[] = prim::ListConstruct(%1740, %1741, %62, %63), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
  %x.114 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.113, %1742), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
  %1744 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
  %value_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.114, %1744), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
  %1746 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.19, %66, %67), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.37 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.19, %1746), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.38 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.37, %68), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:259:0
  %input.185 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.38, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:262:0
  %input.186 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.185, %66, %69), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.186, %71, %70), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self/__module.encoder.layer.18.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.37 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.19, %value_layer.19), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:275:0
  %1753 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
  %1754 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.37, %1753), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.38 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1754, %61), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
  %1756 : int = aten::size(%context_layer.38, %61), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:278:0
  %1757 : int = aten::size(%context_layer.38, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:278:0
  %1758 : int[] = prim::ListConstruct(%1756, %1757, %72), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self
  %input.187 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.38, %1758), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.self # transformers/modeling_bert.py:279:0
  %1760 : __torch__.torch.nn.modules.normalization.___torch_mangle_7726.LayerNorm = prim::GetAttr[name="LayerNorm"](%1708)
  %1761 : __torch__.torch.nn.modules.linear.___torch_mangle_7725.Linear = prim::GetAttr[name="dense"](%1708)
  %1762 : Tensor = prim::GetAttr[name="bias"](%1761)
  %1763 : Tensor = prim::GetAttr[name="weight"](%1761)
  %1764 : Float(1024:1, 1024:1024) = aten::t(%1763), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1676:0
  %output.112 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.187, %1764), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1676:0
  %input.188 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.112, %1762, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.37 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.188, %71, %70), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.dropout # torch/nn/functional.py:973:0
  %input.189 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.37, %input.184, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output # transformers/modeling_bert.py:295:0
  %1769 : Tensor = prim::GetAttr[name="bias"](%1760)
  %1770 : Tensor = prim::GetAttr[name="weight"](%1760)
  %1771 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.LayerNorm
  %input_tensor.19 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.189, %1771, %1770, %1769, %59, %58), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.attention/__module.encoder.layer.18.attention.output/__module.encoder.layer.18.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1773 : __torch__.torch.nn.modules.linear.___torch_mangle_7730.Linear = prim::GetAttr[name="dense"](%1706)
  %1774 : Tensor = prim::GetAttr[name="bias"](%1773)
  %1775 : Tensor = prim::GetAttr[name="weight"](%1773)
  %1776 : Float(1024:1, 4096:1024) = aten::t(%1775), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate/__module.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1676:0
  %output.113 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.19, %1776), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate/__module.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1676:0
  %input.190 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.113, %1774, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate/__module.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1678:0
  %input.191 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.190), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.intermediate # torch/nn/functional.py:1369:0
  %1780 : __torch__.torch.nn.modules.normalization.___torch_mangle_7733.LayerNorm = prim::GetAttr[name="LayerNorm"](%1705)
  %1781 : __torch__.torch.nn.modules.linear.___torch_mangle_7732.Linear = prim::GetAttr[name="dense"](%1705)
  %1782 : Tensor = prim::GetAttr[name="bias"](%1781)
  %1783 : Tensor = prim::GetAttr[name="weight"](%1781)
  %1784 : Float(4096:1, 1024:4096) = aten::t(%1783), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.dense # torch/nn/functional.py:1676:0
  %output.114 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.191, %1784), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.dense # torch/nn/functional.py:1676:0
  %input.192 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.114, %1782, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.38 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.192, %71, %70), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.dropout # torch/nn/functional.py:973:0
  %input.193 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.38, %input_tensor.19, %60), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output # transformers/modeling_bert.py:371:0
  %1789 : Tensor = prim::GetAttr[name="bias"](%1780)
  %1790 : Tensor = prim::GetAttr[name="weight"](%1780)
  %1791 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.LayerNorm
  %input.194 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.193, %1791, %1790, %1789, %59, %58), scope: __module.encoder/__module.encoder.layer.18/__module.encoder.layer.18.output/__module.encoder.layer.18.output.LayerNorm # torch/nn/functional.py:2048:0
  %1793 : __torch__.transformers.modeling_bert.___torch_mangle_7752.BertOutput = prim::GetAttr[name="output"](%82)
  %1794 : __torch__.transformers.modeling_bert.___torch_mangle_7748.BertIntermediate = prim::GetAttr[name="intermediate"](%82)
  %1795 : __torch__.transformers.modeling_bert.___torch_mangle_7746.BertAttention = prim::GetAttr[name="attention"](%82)
  %1796 : __torch__.transformers.modeling_bert.___torch_mangle_7745.BertSelfOutput = prim::GetAttr[name="output"](%1795)
  %1797 : __torch__.transformers.modeling_bert.___torch_mangle_7741.BertSelfAttention = prim::GetAttr[name="self"](%1795)
  %1798 : __torch__.torch.nn.modules.linear.___torch_mangle_7739.Linear = prim::GetAttr[name="value"](%1797)
  %1799 : __torch__.torch.nn.modules.linear.___torch_mangle_7738.Linear = prim::GetAttr[name="key"](%1797)
  %1800 : __torch__.torch.nn.modules.linear.___torch_mangle_7737.Linear = prim::GetAttr[name="query"](%1797)
  %1801 : Tensor = prim::GetAttr[name="bias"](%1800)
  %1802 : Tensor = prim::GetAttr[name="weight"](%1800)
  %1803 : Float(1024:1, 1024:1024) = aten::t(%1802), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.query # torch/nn/functional.py:1676:0
  %output.115 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.194, %1803), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.query # torch/nn/functional.py:1676:0
  %x.115 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.115, %1801, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.query # torch/nn/functional.py:1678:0
  %1806 : Tensor = prim::GetAttr[name="bias"](%1799)
  %1807 : Tensor = prim::GetAttr[name="weight"](%1799)
  %1808 : Float(1024:1, 1024:1024) = aten::t(%1807), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.key # torch/nn/functional.py:1676:0
  %output.116 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.194, %1808), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.key # torch/nn/functional.py:1676:0
  %x.117 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.116, %1806, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.key # torch/nn/functional.py:1678:0
  %1811 : Tensor = prim::GetAttr[name="bias"](%1798)
  %1812 : Tensor = prim::GetAttr[name="weight"](%1798)
  %1813 : Float(1024:1, 1024:1024) = aten::t(%1812), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.value # torch/nn/functional.py:1676:0
  %output.117 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.194, %1813), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.value # torch/nn/functional.py:1676:0
  %x.119 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.117, %1811, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.value # torch/nn/functional.py:1678:0
  %1816 : int = aten::size(%x.115, %61), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1817 : int = aten::size(%x.115, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1818 : int[] = prim::ListConstruct(%1816, %1817, %62, %63), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
  %x.116 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.115, %1818), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
  %1820 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
  %query_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.116, %1820), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
  %1822 : int = aten::size(%x.117, %61), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1823 : int = aten::size(%x.117, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1824 : int[] = prim::ListConstruct(%1822, %1823, %62, %63), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
  %x.118 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.117, %1824), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
  %1826 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
  %key_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.118, %1826), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
  %1828 : int = aten::size(%x.119, %61), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1829 : int = aten::size(%x.119, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1830 : int[] = prim::ListConstruct(%1828, %1829, %62, %63), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
  %x.120 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.119, %1830), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
  %1832 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
  %value_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.120, %1832), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
  %1834 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.20, %66, %67), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.20, %1834), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.39, %68), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:259:0
  %input.195 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.40, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:262:0
  %input.196 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.195, %66, %69), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.196, %71, %70), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self/__module.encoder.layer.19.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.39 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.20, %value_layer.20), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:275:0
  %1841 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
  %1842 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.39, %1841), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.40 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1842, %61), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
  %1844 : int = aten::size(%context_layer.40, %61), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:278:0
  %1845 : int = aten::size(%context_layer.40, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:278:0
  %1846 : int[] = prim::ListConstruct(%1844, %1845, %72), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self
  %input.197 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.40, %1846), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.self # transformers/modeling_bert.py:279:0
  %1848 : __torch__.torch.nn.modules.normalization.___torch_mangle_7743.LayerNorm = prim::GetAttr[name="LayerNorm"](%1796)
  %1849 : __torch__.torch.nn.modules.linear.___torch_mangle_7742.Linear = prim::GetAttr[name="dense"](%1796)
  %1850 : Tensor = prim::GetAttr[name="bias"](%1849)
  %1851 : Tensor = prim::GetAttr[name="weight"](%1849)
  %1852 : Float(1024:1, 1024:1024) = aten::t(%1851), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1676:0
  %output.118 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.197, %1852), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1676:0
  %input.198 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.118, %1850, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.39 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.198, %71, %70), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.dropout # torch/nn/functional.py:973:0
  %input.199 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.39, %input.194, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output # transformers/modeling_bert.py:295:0
  %1857 : Tensor = prim::GetAttr[name="bias"](%1848)
  %1858 : Tensor = prim::GetAttr[name="weight"](%1848)
  %1859 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.LayerNorm
  %input_tensor.20 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.199, %1859, %1858, %1857, %59, %58), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.attention/__module.encoder.layer.19.attention.output/__module.encoder.layer.19.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1861 : __torch__.torch.nn.modules.linear.___torch_mangle_7747.Linear = prim::GetAttr[name="dense"](%1794)
  %1862 : Tensor = prim::GetAttr[name="bias"](%1861)
  %1863 : Tensor = prim::GetAttr[name="weight"](%1861)
  %1864 : Float(1024:1, 4096:1024) = aten::t(%1863), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate/__module.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1676:0
  %output.119 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.20, %1864), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate/__module.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1676:0
  %input.200 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.119, %1862, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate/__module.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1678:0
  %input.201 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.200), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.intermediate # torch/nn/functional.py:1369:0
  %1868 : __torch__.torch.nn.modules.normalization.___torch_mangle_7750.LayerNorm = prim::GetAttr[name="LayerNorm"](%1793)
  %1869 : __torch__.torch.nn.modules.linear.___torch_mangle_7749.Linear = prim::GetAttr[name="dense"](%1793)
  %1870 : Tensor = prim::GetAttr[name="bias"](%1869)
  %1871 : Tensor = prim::GetAttr[name="weight"](%1869)
  %1872 : Float(4096:1, 1024:4096) = aten::t(%1871), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.dense # torch/nn/functional.py:1676:0
  %output.120 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.201, %1872), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.dense # torch/nn/functional.py:1676:0
  %input.202 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.120, %1870, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.40 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.202, %71, %70), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.dropout # torch/nn/functional.py:973:0
  %input.203 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.40, %input_tensor.20, %60), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output # transformers/modeling_bert.py:371:0
  %1877 : Tensor = prim::GetAttr[name="bias"](%1868)
  %1878 : Tensor = prim::GetAttr[name="weight"](%1868)
  %1879 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.LayerNorm
  %input.204 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.203, %1879, %1878, %1877, %59, %58), scope: __module.encoder/__module.encoder.layer.19/__module.encoder.layer.19.output/__module.encoder.layer.19.output.LayerNorm # torch/nn/functional.py:2048:0
  %1881 : __torch__.transformers.modeling_bert.___torch_mangle_7769.BertOutput = prim::GetAttr[name="output"](%80)
  %1882 : __torch__.transformers.modeling_bert.___torch_mangle_7765.BertIntermediate = prim::GetAttr[name="intermediate"](%80)
  %1883 : __torch__.transformers.modeling_bert.___torch_mangle_7763.BertAttention = prim::GetAttr[name="attention"](%80)
  %1884 : __torch__.transformers.modeling_bert.___torch_mangle_7762.BertSelfOutput = prim::GetAttr[name="output"](%1883)
  %1885 : __torch__.transformers.modeling_bert.___torch_mangle_7758.BertSelfAttention = prim::GetAttr[name="self"](%1883)
  %1886 : __torch__.torch.nn.modules.linear.___torch_mangle_7756.Linear = prim::GetAttr[name="value"](%1885)
  %1887 : __torch__.torch.nn.modules.linear.___torch_mangle_7755.Linear = prim::GetAttr[name="key"](%1885)
  %1888 : __torch__.torch.nn.modules.linear.___torch_mangle_7754.Linear = prim::GetAttr[name="query"](%1885)
  %1889 : Tensor = prim::GetAttr[name="bias"](%1888)
  %1890 : Tensor = prim::GetAttr[name="weight"](%1888)
  %1891 : Float(1024:1, 1024:1024) = aten::t(%1890), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.query # torch/nn/functional.py:1676:0
  %output.121 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.204, %1891), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.query # torch/nn/functional.py:1676:0
  %x.121 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.121, %1889, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.query # torch/nn/functional.py:1678:0
  %1894 : Tensor = prim::GetAttr[name="bias"](%1887)
  %1895 : Tensor = prim::GetAttr[name="weight"](%1887)
  %1896 : Float(1024:1, 1024:1024) = aten::t(%1895), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.key # torch/nn/functional.py:1676:0
  %output.122 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.204, %1896), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.key # torch/nn/functional.py:1676:0
  %x.123 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.122, %1894, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.key # torch/nn/functional.py:1678:0
  %1899 : Tensor = prim::GetAttr[name="bias"](%1886)
  %1900 : Tensor = prim::GetAttr[name="weight"](%1886)
  %1901 : Float(1024:1, 1024:1024) = aten::t(%1900), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.value # torch/nn/functional.py:1676:0
  %output.123 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.204, %1901), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.value # torch/nn/functional.py:1676:0
  %x.125 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.123, %1899, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.value # torch/nn/functional.py:1678:0
  %1904 : int = aten::size(%x.121, %61), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1905 : int = aten::size(%x.121, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1906 : int[] = prim::ListConstruct(%1904, %1905, %62, %63), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
  %x.122 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.121, %1906), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
  %1908 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
  %query_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.122, %1908), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
  %1910 : int = aten::size(%x.123, %61), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1911 : int = aten::size(%x.123, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1912 : int[] = prim::ListConstruct(%1910, %1911, %62, %63), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
  %x.124 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.123, %1912), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
  %1914 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
  %key_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.124, %1914), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
  %1916 : int = aten::size(%x.125, %61), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1917 : int = aten::size(%x.125, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1918 : int[] = prim::ListConstruct(%1916, %1917, %62, %63), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
  %x.126 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.125, %1918), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
  %1920 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
  %value_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.126, %1920), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
  %1922 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.21, %66, %67), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.41 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.21, %1922), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.42 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.41, %68), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:259:0
  %input.205 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.42, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:262:0
  %input.206 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.205, %66, %69), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.206, %71, %70), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self/__module.encoder.layer.20.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.41 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.21, %value_layer.21), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:275:0
  %1929 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
  %1930 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.41, %1929), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.42 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1930, %61), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
  %1932 : int = aten::size(%context_layer.42, %61), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:278:0
  %1933 : int = aten::size(%context_layer.42, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:278:0
  %1934 : int[] = prim::ListConstruct(%1932, %1933, %72), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self
  %input.207 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.42, %1934), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.self # transformers/modeling_bert.py:279:0
  %1936 : __torch__.torch.nn.modules.normalization.___torch_mangle_7760.LayerNorm = prim::GetAttr[name="LayerNorm"](%1884)
  %1937 : __torch__.torch.nn.modules.linear.___torch_mangle_7759.Linear = prim::GetAttr[name="dense"](%1884)
  %1938 : Tensor = prim::GetAttr[name="bias"](%1937)
  %1939 : Tensor = prim::GetAttr[name="weight"](%1937)
  %1940 : Float(1024:1, 1024:1024) = aten::t(%1939), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1676:0
  %output.124 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.207, %1940), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1676:0
  %input.208 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.124, %1938, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.41 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.208, %71, %70), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.dropout # torch/nn/functional.py:973:0
  %input.209 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.41, %input.204, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output # transformers/modeling_bert.py:295:0
  %1945 : Tensor = prim::GetAttr[name="bias"](%1936)
  %1946 : Tensor = prim::GetAttr[name="weight"](%1936)
  %1947 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.LayerNorm
  %input_tensor.21 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.209, %1947, %1946, %1945, %59, %58), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.attention/__module.encoder.layer.20.attention.output/__module.encoder.layer.20.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1949 : __torch__.torch.nn.modules.linear.___torch_mangle_7764.Linear = prim::GetAttr[name="dense"](%1882)
  %1950 : Tensor = prim::GetAttr[name="bias"](%1949)
  %1951 : Tensor = prim::GetAttr[name="weight"](%1949)
  %1952 : Float(1024:1, 4096:1024) = aten::t(%1951), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate/__module.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1676:0
  %output.125 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.21, %1952), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate/__module.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1676:0
  %input.210 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.125, %1950, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate/__module.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1678:0
  %input.211 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.210), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.intermediate # torch/nn/functional.py:1369:0
  %1956 : __torch__.torch.nn.modules.normalization.___torch_mangle_7767.LayerNorm = prim::GetAttr[name="LayerNorm"](%1881)
  %1957 : __torch__.torch.nn.modules.linear.___torch_mangle_7766.Linear = prim::GetAttr[name="dense"](%1881)
  %1958 : Tensor = prim::GetAttr[name="bias"](%1957)
  %1959 : Tensor = prim::GetAttr[name="weight"](%1957)
  %1960 : Float(4096:1, 1024:4096) = aten::t(%1959), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.dense # torch/nn/functional.py:1676:0
  %output.126 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.211, %1960), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.dense # torch/nn/functional.py:1676:0
  %input.212 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.126, %1958, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.42 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.212, %71, %70), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.dropout # torch/nn/functional.py:973:0
  %input.213 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.42, %input_tensor.21, %60), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output # transformers/modeling_bert.py:371:0
  %1965 : Tensor = prim::GetAttr[name="bias"](%1956)
  %1966 : Tensor = prim::GetAttr[name="weight"](%1956)
  %1967 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.LayerNorm
  %input.214 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.213, %1967, %1966, %1965, %59, %58), scope: __module.encoder/__module.encoder.layer.20/__module.encoder.layer.20.output/__module.encoder.layer.20.output.LayerNorm # torch/nn/functional.py:2048:0
  %1969 : __torch__.transformers.modeling_bert.___torch_mangle_7786.BertOutput = prim::GetAttr[name="output"](%78)
  %1970 : __torch__.transformers.modeling_bert.___torch_mangle_7782.BertIntermediate = prim::GetAttr[name="intermediate"](%78)
  %1971 : __torch__.transformers.modeling_bert.___torch_mangle_7780.BertAttention = prim::GetAttr[name="attention"](%78)
  %1972 : __torch__.transformers.modeling_bert.___torch_mangle_7779.BertSelfOutput = prim::GetAttr[name="output"](%1971)
  %1973 : __torch__.transformers.modeling_bert.___torch_mangle_7775.BertSelfAttention = prim::GetAttr[name="self"](%1971)
  %1974 : __torch__.torch.nn.modules.linear.___torch_mangle_7773.Linear = prim::GetAttr[name="value"](%1973)
  %1975 : __torch__.torch.nn.modules.linear.___torch_mangle_7772.Linear = prim::GetAttr[name="key"](%1973)
  %1976 : __torch__.torch.nn.modules.linear.___torch_mangle_7771.Linear = prim::GetAttr[name="query"](%1973)
  %1977 : Tensor = prim::GetAttr[name="bias"](%1976)
  %1978 : Tensor = prim::GetAttr[name="weight"](%1976)
  %1979 : Float(1024:1, 1024:1024) = aten::t(%1978), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.query # torch/nn/functional.py:1676:0
  %output.127 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.214, %1979), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.query # torch/nn/functional.py:1676:0
  %x.127 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.127, %1977, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.query # torch/nn/functional.py:1678:0
  %1982 : Tensor = prim::GetAttr[name="bias"](%1975)
  %1983 : Tensor = prim::GetAttr[name="weight"](%1975)
  %1984 : Float(1024:1, 1024:1024) = aten::t(%1983), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.key # torch/nn/functional.py:1676:0
  %output.128 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.214, %1984), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.key # torch/nn/functional.py:1676:0
  %x.129 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.128, %1982, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.key # torch/nn/functional.py:1678:0
  %1987 : Tensor = prim::GetAttr[name="bias"](%1974)
  %1988 : Tensor = prim::GetAttr[name="weight"](%1974)
  %1989 : Float(1024:1, 1024:1024) = aten::t(%1988), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.value # torch/nn/functional.py:1676:0
  %output.129 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.214, %1989), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.value # torch/nn/functional.py:1676:0
  %x.131 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.129, %1987, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.value # torch/nn/functional.py:1678:0
  %1992 : int = aten::size(%x.127, %61), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1993 : int = aten::size(%x.127, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1994 : int[] = prim::ListConstruct(%1992, %1993, %62, %63), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
  %x.128 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.127, %1994), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
  %1996 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
  %query_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.128, %1996), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
  %1998 : int = aten::size(%x.129, %61), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1999 : int = aten::size(%x.129, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %2000 : int[] = prim::ListConstruct(%1998, %1999, %62, %63), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
  %x.130 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.129, %2000), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
  %2002 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
  %key_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.130, %2002), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
  %2004 : int = aten::size(%x.131, %61), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %2005 : int = aten::size(%x.131, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %2006 : int[] = prim::ListConstruct(%2004, %2005, %62, %63), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
  %x.132 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.131, %2006), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
  %2008 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
  %value_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.132, %2008), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
  %2010 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.22, %66, %67), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.43 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.22, %2010), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.44 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.43, %68), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:259:0
  %input.215 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.44, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:262:0
  %input.216 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.215, %66, %69), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.216, %71, %70), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self/__module.encoder.layer.21.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.43 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.22, %value_layer.22), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:275:0
  %2017 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
  %2018 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.43, %2017), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.44 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%2018, %61), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
  %2020 : int = aten::size(%context_layer.44, %61), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:278:0
  %2021 : int = aten::size(%context_layer.44, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:278:0
  %2022 : int[] = prim::ListConstruct(%2020, %2021, %72), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self
  %input.217 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.44, %2022), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.self # transformers/modeling_bert.py:279:0
  %2024 : __torch__.torch.nn.modules.normalization.___torch_mangle_7777.LayerNorm = prim::GetAttr[name="LayerNorm"](%1972)
  %2025 : __torch__.torch.nn.modules.linear.___torch_mangle_7776.Linear = prim::GetAttr[name="dense"](%1972)
  %2026 : Tensor = prim::GetAttr[name="bias"](%2025)
  %2027 : Tensor = prim::GetAttr[name="weight"](%2025)
  %2028 : Float(1024:1, 1024:1024) = aten::t(%2027), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1676:0
  %output.130 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.217, %2028), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1676:0
  %input.218 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.130, %2026, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.43 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.218, %71, %70), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.dropout # torch/nn/functional.py:973:0
  %input.219 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.43, %input.214, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output # transformers/modeling_bert.py:295:0
  %2033 : Tensor = prim::GetAttr[name="bias"](%2024)
  %2034 : Tensor = prim::GetAttr[name="weight"](%2024)
  %2035 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.LayerNorm
  %input_tensor.22 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.219, %2035, %2034, %2033, %59, %58), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.attention/__module.encoder.layer.21.attention.output/__module.encoder.layer.21.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2037 : __torch__.torch.nn.modules.linear.___torch_mangle_7781.Linear = prim::GetAttr[name="dense"](%1970)
  %2038 : Tensor = prim::GetAttr[name="bias"](%2037)
  %2039 : Tensor = prim::GetAttr[name="weight"](%2037)
  %2040 : Float(1024:1, 4096:1024) = aten::t(%2039), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate/__module.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1676:0
  %output.131 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.22, %2040), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate/__module.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1676:0
  %input.220 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.131, %2038, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate/__module.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1678:0
  %input.221 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.220), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.intermediate # torch/nn/functional.py:1369:0
  %2044 : __torch__.torch.nn.modules.normalization.___torch_mangle_7784.LayerNorm = prim::GetAttr[name="LayerNorm"](%1969)
  %2045 : __torch__.torch.nn.modules.linear.___torch_mangle_7783.Linear = prim::GetAttr[name="dense"](%1969)
  %2046 : Tensor = prim::GetAttr[name="bias"](%2045)
  %2047 : Tensor = prim::GetAttr[name="weight"](%2045)
  %2048 : Float(4096:1, 1024:4096) = aten::t(%2047), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.dense # torch/nn/functional.py:1676:0
  %output.132 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.221, %2048), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.dense # torch/nn/functional.py:1676:0
  %input.222 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.132, %2046, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.44 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.222, %71, %70), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.dropout # torch/nn/functional.py:973:0
  %input.223 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.44, %input_tensor.22, %60), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output # transformers/modeling_bert.py:371:0
  %2053 : Tensor = prim::GetAttr[name="bias"](%2044)
  %2054 : Tensor = prim::GetAttr[name="weight"](%2044)
  %2055 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.LayerNorm
  %input.224 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.223, %2055, %2054, %2053, %59, %58), scope: __module.encoder/__module.encoder.layer.21/__module.encoder.layer.21.output/__module.encoder.layer.21.output.LayerNorm # torch/nn/functional.py:2048:0
  %2057 : __torch__.transformers.modeling_bert.___torch_mangle_7803.BertOutput = prim::GetAttr[name="output"](%76)
  %2058 : __torch__.transformers.modeling_bert.___torch_mangle_7799.BertIntermediate = prim::GetAttr[name="intermediate"](%76)
  %2059 : __torch__.transformers.modeling_bert.___torch_mangle_7797.BertAttention = prim::GetAttr[name="attention"](%76)
  %2060 : __torch__.transformers.modeling_bert.___torch_mangle_7796.BertSelfOutput = prim::GetAttr[name="output"](%2059)
  %2061 : __torch__.transformers.modeling_bert.___torch_mangle_7792.BertSelfAttention = prim::GetAttr[name="self"](%2059)
  %2062 : __torch__.torch.nn.modules.linear.___torch_mangle_7790.Linear = prim::GetAttr[name="value"](%2061)
  %2063 : __torch__.torch.nn.modules.linear.___torch_mangle_7789.Linear = prim::GetAttr[name="key"](%2061)
  %2064 : __torch__.torch.nn.modules.linear.___torch_mangle_7788.Linear = prim::GetAttr[name="query"](%2061)
  %2065 : Tensor = prim::GetAttr[name="bias"](%2064)
  %2066 : Tensor = prim::GetAttr[name="weight"](%2064)
  %2067 : Float(1024:1, 1024:1024) = aten::t(%2066), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.query # torch/nn/functional.py:1676:0
  %output.133 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.224, %2067), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.query # torch/nn/functional.py:1676:0
  %x.133 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.133, %2065, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.query # torch/nn/functional.py:1678:0
  %2070 : Tensor = prim::GetAttr[name="bias"](%2063)
  %2071 : Tensor = prim::GetAttr[name="weight"](%2063)
  %2072 : Float(1024:1, 1024:1024) = aten::t(%2071), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.key # torch/nn/functional.py:1676:0
  %output.134 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.224, %2072), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.key # torch/nn/functional.py:1676:0
  %x.135 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.134, %2070, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.key # torch/nn/functional.py:1678:0
  %2075 : Tensor = prim::GetAttr[name="bias"](%2062)
  %2076 : Tensor = prim::GetAttr[name="weight"](%2062)
  %2077 : Float(1024:1, 1024:1024) = aten::t(%2076), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.value # torch/nn/functional.py:1676:0
  %output.135 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.224, %2077), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.value # torch/nn/functional.py:1676:0
  %x.137 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.135, %2075, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.value # torch/nn/functional.py:1678:0
  %2080 : int = aten::size(%x.133, %61), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2081 : int = aten::size(%x.133, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2082 : int[] = prim::ListConstruct(%2080, %2081, %62, %63), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
  %x.134 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.133, %2082), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
  %2084 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
  %query_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.134, %2084), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
  %2086 : int = aten::size(%x.135, %61), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2087 : int = aten::size(%x.135, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2088 : int[] = prim::ListConstruct(%2086, %2087, %62, %63), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
  %x.136 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.135, %2088), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
  %2090 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
  %key_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.136, %2090), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
  %2092 : int = aten::size(%x.137, %61), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2093 : int = aten::size(%x.137, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2094 : int[] = prim::ListConstruct(%2092, %2093, %62, %63), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
  %x.138 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.137, %2094), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
  %2096 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
  %value_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.138, %2096), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
  %2098 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.23, %66, %67), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.23, %2098), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.45, %68), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:259:0
  %input.225 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.46, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:262:0
  %input.226 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.225, %66, %69), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.226, %71, %70), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self/__module.encoder.layer.22.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.45 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.23, %value_layer.23), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:275:0
  %2105 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
  %2106 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.45, %2105), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.46 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%2106, %61), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
  %2108 : int = aten::size(%context_layer.46, %61), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:278:0
  %2109 : int = aten::size(%context_layer.46, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:278:0
  %2110 : int[] = prim::ListConstruct(%2108, %2109, %72), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self
  %input.227 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.46, %2110), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.self # transformers/modeling_bert.py:279:0
  %2112 : __torch__.torch.nn.modules.normalization.___torch_mangle_7794.LayerNorm = prim::GetAttr[name="LayerNorm"](%2060)
  %2113 : __torch__.torch.nn.modules.linear.___torch_mangle_7793.Linear = prim::GetAttr[name="dense"](%2060)
  %2114 : Tensor = prim::GetAttr[name="bias"](%2113)
  %2115 : Tensor = prim::GetAttr[name="weight"](%2113)
  %2116 : Float(1024:1, 1024:1024) = aten::t(%2115), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1676:0
  %output.136 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.227, %2116), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1676:0
  %input.228 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.136, %2114, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.45 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.228, %71, %70), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.dropout # torch/nn/functional.py:973:0
  %input.229 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.45, %input.224, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output # transformers/modeling_bert.py:295:0
  %2121 : Tensor = prim::GetAttr[name="bias"](%2112)
  %2122 : Tensor = prim::GetAttr[name="weight"](%2112)
  %2123 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.LayerNorm
  %input_tensor.23 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.229, %2123, %2122, %2121, %59, %58), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.attention/__module.encoder.layer.22.attention.output/__module.encoder.layer.22.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2125 : __torch__.torch.nn.modules.linear.___torch_mangle_7798.Linear = prim::GetAttr[name="dense"](%2058)
  %2126 : Tensor = prim::GetAttr[name="bias"](%2125)
  %2127 : Tensor = prim::GetAttr[name="weight"](%2125)
  %2128 : Float(1024:1, 4096:1024) = aten::t(%2127), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate/__module.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1676:0
  %output.137 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.23, %2128), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate/__module.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1676:0
  %input.230 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.137, %2126, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate/__module.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1678:0
  %input.231 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.230), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.intermediate # torch/nn/functional.py:1369:0
  %2132 : __torch__.torch.nn.modules.normalization.___torch_mangle_7801.LayerNorm = prim::GetAttr[name="LayerNorm"](%2057)
  %2133 : __torch__.torch.nn.modules.linear.___torch_mangle_7800.Linear = prim::GetAttr[name="dense"](%2057)
  %2134 : Tensor = prim::GetAttr[name="bias"](%2133)
  %2135 : Tensor = prim::GetAttr[name="weight"](%2133)
  %2136 : Float(4096:1, 1024:4096) = aten::t(%2135), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.dense # torch/nn/functional.py:1676:0
  %output.138 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.231, %2136), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.dense # torch/nn/functional.py:1676:0
  %input.232 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.138, %2134, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.46 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.232, %71, %70), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.dropout # torch/nn/functional.py:973:0
  %input.233 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.46, %input_tensor.23, %60), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output # transformers/modeling_bert.py:371:0
  %2141 : Tensor = prim::GetAttr[name="bias"](%2132)
  %2142 : Tensor = prim::GetAttr[name="weight"](%2132)
  %2143 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.LayerNorm
  %input.234 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.233, %2143, %2142, %2141, %59, %58), scope: __module.encoder/__module.encoder.layer.22/__module.encoder.layer.22.output/__module.encoder.layer.22.output.LayerNorm # torch/nn/functional.py:2048:0
  %2145 : __torch__.transformers.modeling_bert.___torch_mangle_7820.BertOutput = prim::GetAttr[name="output"](%74)
  %2146 : __torch__.transformers.modeling_bert.___torch_mangle_7816.BertIntermediate = prim::GetAttr[name="intermediate"](%74)
  %2147 : __torch__.transformers.modeling_bert.___torch_mangle_7814.BertAttention = prim::GetAttr[name="attention"](%74)
  %2148 : __torch__.transformers.modeling_bert.___torch_mangle_7813.BertSelfOutput = prim::GetAttr[name="output"](%2147)
  %2149 : __torch__.transformers.modeling_bert.___torch_mangle_7809.BertSelfAttention = prim::GetAttr[name="self"](%2147)
  %2150 : __torch__.torch.nn.modules.linear.___torch_mangle_7807.Linear = prim::GetAttr[name="value"](%2149)
  %2151 : __torch__.torch.nn.modules.linear.___torch_mangle_7806.Linear = prim::GetAttr[name="key"](%2149)
  %2152 : __torch__.torch.nn.modules.linear.___torch_mangle_7805.Linear = prim::GetAttr[name="query"](%2149)
  %2153 : Tensor = prim::GetAttr[name="bias"](%2152)
  %2154 : Tensor = prim::GetAttr[name="weight"](%2152)
  %2155 : Float(1024:1, 1024:1024) = aten::t(%2154), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.query # torch/nn/functional.py:1676:0
  %output.139 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.234, %2155), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.query # torch/nn/functional.py:1676:0
  %x.139 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.139, %2153, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.query # torch/nn/functional.py:1678:0
  %2158 : Tensor = prim::GetAttr[name="bias"](%2151)
  %2159 : Tensor = prim::GetAttr[name="weight"](%2151)
  %2160 : Float(1024:1, 1024:1024) = aten::t(%2159), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.key # torch/nn/functional.py:1676:0
  %output.140 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.234, %2160), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.key # torch/nn/functional.py:1676:0
  %x.141 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.140, %2158, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.key # torch/nn/functional.py:1678:0
  %2163 : Tensor = prim::GetAttr[name="bias"](%2150)
  %2164 : Tensor = prim::GetAttr[name="weight"](%2150)
  %2165 : Float(1024:1, 1024:1024) = aten::t(%2164), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.value # torch/nn/functional.py:1676:0
  %output.141 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.234, %2165), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.value # torch/nn/functional.py:1676:0
  %x.143 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.141, %2163, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.value # torch/nn/functional.py:1678:0
  %2168 : int = aten::size(%x.139, %61), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2169 : int = aten::size(%x.139, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2170 : int[] = prim::ListConstruct(%2168, %2169, %62, %63), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
  %x.140 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.139, %2170), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
  %2172 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
  %query_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.140, %2172), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
  %2174 : int = aten::size(%x.141, %61), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2175 : int = aten::size(%x.141, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2176 : int[] = prim::ListConstruct(%2174, %2175, %62, %63), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
  %x.142 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.141, %2176), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
  %2178 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
  %key_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.142, %2178), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
  %2180 : int = aten::size(%x.143, %61), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2181 : int = aten::size(%x.143, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2182 : int[] = prim::ListConstruct(%2180, %2181, %62, %63), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
  %x : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.143, %2182), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
  %2184 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
  %value_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x, %2184), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
  %2186 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer, %66, %67), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.47 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer, %2186), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.47, %68), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:259:0
  %input.235 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:262:0
  %input.236 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.235, %66, %69), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.236, %71, %70), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self/__module.encoder.layer.23.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.47 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:275:0
  %2193 : int[] = prim::ListConstruct(%61, %64, %60, %65), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
  %2194 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.47, %2193), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
  %context_layer : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%2194, %61), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
  %2196 : int = aten::size(%context_layer, %61), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:278:0
  %2197 : int = aten::size(%context_layer, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:278:0
  %2198 : int[] = prim::ListConstruct(%2196, %2197, %72), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self
  %input.237 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer, %2198), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.self # transformers/modeling_bert.py:279:0
  %2200 : __torch__.torch.nn.modules.normalization.___torch_mangle_7811.LayerNorm = prim::GetAttr[name="LayerNorm"](%2148)
  %2201 : __torch__.torch.nn.modules.linear.___torch_mangle_7810.Linear = prim::GetAttr[name="dense"](%2148)
  %2202 : Tensor = prim::GetAttr[name="bias"](%2201)
  %2203 : Tensor = prim::GetAttr[name="weight"](%2201)
  %2204 : Float(1024:1, 1024:1024) = aten::t(%2203), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1676:0
  %output.142 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.237, %2204), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1676:0
  %input.238 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.142, %2202, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.47 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.238, %71, %70), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.dropout # torch/nn/functional.py:973:0
  %input.239 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.47, %input.234, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output # transformers/modeling_bert.py:295:0
  %2209 : Tensor = prim::GetAttr[name="bias"](%2200)
  %2210 : Tensor = prim::GetAttr[name="weight"](%2200)
  %2211 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.LayerNorm
  %input_tensor : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.239, %2211, %2210, %2209, %59, %58), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.attention/__module.encoder.layer.23.attention.output/__module.encoder.layer.23.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2213 : __torch__.torch.nn.modules.linear.___torch_mangle_7815.Linear = prim::GetAttr[name="dense"](%2146)
  %2214 : Tensor = prim::GetAttr[name="bias"](%2213)
  %2215 : Tensor = prim::GetAttr[name="weight"](%2213)
  %2216 : Float(1024:1, 4096:1024) = aten::t(%2215), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate/__module.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1676:0
  %output.143 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor, %2216), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate/__module.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1676:0
  %input.240 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.143, %2214, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate/__module.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1678:0
  %input.241 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.240), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.intermediate # torch/nn/functional.py:1369:0
  %2220 : __torch__.torch.nn.modules.normalization.___torch_mangle_7818.LayerNorm = prim::GetAttr[name="LayerNorm"](%2145)
  %2221 : __torch__.torch.nn.modules.linear.___torch_mangle_7817.Linear = prim::GetAttr[name="dense"](%2145)
  %2222 : Tensor = prim::GetAttr[name="bias"](%2221)
  %2223 : Tensor = prim::GetAttr[name="weight"](%2221)
  %2224 : Float(4096:1, 1024:4096) = aten::t(%2223), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.dense # torch/nn/functional.py:1676:0
  %output : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.241, %2224), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.dense # torch/nn/functional.py:1676:0
  %input.242 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output, %2222, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.242, %71, %70), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.dropout # torch/nn/functional.py:973:0
  %input : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states, %input_tensor, %60), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output # transformers/modeling_bert.py:371:0
  %2229 : Tensor = prim::GetAttr[name="bias"](%2220)
  %2230 : Tensor = prim::GetAttr[name="weight"](%2220)
  %2231 : int[] = prim::ListConstruct(%72), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.LayerNorm
  %2232 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input, %2231, %2230, %2229, %59, %58), scope: __module.encoder/__module.encoder.layer.23/__module.encoder.layer.23.output/__module.encoder.layer.23.output.LayerNorm # torch/nn/functional.py:2048:0
  %31 : (Float(17:13312, 13:1024, 1024:1)) = prim::TupleConstruct(%2232)
  return (%31)
