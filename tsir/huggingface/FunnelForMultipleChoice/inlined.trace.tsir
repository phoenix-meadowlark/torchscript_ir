graph(%self.1 : __torch__.transformers.modeling_funnel.FunnelForMultipleChoice,
      %input_ids.1 : Long(17:91, 7:13, 13:1),
      %attention_mask.1 : Long(17:91, 7:13, 13:1)):
  %3 : __torch__.transformers.modeling_funnel.FunnelClassificationHead = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.transformers.modeling_funnel.___torch_mangle_2590.FunnelBaseModel = prim::GetAttr[name="funnel"](%self.1)
  %5 : int = prim::Constant[value=1]() # transformers/modeling_funnel.py:1338:0
  %6 : int = aten::size(%input_ids.1, %5) # transformers/modeling_funnel.py:1338:0
  %num_choices : Long() = prim::NumToTensor(%6)
  %8 : int = aten::Int(%num_choices)
  %9 : int = prim::Constant[value=-1]() # transformers/modeling_funnel.py:1340:0
  %10 : int = aten::size(%input_ids.1, %9) # transformers/modeling_funnel.py:1340:0
  %11 : Long() = prim::NumToTensor(%10)
  %12 : int = aten::Int(%11)
  %13 : int = prim::Constant[value=-1]() # transformers/modeling_funnel.py:1340:0
  %14 : int[] = prim::ListConstruct(%13, %12)
  %input_ids : Long(119:13, 13:1) = aten::view(%input_ids.1, %14) # transformers/modeling_funnel.py:1340:0
  %16 : int = prim::Constant[value=-1]() # transformers/modeling_funnel.py:1341:0
  %17 : int = aten::size(%attention_mask.1, %16) # transformers/modeling_funnel.py:1341:0
  %18 : Long() = prim::NumToTensor(%17)
  %19 : int = aten::Int(%18)
  %20 : int = prim::Constant[value=-1]() # transformers/modeling_funnel.py:1341:0
  %21 : int[] = prim::ListConstruct(%20, %19)
  %attention_mask.2 : Long(119:13, 13:1) = aten::view(%attention_mask.1, %21) # transformers/modeling_funnel.py:1341:0
  %37 : int = prim::Constant[value=384](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %38 : float = prim::Constant[value=1.](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %39 : int = prim::Constant[value=6](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %40 : Long() = prim::Constant[value={384}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %41 : Float() = prim::Constant[value={10000}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %42 : None = prim::Constant(), scope: __module.funnel/__module.funnel.encoder
  %43 : Long() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %44 : Long() = prim::Constant[value={2}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %45 : int = prim::Constant[value=9223372036854775807](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %46 : Long() = prim::Constant[value={13}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %47 : Float(1:1) = prim::Constant[value={-1}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %48 : int = prim::Constant[value=2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %49 : Long() = prim::Constant[value={14}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %50 : int = prim::Constant[value=-2](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %51 : Float(1:1) = prim::Constant[value={-3}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %52 : Long() = prim::Constant[value={16}](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %53 : int = prim::Constant[value=-4](), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %54 : Double() = prim::Constant[value={0.5}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %55 : float = prim::Constant[value=3.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %56 : Double() = prim::Constant[value={0.044715}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %57 : Double() = prim::Constant[value={0.797885}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %58 : Double() = prim::Constant[value={1}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %59 : float = prim::Constant[value=0.](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %60 : int = prim::Constant[value=12](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %61 : int = prim::Constant[value=64](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %62 : Double() = prim::Constant[value={0.125}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %63 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %64 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %65 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %66 : int = prim::Constant[value=3](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %67 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %68 : Double() = prim::Constant[value={1e+06}](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %69 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %70 : int = prim::Constant[value=-1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %71 : bool = prim::Constant[value=1](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %72 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %73 : int = prim::Constant[value=768](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %74 : float = prim::Constant[value=0.10000000000000001](), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %75 : bool = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:945:0
  %76 : Device = prim::Constant[value="cpu"](), scope: __module.funnel # transformers/modeling_funnel.py:945:0
  %77 : int = prim::Constant[value=4](), scope: __module.funnel # transformers/modeling_funnel.py:945:0
  %78 : int = prim::Constant[value=1](), scope: __module.funnel # transformers/modeling_funnel.py:934:0
  %79 : int = prim::Constant[value=0](), scope: __module.funnel # transformers/modeling_funnel.py:934:0
  %80 : __torch__.transformers.modeling_funnel.___torch_mangle_2589.FunnelEncoder = prim::GetAttr[name="encoder"](%4)
  %81 : __torch__.transformers.modeling_funnel.___torch_mangle_2401.FunnelEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %82 : int = aten::size(%input_ids, %79), scope: __module.funnel # transformers/modeling_funnel.py:934:0
  %83 : int = aten::size(%input_ids, %78), scope: __module.funnel # transformers/modeling_funnel.py:934:0
  %84 : int[] = prim::ListConstruct(%82, %83), scope: __module.funnel
  %token_type_ids : Long(119:13, 13:1) = aten::zeros(%84, %77, %79, %76, %75), scope: __module.funnel # transformers/modeling_funnel.py:945:0
  %86 : __torch__.torch.nn.modules.normalization.___torch_mangle_2399.LayerNorm = prim::GetAttr[name="layer_norm"](%81)
  %87 : __torch__.torch.nn.modules.sparse.___torch_mangle_2398.Embedding = prim::GetAttr[name="word_embeddings"](%81)
  %88 : Tensor = prim::GetAttr[name="weight"](%87)
  %input.1 : Float(119:9984, 13:768, 768:1) = aten::embedding(%88, %input_ids, %70, %75, %75), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %90 : Tensor = prim::GetAttr[name="bias"](%86)
  %91 : Tensor = prim::GetAttr[name="weight"](%86)
  %92 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm
  %input.2 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.1, %92, %91, %90, %72, %71), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %inputs_embeds : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.2, %74, %75), scope: __module.funnel/__module.funnel.embeddings/__module.funnel.embeddings.dropout # torch/nn/functional.py:973:0
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_2587.ModuleList = prim::GetAttr[name="2"](%95)
  %97 : __torch__.transformers.modeling_funnel.___torch_mangle_2586.FunnelLayer = prim::GetAttr[name="3"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_2587.ModuleList = prim::GetAttr[name="2"](%98)
  %100 : __torch__.transformers.modeling_funnel.___torch_mangle_2571.FunnelLayer = prim::GetAttr[name="2"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_2587.ModuleList = prim::GetAttr[name="2"](%101)
  %103 : __torch__.transformers.modeling_funnel.___torch_mangle_2556.FunnelLayer = prim::GetAttr[name="1"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_2587.ModuleList = prim::GetAttr[name="2"](%104)
  %106 : __torch__.transformers.modeling_funnel.___torch_mangle_2541.FunnelLayer = prim::GetAttr[name="0"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_2526.ModuleList = prim::GetAttr[name="1"](%107)
  %109 : __torch__.transformers.modeling_funnel.___torch_mangle_2525.FunnelLayer = prim::GetAttr[name="3"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_2526.ModuleList = prim::GetAttr[name="1"](%110)
  %112 : __torch__.transformers.modeling_funnel.___torch_mangle_2510.FunnelLayer = prim::GetAttr[name="2"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_2526.ModuleList = prim::GetAttr[name="1"](%113)
  %115 : __torch__.transformers.modeling_funnel.___torch_mangle_2495.FunnelLayer = prim::GetAttr[name="1"](%114)
  %116 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_2526.ModuleList = prim::GetAttr[name="1"](%116)
  %118 : __torch__.transformers.modeling_funnel.___torch_mangle_2480.FunnelLayer = prim::GetAttr[name="0"](%117)
  %119 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %120 : __torch__.torch.nn.modules.container.___torch_mangle_2465.ModuleList = prim::GetAttr[name="0"](%119)
  %121 : __torch__.transformers.modeling_funnel.___torch_mangle_2464.FunnelLayer = prim::GetAttr[name="3"](%120)
  %122 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %123 : __torch__.torch.nn.modules.container.___torch_mangle_2465.ModuleList = prim::GetAttr[name="0"](%122)
  %124 : __torch__.transformers.modeling_funnel.___torch_mangle_2449.FunnelLayer = prim::GetAttr[name="2"](%123)
  %125 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %126 : __torch__.torch.nn.modules.container.___torch_mangle_2465.ModuleList = prim::GetAttr[name="0"](%125)
  %127 : __torch__.transformers.modeling_funnel.___torch_mangle_2434.FunnelLayer = prim::GetAttr[name="1"](%126)
  %128 : __torch__.torch.nn.modules.container.___torch_mangle_2588.ModuleList = prim::GetAttr[name="blocks"](%80)
  %129 : __torch__.torch.nn.modules.container.___torch_mangle_2465.ModuleList = prim::GetAttr[name="0"](%128)
  %130 : __torch__.transformers.modeling_funnel.___torch_mangle_2419.FunnelLayer = prim::GetAttr[name="0"](%129)
  %attention_mask.3 : Float(119:13, 13:1) = aten::type_as(%attention_mask.2, %inputs_embeds), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:625:0
  %132 : int = aten::size(%inputs_embeds, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:195:0
  %seq_len.1 : Long() = prim::NumToTensor(%132), scope: __module.funnel/__module.funnel.encoder
  %freq_seq : Float(384:1) = aten::arange(%79, %37, %38, %39, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:247:0
  %135 : Float(384:1) = aten::div(%freq_seq, %40), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:248:0
  %136 : Float() = aten::to(%41, %76, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %137 : Float() = aten::detach(%136), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %138 : Float(384:1) = aten::pow(%137, %135), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:420:0
  %139 : Float(384:1) = aten::reciprocal(%138), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %inv_freq : Float(384:1) = aten::mul(%139, %43), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:400:0
  %141 : Long() = aten::neg(%seq_len.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %142 : Long() = aten::mul(%141, %44), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %143 : Scalar = aten::ScalarImplicit(%142), scope: __module.funnel/__module.funnel.encoder
  %144 : Long() = aten::mul(%seq_len.1, %44), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %145 : Scalar = aten::ScalarImplicit(%144), scope: __module.funnel/__module.funnel.encoder
  %rel_pos_id : Float(52:1) = aten::arange(%143, %145, %38, %39, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:250:0
  %zero_offset : Long() = aten::mul(%seq_len.1, %44), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:251:0
  %148 : Float(52:1) = aten::slice(%rel_pos_id, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %149 : Float(52:1, 1:1) = aten::unsqueeze(%148, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %150 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %sinusoid : Float(52:384, 384:1) = aten::mul(%149, %150), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:252:0
  %input.3 : Float(52:384, 384:1) = aten::sin(%sinusoid), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:253:0
  %sin_embed : Float(52:384, 384:1) = aten::dropout(%input.3, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.4 : Float(52:384, 384:1) = aten::cos(%sinusoid), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:254:0
  %cos_embed : Float(52:384, 384:1) = aten::dropout(%input.4, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %156 : Tensor[] = prim::ListConstruct(%sin_embed, %cos_embed), scope: __module.funnel/__module.funnel.encoder
  %pos_embed : Float(52:768, 768:1) = aten::cat(%156, %70), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:255:0
  %pos : Float(13:1) = aten::arange(%79, %132, %78, %39, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:257:0
  %159 : Float() = aten::select(%pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %160 : Float() = aten::select(%pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.1 : Float() = aten::sub(%159, %160, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.1 : Float() = aten::add(%ref_point.1, %46, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %163 : Scalar = aten::ScalarImplicit(%max_dist.1), scope: __module.funnel/__module.funnel.encoder
  %164 : Float() = aten::select(%pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %165 : Float() = aten::select(%pos, %79, %70), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.1 : Float() = aten::sub(%164, %165, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %167 : Float() = aten::sub(%min_dist.1, %43, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %168 : Scalar = aten::ScalarImplicit(%167), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.1 : Long(26:1) = aten::arange(%163, %168, %70, %77, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %170 : Long(26:1) = aten::slice(%rel_pos.1, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %171 : Long(26:1, 1:1) = aten::unsqueeze(%170, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.2 : Long(26:1, 1:1) = aten::add(%171, %zero_offset, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %173 : int = aten::size(%rel_pos.2, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %174 : int[] = prim::ListConstruct(%173, %73), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.3 : Long(26:1, 768:0) = aten::expand(%rel_pos.2, %174, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %176 : Float(26:768, 768:1) = aten::gather(%pos_embed, %79, %rel_pos.3, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %177 : Float(1:1) = aten::to(%47, %76, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.1 : Float(1:1) = aten::detach(%177), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.1 : Float(11:1) = aten::slice(%pos, %79, %78, %70, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %180 : Float(6:2) = aten::slice(%pooled_pos_id.1, %79, %79, %45, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %181 : Tensor[] = prim::ListConstruct(%cls_pos.1, %180), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos.1 : Float(7:1) = aten::cat(%181, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %183 : Float() = aten::select(%pooled_pos.1, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %184 : Float() = aten::select(%pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.2 : Float() = aten::sub(%183, %184, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.2 : Float() = aten::add(%ref_point.2, %49, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %187 : Scalar = aten::ScalarImplicit(%max_dist.2), scope: __module.funnel/__module.funnel.encoder
  %188 : Float() = aten::select(%pooled_pos.1, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %189 : Float() = aten::select(%pos, %79, %70), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.2 : Float() = aten::sub(%188, %189, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %191 : Float() = aten::sub(%min_dist.2, %43, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %192 : Scalar = aten::ScalarImplicit(%191), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.4 : Long(27:1) = aten::arange(%187, %192, %70, %77, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %194 : Long(27:1) = aten::slice(%rel_pos.4, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %195 : Long(27:1, 1:1) = aten::unsqueeze(%194, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.5 : Long(27:1, 1:1) = aten::add(%195, %zero_offset, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %197 : int = aten::size(%rel_pos.5, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %198 : int[] = prim::ListConstruct(%197, %73), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.6 : Long(27:1, 768:0) = aten::expand(%rel_pos.5, %198, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %200 : Float(27:768, 768:1) = aten::gather(%pos_embed, %79, %rel_pos.6, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %201 : Float() = aten::select(%pooled_pos.1, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %202 : Float() = aten::select(%pooled_pos.1, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.3 : Float() = aten::sub(%201, %202, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.3 : Float() = aten::add(%ref_point.3, %49, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %205 : Scalar = aten::ScalarImplicit(%max_dist.3), scope: __module.funnel/__module.funnel.encoder
  %206 : Float() = aten::select(%pooled_pos.1, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %207 : Float() = aten::select(%pooled_pos.1, %79, %70), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.3 : Float() = aten::sub(%206, %207, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %209 : Float() = aten::sub(%min_dist.3, %43, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %210 : Scalar = aten::ScalarImplicit(%209), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.7 : Long(14:1) = aten::arange(%205, %210, %50, %77, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %212 : Long(14:1) = aten::slice(%rel_pos.7, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %213 : Long(14:1, 1:1) = aten::unsqueeze(%212, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.8 : Long(14:1, 1:1) = aten::add(%213, %zero_offset, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %215 : int = aten::size(%rel_pos.8, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %216 : int[] = prim::ListConstruct(%215, %73), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.9 : Long(14:1, 768:0) = aten::expand(%rel_pos.8, %216, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %218 : Float(14:768, 768:1) = aten::gather(%pos_embed, %79, %rel_pos.9, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %219 : Float(1:1) = aten::to(%51, %76, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos : Float(1:1) = aten::detach(%219), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id : Float(5:1) = aten::slice(%pooled_pos.1, %79, %78, %70, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:301:0
  %222 : Float(3:2) = aten::slice(%pooled_pos_id, %79, %79, %45, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %223 : Tensor[] = prim::ListConstruct(%cls_pos, %222), scope: __module.funnel/__module.funnel.encoder
  %pooled_pos : Float(4:1) = aten::cat(%223, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:302:0
  %225 : Float() = aten::select(%pooled_pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %226 : Float() = aten::select(%pooled_pos.1, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.4 : Float() = aten::sub(%225, %226, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.4 : Float() = aten::add(%ref_point.4, %52, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %229 : Scalar = aten::ScalarImplicit(%max_dist.4), scope: __module.funnel/__module.funnel.encoder
  %230 : Float() = aten::select(%pooled_pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %231 : Float() = aten::select(%pooled_pos.1, %79, %70), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.4 : Float() = aten::sub(%230, %231, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %233 : Float() = aten::sub(%min_dist.4, %43, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %234 : Scalar = aten::ScalarImplicit(%233), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.10 : Long(15:1) = aten::arange(%229, %234, %50, %77, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %236 : Long(15:1) = aten::slice(%rel_pos.10, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %237 : Long(15:1, 1:1) = aten::unsqueeze(%236, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.11 : Long(15:1, 1:1) = aten::add(%237, %zero_offset, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:275:0
  %239 : int = aten::size(%rel_pos.11, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %240 : int[] = prim::ListConstruct(%239, %73), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.12 : Long(15:1, 768:0) = aten::expand(%rel_pos.11, %240, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:276:0
  %242 : Float(15:768, 768:1) = aten::gather(%pos_embed, %79, %rel_pos.12, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:277:0
  %243 : Float() = aten::select(%pooled_pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %244 : Float() = aten::select(%pooled_pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %ref_point : Float() = aten::sub(%243, %244, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:313:0
  %max_dist : Float() = aten::add(%ref_point, %52, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:315:0
  %247 : Scalar = aten::ScalarImplicit(%max_dist), scope: __module.funnel/__module.funnel.encoder
  %248 : Float() = aten::select(%pooled_pos, %79, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %249 : Float() = aten::select(%pooled_pos, %79, %70), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %min_dist : Float() = aten::sub(%248, %249, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:316:0
  %251 : Float() = aten::sub(%min_dist, %43, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %252 : Scalar = aten::ScalarImplicit(%251), scope: __module.funnel/__module.funnel.encoder
  %rel_pos.13 : Long(8:1) = aten::arange(%247, %252, %53, %77, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:318:0
  %254 : Long(8:1) = aten::slice(%rel_pos.13, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %255 : Long(8:1, 1:1) = aten::unsqueeze(%254, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.14 : Long(8:1, 1:1) = aten::add(%255, %zero_offset, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:284:0
  %257 : int = aten::size(%rel_pos.14, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %258 : int[] = prim::ListConstruct(%257, %73), scope: __module.funnel/__module.funnel.encoder
  %rel_pos : Long(8:1, 768:0) = aten::expand(%rel_pos.14, %258, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:285:0
  %260 : Float(8:768, 768:1) = aten::gather(%pos_embed, %79, %rel_pos, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:286:0
  %261 : Long(119:13, 13:1) = aten::slice(%token_type_ids, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %262 : Long(119:13, 13:1) = aten::slice(%261, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %263 : Long(119:13, 13:1, 1:1) = aten::unsqueeze(%262, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %264 : Long(119:13, 13:1) = aten::slice(%token_type_ids, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %265 : Long(119:13, 1:13, 13:1) = aten::unsqueeze(%264, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.1 : Bool(119:169, 13:13, 13:1) = aten::eq(%263, %265), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %cls_ids : Bool(119:13, 13:1) = aten::eq(%token_type_ids, %48), scope: __module.funnel/__module.funnel.encoder # torch/tensor.py:22:0
  %268 : Bool(119:13, 13:1) = aten::slice(%cls_ids, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %269 : Bool(119:13, 13:1) = aten::slice(%268, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %270 : Bool(119:13, 13:1, 1:1) = aten::unsqueeze(%269, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %271 : Bool(119:13, 13:1) = aten::slice(%cls_ids, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %272 : Bool(119:13, 1:13, 13:1) = aten::unsqueeze(%271, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %cls_mat : Bool(119:169, 13:13, 13:1) = aten::__or__(%270, %272), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.2 : Bool(119:169, 13:13, 13:1) = aten::__or__(%cls_mat, %token_type_mat.1), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:211:0
  %275 : Long() = aten::sub(%seq_len.1, %43, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %276 : int = aten::Int(%275), scope: __module.funnel/__module.funnel.encoder
  %277 : Long() = aten::sub(%seq_len.1, %43, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %278 : int = aten::Int(%277), scope: __module.funnel/__module.funnel.encoder
  %279 : int[] = prim::ListConstruct(%276, %278), scope: __module.funnel/__module.funnel.encoder
  %input.5 : Float(12:12, 12:1) = aten::ones(%279, %39, %79, %76, %75), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:199:0
  %281 : int[] = prim::ListConstruct(%78, %79, %78, %79), scope: __module.funnel/__module.funnel.encoder
  %cls_mask.1 : Float(13:13, 13:1) = aten::constant_pad_nd(%input.5, %281, %79), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:3552:0
  %283 : __torch__.transformers.modeling_funnel.___torch_mangle_2418.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%130)
  %284 : __torch__.transformers.modeling_funnel.___torch_mangle_2412.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%130)
  %285 : __torch__.torch.nn.modules.normalization.___torch_mangle_2411.LayerNorm = prim::GetAttr[name="layer_norm"](%284)
  %286 : __torch__.torch.nn.modules.linear.___torch_mangle_2410.Linear = prim::GetAttr[name="post_proj"](%284)
  %287 : Tensor = prim::GetAttr[name="seg_embed"](%284)
  %288 : Tensor = prim::GetAttr[name="r_s_bias"](%284)
  %289 : Tensor = prim::GetAttr[name="r_kernel"](%284)
  %290 : Tensor = prim::GetAttr[name="r_r_bias"](%284)
  %291 : Tensor = prim::GetAttr[name="r_w_bias"](%284)
  %292 : __torch__.torch.nn.modules.linear.___torch_mangle_2409.Linear = prim::GetAttr[name="v_head"](%284)
  %293 : __torch__.torch.nn.modules.linear.___torch_mangle_2408.Linear = prim::GetAttr[name="k_head"](%284)
  %294 : __torch__.torch.nn.modules.linear.___torch_mangle_2407.Linear = prim::GetAttr[name="q_head"](%284)
  %295 : int = aten::size(%inputs_embeds, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %296 : int = aten::size(%inputs_embeds, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %297 : int = aten::size(%inputs_embeds, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
  %298 : Tensor = prim::GetAttr[name="weight"](%294)
  %299 : Float(768:1, 768:768) = aten::t(%298), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %300 : Float(119:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %299), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %301 : int[] = prim::ListConstruct(%295, %296, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %q_head.1 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%300, %301), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %303 : Tensor = prim::GetAttr[name="bias"](%293)
  %304 : Tensor = prim::GetAttr[name="weight"](%293)
  %305 : Float(768:1, 768:768) = aten::t(%304), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.1 : Float(119:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %305), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %307 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.1, %303, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
  %308 : int[] = prim::ListConstruct(%295, %297, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %309 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%307, %308), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
  %310 : Tensor = prim::GetAttr[name="bias"](%292)
  %311 : Tensor = prim::GetAttr[name="weight"](%292)
  %312 : Float(768:1, 768:768) = aten::t(%311), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.2 : Float(119:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %312), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %314 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.2, %310, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
  %315 : int[] = prim::ListConstruct(%295, %297, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %316 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%314, %315), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.2 : Float(119:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.1, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.1 : Float(12:64, 64:1) = aten::mul(%291, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
  %319 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_w_bias.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
  %320 : Tensor[] = prim::ListConstruct(%319, %309), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %content_score.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::einsum(%63, %320), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %v.1 : Float(12:64, 64:1) = aten::mul(%290, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
  %323 : Tensor[] = prim::ListConstruct(%176, %289), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %324 : Float(26:768, 12:64, 64:1) = aten::einsum(%64, %323), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %325 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %v.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
  %326 : Tensor[] = prim::ListConstruct(%325, %324), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.1 : Float(119:338, 12:40222, 13:26, 26:1) = aten::einsum(%65, %326), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %328 : int = aten::size(%positional_attn.1, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %329 : int = aten::size(%positional_attn.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %330 : int = aten::size(%positional_attn.1, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %331 : int = aten::size(%positional_attn.1, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.1 : Long() = prim::NumToTensor(%331), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %333 : int[] = prim::ListConstruct(%328, %329, %331, %330), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.2 : Float(119:338, 12:40222, 26:13, 13:1) = aten::reshape(%positional_attn.1, %333), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:428:0
  %335 : Float(119:338, 12:40222, 26:13, 13:1) = aten::slice(%positional_attn.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %336 : Float(119:338, 12:40222, 26:13, 13:1) = aten::slice(%335, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %337 : Float(119:338, 12:40222, 25:13, 13:1) = aten::slice(%336, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.3 : Float(119:338, 12:40222, 25:13, 13:1) = aten::slice(%337, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %339 : Long() = aten::sub(%max_rel_len.1, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %340 : int = aten::Int(%339), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %341 : int[] = prim::ListConstruct(%328, %329, %330, %340), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %positional_attn.4 : Float(119:338, 12:40222, 13:25, 25:1) = aten::reshape(%positional_attn.3, %341), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.5 : Float(119:338, 12:40222, 13:25, 13:1) = aten::slice(%positional_attn.4, %66, %79, %297, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.6 : Float(119:338, 12:40222, 13:25, 13:1) = aten::mul_(%positional_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:498:0
  %345 : int = aten::size(%token_type_mat.2, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %346 : int = aten::size(%token_type_mat.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %347 : int = aten::size(%token_type_mat.2, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.1 : Float(12:64, 64:1) = aten::mul(%288, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
  %349 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_s_bias.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
  %350 : Tensor[] = prim::ListConstruct(%349, %287), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %351 : Float(119:26, 12:3094, 13:2, 2:1) = aten::einsum(%67, %350), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %352 : Bool(119:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %353 : Bool(119:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%352, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %354 : int = aten::size(%q_head.2, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %355 : int[] = prim::ListConstruct(%345, %354, %346, %347), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %token_type_mat.3 : Bool(119:169, 12:0, 13:13, 13:1) = aten::expand(%353, %355, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %357 : Tensor[] = aten::split(%351, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:371:0
  %diff_token_type.1 : Float(119:26, 12:3094, 13:2, 1:1), %same_token_type.1 : Float(119:26, 12:3094, 13:2, 1:1) = prim::ListUnpack(%357), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %360 : int = aten::size(%token_type_mat.3, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %361 : int = aten::size(%token_type_mat.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %362 : int = aten::size(%token_type_mat.3, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %363 : int = aten::size(%token_type_mat.3, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %364 : int[] = prim::ListConstruct(%360, %361, %362, %363), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %365 : Float(119:26, 12:3094, 13:2, 13:0) = aten::expand(%same_token_type.1, %364, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %366 : int = aten::size(%token_type_mat.3, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %367 : int = aten::size(%token_type_mat.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %368 : int = aten::size(%token_type_mat.3, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %369 : int = aten::size(%token_type_mat.3, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %370 : int[] = prim::ListConstruct(%366, %367, %368, %369), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %371 : Float(119:26, 12:3094, 13:2, 13:0) = aten::expand(%diff_token_type.1, %370, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.3, %365, %371), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.1, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:522:0
  %374 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.1, %positional_attn.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%374, %token_type_attn.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.1, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
  %377 : Float(119:13, 13:1) = aten::slice(%attention_mask.3, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %378 : Float(119:13, 1:13, 13:1) = aten::unsqueeze(%377, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %379 : Float(119:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%378, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %380 : Float(119:13, 1:13, 1:13, 13:1) = aten::to(%379, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %381 : Float(119:13, 1:13, 1:13, 13:1) = aten::rsub(%380, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/tensor.py:396:0
  %382 : Float(119:13, 1:13, 1:13, 13:1) = aten::mul(%381, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.2, %382, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %input.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.3, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
  %385 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.6, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %386 : Tensor[] = prim::ListConstruct(%385, %316), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %attn_vec.1 : Float(119:9984, 13:64, 12:832, 64:1) = aten::einsum(%69, %386), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %388 : int[] = prim::ListConstruct(%295, %296, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention
  %input.7 : Float(119:9984, 13:768, 768:1) = aten::reshape(%attn_vec.1, %388), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
  %390 : Tensor = prim::GetAttr[name="bias"](%286)
  %391 : Tensor = prim::GetAttr[name="weight"](%286)
  %392 : Float(768:1, 768:768) = aten::t(%391), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.7, %392), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.8 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.3, %390, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.1 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.8, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.9 : Float(119:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %attn_out.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
  %397 : Tensor = prim::GetAttr[name="bias"](%285)
  %398 : Tensor = prim::GetAttr[name="weight"](%285)
  %399 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm
  %input.10 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %399, %398, %397, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.attention/__module.funnel.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %401 : __torch__.torch.nn.modules.normalization.___torch_mangle_2417.LayerNorm = prim::GetAttr[name="layer_norm"](%283)
  %402 : __torch__.torch.nn.modules.linear.___torch_mangle_2415.Linear = prim::GetAttr[name="linear_2"](%283)
  %403 : __torch__.torch.nn.modules.linear.___torch_mangle_2413.Linear = prim::GetAttr[name="linear_1"](%283)
  %404 : Tensor = prim::GetAttr[name="bias"](%403)
  %405 : Tensor = prim::GetAttr[name="weight"](%403)
  %406 : Float(768:1, 3072:768) = aten::t(%405), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input.10, %406), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.1 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.4, %404, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %409 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%x.1, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %410 : Float(119:39936, 13:3072, 3072:1) = aten::pow(%x.1, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %411 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%410, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %412 : Float(119:39936, 13:3072, 3072:1) = aten::add(%x.1, %411, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %413 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%412, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %414 : Float(119:39936, 13:3072, 3072:1) = aten::tanh(%413), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %415 : Float(119:39936, 13:3072, 3072:1) = aten::add(%414, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.11 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%409, %415), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.12 : Float(119:39936, 13:3072, 3072:1) = aten::dropout(%input.11, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %418 : Tensor = prim::GetAttr[name="bias"](%402)
  %419 : Tensor = prim::GetAttr[name="weight"](%402)
  %420 : Float(3072:1, 768:3072) = aten::t(%419), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.12, %420), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.13 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.5, %418, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.1 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.13, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(119:9984, 13:768, 768:1) = aten::add(%input.10, %h.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
  %425 : Tensor = prim::GetAttr[name="bias"](%401)
  %426 : Tensor = prim::GetAttr[name="weight"](%401)
  %427 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm
  %query.1 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %427, %426, %425, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.0/__module.funnel.encoder.blocks.0.0.ffn/__module.funnel.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %429 : __torch__.transformers.modeling_funnel.___torch_mangle_2433.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%127)
  %430 : __torch__.transformers.modeling_funnel.___torch_mangle_2427.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%127)
  %431 : __torch__.torch.nn.modules.normalization.___torch_mangle_2426.LayerNorm = prim::GetAttr[name="layer_norm"](%430)
  %432 : __torch__.torch.nn.modules.linear.___torch_mangle_2425.Linear = prim::GetAttr[name="post_proj"](%430)
  %433 : Tensor = prim::GetAttr[name="seg_embed"](%430)
  %434 : Tensor = prim::GetAttr[name="r_s_bias"](%430)
  %435 : Tensor = prim::GetAttr[name="r_kernel"](%430)
  %436 : Tensor = prim::GetAttr[name="r_r_bias"](%430)
  %437 : Tensor = prim::GetAttr[name="r_w_bias"](%430)
  %438 : __torch__.torch.nn.modules.linear.___torch_mangle_2424.Linear = prim::GetAttr[name="v_head"](%430)
  %439 : __torch__.torch.nn.modules.linear.___torch_mangle_2423.Linear = prim::GetAttr[name="k_head"](%430)
  %440 : __torch__.torch.nn.modules.linear.___torch_mangle_2422.Linear = prim::GetAttr[name="q_head"](%430)
  %441 : int = aten::size(%query.1, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %442 : int = aten::size(%query.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %443 : int = aten::size(%query.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
  %444 : Tensor = prim::GetAttr[name="weight"](%440)
  %445 : Float(768:1, 768:768) = aten::t(%444), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %446 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.1, %445), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %447 : int[] = prim::ListConstruct(%441, %442, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %q_head.3 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%446, %447), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
  %449 : Tensor = prim::GetAttr[name="bias"](%439)
  %450 : Tensor = prim::GetAttr[name="weight"](%439)
  %451 : Float(768:1, 768:768) = aten::t(%450), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.6 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.1, %451), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %453 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.6, %449, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
  %454 : int[] = prim::ListConstruct(%441, %443, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %455 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%453, %454), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
  %456 : Tensor = prim::GetAttr[name="bias"](%438)
  %457 : Tensor = prim::GetAttr[name="weight"](%438)
  %458 : Float(768:1, 768:768) = aten::t(%457), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.7 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.1, %458), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %460 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.7, %456, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
  %461 : int[] = prim::ListConstruct(%441, %443, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %462 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%460, %461), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.4 : Float(119:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.3, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.2 : Float(12:64, 64:1) = aten::mul(%437, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
  %465 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_w_bias.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
  %466 : Tensor[] = prim::ListConstruct(%465, %455), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %content_score.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::einsum(%63, %466), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %v.2 : Float(12:64, 64:1) = aten::mul(%436, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
  %469 : Tensor[] = prim::ListConstruct(%176, %435), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %470 : Float(26:768, 12:64, 64:1) = aten::einsum(%64, %469), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %471 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %v.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
  %472 : Tensor[] = prim::ListConstruct(%471, %470), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.7 : Float(119:338, 12:40222, 13:26, 26:1) = aten::einsum(%65, %472), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %474 : int = aten::size(%positional_attn.7, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %475 : int = aten::size(%positional_attn.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %476 : int = aten::size(%positional_attn.7, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %477 : int = aten::size(%positional_attn.7, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.2 : Long() = prim::NumToTensor(%477), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %479 : int[] = prim::ListConstruct(%474, %475, %477, %476), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.8 : Float(119:338, 12:40222, 26:13, 13:1) = aten::reshape(%positional_attn.7, %479), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:428:0
  %481 : Float(119:338, 12:40222, 26:13, 13:1) = aten::slice(%positional_attn.8, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %482 : Float(119:338, 12:40222, 26:13, 13:1) = aten::slice(%481, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %483 : Float(119:338, 12:40222, 25:13, 13:1) = aten::slice(%482, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.9 : Float(119:338, 12:40222, 25:13, 13:1) = aten::slice(%483, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %485 : Long() = aten::sub(%max_rel_len.2, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %486 : int = aten::Int(%485), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %487 : int[] = prim::ListConstruct(%474, %475, %476, %486), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %positional_attn.10 : Float(119:338, 12:40222, 13:25, 25:1) = aten::reshape(%positional_attn.9, %487), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.11 : Float(119:338, 12:40222, 13:25, 13:1) = aten::slice(%positional_attn.10, %66, %79, %443, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.12 : Float(119:338, 12:40222, 13:25, 13:1) = aten::mul_(%positional_attn.11, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:498:0
  %491 : int = aten::size(%token_type_mat.2, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %492 : int = aten::size(%token_type_mat.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %493 : int = aten::size(%token_type_mat.2, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.2 : Float(12:64, 64:1) = aten::mul(%434, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
  %495 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_s_bias.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
  %496 : Tensor[] = prim::ListConstruct(%495, %433), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %497 : Float(119:26, 12:3094, 13:2, 2:1) = aten::einsum(%67, %496), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %498 : Bool(119:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %499 : Bool(119:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%498, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %500 : int = aten::size(%q_head.4, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %501 : int[] = prim::ListConstruct(%491, %500, %492, %493), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %token_type_mat.4 : Bool(119:169, 12:0, 13:13, 13:1) = aten::expand(%499, %501, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %503 : Tensor[] = aten::split(%497, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:371:0
  %diff_token_type.2 : Float(119:26, 12:3094, 13:2, 1:1), %same_token_type.2 : Float(119:26, 12:3094, 13:2, 1:1) = prim::ListUnpack(%503), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %506 : int = aten::size(%token_type_mat.4, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %507 : int = aten::size(%token_type_mat.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %508 : int = aten::size(%token_type_mat.4, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %509 : int = aten::size(%token_type_mat.4, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %510 : int[] = prim::ListConstruct(%506, %507, %508, %509), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %511 : Float(119:26, 12:3094, 13:2, 13:0) = aten::expand(%same_token_type.2, %510, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %512 : int = aten::size(%token_type_mat.4, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %513 : int = aten::size(%token_type_mat.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %514 : int = aten::size(%token_type_mat.4, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %515 : int = aten::size(%token_type_mat.4, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %516 : int[] = prim::ListConstruct(%512, %513, %514, %515), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %517 : Float(119:26, 12:3094, 13:2, 13:0) = aten::expand(%diff_token_type.2, %516, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.4, %511, %517), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.3, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:522:0
  %520 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.2, %positional_attn.12, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%520, %token_type_attn.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.4, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
  %523 : Float(119:13, 13:1) = aten::slice(%attention_mask.3, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %524 : Float(119:13, 1:13, 13:1) = aten::unsqueeze(%523, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %525 : Float(119:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%524, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %526 : Float(119:13, 1:13, 1:13, 13:1) = aten::to(%525, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %527 : Float(119:13, 1:13, 1:13, 13:1) = aten::rsub(%526, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/tensor.py:396:0
  %528 : Float(119:13, 1:13, 1:13, 13:1) = aten::mul(%527, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.5, %528, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %input.15 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.6, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
  %531 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.15, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %532 : Tensor[] = prim::ListConstruct(%531, %462), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %attn_vec.2 : Float(119:9984, 13:64, 12:832, 64:1) = aten::einsum(%69, %532), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %534 : int[] = prim::ListConstruct(%441, %442, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention
  %input.16 : Float(119:9984, 13:768, 768:1) = aten::reshape(%attn_vec.2, %534), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
  %536 : Tensor = prim::GetAttr[name="bias"](%432)
  %537 : Tensor = prim::GetAttr[name="weight"](%432)
  %538 : Float(768:1, 768:768) = aten::t(%537), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.16, %538), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.17 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.8, %536, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.2 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.17, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.18 : Float(119:9984, 13:768, 768:1) = aten::add(%query.1, %attn_out.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
  %543 : Tensor = prim::GetAttr[name="bias"](%431)
  %544 : Tensor = prim::GetAttr[name="weight"](%431)
  %545 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm
  %input.19 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.18, %545, %544, %543, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.attention/__module.funnel.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %547 : __torch__.torch.nn.modules.normalization.___torch_mangle_2432.LayerNorm = prim::GetAttr[name="layer_norm"](%429)
  %548 : __torch__.torch.nn.modules.linear.___torch_mangle_2430.Linear = prim::GetAttr[name="linear_2"](%429)
  %549 : __torch__.torch.nn.modules.linear.___torch_mangle_2428.Linear = prim::GetAttr[name="linear_1"](%429)
  %550 : Tensor = prim::GetAttr[name="bias"](%549)
  %551 : Tensor = prim::GetAttr[name="weight"](%549)
  %552 : Float(768:1, 3072:768) = aten::t(%551), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.9 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input.19, %552), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.2 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.9, %550, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %555 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%x.2, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %556 : Float(119:39936, 13:3072, 3072:1) = aten::pow(%x.2, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %557 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%556, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %558 : Float(119:39936, 13:3072, 3072:1) = aten::add(%x.2, %557, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %559 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%558, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %560 : Float(119:39936, 13:3072, 3072:1) = aten::tanh(%559), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %561 : Float(119:39936, 13:3072, 3072:1) = aten::add(%560, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.20 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%555, %561), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.21 : Float(119:39936, 13:3072, 3072:1) = aten::dropout(%input.20, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %564 : Tensor = prim::GetAttr[name="bias"](%548)
  %565 : Tensor = prim::GetAttr[name="weight"](%548)
  %566 : Float(3072:1, 768:3072) = aten::t(%565), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.10 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.21, %566), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.22 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.10, %564, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.2 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.22, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(119:9984, 13:768, 768:1) = aten::add(%input.19, %h.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
  %571 : Tensor = prim::GetAttr[name="bias"](%547)
  %572 : Tensor = prim::GetAttr[name="weight"](%547)
  %573 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm
  %query.2 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %573, %572, %571, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.1/__module.funnel.encoder.blocks.0.1.ffn/__module.funnel.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %575 : __torch__.transformers.modeling_funnel.___torch_mangle_2448.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%124)
  %576 : __torch__.transformers.modeling_funnel.___torch_mangle_2442.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%124)
  %577 : __torch__.torch.nn.modules.normalization.___torch_mangle_2441.LayerNorm = prim::GetAttr[name="layer_norm"](%576)
  %578 : __torch__.torch.nn.modules.linear.___torch_mangle_2440.Linear = prim::GetAttr[name="post_proj"](%576)
  %579 : Tensor = prim::GetAttr[name="seg_embed"](%576)
  %580 : Tensor = prim::GetAttr[name="r_s_bias"](%576)
  %581 : Tensor = prim::GetAttr[name="r_kernel"](%576)
  %582 : Tensor = prim::GetAttr[name="r_r_bias"](%576)
  %583 : Tensor = prim::GetAttr[name="r_w_bias"](%576)
  %584 : __torch__.torch.nn.modules.linear.___torch_mangle_2439.Linear = prim::GetAttr[name="v_head"](%576)
  %585 : __torch__.torch.nn.modules.linear.___torch_mangle_2438.Linear = prim::GetAttr[name="k_head"](%576)
  %586 : __torch__.torch.nn.modules.linear.___torch_mangle_2437.Linear = prim::GetAttr[name="q_head"](%576)
  %587 : int = aten::size(%query.2, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %588 : int = aten::size(%query.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %589 : int = aten::size(%query.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
  %590 : Tensor = prim::GetAttr[name="weight"](%586)
  %591 : Float(768:1, 768:768) = aten::t(%590), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %592 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.2, %591), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %593 : int[] = prim::ListConstruct(%587, %588, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %q_head.5 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%592, %593), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
  %595 : Tensor = prim::GetAttr[name="bias"](%585)
  %596 : Tensor = prim::GetAttr[name="weight"](%585)
  %597 : Float(768:1, 768:768) = aten::t(%596), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.11 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.2, %597), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %599 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.11, %595, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
  %600 : int[] = prim::ListConstruct(%587, %589, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %601 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%599, %600), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
  %602 : Tensor = prim::GetAttr[name="bias"](%584)
  %603 : Tensor = prim::GetAttr[name="weight"](%584)
  %604 : Float(768:1, 768:768) = aten::t(%603), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.12 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.2, %604), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %606 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.12, %602, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
  %607 : int[] = prim::ListConstruct(%587, %589, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %608 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%606, %607), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.6 : Float(119:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.5, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.3 : Float(12:64, 64:1) = aten::mul(%583, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
  %611 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_w_bias.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
  %612 : Tensor[] = prim::ListConstruct(%611, %601), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %content_score.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::einsum(%63, %612), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %v.3 : Float(12:64, 64:1) = aten::mul(%582, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
  %615 : Tensor[] = prim::ListConstruct(%176, %581), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %616 : Float(26:768, 12:64, 64:1) = aten::einsum(%64, %615), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %617 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %v.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
  %618 : Tensor[] = prim::ListConstruct(%617, %616), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.13 : Float(119:338, 12:40222, 13:26, 26:1) = aten::einsum(%65, %618), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %620 : int = aten::size(%positional_attn.13, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %621 : int = aten::size(%positional_attn.13, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %622 : int = aten::size(%positional_attn.13, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %623 : int = aten::size(%positional_attn.13, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.3 : Long() = prim::NumToTensor(%623), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %625 : int[] = prim::ListConstruct(%620, %621, %623, %622), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.14 : Float(119:338, 12:40222, 26:13, 13:1) = aten::reshape(%positional_attn.13, %625), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:428:0
  %627 : Float(119:338, 12:40222, 26:13, 13:1) = aten::slice(%positional_attn.14, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %628 : Float(119:338, 12:40222, 26:13, 13:1) = aten::slice(%627, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %629 : Float(119:338, 12:40222, 25:13, 13:1) = aten::slice(%628, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.15 : Float(119:338, 12:40222, 25:13, 13:1) = aten::slice(%629, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %631 : Long() = aten::sub(%max_rel_len.3, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %632 : int = aten::Int(%631), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %633 : int[] = prim::ListConstruct(%620, %621, %622, %632), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %positional_attn.16 : Float(119:338, 12:40222, 13:25, 25:1) = aten::reshape(%positional_attn.15, %633), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.17 : Float(119:338, 12:40222, 13:25, 13:1) = aten::slice(%positional_attn.16, %66, %79, %589, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.18 : Float(119:338, 12:40222, 13:25, 13:1) = aten::mul_(%positional_attn.17, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:498:0
  %637 : int = aten::size(%token_type_mat.2, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %638 : int = aten::size(%token_type_mat.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %639 : int = aten::size(%token_type_mat.2, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.3 : Float(12:64, 64:1) = aten::mul(%580, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
  %641 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_s_bias.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
  %642 : Tensor[] = prim::ListConstruct(%641, %579), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %643 : Float(119:26, 12:3094, 13:2, 2:1) = aten::einsum(%67, %642), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %644 : Bool(119:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %645 : Bool(119:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%644, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %646 : int = aten::size(%q_head.6, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %647 : int[] = prim::ListConstruct(%637, %646, %638, %639), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %token_type_mat.5 : Bool(119:169, 12:0, 13:13, 13:1) = aten::expand(%645, %647, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %649 : Tensor[] = aten::split(%643, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:371:0
  %diff_token_type.3 : Float(119:26, 12:3094, 13:2, 1:1), %same_token_type.3 : Float(119:26, 12:3094, 13:2, 1:1) = prim::ListUnpack(%649), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %652 : int = aten::size(%token_type_mat.5, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %653 : int = aten::size(%token_type_mat.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %654 : int = aten::size(%token_type_mat.5, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %655 : int = aten::size(%token_type_mat.5, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %656 : int[] = prim::ListConstruct(%652, %653, %654, %655), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %657 : Float(119:26, 12:3094, 13:2, 13:0) = aten::expand(%same_token_type.3, %656, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %658 : int = aten::size(%token_type_mat.5, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %659 : int = aten::size(%token_type_mat.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %660 : int = aten::size(%token_type_mat.5, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %661 : int = aten::size(%token_type_mat.5, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %662 : int[] = prim::ListConstruct(%658, %659, %660, %661), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %663 : Float(119:26, 12:3094, 13:2, 13:0) = aten::expand(%diff_token_type.3, %662, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.5, %657, %663), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.5, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:522:0
  %666 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.3, %positional_attn.18, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.7 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%666, %token_type_attn.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.8 : Float(119:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.7, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
  %669 : Float(119:13, 13:1) = aten::slice(%attention_mask.3, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %670 : Float(119:13, 1:13, 13:1) = aten::unsqueeze(%669, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %671 : Float(119:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%670, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %672 : Float(119:13, 1:13, 1:13, 13:1) = aten::to(%671, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %673 : Float(119:13, 1:13, 1:13, 13:1) = aten::rsub(%672, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/tensor.py:396:0
  %674 : Float(119:13, 1:13, 1:13, 13:1) = aten::mul(%673, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.9 : Float(119:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.8, %674, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %input.24 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.9, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
  %677 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.24, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %678 : Tensor[] = prim::ListConstruct(%677, %608), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %attn_vec.3 : Float(119:9984, 13:64, 12:832, 64:1) = aten::einsum(%69, %678), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %680 : int[] = prim::ListConstruct(%587, %588, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention
  %input.25 : Float(119:9984, 13:768, 768:1) = aten::reshape(%attn_vec.3, %680), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
  %682 : Tensor = prim::GetAttr[name="bias"](%578)
  %683 : Tensor = prim::GetAttr[name="weight"](%578)
  %684 : Float(768:1, 768:768) = aten::t(%683), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.25, %684), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.26 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.13, %682, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.3 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.26, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.27 : Float(119:9984, 13:768, 768:1) = aten::add(%query.2, %attn_out.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
  %689 : Tensor = prim::GetAttr[name="bias"](%577)
  %690 : Tensor = prim::GetAttr[name="weight"](%577)
  %691 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm
  %input.28 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %691, %690, %689, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.attention/__module.funnel.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %693 : __torch__.torch.nn.modules.normalization.___torch_mangle_2447.LayerNorm = prim::GetAttr[name="layer_norm"](%575)
  %694 : __torch__.torch.nn.modules.linear.___torch_mangle_2445.Linear = prim::GetAttr[name="linear_2"](%575)
  %695 : __torch__.torch.nn.modules.linear.___torch_mangle_2443.Linear = prim::GetAttr[name="linear_1"](%575)
  %696 : Tensor = prim::GetAttr[name="bias"](%695)
  %697 : Tensor = prim::GetAttr[name="weight"](%695)
  %698 : Float(768:1, 3072:768) = aten::t(%697), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.14 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input.28, %698), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.3 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.14, %696, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %701 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%x.3, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %702 : Float(119:39936, 13:3072, 3072:1) = aten::pow(%x.3, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %703 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%702, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %704 : Float(119:39936, 13:3072, 3072:1) = aten::add(%x.3, %703, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %705 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%704, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %706 : Float(119:39936, 13:3072, 3072:1) = aten::tanh(%705), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %707 : Float(119:39936, 13:3072, 3072:1) = aten::add(%706, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.29 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%701, %707), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.30 : Float(119:39936, 13:3072, 3072:1) = aten::dropout(%input.29, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %710 : Tensor = prim::GetAttr[name="bias"](%694)
  %711 : Tensor = prim::GetAttr[name="weight"](%694)
  %712 : Float(3072:1, 768:3072) = aten::t(%711), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.15 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.30, %712), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.31 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.15, %710, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.3 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.31, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.32 : Float(119:9984, 13:768, 768:1) = aten::add(%input.28, %h.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
  %717 : Tensor = prim::GetAttr[name="bias"](%693)
  %718 : Tensor = prim::GetAttr[name="weight"](%693)
  %719 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm
  %query.3 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.32, %719, %718, %717, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.2/__module.funnel.encoder.blocks.0.2.ffn/__module.funnel.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %721 : __torch__.transformers.modeling_funnel.___torch_mangle_2463.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%121)
  %722 : __torch__.transformers.modeling_funnel.___torch_mangle_2457.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%121)
  %723 : __torch__.torch.nn.modules.normalization.___torch_mangle_2456.LayerNorm = prim::GetAttr[name="layer_norm"](%722)
  %724 : __torch__.torch.nn.modules.linear.___torch_mangle_2455.Linear = prim::GetAttr[name="post_proj"](%722)
  %725 : Tensor = prim::GetAttr[name="seg_embed"](%722)
  %726 : Tensor = prim::GetAttr[name="r_s_bias"](%722)
  %727 : Tensor = prim::GetAttr[name="r_kernel"](%722)
  %728 : Tensor = prim::GetAttr[name="r_r_bias"](%722)
  %729 : Tensor = prim::GetAttr[name="r_w_bias"](%722)
  %730 : __torch__.torch.nn.modules.linear.___torch_mangle_2454.Linear = prim::GetAttr[name="v_head"](%722)
  %731 : __torch__.torch.nn.modules.linear.___torch_mangle_2453.Linear = prim::GetAttr[name="k_head"](%722)
  %732 : __torch__.torch.nn.modules.linear.___torch_mangle_2452.Linear = prim::GetAttr[name="q_head"](%722)
  %733 : int = aten::size(%query.3, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %734 : int = aten::size(%query.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %735 : int = aten::size(%query.3, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
  %736 : Tensor = prim::GetAttr[name="weight"](%732)
  %737 : Float(768:1, 768:768) = aten::t(%736), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %738 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.3, %737), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %739 : int[] = prim::ListConstruct(%733, %734, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %q_head.7 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%738, %739), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
  %741 : Tensor = prim::GetAttr[name="bias"](%731)
  %742 : Tensor = prim::GetAttr[name="weight"](%731)
  %743 : Float(768:1, 768:768) = aten::t(%742), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.16 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.3, %743), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %745 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.16, %741, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
  %746 : int[] = prim::ListConstruct(%733, %735, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %747 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%745, %746), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
  %748 : Tensor = prim::GetAttr[name="bias"](%730)
  %749 : Tensor = prim::GetAttr[name="weight"](%730)
  %750 : Float(768:1, 768:768) = aten::t(%749), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.17 : Float(119:9984, 13:768, 768:1) = aten::matmul(%query.3, %750), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %752 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.17, %748, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
  %753 : int[] = prim::ListConstruct(%733, %735, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %754 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%752, %753), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.8 : Float(119:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.7, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.4 : Float(12:64, 64:1) = aten::mul(%729, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
  %757 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_w_bias.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
  %758 : Tensor[] = prim::ListConstruct(%757, %747), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %content_score.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::einsum(%63, %758), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %v.4 : Float(12:64, 64:1) = aten::mul(%728, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
  %761 : Tensor[] = prim::ListConstruct(%176, %727), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %762 : Float(26:768, 12:64, 64:1) = aten::einsum(%64, %761), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %763 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %v.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
  %764 : Tensor[] = prim::ListConstruct(%763, %762), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.19 : Float(119:338, 12:40222, 13:26, 26:1) = aten::einsum(%65, %764), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %766 : int = aten::size(%positional_attn.19, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %767 : int = aten::size(%positional_attn.19, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %768 : int = aten::size(%positional_attn.19, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %769 : int = aten::size(%positional_attn.19, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.4 : Long() = prim::NumToTensor(%769), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %771 : int[] = prim::ListConstruct(%766, %767, %769, %768), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.20 : Float(119:338, 12:40222, 26:13, 13:1) = aten::reshape(%positional_attn.19, %771), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:428:0
  %773 : Float(119:338, 12:40222, 26:13, 13:1) = aten::slice(%positional_attn.20, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %774 : Float(119:338, 12:40222, 26:13, 13:1) = aten::slice(%773, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %775 : Float(119:338, 12:40222, 25:13, 13:1) = aten::slice(%774, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.21 : Float(119:338, 12:40222, 25:13, 13:1) = aten::slice(%775, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %777 : Long() = aten::sub(%max_rel_len.4, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %778 : int = aten::Int(%777), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %779 : int[] = prim::ListConstruct(%766, %767, %768, %778), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %positional_attn.22 : Float(119:338, 12:40222, 13:25, 25:1) = aten::reshape(%positional_attn.21, %779), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.23 : Float(119:338, 12:40222, 13:25, 13:1) = aten::slice(%positional_attn.22, %66, %79, %735, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.24 : Float(119:338, 12:40222, 13:25, 13:1) = aten::mul_(%positional_attn.23, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:498:0
  %783 : int = aten::size(%token_type_mat.2, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %784 : int = aten::size(%token_type_mat.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %785 : int = aten::size(%token_type_mat.2, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.4 : Float(12:64, 64:1) = aten::mul(%726, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
  %787 : Float(119:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_s_bias.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
  %788 : Tensor[] = prim::ListConstruct(%787, %725), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %789 : Float(119:26, 12:3094, 13:2, 2:1) = aten::einsum(%67, %788), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %790 : Bool(119:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %791 : Bool(119:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%790, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %792 : int = aten::size(%q_head.8, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %793 : int[] = prim::ListConstruct(%783, %792, %784, %785), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %token_type_mat.6 : Bool(119:169, 12:0, 13:13, 13:1) = aten::expand(%791, %793, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %795 : Tensor[] = aten::split(%789, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:371:0
  %diff_token_type.4 : Float(119:26, 12:3094, 13:2, 1:1), %same_token_type.4 : Float(119:26, 12:3094, 13:2, 1:1) = prim::ListUnpack(%795), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %798 : int = aten::size(%token_type_mat.6, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %799 : int = aten::size(%token_type_mat.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %800 : int = aten::size(%token_type_mat.6, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %801 : int = aten::size(%token_type_mat.6, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %802 : int[] = prim::ListConstruct(%798, %799, %800, %801), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %803 : Float(119:26, 12:3094, 13:2, 13:0) = aten::expand(%same_token_type.4, %802, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %804 : int = aten::size(%token_type_mat.6, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %805 : int = aten::size(%token_type_mat.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %806 : int = aten::size(%token_type_mat.6, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %807 : int = aten::size(%token_type_mat.6, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %808 : int[] = prim::ListConstruct(%804, %805, %806, %807), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %809 : Float(119:26, 12:3094, 13:2, 13:0) = aten::expand(%diff_token_type.4, %808, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.7 : Float(119:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.6, %803, %809), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.8 : Float(119:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.7, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:522:0
  %812 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.4, %positional_attn.24, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.10 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%812, %token_type_attn.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.11 : Float(119:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.10, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
  %815 : Float(119:13, 13:1) = aten::slice(%attention_mask.3, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %816 : Float(119:13, 1:13, 13:1) = aten::unsqueeze(%815, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %817 : Float(119:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%816, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %818 : Float(119:13, 1:13, 1:13, 13:1) = aten::to(%817, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %819 : Float(119:13, 1:13, 1:13, 13:1) = aten::rsub(%818, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/tensor.py:396:0
  %820 : Float(119:13, 1:13, 1:13, 13:1) = aten::mul(%819, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.12 : Float(119:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.11, %820, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %input.33 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.12, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
  %823 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.33, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %824 : Tensor[] = prim::ListConstruct(%823, %754), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %attn_vec.4 : Float(119:9984, 13:64, 12:832, 64:1) = aten::einsum(%69, %824), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %826 : int[] = prim::ListConstruct(%733, %734, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention
  %input.34 : Float(119:9984, 13:768, 768:1) = aten::reshape(%attn_vec.4, %826), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
  %828 : Tensor = prim::GetAttr[name="bias"](%724)
  %829 : Tensor = prim::GetAttr[name="weight"](%724)
  %830 : Float(768:1, 768:768) = aten::t(%829), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.18 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.34, %830), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.35 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.18, %828, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.4 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.35, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.36 : Float(119:9984, 13:768, 768:1) = aten::add(%query.3, %attn_out.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
  %835 : Tensor = prim::GetAttr[name="bias"](%723)
  %836 : Tensor = prim::GetAttr[name="weight"](%723)
  %837 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm
  %input.37 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.36, %837, %836, %835, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.attention/__module.funnel.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %839 : __torch__.torch.nn.modules.normalization.___torch_mangle_2462.LayerNorm = prim::GetAttr[name="layer_norm"](%721)
  %840 : __torch__.torch.nn.modules.linear.___torch_mangle_2460.Linear = prim::GetAttr[name="linear_2"](%721)
  %841 : __torch__.torch.nn.modules.linear.___torch_mangle_2458.Linear = prim::GetAttr[name="linear_1"](%721)
  %842 : Tensor = prim::GetAttr[name="bias"](%841)
  %843 : Tensor = prim::GetAttr[name="weight"](%841)
  %844 : Float(768:1, 3072:768) = aten::t(%843), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input.37, %844), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.4 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.19, %842, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %847 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%x.4, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %848 : Float(119:39936, 13:3072, 3072:1) = aten::pow(%x.4, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %849 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%848, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %850 : Float(119:39936, 13:3072, 3072:1) = aten::add(%x.4, %849, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %851 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%850, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %852 : Float(119:39936, 13:3072, 3072:1) = aten::tanh(%851), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %853 : Float(119:39936, 13:3072, 3072:1) = aten::add(%852, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.38 : Float(119:39936, 13:3072, 3072:1) = aten::mul(%847, %853), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.39 : Float(119:39936, 13:3072, 3072:1) = aten::dropout(%input.38, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %856 : Tensor = prim::GetAttr[name="bias"](%840)
  %857 : Tensor = prim::GetAttr[name="weight"](%840)
  %858 : Float(3072:1, 768:3072) = aten::t(%857), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.39, %858), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.40 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.20, %856, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.4 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.40, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(119:9984, 13:768, 768:1) = aten::add(%input.37, %h.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
  %863 : Tensor = prim::GetAttr[name="bias"](%839)
  %864 : Tensor = prim::GetAttr[name="weight"](%839)
  %865 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm
  %hidden.1 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.41, %865, %864, %863, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.0.3/__module.funnel.encoder.blocks.0.3.ffn/__module.funnel.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %867 : Bool(119:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %868 : Bool(119:169, 1:13, 13:1) = aten::slice(%867, %78, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %869 : Tensor[] = prim::ListConstruct(%868, %token_type_mat.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.1 : Bool(119:182, 14:13, 13:1) = aten::cat(%869, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %871 : Bool(119:182, 14:13, 13:1) = aten::slice(%tensor.1, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.7 : Bool(119:182, 7:26, 13:1) = aten::slice(%871, %78, %79, %70, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %873 : Float(1:13, 13:1) = aten::slice(%cls_mask.1, %79, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %874 : Tensor[] = prim::ListConstruct(%873, %cls_mask.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.2 : Float(14:13, 13:1) = aten::cat(%874, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.2 : Float(7:26, 13:1) = aten::slice(%tensor.2, %79, %79, %70, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %877 : Float(119:9984, 13:768, 768:1) = aten::slice(%hidden.1, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.1 : Float(119:9984, 12:768, 768:1) = aten::slice(%877, %78, %79, %70, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %879 : Float(119:9984, 13:768, 768:1) = aten::slice(%hidden.1, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %880 : Float(119:9984, 1:768, 768:1) = aten::slice(%879, %78, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %881 : Tensor[] = prim::ListConstruct(%880, %suffix.1), scope: __module.funnel/__module.funnel.encoder
  %tensor.3 : Float(119:9984, 13:768, 768:1) = aten::cat(%881, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %883 : Float(119:9984, 13:768, 768:1) = aten::slice(%tensor.3, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %884 : Float(119:9984, 1:9984, 13:768, 768:1) = aten::unsqueeze(%883, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %885 : Float(119:9984, 1:9984, 13:768, 768:1) = aten::slice(%884, %48, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.4 : Float(119:9984, 1:9984, 13:768, 768:1) = aten::slice(%885, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %887 : int[] = prim::ListConstruct(%48, %78), scope: __module.funnel/__module.funnel.encoder
  %888 : int[] = prim::ListConstruct(%48, %78), scope: __module.funnel/__module.funnel.encoder
  %889 : int[] = prim::ListConstruct(%79, %79), scope: __module.funnel/__module.funnel.encoder
  %tensor.5 : Float(119:5376, 1:5376, 7:768, 768:1) = aten::avg_pool2d(%tensor.4, %887, %888, %889, %71, %71, %42), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %891 : Float(119:5376, 1:5376, 7:768, 768:1) = aten::slice(%tensor.5, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.4 : Float(119:5376, 7:768, 768:1) = aten::select(%891, %78, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %893 : __torch__.transformers.modeling_funnel.___torch_mangle_2479.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%118)
  %894 : __torch__.transformers.modeling_funnel.___torch_mangle_2473.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%118)
  %895 : __torch__.torch.nn.modules.normalization.___torch_mangle_2472.LayerNorm = prim::GetAttr[name="layer_norm"](%894)
  %896 : __torch__.torch.nn.modules.linear.___torch_mangle_2471.Linear = prim::GetAttr[name="post_proj"](%894)
  %897 : Tensor = prim::GetAttr[name="seg_embed"](%894)
  %898 : Tensor = prim::GetAttr[name="r_s_bias"](%894)
  %899 : Tensor = prim::GetAttr[name="r_kernel"](%894)
  %900 : Tensor = prim::GetAttr[name="r_r_bias"](%894)
  %901 : Tensor = prim::GetAttr[name="r_w_bias"](%894)
  %902 : __torch__.torch.nn.modules.linear.___torch_mangle_2470.Linear = prim::GetAttr[name="v_head"](%894)
  %903 : __torch__.torch.nn.modules.linear.___torch_mangle_2469.Linear = prim::GetAttr[name="k_head"](%894)
  %904 : __torch__.torch.nn.modules.linear.___torch_mangle_2468.Linear = prim::GetAttr[name="q_head"](%894)
  %905 : int = aten::size(%query.4, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %906 : int = aten::size(%query.4, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %907 : int = aten::size(%hidden.1, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
  %908 : Tensor = prim::GetAttr[name="weight"](%904)
  %909 : Float(768:1, 768:768) = aten::t(%908), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %910 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.4, %909), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %911 : int[] = prim::ListConstruct(%905, %906, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %q_head.9 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%910, %911), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
  %913 : Tensor = prim::GetAttr[name="bias"](%903)
  %914 : Tensor = prim::GetAttr[name="weight"](%903)
  %915 : Float(768:1, 768:768) = aten::t(%914), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.21 : Float(119:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %915), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %917 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.21, %913, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
  %918 : int[] = prim::ListConstruct(%905, %907, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %919 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%917, %918), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
  %920 : Tensor = prim::GetAttr[name="bias"](%902)
  %921 : Tensor = prim::GetAttr[name="weight"](%902)
  %922 : Float(768:1, 768:768) = aten::t(%921), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.22 : Float(119:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %922), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %924 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.22, %920, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
  %925 : int[] = prim::ListConstruct(%905, %907, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %926 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%924, %925), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.10 : Float(119:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.9, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.5 : Float(12:64, 64:1) = aten::mul(%901, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
  %929 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_w_bias.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
  %930 : Tensor[] = prim::ListConstruct(%929, %919), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %content_score.5 : Float(119:1092, 12:91, 7:13, 13:1) = aten::einsum(%63, %930), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %v.5 : Float(12:64, 64:1) = aten::mul(%900, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
  %933 : Tensor[] = prim::ListConstruct(%200, %899), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %934 : Float(27:768, 12:64, 64:1) = aten::einsum(%64, %933), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %935 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %v.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
  %936 : Tensor[] = prim::ListConstruct(%935, %934), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.25 : Float(119:189, 12:22491, 7:27, 27:1) = aten::einsum(%65, %936), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %938 : int = aten::size(%positional_attn.25, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %939 : int = aten::size(%positional_attn.25, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %940 : int = aten::size(%positional_attn.25, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %941 : int = aten::size(%positional_attn.25, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.5 : Long() = prim::NumToTensor(%941), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %943 : int[] = prim::ListConstruct(%938, %939, %941, %940), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.26 : Float(119:189, 12:22491, 27:7, 7:1) = aten::reshape(%positional_attn.25, %943), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:428:0
  %945 : Float(119:189, 12:22491, 27:7, 7:1) = aten::slice(%positional_attn.26, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %946 : Float(119:189, 12:22491, 27:7, 7:1) = aten::slice(%945, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %947 : Float(119:189, 12:22491, 25:7, 7:1) = aten::slice(%946, %48, %48, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.27 : Float(119:189, 12:22491, 25:7, 7:1) = aten::slice(%947, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %949 : Long() = aten::sub(%max_rel_len.5, %44, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %950 : int = aten::Int(%949), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %951 : int[] = prim::ListConstruct(%938, %939, %940, %950), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %positional_attn.28 : Float(119:189, 12:22491, 7:25, 25:1) = aten::reshape(%positional_attn.27, %951), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.29 : Float(119:189, 12:22491, 7:25, 13:1) = aten::slice(%positional_attn.28, %66, %79, %907, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.30 : Float(119:189, 12:22491, 7:25, 13:1) = aten::mul_(%positional_attn.29, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:498:0
  %955 : int = aten::size(%token_type_mat.7, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %956 : int = aten::size(%token_type_mat.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %957 : int = aten::size(%token_type_mat.7, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.5 : Float(12:64, 64:1) = aten::mul(%898, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
  %959 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_s_bias.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
  %960 : Tensor[] = prim::ListConstruct(%959, %897), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %961 : Float(119:14, 12:1666, 7:2, 2:1) = aten::einsum(%67, %960), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %962 : Bool(119:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %963 : Bool(119:182, 1:182, 7:26, 13:1) = aten::unsqueeze(%962, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %964 : int = aten::size(%q_head.10, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %965 : int[] = prim::ListConstruct(%955, %964, %956, %957), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %token_type_mat.8 : Bool(119:182, 12:0, 7:26, 13:1) = aten::expand(%963, %965, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %967 : Tensor[] = aten::split(%961, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:371:0
  %diff_token_type.5 : Float(119:14, 12:1666, 7:2, 1:1), %same_token_type.5 : Float(119:14, 12:1666, 7:2, 1:1) = prim::ListUnpack(%967), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %970 : int = aten::size(%token_type_mat.8, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %971 : int = aten::size(%token_type_mat.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %972 : int = aten::size(%token_type_mat.8, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %973 : int = aten::size(%token_type_mat.8, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %974 : int[] = prim::ListConstruct(%970, %971, %972, %973), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %975 : Float(119:14, 12:1666, 7:2, 13:0) = aten::expand(%same_token_type.5, %974, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %976 : int = aten::size(%token_type_mat.8, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %977 : int = aten::size(%token_type_mat.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %978 : int = aten::size(%token_type_mat.8, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %979 : int = aten::size(%token_type_mat.8, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %980 : int[] = prim::ListConstruct(%976, %977, %978, %979), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %981 : Float(119:14, 12:1666, 7:2, 13:0) = aten::expand(%diff_token_type.5, %980, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.9 : Float(119:1092, 12:91, 7:13, 13:1) = aten::where(%token_type_mat.8, %975, %981), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.10 : Float(119:1092, 12:91, 7:13, 13:1) = aten::mul_(%token_type_attn.9, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:522:0
  %984 : Float(119:1092, 12:91, 7:13, 13:1) = aten::add(%content_score.5, %positional_attn.30, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.13 : Float(119:1092, 12:91, 7:13, 13:1) = aten::add(%984, %token_type_attn.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.14 : Float(119:1092, 12:91, 7:13, 13:1) = aten::to(%attn_score.13, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
  %987 : Float(119:13, 13:1) = aten::slice(%attention_mask.3, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %988 : Float(119:13, 1:13, 13:1) = aten::unsqueeze(%987, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %989 : Float(119:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%988, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %990 : Float(119:13, 1:13, 1:13, 13:1) = aten::to(%989, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %991 : Float(119:13, 1:13, 1:13, 13:1) = aten::rsub(%990, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/tensor.py:396:0
  %992 : Float(119:13, 1:13, 1:13, 13:1) = aten::mul(%991, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.15 : Float(119:1092, 12:91, 7:13, 13:1) = aten::sub(%attn_score.14, %992, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %input.42 : Float(119:1092, 12:91, 7:13, 13:1) = aten::softmax(%attn_score.15, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
  %995 : Float(119:1092, 12:91, 7:13, 13:1) = aten::dropout(%input.42, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %996 : Tensor[] = prim::ListConstruct(%995, %926), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %attn_vec.5 : Float(119:5376, 7:64, 12:448, 64:1) = aten::einsum(%69, %996), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %998 : int[] = prim::ListConstruct(%905, %906, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention
  %input.43 : Float(119:5376, 7:768, 768:1) = aten::reshape(%attn_vec.5, %998), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
  %1000 : Tensor = prim::GetAttr[name="bias"](%896)
  %1001 : Tensor = prim::GetAttr[name="weight"](%896)
  %1002 : Float(768:1, 768:768) = aten::t(%1001), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.23 : Float(119:5376, 7:768, 768:1) = aten::matmul(%input.43, %1002), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.44 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.23, %1000, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.5 : Float(119:5376, 7:768, 768:1) = aten::dropout(%input.44, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(119:5376, 7:768, 768:1) = aten::add(%query.4, %attn_out.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
  %1007 : Tensor = prim::GetAttr[name="bias"](%895)
  %1008 : Tensor = prim::GetAttr[name="weight"](%895)
  %1009 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm
  %input.46 : Float(119:5376, 7:768, 768:1) = aten::layer_norm(%input.45, %1009, %1008, %1007, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.attention/__module.funnel.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1011 : __torch__.torch.nn.modules.normalization.___torch_mangle_2478.LayerNorm = prim::GetAttr[name="layer_norm"](%893)
  %1012 : __torch__.torch.nn.modules.linear.___torch_mangle_2476.Linear = prim::GetAttr[name="linear_2"](%893)
  %1013 : __torch__.torch.nn.modules.linear.___torch_mangle_2474.Linear = prim::GetAttr[name="linear_1"](%893)
  %1014 : Tensor = prim::GetAttr[name="bias"](%1013)
  %1015 : Tensor = prim::GetAttr[name="weight"](%1013)
  %1016 : Float(768:1, 3072:768) = aten::t(%1015), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.24 : Float(119:21504, 7:3072, 3072:1) = aten::matmul(%input.46, %1016), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.5 : Float(119:21504, 7:3072, 3072:1) = aten::add_(%output.24, %1014, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1019 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%x.5, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1020 : Float(119:21504, 7:3072, 3072:1) = aten::pow(%x.5, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1021 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1020, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1022 : Float(119:21504, 7:3072, 3072:1) = aten::add(%x.5, %1021, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1023 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1022, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1024 : Float(119:21504, 7:3072, 3072:1) = aten::tanh(%1023), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1025 : Float(119:21504, 7:3072, 3072:1) = aten::add(%1024, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.47 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1019, %1025), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.48 : Float(119:21504, 7:3072, 3072:1) = aten::dropout(%input.47, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1028 : Tensor = prim::GetAttr[name="bias"](%1012)
  %1029 : Tensor = prim::GetAttr[name="weight"](%1012)
  %1030 : Float(3072:1, 768:3072) = aten::t(%1029), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.25 : Float(119:5376, 7:768, 768:1) = aten::matmul(%input.48, %1030), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.49 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.25, %1028, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.5 : Float(119:5376, 7:768, 768:1) = aten::dropout(%input.49, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(119:5376, 7:768, 768:1) = aten::add(%input.46, %h.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
  %1035 : Tensor = prim::GetAttr[name="bias"](%1011)
  %1036 : Tensor = prim::GetAttr[name="weight"](%1011)
  %1037 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm
  %query.5 : Float(119:5376, 7:768, 768:1) = aten::layer_norm(%input.50, %1037, %1036, %1035, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.0/__module.funnel.encoder.blocks.1.0.ffn/__module.funnel.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1039 : Bool(119:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1040 : Bool(119:182, 7:26, 13:1) = aten::slice(%1039, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1041 : Bool(119:182, 7:26, 1:1) = aten::slice(%1040, %48, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1042 : Tensor[] = prim::ListConstruct(%1041, %token_type_mat.7), scope: __module.funnel/__module.funnel.encoder
  %tensor.6 : Bool(119:98, 7:14, 14:1) = aten::cat(%1042, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1044 : Bool(119:98, 7:14, 14:1) = aten::slice(%tensor.6, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1045 : Bool(119:98, 7:14, 14:1) = aten::slice(%1044, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.9 : Bool(119:98, 7:14, 7:2) = aten::slice(%1045, %48, %79, %70, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1047 : Float(7:26, 13:1) = aten::slice(%cls_mask.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1048 : Float(7:26, 1:1) = aten::slice(%1047, %78, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1049 : Tensor[] = prim::ListConstruct(%1048, %cls_mask.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.7 : Float(7:14, 14:1) = aten::cat(%1049, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1051 : Float(7:14, 14:1) = aten::slice(%tensor.7, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.3 : Float(7:14, 7:2) = aten::slice(%1051, %78, %79, %70, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1053 : Float(119:13, 13:1) = aten::slice(%attention_mask.3, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.2 : Float(119:13, 12:1) = aten::slice(%1053, %78, %79, %70, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1055 : Float(119:13, 13:1) = aten::slice(%attention_mask.3, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1056 : Float(119:13, 1:1) = aten::slice(%1055, %78, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1057 : Tensor[] = prim::ListConstruct(%1056, %suffix.2), scope: __module.funnel/__module.funnel.encoder
  %tensor.8 : Float(119:13, 13:1) = aten::cat(%1057, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1059 : Float(119:13, 13:1) = aten::slice(%tensor.8, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1060 : Float(119:13, 1:13, 13:1) = aten::unsqueeze(%1059, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1061 : Float(119:13, 1:13, 13:1) = aten::slice(%1060, %48, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.9 : Float(119:13, 1:13, 13:1, 1:1) = aten::unsqueeze(%1061, %66), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.51 : Float(119:13, 1:13, 13:1, 1:1) = aten::neg(%tensor.9), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1064 : int[] = prim::ListConstruct(%48, %78), scope: __module.funnel/__module.funnel.encoder
  %1065 : int[] = prim::ListConstruct(%48, %78), scope: __module.funnel/__module.funnel.encoder
  %1066 : int[] = prim::ListConstruct(%79, %79), scope: __module.funnel/__module.funnel.encoder
  %1067 : int[] = prim::ListConstruct(%78, %78), scope: __module.funnel/__module.funnel.encoder
  %1068 : Float(119:7, 1:7, 7:1, 1:1) = aten::max_pool2d(%input.51, %1064, %1065, %1066, %1067, %71), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor.10 : Float(119:7, 1:7, 7:1, 1:1) = aten::neg(%1068), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1070 : Float(119:7, 1:7, 7:1, 1:1) = aten::slice(%tensor.10, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1071 : Float(119:7, 7:1, 1:1) = aten::select(%1070, %78, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1072 : Float(119:7, 7:1, 1:1) = aten::slice(%1071, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask.4 : Float(119:7, 7:1) = aten::select(%1072, %48, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1074 : __torch__.transformers.modeling_funnel.___torch_mangle_2494.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%115)
  %1075 : __torch__.transformers.modeling_funnel.___torch_mangle_2488.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%115)
  %1076 : __torch__.torch.nn.modules.normalization.___torch_mangle_2487.LayerNorm = prim::GetAttr[name="layer_norm"](%1075)
  %1077 : __torch__.torch.nn.modules.linear.___torch_mangle_2486.Linear = prim::GetAttr[name="post_proj"](%1075)
  %1078 : Tensor = prim::GetAttr[name="seg_embed"](%1075)
  %1079 : Tensor = prim::GetAttr[name="r_s_bias"](%1075)
  %1080 : Tensor = prim::GetAttr[name="r_kernel"](%1075)
  %1081 : Tensor = prim::GetAttr[name="r_r_bias"](%1075)
  %1082 : Tensor = prim::GetAttr[name="r_w_bias"](%1075)
  %1083 : __torch__.torch.nn.modules.linear.___torch_mangle_2485.Linear = prim::GetAttr[name="v_head"](%1075)
  %1084 : __torch__.torch.nn.modules.linear.___torch_mangle_2484.Linear = prim::GetAttr[name="k_head"](%1075)
  %1085 : __torch__.torch.nn.modules.linear.___torch_mangle_2483.Linear = prim::GetAttr[name="q_head"](%1075)
  %1086 : int = aten::size(%query.5, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1087 : int = aten::size(%query.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1088 : int = aten::size(%query.5, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
  %1089 : Tensor = prim::GetAttr[name="weight"](%1085)
  %1090 : Float(768:1, 768:768) = aten::t(%1089), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1091 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.5, %1090), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1092 : int[] = prim::ListConstruct(%1086, %1087, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %q_head.11 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1091, %1092), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
  %1094 : Tensor = prim::GetAttr[name="bias"](%1084)
  %1095 : Tensor = prim::GetAttr[name="weight"](%1084)
  %1096 : Float(768:1, 768:768) = aten::t(%1095), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.26 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.5, %1096), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %1098 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.26, %1094, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
  %1099 : int[] = prim::ListConstruct(%1086, %1088, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1100 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1098, %1099), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
  %1101 : Tensor = prim::GetAttr[name="bias"](%1083)
  %1102 : Tensor = prim::GetAttr[name="weight"](%1083)
  %1103 : Float(768:1, 768:768) = aten::t(%1102), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.27 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.5, %1103), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %1105 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.27, %1101, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
  %1106 : int[] = prim::ListConstruct(%1086, %1088, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1107 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1105, %1106), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.12 : Float(119:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.11, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.6 : Float(12:64, 64:1) = aten::mul(%1082, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
  %1110 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_w_bias.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
  %1111 : Tensor[] = prim::ListConstruct(%1110, %1100), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %content_score.6 : Float(119:588, 12:49, 7:7, 7:1) = aten::einsum(%63, %1111), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %v.6 : Float(12:64, 64:1) = aten::mul(%1081, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
  %1114 : Tensor[] = prim::ListConstruct(%218, %1080), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1115 : Float(14:768, 12:64, 64:1) = aten::einsum(%64, %1114), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1116 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %v.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
  %1117 : Tensor[] = prim::ListConstruct(%1116, %1115), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.31 : Float(119:98, 12:11662, 7:14, 14:1) = aten::einsum(%65, %1117), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1119 : int = aten::size(%positional_attn.31, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1120 : int = aten::size(%positional_attn.31, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1121 : int = aten::size(%positional_attn.31, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1122 : int = aten::size(%positional_attn.31, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.6 : Long() = prim::NumToTensor(%1122), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1124 : int[] = prim::ListConstruct(%1119, %1120, %1122, %1121), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.32 : Float(119:98, 12:11662, 14:7, 7:1) = aten::reshape(%positional_attn.31, %1124), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:428:0
  %1126 : Float(119:98, 12:11662, 14:7, 7:1) = aten::slice(%positional_attn.32, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1127 : Float(119:98, 12:11662, 14:7, 7:1) = aten::slice(%1126, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1128 : Float(119:98, 12:11662, 13:7, 7:1) = aten::slice(%1127, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.33 : Float(119:98, 12:11662, 13:7, 7:1) = aten::slice(%1128, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1130 : Long() = aten::sub(%max_rel_len.6, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %1131 : int = aten::Int(%1130), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1132 : int[] = prim::ListConstruct(%1119, %1120, %1121, %1131), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %positional_attn.34 : Float(119:98, 12:11662, 7:13, 13:1) = aten::reshape(%positional_attn.33, %1132), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.35 : Float(119:98, 12:11662, 7:13, 7:1) = aten::slice(%positional_attn.34, %66, %79, %1088, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.36 : Float(119:98, 12:11662, 7:13, 7:1) = aten::mul_(%positional_attn.35, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:498:0
  %1136 : int = aten::size(%token_type_mat.9, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1137 : int = aten::size(%token_type_mat.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1138 : int = aten::size(%token_type_mat.9, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.6 : Float(12:64, 64:1) = aten::mul(%1079, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
  %1140 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_s_bias.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
  %1141 : Tensor[] = prim::ListConstruct(%1140, %1078), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1142 : Float(119:14, 12:1666, 7:2, 2:1) = aten::einsum(%67, %1141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1143 : Bool(119:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1144 : Bool(119:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1143, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1145 : int = aten::size(%q_head.12, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1146 : int[] = prim::ListConstruct(%1136, %1145, %1137, %1138), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %token_type_mat.10 : Bool(119:98, 12:0, 7:14, 7:2) = aten::expand(%1144, %1146, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1148 : Tensor[] = aten::split(%1142, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:371:0
  %diff_token_type.6 : Float(119:14, 12:1666, 7:2, 1:1), %same_token_type.6 : Float(119:14, 12:1666, 7:2, 1:1) = prim::ListUnpack(%1148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1151 : int = aten::size(%token_type_mat.10, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1152 : int = aten::size(%token_type_mat.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1153 : int = aten::size(%token_type_mat.10, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1154 : int = aten::size(%token_type_mat.10, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1155 : int[] = prim::ListConstruct(%1151, %1152, %1153, %1154), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1156 : Float(119:14, 12:1666, 7:2, 7:0) = aten::expand(%same_token_type.6, %1155, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1157 : int = aten::size(%token_type_mat.10, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1158 : int = aten::size(%token_type_mat.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1159 : int = aten::size(%token_type_mat.10, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1160 : int = aten::size(%token_type_mat.10, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1161 : int[] = prim::ListConstruct(%1157, %1158, %1159, %1160), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %1162 : Float(119:14, 12:1666, 7:2, 7:0) = aten::expand(%diff_token_type.6, %1161, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.11 : Float(119:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.10, %1156, %1162), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.12 : Float(119:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.11, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:522:0
  %1165 : Float(119:588, 12:49, 7:7, 7:1) = aten::add(%content_score.6, %positional_attn.36, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.16 : Float(119:588, 12:49, 7:7, 7:1) = aten::add(%1165, %token_type_attn.12, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.17 : Float(119:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.16, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
  %1168 : Float(119:7, 7:1) = aten::slice(%attention_mask.4, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1169 : Float(119:7, 1:7, 7:1) = aten::unsqueeze(%1168, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1170 : Float(119:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1169, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1171 : Float(119:7, 1:7, 1:7, 7:1) = aten::to(%1170, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1172 : Float(119:7, 1:7, 1:7, 7:1) = aten::rsub(%1171, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/tensor.py:396:0
  %1173 : Float(119:7, 1:7, 1:7, 7:1) = aten::mul(%1172, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.18 : Float(119:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.17, %1173, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %input.52 : Float(119:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.18, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
  %1176 : Float(119:588, 12:49, 7:7, 7:1) = aten::dropout(%input.52, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1177 : Tensor[] = prim::ListConstruct(%1176, %1107), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %attn_vec.6 : Float(119:5376, 7:64, 12:448, 64:1) = aten::einsum(%69, %1177), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1179 : int[] = prim::ListConstruct(%1086, %1087, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention
  %input.53 : Float(119:5376, 7:768, 768:1) = aten::reshape(%attn_vec.6, %1179), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
  %1181 : Tensor = prim::GetAttr[name="bias"](%1077)
  %1182 : Tensor = prim::GetAttr[name="weight"](%1077)
  %1183 : Float(768:1, 768:768) = aten::t(%1182), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(119:5376, 7:768, 768:1) = aten::matmul(%input.53, %1183), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.54 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.28, %1181, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.6 : Float(119:5376, 7:768, 768:1) = aten::dropout(%input.54, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.55 : Float(119:5376, 7:768, 768:1) = aten::add(%query.5, %attn_out.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
  %1188 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1189 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1190 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm
  %input.56 : Float(119:5376, 7:768, 768:1) = aten::layer_norm(%input.55, %1190, %1189, %1188, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.attention/__module.funnel.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1192 : __torch__.torch.nn.modules.normalization.___torch_mangle_2493.LayerNorm = prim::GetAttr[name="layer_norm"](%1074)
  %1193 : __torch__.torch.nn.modules.linear.___torch_mangle_2491.Linear = prim::GetAttr[name="linear_2"](%1074)
  %1194 : __torch__.torch.nn.modules.linear.___torch_mangle_2489.Linear = prim::GetAttr[name="linear_1"](%1074)
  %1195 : Tensor = prim::GetAttr[name="bias"](%1194)
  %1196 : Tensor = prim::GetAttr[name="weight"](%1194)
  %1197 : Float(768:1, 3072:768) = aten::t(%1196), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.29 : Float(119:21504, 7:3072, 3072:1) = aten::matmul(%input.56, %1197), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.6 : Float(119:21504, 7:3072, 3072:1) = aten::add_(%output.29, %1195, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1200 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%x.6, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1201 : Float(119:21504, 7:3072, 3072:1) = aten::pow(%x.6, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1202 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1201, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1203 : Float(119:21504, 7:3072, 3072:1) = aten::add(%x.6, %1202, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1204 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1203, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1205 : Float(119:21504, 7:3072, 3072:1) = aten::tanh(%1204), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1206 : Float(119:21504, 7:3072, 3072:1) = aten::add(%1205, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.57 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1200, %1206), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.58 : Float(119:21504, 7:3072, 3072:1) = aten::dropout(%input.57, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1209 : Tensor = prim::GetAttr[name="bias"](%1193)
  %1210 : Tensor = prim::GetAttr[name="weight"](%1193)
  %1211 : Float(3072:1, 768:3072) = aten::t(%1210), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.30 : Float(119:5376, 7:768, 768:1) = aten::matmul(%input.58, %1211), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.59 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.30, %1209, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.6 : Float(119:5376, 7:768, 768:1) = aten::dropout(%input.59, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(119:5376, 7:768, 768:1) = aten::add(%input.56, %h.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
  %1216 : Tensor = prim::GetAttr[name="bias"](%1192)
  %1217 : Tensor = prim::GetAttr[name="weight"](%1192)
  %1218 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm
  %query.6 : Float(119:5376, 7:768, 768:1) = aten::layer_norm(%input.60, %1218, %1217, %1216, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.1/__module.funnel.encoder.blocks.1.1.ffn/__module.funnel.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1220 : __torch__.transformers.modeling_funnel.___torch_mangle_2509.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%112)
  %1221 : __torch__.transformers.modeling_funnel.___torch_mangle_2503.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%112)
  %1222 : __torch__.torch.nn.modules.normalization.___torch_mangle_2502.LayerNorm = prim::GetAttr[name="layer_norm"](%1221)
  %1223 : __torch__.torch.nn.modules.linear.___torch_mangle_2501.Linear = prim::GetAttr[name="post_proj"](%1221)
  %1224 : Tensor = prim::GetAttr[name="seg_embed"](%1221)
  %1225 : Tensor = prim::GetAttr[name="r_s_bias"](%1221)
  %1226 : Tensor = prim::GetAttr[name="r_kernel"](%1221)
  %1227 : Tensor = prim::GetAttr[name="r_r_bias"](%1221)
  %1228 : Tensor = prim::GetAttr[name="r_w_bias"](%1221)
  %1229 : __torch__.torch.nn.modules.linear.___torch_mangle_2500.Linear = prim::GetAttr[name="v_head"](%1221)
  %1230 : __torch__.torch.nn.modules.linear.___torch_mangle_2499.Linear = prim::GetAttr[name="k_head"](%1221)
  %1231 : __torch__.torch.nn.modules.linear.___torch_mangle_2498.Linear = prim::GetAttr[name="q_head"](%1221)
  %1232 : int = aten::size(%query.6, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1233 : int = aten::size(%query.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1234 : int = aten::size(%query.6, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
  %1235 : Tensor = prim::GetAttr[name="weight"](%1231)
  %1236 : Float(768:1, 768:768) = aten::t(%1235), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1237 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.6, %1236), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1238 : int[] = prim::ListConstruct(%1232, %1233, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %q_head.13 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1237, %1238), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
  %1240 : Tensor = prim::GetAttr[name="bias"](%1230)
  %1241 : Tensor = prim::GetAttr[name="weight"](%1230)
  %1242 : Float(768:1, 768:768) = aten::t(%1241), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.31 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.6, %1242), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %1244 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.31, %1240, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
  %1245 : int[] = prim::ListConstruct(%1232, %1234, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1246 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1244, %1245), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
  %1247 : Tensor = prim::GetAttr[name="bias"](%1229)
  %1248 : Tensor = prim::GetAttr[name="weight"](%1229)
  %1249 : Float(768:1, 768:768) = aten::t(%1248), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.32 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.6, %1249), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %1251 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.32, %1247, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
  %1252 : int[] = prim::ListConstruct(%1232, %1234, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1253 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1251, %1252), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.14 : Float(119:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.13, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.7 : Float(12:64, 64:1) = aten::mul(%1228, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
  %1256 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_w_bias.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
  %1257 : Tensor[] = prim::ListConstruct(%1256, %1246), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %content_score.7 : Float(119:588, 12:49, 7:7, 7:1) = aten::einsum(%63, %1257), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %v.7 : Float(12:64, 64:1) = aten::mul(%1227, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
  %1260 : Tensor[] = prim::ListConstruct(%218, %1226), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1261 : Float(14:768, 12:64, 64:1) = aten::einsum(%64, %1260), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1262 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %v.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
  %1263 : Tensor[] = prim::ListConstruct(%1262, %1261), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.37 : Float(119:98, 12:11662, 7:14, 14:1) = aten::einsum(%65, %1263), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1265 : int = aten::size(%positional_attn.37, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1266 : int = aten::size(%positional_attn.37, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1267 : int = aten::size(%positional_attn.37, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1268 : int = aten::size(%positional_attn.37, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.7 : Long() = prim::NumToTensor(%1268), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1270 : int[] = prim::ListConstruct(%1265, %1266, %1268, %1267), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.38 : Float(119:98, 12:11662, 14:7, 7:1) = aten::reshape(%positional_attn.37, %1270), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:428:0
  %1272 : Float(119:98, 12:11662, 14:7, 7:1) = aten::slice(%positional_attn.38, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1273 : Float(119:98, 12:11662, 14:7, 7:1) = aten::slice(%1272, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1274 : Float(119:98, 12:11662, 13:7, 7:1) = aten::slice(%1273, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.39 : Float(119:98, 12:11662, 13:7, 7:1) = aten::slice(%1274, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1276 : Long() = aten::sub(%max_rel_len.7, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %1277 : int = aten::Int(%1276), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1278 : int[] = prim::ListConstruct(%1265, %1266, %1267, %1277), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %positional_attn.40 : Float(119:98, 12:11662, 7:13, 13:1) = aten::reshape(%positional_attn.39, %1278), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.41 : Float(119:98, 12:11662, 7:13, 7:1) = aten::slice(%positional_attn.40, %66, %79, %1234, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.42 : Float(119:98, 12:11662, 7:13, 7:1) = aten::mul_(%positional_attn.41, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:498:0
  %1282 : int = aten::size(%token_type_mat.9, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1283 : int = aten::size(%token_type_mat.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1284 : int = aten::size(%token_type_mat.9, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.7 : Float(12:64, 64:1) = aten::mul(%1225, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
  %1286 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_s_bias.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
  %1287 : Tensor[] = prim::ListConstruct(%1286, %1224), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1288 : Float(119:14, 12:1666, 7:2, 2:1) = aten::einsum(%67, %1287), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1289 : Bool(119:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1290 : Bool(119:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1289, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1291 : int = aten::size(%q_head.14, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1292 : int[] = prim::ListConstruct(%1282, %1291, %1283, %1284), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %token_type_mat.11 : Bool(119:98, 12:0, 7:14, 7:2) = aten::expand(%1290, %1292, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1294 : Tensor[] = aten::split(%1288, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:371:0
  %diff_token_type.7 : Float(119:14, 12:1666, 7:2, 1:1), %same_token_type.7 : Float(119:14, 12:1666, 7:2, 1:1) = prim::ListUnpack(%1294), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1297 : int = aten::size(%token_type_mat.11, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1298 : int = aten::size(%token_type_mat.11, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1299 : int = aten::size(%token_type_mat.11, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1300 : int = aten::size(%token_type_mat.11, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1301 : int[] = prim::ListConstruct(%1297, %1298, %1299, %1300), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1302 : Float(119:14, 12:1666, 7:2, 7:0) = aten::expand(%same_token_type.7, %1301, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1303 : int = aten::size(%token_type_mat.11, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1304 : int = aten::size(%token_type_mat.11, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1305 : int = aten::size(%token_type_mat.11, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1306 : int = aten::size(%token_type_mat.11, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1307 : int[] = prim::ListConstruct(%1303, %1304, %1305, %1306), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %1308 : Float(119:14, 12:1666, 7:2, 7:0) = aten::expand(%diff_token_type.7, %1307, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.13 : Float(119:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.11, %1302, %1308), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.14 : Float(119:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.13, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:522:0
  %1311 : Float(119:588, 12:49, 7:7, 7:1) = aten::add(%content_score.7, %positional_attn.42, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.19 : Float(119:588, 12:49, 7:7, 7:1) = aten::add(%1311, %token_type_attn.14, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.20 : Float(119:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.19, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
  %1314 : Float(119:7, 7:1) = aten::slice(%attention_mask.4, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1315 : Float(119:7, 1:7, 7:1) = aten::unsqueeze(%1314, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1316 : Float(119:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1315, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1317 : Float(119:7, 1:7, 1:7, 7:1) = aten::to(%1316, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1318 : Float(119:7, 1:7, 1:7, 7:1) = aten::rsub(%1317, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/tensor.py:396:0
  %1319 : Float(119:7, 1:7, 1:7, 7:1) = aten::mul(%1318, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.21 : Float(119:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.20, %1319, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %input.61 : Float(119:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.21, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
  %1322 : Float(119:588, 12:49, 7:7, 7:1) = aten::dropout(%input.61, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1323 : Tensor[] = prim::ListConstruct(%1322, %1253), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %attn_vec.7 : Float(119:5376, 7:64, 12:448, 64:1) = aten::einsum(%69, %1323), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1325 : int[] = prim::ListConstruct(%1232, %1233, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention
  %input.62 : Float(119:5376, 7:768, 768:1) = aten::reshape(%attn_vec.7, %1325), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
  %1327 : Tensor = prim::GetAttr[name="bias"](%1223)
  %1328 : Tensor = prim::GetAttr[name="weight"](%1223)
  %1329 : Float(768:1, 768:768) = aten::t(%1328), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(119:5376, 7:768, 768:1) = aten::matmul(%input.62, %1329), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.63 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.33, %1327, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.7 : Float(119:5376, 7:768, 768:1) = aten::dropout(%input.63, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.64 : Float(119:5376, 7:768, 768:1) = aten::add(%query.6, %attn_out.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
  %1334 : Tensor = prim::GetAttr[name="bias"](%1222)
  %1335 : Tensor = prim::GetAttr[name="weight"](%1222)
  %1336 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm
  %input.65 : Float(119:5376, 7:768, 768:1) = aten::layer_norm(%input.64, %1336, %1335, %1334, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.attention/__module.funnel.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1338 : __torch__.torch.nn.modules.normalization.___torch_mangle_2508.LayerNorm = prim::GetAttr[name="layer_norm"](%1220)
  %1339 : __torch__.torch.nn.modules.linear.___torch_mangle_2506.Linear = prim::GetAttr[name="linear_2"](%1220)
  %1340 : __torch__.torch.nn.modules.linear.___torch_mangle_2504.Linear = prim::GetAttr[name="linear_1"](%1220)
  %1341 : Tensor = prim::GetAttr[name="bias"](%1340)
  %1342 : Tensor = prim::GetAttr[name="weight"](%1340)
  %1343 : Float(768:1, 3072:768) = aten::t(%1342), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(119:21504, 7:3072, 3072:1) = aten::matmul(%input.65, %1343), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.7 : Float(119:21504, 7:3072, 3072:1) = aten::add_(%output.34, %1341, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1346 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%x.7, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1347 : Float(119:21504, 7:3072, 3072:1) = aten::pow(%x.7, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1348 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1347, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1349 : Float(119:21504, 7:3072, 3072:1) = aten::add(%x.7, %1348, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1350 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1349, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1351 : Float(119:21504, 7:3072, 3072:1) = aten::tanh(%1350), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1352 : Float(119:21504, 7:3072, 3072:1) = aten::add(%1351, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.66 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1346, %1352), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.67 : Float(119:21504, 7:3072, 3072:1) = aten::dropout(%input.66, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1355 : Tensor = prim::GetAttr[name="bias"](%1339)
  %1356 : Tensor = prim::GetAttr[name="weight"](%1339)
  %1357 : Float(3072:1, 768:3072) = aten::t(%1356), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(119:5376, 7:768, 768:1) = aten::matmul(%input.67, %1357), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.68 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.35, %1355, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.7 : Float(119:5376, 7:768, 768:1) = aten::dropout(%input.68, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(119:5376, 7:768, 768:1) = aten::add(%input.65, %h.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
  %1362 : Tensor = prim::GetAttr[name="bias"](%1338)
  %1363 : Tensor = prim::GetAttr[name="weight"](%1338)
  %1364 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm
  %query.7 : Float(119:5376, 7:768, 768:1) = aten::layer_norm(%input.69, %1364, %1363, %1362, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.2/__module.funnel.encoder.blocks.1.2.ffn/__module.funnel.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1366 : __torch__.transformers.modeling_funnel.___torch_mangle_2524.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%109)
  %1367 : __torch__.transformers.modeling_funnel.___torch_mangle_2518.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%109)
  %1368 : __torch__.torch.nn.modules.normalization.___torch_mangle_2517.LayerNorm = prim::GetAttr[name="layer_norm"](%1367)
  %1369 : __torch__.torch.nn.modules.linear.___torch_mangle_2516.Linear = prim::GetAttr[name="post_proj"](%1367)
  %1370 : Tensor = prim::GetAttr[name="seg_embed"](%1367)
  %1371 : Tensor = prim::GetAttr[name="r_s_bias"](%1367)
  %1372 : Tensor = prim::GetAttr[name="r_kernel"](%1367)
  %1373 : Tensor = prim::GetAttr[name="r_r_bias"](%1367)
  %1374 : Tensor = prim::GetAttr[name="r_w_bias"](%1367)
  %1375 : __torch__.torch.nn.modules.linear.___torch_mangle_2515.Linear = prim::GetAttr[name="v_head"](%1367)
  %1376 : __torch__.torch.nn.modules.linear.___torch_mangle_2514.Linear = prim::GetAttr[name="k_head"](%1367)
  %1377 : __torch__.torch.nn.modules.linear.___torch_mangle_2513.Linear = prim::GetAttr[name="q_head"](%1367)
  %1378 : int = aten::size(%query.7, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1379 : int = aten::size(%query.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1380 : int = aten::size(%query.7, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
  %1381 : Tensor = prim::GetAttr[name="weight"](%1377)
  %1382 : Float(768:1, 768:768) = aten::t(%1381), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1383 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.7, %1382), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1384 : int[] = prim::ListConstruct(%1378, %1379, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %q_head.15 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1383, %1384), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
  %1386 : Tensor = prim::GetAttr[name="bias"](%1376)
  %1387 : Tensor = prim::GetAttr[name="weight"](%1376)
  %1388 : Float(768:1, 768:768) = aten::t(%1387), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.36 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.7, %1388), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %1390 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.36, %1386, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
  %1391 : int[] = prim::ListConstruct(%1378, %1380, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1392 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1390, %1391), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
  %1393 : Tensor = prim::GetAttr[name="bias"](%1375)
  %1394 : Tensor = prim::GetAttr[name="weight"](%1375)
  %1395 : Float(768:1, 768:768) = aten::t(%1394), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.37 : Float(119:5376, 7:768, 768:1) = aten::matmul(%query.7, %1395), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %1397 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.37, %1393, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
  %1398 : int[] = prim::ListConstruct(%1378, %1380, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1399 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1397, %1398), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.16 : Float(119:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.15, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.8 : Float(12:64, 64:1) = aten::mul(%1374, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
  %1402 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_w_bias.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
  %1403 : Tensor[] = prim::ListConstruct(%1402, %1392), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %content_score.8 : Float(119:588, 12:49, 7:7, 7:1) = aten::einsum(%63, %1403), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %v.8 : Float(12:64, 64:1) = aten::mul(%1373, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
  %1406 : Tensor[] = prim::ListConstruct(%218, %1372), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1407 : Float(14:768, 12:64, 64:1) = aten::einsum(%64, %1406), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1408 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %v.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
  %1409 : Tensor[] = prim::ListConstruct(%1408, %1407), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.43 : Float(119:98, 12:11662, 7:14, 14:1) = aten::einsum(%65, %1409), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1411 : int = aten::size(%positional_attn.43, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1412 : int = aten::size(%positional_attn.43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1413 : int = aten::size(%positional_attn.43, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1414 : int = aten::size(%positional_attn.43, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.8 : Long() = prim::NumToTensor(%1414), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1416 : int[] = prim::ListConstruct(%1411, %1412, %1414, %1413), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.44 : Float(119:98, 12:11662, 14:7, 7:1) = aten::reshape(%positional_attn.43, %1416), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:428:0
  %1418 : Float(119:98, 12:11662, 14:7, 7:1) = aten::slice(%positional_attn.44, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1419 : Float(119:98, 12:11662, 14:7, 7:1) = aten::slice(%1418, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1420 : Float(119:98, 12:11662, 13:7, 7:1) = aten::slice(%1419, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.45 : Float(119:98, 12:11662, 13:7, 7:1) = aten::slice(%1420, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1422 : Long() = aten::sub(%max_rel_len.8, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %1423 : int = aten::Int(%1422), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1424 : int[] = prim::ListConstruct(%1411, %1412, %1413, %1423), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %positional_attn.46 : Float(119:98, 12:11662, 7:13, 13:1) = aten::reshape(%positional_attn.45, %1424), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.47 : Float(119:98, 12:11662, 7:13, 7:1) = aten::slice(%positional_attn.46, %66, %79, %1380, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.48 : Float(119:98, 12:11662, 7:13, 7:1) = aten::mul_(%positional_attn.47, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:498:0
  %1428 : int = aten::size(%token_type_mat.9, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1429 : int = aten::size(%token_type_mat.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1430 : int = aten::size(%token_type_mat.9, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.8 : Float(12:64, 64:1) = aten::mul(%1371, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
  %1432 : Float(119:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_s_bias.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
  %1433 : Tensor[] = prim::ListConstruct(%1432, %1370), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1434 : Float(119:14, 12:1666, 7:2, 2:1) = aten::einsum(%67, %1433), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1435 : Bool(119:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1436 : Bool(119:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1435, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1437 : int = aten::size(%q_head.16, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1438 : int[] = prim::ListConstruct(%1428, %1437, %1429, %1430), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %token_type_mat.12 : Bool(119:98, 12:0, 7:14, 7:2) = aten::expand(%1436, %1438, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1440 : Tensor[] = aten::split(%1434, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:371:0
  %diff_token_type.8 : Float(119:14, 12:1666, 7:2, 1:1), %same_token_type.8 : Float(119:14, 12:1666, 7:2, 1:1) = prim::ListUnpack(%1440), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1443 : int = aten::size(%token_type_mat.12, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1444 : int = aten::size(%token_type_mat.12, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1445 : int = aten::size(%token_type_mat.12, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1446 : int = aten::size(%token_type_mat.12, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1447 : int[] = prim::ListConstruct(%1443, %1444, %1445, %1446), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1448 : Float(119:14, 12:1666, 7:2, 7:0) = aten::expand(%same_token_type.8, %1447, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1449 : int = aten::size(%token_type_mat.12, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1450 : int = aten::size(%token_type_mat.12, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1451 : int = aten::size(%token_type_mat.12, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1452 : int = aten::size(%token_type_mat.12, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1453 : int[] = prim::ListConstruct(%1449, %1450, %1451, %1452), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %1454 : Float(119:14, 12:1666, 7:2, 7:0) = aten::expand(%diff_token_type.8, %1453, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.15 : Float(119:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.12, %1448, %1454), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.16 : Float(119:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.15, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:522:0
  %1457 : Float(119:588, 12:49, 7:7, 7:1) = aten::add(%content_score.8, %positional_attn.48, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.22 : Float(119:588, 12:49, 7:7, 7:1) = aten::add(%1457, %token_type_attn.16, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.23 : Float(119:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.22, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
  %1460 : Float(119:7, 7:1) = aten::slice(%attention_mask.4, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1461 : Float(119:7, 1:7, 7:1) = aten::unsqueeze(%1460, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1462 : Float(119:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1461, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1463 : Float(119:7, 1:7, 1:7, 7:1) = aten::to(%1462, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1464 : Float(119:7, 1:7, 1:7, 7:1) = aten::rsub(%1463, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/tensor.py:396:0
  %1465 : Float(119:7, 1:7, 1:7, 7:1) = aten::mul(%1464, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.24 : Float(119:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.23, %1465, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %input.70 : Float(119:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.24, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
  %1468 : Float(119:588, 12:49, 7:7, 7:1) = aten::dropout(%input.70, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %1469 : Tensor[] = prim::ListConstruct(%1468, %1399), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %attn_vec.8 : Float(119:5376, 7:64, 12:448, 64:1) = aten::einsum(%69, %1469), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1471 : int[] = prim::ListConstruct(%1378, %1379, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention
  %input.71 : Float(119:5376, 7:768, 768:1) = aten::reshape(%attn_vec.8, %1471), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
  %1473 : Tensor = prim::GetAttr[name="bias"](%1369)
  %1474 : Tensor = prim::GetAttr[name="weight"](%1369)
  %1475 : Float(768:1, 768:768) = aten::t(%1474), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(119:5376, 7:768, 768:1) = aten::matmul(%input.71, %1475), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.72 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.38, %1473, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.8 : Float(119:5376, 7:768, 768:1) = aten::dropout(%input.72, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(119:5376, 7:768, 768:1) = aten::add(%query.7, %attn_out.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
  %1480 : Tensor = prim::GetAttr[name="bias"](%1368)
  %1481 : Tensor = prim::GetAttr[name="weight"](%1368)
  %1482 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm
  %input.74 : Float(119:5376, 7:768, 768:1) = aten::layer_norm(%input.73, %1482, %1481, %1480, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.attention/__module.funnel.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %1484 : __torch__.torch.nn.modules.normalization.___torch_mangle_2523.LayerNorm = prim::GetAttr[name="layer_norm"](%1366)
  %1485 : __torch__.torch.nn.modules.linear.___torch_mangle_2521.Linear = prim::GetAttr[name="linear_2"](%1366)
  %1486 : __torch__.torch.nn.modules.linear.___torch_mangle_2519.Linear = prim::GetAttr[name="linear_1"](%1366)
  %1487 : Tensor = prim::GetAttr[name="bias"](%1486)
  %1488 : Tensor = prim::GetAttr[name="weight"](%1486)
  %1489 : Float(768:1, 3072:768) = aten::t(%1488), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.39 : Float(119:21504, 7:3072, 3072:1) = aten::matmul(%input.74, %1489), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.8 : Float(119:21504, 7:3072, 3072:1) = aten::add_(%output.39, %1487, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1492 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%x.8, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1493 : Float(119:21504, 7:3072, 3072:1) = aten::pow(%x.8, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1494 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1493, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1495 : Float(119:21504, 7:3072, 3072:1) = aten::add(%x.8, %1494, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1496 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1495, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1497 : Float(119:21504, 7:3072, 3072:1) = aten::tanh(%1496), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1498 : Float(119:21504, 7:3072, 3072:1) = aten::add(%1497, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.75 : Float(119:21504, 7:3072, 3072:1) = aten::mul(%1492, %1498), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.76 : Float(119:21504, 7:3072, 3072:1) = aten::dropout(%input.75, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1501 : Tensor = prim::GetAttr[name="bias"](%1485)
  %1502 : Tensor = prim::GetAttr[name="weight"](%1485)
  %1503 : Float(3072:1, 768:3072) = aten::t(%1502), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.40 : Float(119:5376, 7:768, 768:1) = aten::matmul(%input.76, %1503), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.77 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.40, %1501, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.8 : Float(119:5376, 7:768, 768:1) = aten::dropout(%input.77, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(119:5376, 7:768, 768:1) = aten::add(%input.74, %h.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
  %1508 : Tensor = prim::GetAttr[name="bias"](%1484)
  %1509 : Tensor = prim::GetAttr[name="weight"](%1484)
  %1510 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm
  %hidden.2 : Float(119:5376, 7:768, 768:1) = aten::layer_norm(%input.78, %1510, %1509, %1508, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.1.3/__module.funnel.encoder.blocks.1.3.ffn/__module.funnel.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1512 : Bool(119:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1513 : Bool(119:98, 1:14, 7:2) = aten::slice(%1512, %78, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1514 : Tensor[] = prim::ListConstruct(%1513, %token_type_mat.9), scope: __module.funnel/__module.funnel.encoder
  %tensor.11 : Bool(119:56, 8:7, 7:1) = aten::cat(%1514, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1516 : Bool(119:56, 8:7, 7:1) = aten::slice(%tensor.11, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.13 : Bool(119:56, 4:14, 7:1) = aten::slice(%1516, %78, %79, %70, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1518 : Float(1:14, 7:2) = aten::slice(%cls_mask.3, %79, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1519 : Tensor[] = prim::ListConstruct(%1518, %cls_mask.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.12 : Float(8:7, 7:1) = aten::cat(%1519, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.4 : Float(4:14, 7:1) = aten::slice(%tensor.12, %79, %79, %70, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1522 : Float(119:5376, 7:768, 768:1) = aten::slice(%hidden.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix.3 : Float(119:5376, 6:768, 768:1) = aten::slice(%1522, %78, %79, %70, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1524 : Float(119:5376, 7:768, 768:1) = aten::slice(%hidden.2, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1525 : Float(119:5376, 1:768, 768:1) = aten::slice(%1524, %78, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1526 : Tensor[] = prim::ListConstruct(%1525, %suffix.3), scope: __module.funnel/__module.funnel.encoder
  %tensor.13 : Float(119:5376, 7:768, 768:1) = aten::cat(%1526, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1528 : Float(119:5376, 7:768, 768:1) = aten::slice(%tensor.13, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1529 : Float(119:5376, 1:5376, 7:768, 768:1) = aten::unsqueeze(%1528, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1530 : Float(119:5376, 1:5376, 7:768, 768:1) = aten::slice(%1529, %48, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %tensor.14 : Float(119:5376, 1:5376, 7:768, 768:1) = aten::slice(%1530, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:366:0
  %1532 : int[] = prim::ListConstruct(%48, %78), scope: __module.funnel/__module.funnel.encoder
  %1533 : int[] = prim::ListConstruct(%48, %78), scope: __module.funnel/__module.funnel.encoder
  %1534 : int[] = prim::ListConstruct(%79, %79), scope: __module.funnel/__module.funnel.encoder
  %tensor.15 : Float(119:3072, 1:3072, 4:768, 768:1) = aten::avg_pool2d(%tensor.14, %1532, %1533, %1534, %71, %71, %42), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:371:0
  %1536 : Float(119:3072, 1:3072, 4:768, 768:1) = aten::slice(%tensor.15, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %query.8 : Float(119:3072, 4:768, 768:1) = aten::select(%1536, %78, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:382:0
  %1538 : __torch__.transformers.modeling_funnel.___torch_mangle_2540.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%106)
  %1539 : __torch__.transformers.modeling_funnel.___torch_mangle_2534.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%106)
  %1540 : __torch__.torch.nn.modules.normalization.___torch_mangle_2533.LayerNorm = prim::GetAttr[name="layer_norm"](%1539)
  %1541 : __torch__.torch.nn.modules.linear.___torch_mangle_2532.Linear = prim::GetAttr[name="post_proj"](%1539)
  %1542 : Tensor = prim::GetAttr[name="seg_embed"](%1539)
  %1543 : Tensor = prim::GetAttr[name="r_s_bias"](%1539)
  %1544 : Tensor = prim::GetAttr[name="r_kernel"](%1539)
  %1545 : Tensor = prim::GetAttr[name="r_r_bias"](%1539)
  %1546 : Tensor = prim::GetAttr[name="r_w_bias"](%1539)
  %1547 : __torch__.torch.nn.modules.linear.___torch_mangle_2531.Linear = prim::GetAttr[name="v_head"](%1539)
  %1548 : __torch__.torch.nn.modules.linear.___torch_mangle_2530.Linear = prim::GetAttr[name="k_head"](%1539)
  %1549 : __torch__.torch.nn.modules.linear.___torch_mangle_2529.Linear = prim::GetAttr[name="q_head"](%1539)
  %1550 : int = aten::size(%query.8, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1551 : int = aten::size(%query.8, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1552 : int = aten::size(%hidden.2, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
  %1553 : Tensor = prim::GetAttr[name="weight"](%1549)
  %1554 : Float(768:1, 768:768) = aten::t(%1553), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1555 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query.8, %1554), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1556 : int[] = prim::ListConstruct(%1550, %1551, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %q_head.17 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%1555, %1556), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
  %1558 : Tensor = prim::GetAttr[name="bias"](%1548)
  %1559 : Tensor = prim::GetAttr[name="weight"](%1548)
  %1560 : Float(768:1, 768:768) = aten::t(%1559), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.41 : Float(119:5376, 7:768, 768:1) = aten::matmul(%hidden.2, %1560), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %1562 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.41, %1558, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
  %1563 : int[] = prim::ListConstruct(%1550, %1552, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1564 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1562, %1563), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
  %1565 : Tensor = prim::GetAttr[name="bias"](%1547)
  %1566 : Tensor = prim::GetAttr[name="weight"](%1547)
  %1567 : Float(768:1, 768:768) = aten::t(%1566), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.42 : Float(119:5376, 7:768, 768:1) = aten::matmul(%hidden.2, %1567), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %1569 : Float(119:5376, 7:768, 768:1) = aten::add_(%output.42, %1565, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
  %1570 : int[] = prim::ListConstruct(%1550, %1552, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1571 : Float(119:5376, 7:768, 12:64, 64:1) = aten::view(%1569, %1570), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.18 : Float(119:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.17, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.9 : Float(12:64, 64:1) = aten::mul(%1546, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
  %1574 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_w_bias.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
  %1575 : Tensor[] = prim::ListConstruct(%1574, %1564), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %content_score.9 : Float(119:336, 12:28, 4:7, 7:1) = aten::einsum(%63, %1575), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %v.9 : Float(12:64, 64:1) = aten::mul(%1545, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
  %1578 : Tensor[] = prim::ListConstruct(%242, %1544), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1579 : Float(15:768, 12:64, 64:1) = aten::einsum(%64, %1578), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1580 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %v.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
  %1581 : Tensor[] = prim::ListConstruct(%1580, %1579), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.49 : Float(119:60, 12:7140, 4:15, 15:1) = aten::einsum(%65, %1581), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1583 : int = aten::size(%positional_attn.49, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1584 : int = aten::size(%positional_attn.49, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1585 : int = aten::size(%positional_attn.49, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1586 : int = aten::size(%positional_attn.49, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.9 : Long() = prim::NumToTensor(%1586), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1588 : int[] = prim::ListConstruct(%1583, %1584, %1586, %1585), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.50 : Float(119:60, 12:7140, 15:4, 4:1) = aten::reshape(%positional_attn.49, %1588), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:428:0
  %1590 : Float(119:60, 12:7140, 15:4, 4:1) = aten::slice(%positional_attn.50, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1591 : Float(119:60, 12:7140, 15:4, 4:1) = aten::slice(%1590, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1592 : Float(119:60, 12:7140, 13:4, 4:1) = aten::slice(%1591, %48, %48, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.51 : Float(119:60, 12:7140, 13:4, 4:1) = aten::slice(%1592, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1594 : Long() = aten::sub(%max_rel_len.9, %44, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %1595 : int = aten::Int(%1594), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1596 : int[] = prim::ListConstruct(%1583, %1584, %1585, %1595), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %positional_attn.52 : Float(119:60, 12:7140, 4:13, 13:1) = aten::reshape(%positional_attn.51, %1596), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.53 : Float(119:60, 12:7140, 4:13, 7:1) = aten::slice(%positional_attn.52, %66, %79, %1552, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.54 : Float(119:60, 12:7140, 4:13, 7:1) = aten::mul_(%positional_attn.53, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:498:0
  %1600 : int = aten::size(%token_type_mat.13, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1601 : int = aten::size(%token_type_mat.13, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1602 : int = aten::size(%token_type_mat.13, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.9 : Float(12:64, 64:1) = aten::mul(%1543, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
  %1604 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_s_bias.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
  %1605 : Tensor[] = prim::ListConstruct(%1604, %1542), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1606 : Float(119:8, 12:952, 4:2, 2:1) = aten::einsum(%67, %1605), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1607 : Bool(119:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1608 : Bool(119:56, 1:56, 4:14, 7:1) = aten::unsqueeze(%1607, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1609 : int = aten::size(%q_head.18, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1610 : int[] = prim::ListConstruct(%1600, %1609, %1601, %1602), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %token_type_mat.14 : Bool(119:56, 12:0, 4:14, 7:1) = aten::expand(%1608, %1610, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1612 : Tensor[] = aten::split(%1606, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:371:0
  %diff_token_type.9 : Float(119:8, 12:952, 4:2, 1:1), %same_token_type.9 : Float(119:8, 12:952, 4:2, 1:1) = prim::ListUnpack(%1612), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1615 : int = aten::size(%token_type_mat.14, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1616 : int = aten::size(%token_type_mat.14, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1617 : int = aten::size(%token_type_mat.14, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1618 : int = aten::size(%token_type_mat.14, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1619 : int[] = prim::ListConstruct(%1615, %1616, %1617, %1618), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1620 : Float(119:8, 12:952, 4:2, 7:0) = aten::expand(%same_token_type.9, %1619, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1621 : int = aten::size(%token_type_mat.14, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1622 : int = aten::size(%token_type_mat.14, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1623 : int = aten::size(%token_type_mat.14, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1624 : int = aten::size(%token_type_mat.14, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1625 : int[] = prim::ListConstruct(%1621, %1622, %1623, %1624), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %1626 : Float(119:8, 12:952, 4:2, 7:0) = aten::expand(%diff_token_type.9, %1625, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.17 : Float(119:336, 12:28, 4:7, 7:1) = aten::where(%token_type_mat.14, %1620, %1626), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.18 : Float(119:336, 12:28, 4:7, 7:1) = aten::mul_(%token_type_attn.17, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:522:0
  %1629 : Float(119:336, 12:28, 4:7, 7:1) = aten::add(%content_score.9, %positional_attn.54, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.25 : Float(119:336, 12:28, 4:7, 7:1) = aten::add(%1629, %token_type_attn.18, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.26 : Float(119:336, 12:28, 4:7, 7:1) = aten::to(%attn_score.25, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
  %1632 : Float(119:7, 7:1) = aten::slice(%attention_mask.4, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1633 : Float(119:7, 1:7, 7:1) = aten::unsqueeze(%1632, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1634 : Float(119:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1633, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1635 : Float(119:7, 1:7, 1:7, 7:1) = aten::to(%1634, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1636 : Float(119:7, 1:7, 1:7, 7:1) = aten::rsub(%1635, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/tensor.py:396:0
  %1637 : Float(119:7, 1:7, 1:7, 7:1) = aten::mul(%1636, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.27 : Float(119:336, 12:28, 4:7, 7:1) = aten::sub(%attn_score.26, %1637, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %input.79 : Float(119:336, 12:28, 4:7, 7:1) = aten::softmax(%attn_score.27, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
  %1640 : Float(119:336, 12:28, 4:7, 7:1) = aten::dropout(%input.79, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %1641 : Tensor[] = prim::ListConstruct(%1640, %1571), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %attn_vec.9 : Float(119:3072, 4:64, 12:256, 64:1) = aten::einsum(%69, %1641), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1643 : int[] = prim::ListConstruct(%1550, %1551, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention
  %input.80 : Float(119:3072, 4:768, 768:1) = aten::reshape(%attn_vec.9, %1643), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
  %1645 : Tensor = prim::GetAttr[name="bias"](%1541)
  %1646 : Tensor = prim::GetAttr[name="weight"](%1541)
  %1647 : Float(768:1, 768:768) = aten::t(%1646), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(119:3072, 4:768, 768:1) = aten::matmul(%input.80, %1647), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.81 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.43, %1645, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.9 : Float(119:3072, 4:768, 768:1) = aten::dropout(%input.81, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.82 : Float(119:3072, 4:768, 768:1) = aten::add(%query.8, %attn_out.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
  %1652 : Tensor = prim::GetAttr[name="bias"](%1540)
  %1653 : Tensor = prim::GetAttr[name="weight"](%1540)
  %1654 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm
  %input.83 : Float(119:3072, 4:768, 768:1) = aten::layer_norm(%input.82, %1654, %1653, %1652, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.attention/__module.funnel.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1656 : __torch__.torch.nn.modules.normalization.___torch_mangle_2539.LayerNorm = prim::GetAttr[name="layer_norm"](%1538)
  %1657 : __torch__.torch.nn.modules.linear.___torch_mangle_2537.Linear = prim::GetAttr[name="linear_2"](%1538)
  %1658 : __torch__.torch.nn.modules.linear.___torch_mangle_2535.Linear = prim::GetAttr[name="linear_1"](%1538)
  %1659 : Tensor = prim::GetAttr[name="bias"](%1658)
  %1660 : Tensor = prim::GetAttr[name="weight"](%1658)
  %1661 : Float(768:1, 3072:768) = aten::t(%1660), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.44 : Float(119:12288, 4:3072, 3072:1) = aten::matmul(%input.83, %1661), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.9 : Float(119:12288, 4:3072, 3072:1) = aten::add_(%output.44, %1659, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1664 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%x.9, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1665 : Float(119:12288, 4:3072, 3072:1) = aten::pow(%x.9, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1666 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1665, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1667 : Float(119:12288, 4:3072, 3072:1) = aten::add(%x.9, %1666, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1668 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1667, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1669 : Float(119:12288, 4:3072, 3072:1) = aten::tanh(%1668), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1670 : Float(119:12288, 4:3072, 3072:1) = aten::add(%1669, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.84 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1664, %1670), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.85 : Float(119:12288, 4:3072, 3072:1) = aten::dropout(%input.84, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1673 : Tensor = prim::GetAttr[name="bias"](%1657)
  %1674 : Tensor = prim::GetAttr[name="weight"](%1657)
  %1675 : Float(3072:1, 768:3072) = aten::t(%1674), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.45 : Float(119:3072, 4:768, 768:1) = aten::matmul(%input.85, %1675), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.86 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.45, %1673, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.9 : Float(119:3072, 4:768, 768:1) = aten::dropout(%input.86, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(119:3072, 4:768, 768:1) = aten::add(%input.83, %h.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
  %1680 : Tensor = prim::GetAttr[name="bias"](%1656)
  %1681 : Tensor = prim::GetAttr[name="weight"](%1656)
  %1682 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm
  %query.9 : Float(119:3072, 4:768, 768:1) = aten::layer_norm(%input.87, %1682, %1681, %1680, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.0/__module.funnel.encoder.blocks.2.0.ffn/__module.funnel.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1684 : Bool(119:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1685 : Bool(119:56, 4:14, 7:1) = aten::slice(%1684, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1686 : Bool(119:56, 4:14, 1:1) = aten::slice(%1685, %48, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1687 : Tensor[] = prim::ListConstruct(%1686, %token_type_mat.13), scope: __module.funnel/__module.funnel.encoder
  %tensor.16 : Bool(119:32, 4:8, 8:1) = aten::cat(%1687, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1689 : Bool(119:32, 4:8, 8:1) = aten::slice(%tensor.16, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1690 : Bool(119:32, 4:8, 8:1) = aten::slice(%1689, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.15 : Bool(119:32, 4:8, 4:2) = aten::slice(%1690, %48, %79, %70, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1692 : Float(4:14, 7:1) = aten::slice(%cls_mask.4, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1693 : Float(4:14, 1:1) = aten::slice(%1692, %78, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1694 : Tensor[] = prim::ListConstruct(%1693, %cls_mask.4), scope: __module.funnel/__module.funnel.encoder
  %tensor.17 : Float(4:8, 8:1) = aten::cat(%1694, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:346:0
  %1696 : Float(4:8, 8:1) = aten::slice(%tensor.17, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask : Float(4:8, 4:2) = aten::slice(%1696, %78, %79, %70, %48), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:347:0
  %1698 : Float(119:7, 7:1) = aten::slice(%attention_mask.4, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %suffix : Float(119:7, 6:1) = aten::slice(%1698, %78, %79, %70, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:359:0
  %1700 : Float(119:7, 7:1) = aten::slice(%attention_mask.4, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1701 : Float(119:7, 1:1) = aten::slice(%1700, %78, %79, %78, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1702 : Tensor[] = prim::ListConstruct(%1701, %suffix), scope: __module.funnel/__module.funnel.encoder
  %tensor.18 : Float(119:7, 7:1) = aten::cat(%1702, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:360:0
  %1704 : Float(119:7, 7:1) = aten::slice(%tensor.18, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1705 : Float(119:7, 1:7, 7:1) = aten::unsqueeze(%1704, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %1706 : Float(119:7, 1:7, 7:1) = aten::slice(%1705, %48, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %tensor.19 : Float(119:7, 1:7, 7:1, 1:1) = aten::unsqueeze(%1706, %66), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:364:0
  %input.88 : Float(119:7, 1:7, 7:1, 1:1) = aten::neg(%tensor.19), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1709 : int[] = prim::ListConstruct(%48, %78), scope: __module.funnel/__module.funnel.encoder
  %1710 : int[] = prim::ListConstruct(%48, %78), scope: __module.funnel/__module.funnel.encoder
  %1711 : int[] = prim::ListConstruct(%79, %79), scope: __module.funnel/__module.funnel.encoder
  %1712 : int[] = prim::ListConstruct(%78, %78), scope: __module.funnel/__module.funnel.encoder
  %1713 : Float(119:4, 1:4, 4:1, 1:1) = aten::max_pool2d(%input.88, %1709, %1710, %1711, %1712, %71), scope: __module.funnel/__module.funnel.encoder # torch/nn/functional.py:575:0
  %tensor : Float(119:4, 1:4, 4:1, 1:1) = aten::neg(%1713), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:375:0
  %1715 : Float(119:4, 1:4, 4:1, 1:1) = aten::slice(%tensor, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1716 : Float(119:4, 4:1, 1:1) = aten::select(%1715, %78, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1717 : Float(119:4, 4:1, 1:1) = aten::slice(%1716, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask : Float(119:4, 4:1) = aten::select(%1717, %48, %79), scope: __module.funnel/__module.funnel.encoder # transformers/modeling_funnel.py:380:0
  %1719 : __torch__.transformers.modeling_funnel.___torch_mangle_2555.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%103)
  %1720 : __torch__.transformers.modeling_funnel.___torch_mangle_2549.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%103)
  %1721 : __torch__.torch.nn.modules.normalization.___torch_mangle_2548.LayerNorm = prim::GetAttr[name="layer_norm"](%1720)
  %1722 : __torch__.torch.nn.modules.linear.___torch_mangle_2547.Linear = prim::GetAttr[name="post_proj"](%1720)
  %1723 : Tensor = prim::GetAttr[name="seg_embed"](%1720)
  %1724 : Tensor = prim::GetAttr[name="r_s_bias"](%1720)
  %1725 : Tensor = prim::GetAttr[name="r_kernel"](%1720)
  %1726 : Tensor = prim::GetAttr[name="r_r_bias"](%1720)
  %1727 : Tensor = prim::GetAttr[name="r_w_bias"](%1720)
  %1728 : __torch__.torch.nn.modules.linear.___torch_mangle_2546.Linear = prim::GetAttr[name="v_head"](%1720)
  %1729 : __torch__.torch.nn.modules.linear.___torch_mangle_2545.Linear = prim::GetAttr[name="k_head"](%1720)
  %1730 : __torch__.torch.nn.modules.linear.___torch_mangle_2544.Linear = prim::GetAttr[name="q_head"](%1720)
  %1731 : int = aten::size(%query.9, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1732 : int = aten::size(%query.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1733 : int = aten::size(%query.9, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
  %1734 : Tensor = prim::GetAttr[name="weight"](%1730)
  %1735 : Float(768:1, 768:768) = aten::t(%1734), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1736 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query.9, %1735), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1737 : int[] = prim::ListConstruct(%1731, %1732, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %q_head.19 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%1736, %1737), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
  %1739 : Tensor = prim::GetAttr[name="bias"](%1729)
  %1740 : Tensor = prim::GetAttr[name="weight"](%1729)
  %1741 : Float(768:1, 768:768) = aten::t(%1740), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.46 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query.9, %1741), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %1743 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.46, %1739, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
  %1744 : int[] = prim::ListConstruct(%1731, %1733, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1745 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%1743, %1744), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
  %1746 : Tensor = prim::GetAttr[name="bias"](%1728)
  %1747 : Tensor = prim::GetAttr[name="weight"](%1728)
  %1748 : Float(768:1, 768:768) = aten::t(%1747), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.47 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query.9, %1748), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %1750 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.47, %1746, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
  %1751 : int[] = prim::ListConstruct(%1731, %1733, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1752 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%1750, %1751), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.20 : Float(119:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.19, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.10 : Float(12:64, 64:1) = aten::mul(%1727, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
  %1755 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_w_bias.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
  %1756 : Tensor[] = prim::ListConstruct(%1755, %1745), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %content_score.10 : Float(119:192, 12:16, 4:4, 4:1) = aten::einsum(%63, %1756), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %v.10 : Float(12:64, 64:1) = aten::mul(%1726, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
  %1759 : Tensor[] = prim::ListConstruct(%260, %1725), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1760 : Float(8:768, 12:64, 64:1) = aten::einsum(%64, %1759), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1761 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %v.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
  %1762 : Tensor[] = prim::ListConstruct(%1761, %1760), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.55 : Float(119:32, 12:3808, 4:8, 8:1) = aten::einsum(%65, %1762), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1764 : int = aten::size(%positional_attn.55, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1765 : int = aten::size(%positional_attn.55, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1766 : int = aten::size(%positional_attn.55, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1767 : int = aten::size(%positional_attn.55, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.10 : Long() = prim::NumToTensor(%1767), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1769 : int[] = prim::ListConstruct(%1764, %1765, %1767, %1766), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.56 : Float(119:32, 12:3808, 8:4, 4:1) = aten::reshape(%positional_attn.55, %1769), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:428:0
  %1771 : Float(119:32, 12:3808, 8:4, 4:1) = aten::slice(%positional_attn.56, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1772 : Float(119:32, 12:3808, 8:4, 4:1) = aten::slice(%1771, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1773 : Float(119:32, 12:3808, 7:4, 4:1) = aten::slice(%1772, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.57 : Float(119:32, 12:3808, 7:4, 4:1) = aten::slice(%1773, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1775 : Long() = aten::sub(%max_rel_len.10, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %1776 : int = aten::Int(%1775), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1777 : int[] = prim::ListConstruct(%1764, %1765, %1766, %1776), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %positional_attn.58 : Float(119:32, 12:3808, 4:7, 7:1) = aten::reshape(%positional_attn.57, %1777), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.59 : Float(119:32, 12:3808, 4:7, 4:1) = aten::slice(%positional_attn.58, %66, %79, %1733, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.60 : Float(119:32, 12:3808, 4:7, 4:1) = aten::mul_(%positional_attn.59, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:498:0
  %1781 : int = aten::size(%token_type_mat.15, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1782 : int = aten::size(%token_type_mat.15, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1783 : int = aten::size(%token_type_mat.15, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.10 : Float(12:64, 64:1) = aten::mul(%1724, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
  %1785 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_s_bias.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
  %1786 : Tensor[] = prim::ListConstruct(%1785, %1723), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1787 : Float(119:8, 12:952, 4:2, 2:1) = aten::einsum(%67, %1786), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1788 : Bool(119:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1789 : Bool(119:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1788, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1790 : int = aten::size(%q_head.20, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1791 : int[] = prim::ListConstruct(%1781, %1790, %1782, %1783), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %token_type_mat.16 : Bool(119:32, 12:0, 4:8, 4:2) = aten::expand(%1789, %1791, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1793 : Tensor[] = aten::split(%1787, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:371:0
  %diff_token_type.10 : Float(119:8, 12:952, 4:2, 1:1), %same_token_type.10 : Float(119:8, 12:952, 4:2, 1:1) = prim::ListUnpack(%1793), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1796 : int = aten::size(%token_type_mat.16, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1797 : int = aten::size(%token_type_mat.16, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1798 : int = aten::size(%token_type_mat.16, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1799 : int = aten::size(%token_type_mat.16, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1800 : int[] = prim::ListConstruct(%1796, %1797, %1798, %1799), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1801 : Float(119:8, 12:952, 4:2, 4:0) = aten::expand(%same_token_type.10, %1800, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1802 : int = aten::size(%token_type_mat.16, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1803 : int = aten::size(%token_type_mat.16, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1804 : int = aten::size(%token_type_mat.16, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1805 : int = aten::size(%token_type_mat.16, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1806 : int[] = prim::ListConstruct(%1802, %1803, %1804, %1805), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %1807 : Float(119:8, 12:952, 4:2, 4:0) = aten::expand(%diff_token_type.10, %1806, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.19 : Float(119:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.16, %1801, %1807), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.20 : Float(119:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.19, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:522:0
  %1810 : Float(119:192, 12:16, 4:4, 4:1) = aten::add(%content_score.10, %positional_attn.60, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.28 : Float(119:192, 12:16, 4:4, 4:1) = aten::add(%1810, %token_type_attn.20, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.29 : Float(119:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.28, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
  %1813 : Float(119:4, 4:1) = aten::slice(%attention_mask, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1814 : Float(119:4, 1:4, 4:1) = aten::unsqueeze(%1813, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1815 : Float(119:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1814, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1816 : Float(119:4, 1:4, 1:4, 4:1) = aten::to(%1815, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1817 : Float(119:4, 1:4, 1:4, 4:1) = aten::rsub(%1816, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/tensor.py:396:0
  %1818 : Float(119:4, 1:4, 1:4, 4:1) = aten::mul(%1817, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.30 : Float(119:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.29, %1818, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %input.89 : Float(119:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.30, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
  %1821 : Float(119:192, 12:16, 4:4, 4:1) = aten::dropout(%input.89, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1822 : Tensor[] = prim::ListConstruct(%1821, %1752), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %attn_vec.10 : Float(119:3072, 4:64, 12:256, 64:1) = aten::einsum(%69, %1822), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1824 : int[] = prim::ListConstruct(%1731, %1732, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention
  %input.90 : Float(119:3072, 4:768, 768:1) = aten::reshape(%attn_vec.10, %1824), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
  %1826 : Tensor = prim::GetAttr[name="bias"](%1722)
  %1827 : Tensor = prim::GetAttr[name="weight"](%1722)
  %1828 : Float(768:1, 768:768) = aten::t(%1827), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.48 : Float(119:3072, 4:768, 768:1) = aten::matmul(%input.90, %1828), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.91 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.48, %1826, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.10 : Float(119:3072, 4:768, 768:1) = aten::dropout(%input.91, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.92 : Float(119:3072, 4:768, 768:1) = aten::add(%query.9, %attn_out.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
  %1833 : Tensor = prim::GetAttr[name="bias"](%1721)
  %1834 : Tensor = prim::GetAttr[name="weight"](%1721)
  %1835 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm
  %input.93 : Float(119:3072, 4:768, 768:1) = aten::layer_norm(%input.92, %1835, %1834, %1833, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.attention/__module.funnel.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1837 : __torch__.torch.nn.modules.normalization.___torch_mangle_2554.LayerNorm = prim::GetAttr[name="layer_norm"](%1719)
  %1838 : __torch__.torch.nn.modules.linear.___torch_mangle_2552.Linear = prim::GetAttr[name="linear_2"](%1719)
  %1839 : __torch__.torch.nn.modules.linear.___torch_mangle_2550.Linear = prim::GetAttr[name="linear_1"](%1719)
  %1840 : Tensor = prim::GetAttr[name="bias"](%1839)
  %1841 : Tensor = prim::GetAttr[name="weight"](%1839)
  %1842 : Float(768:1, 3072:768) = aten::t(%1841), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(119:12288, 4:3072, 3072:1) = aten::matmul(%input.93, %1842), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.10 : Float(119:12288, 4:3072, 3072:1) = aten::add_(%output.49, %1840, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1845 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%x.10, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1846 : Float(119:12288, 4:3072, 3072:1) = aten::pow(%x.10, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1847 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1846, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1848 : Float(119:12288, 4:3072, 3072:1) = aten::add(%x.10, %1847, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1849 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1848, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1850 : Float(119:12288, 4:3072, 3072:1) = aten::tanh(%1849), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1851 : Float(119:12288, 4:3072, 3072:1) = aten::add(%1850, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.94 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1845, %1851), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.95 : Float(119:12288, 4:3072, 3072:1) = aten::dropout(%input.94, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1854 : Tensor = prim::GetAttr[name="bias"](%1838)
  %1855 : Tensor = prim::GetAttr[name="weight"](%1838)
  %1856 : Float(3072:1, 768:3072) = aten::t(%1855), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(119:3072, 4:768, 768:1) = aten::matmul(%input.95, %1856), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.96 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.50, %1854, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.10 : Float(119:3072, 4:768, 768:1) = aten::dropout(%input.96, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.97 : Float(119:3072, 4:768, 768:1) = aten::add(%input.93, %h.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
  %1861 : Tensor = prim::GetAttr[name="bias"](%1837)
  %1862 : Tensor = prim::GetAttr[name="weight"](%1837)
  %1863 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm
  %query.10 : Float(119:3072, 4:768, 768:1) = aten::layer_norm(%input.97, %1863, %1862, %1861, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.1/__module.funnel.encoder.blocks.2.1.ffn/__module.funnel.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1865 : __torch__.transformers.modeling_funnel.___torch_mangle_2570.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%100)
  %1866 : __torch__.transformers.modeling_funnel.___torch_mangle_2564.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%100)
  %1867 : __torch__.torch.nn.modules.normalization.___torch_mangle_2563.LayerNorm = prim::GetAttr[name="layer_norm"](%1866)
  %1868 : __torch__.torch.nn.modules.linear.___torch_mangle_2562.Linear = prim::GetAttr[name="post_proj"](%1866)
  %1869 : Tensor = prim::GetAttr[name="seg_embed"](%1866)
  %1870 : Tensor = prim::GetAttr[name="r_s_bias"](%1866)
  %1871 : Tensor = prim::GetAttr[name="r_kernel"](%1866)
  %1872 : Tensor = prim::GetAttr[name="r_r_bias"](%1866)
  %1873 : Tensor = prim::GetAttr[name="r_w_bias"](%1866)
  %1874 : __torch__.torch.nn.modules.linear.___torch_mangle_2561.Linear = prim::GetAttr[name="v_head"](%1866)
  %1875 : __torch__.torch.nn.modules.linear.___torch_mangle_2560.Linear = prim::GetAttr[name="k_head"](%1866)
  %1876 : __torch__.torch.nn.modules.linear.___torch_mangle_2559.Linear = prim::GetAttr[name="q_head"](%1866)
  %1877 : int = aten::size(%query.10, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1878 : int = aten::size(%query.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1879 : int = aten::size(%query.10, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
  %1880 : Tensor = prim::GetAttr[name="weight"](%1876)
  %1881 : Float(768:1, 768:768) = aten::t(%1880), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1882 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query.10, %1881), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1883 : int[] = prim::ListConstruct(%1877, %1878, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %q_head.21 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%1882, %1883), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
  %1885 : Tensor = prim::GetAttr[name="bias"](%1875)
  %1886 : Tensor = prim::GetAttr[name="weight"](%1875)
  %1887 : Float(768:1, 768:768) = aten::t(%1886), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.51 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query.10, %1887), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %1889 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.51, %1885, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
  %1890 : int[] = prim::ListConstruct(%1877, %1879, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1891 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%1889, %1890), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
  %1892 : Tensor = prim::GetAttr[name="bias"](%1874)
  %1893 : Tensor = prim::GetAttr[name="weight"](%1874)
  %1894 : Float(768:1, 768:768) = aten::t(%1893), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.52 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query.10, %1894), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %1896 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.52, %1892, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
  %1897 : int[] = prim::ListConstruct(%1877, %1879, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1898 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%1896, %1897), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.22 : Float(119:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.21, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.11 : Float(12:64, 64:1) = aten::mul(%1873, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
  %1901 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_w_bias.11, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
  %1902 : Tensor[] = prim::ListConstruct(%1901, %1891), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %content_score.11 : Float(119:192, 12:16, 4:4, 4:1) = aten::einsum(%63, %1902), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %v.11 : Float(12:64, 64:1) = aten::mul(%1872, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
  %1905 : Tensor[] = prim::ListConstruct(%260, %1871), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1906 : Float(8:768, 12:64, 64:1) = aten::einsum(%64, %1905), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1907 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %v.11, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
  %1908 : Tensor[] = prim::ListConstruct(%1907, %1906), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.61 : Float(119:32, 12:3808, 4:8, 8:1) = aten::einsum(%65, %1908), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1910 : int = aten::size(%positional_attn.61, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1911 : int = aten::size(%positional_attn.61, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1912 : int = aten::size(%positional_attn.61, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1913 : int = aten::size(%positional_attn.61, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.11 : Long() = prim::NumToTensor(%1913), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1915 : int[] = prim::ListConstruct(%1910, %1911, %1913, %1912), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.62 : Float(119:32, 12:3808, 8:4, 4:1) = aten::reshape(%positional_attn.61, %1915), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:428:0
  %1917 : Float(119:32, 12:3808, 8:4, 4:1) = aten::slice(%positional_attn.62, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1918 : Float(119:32, 12:3808, 8:4, 4:1) = aten::slice(%1917, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1919 : Float(119:32, 12:3808, 7:4, 4:1) = aten::slice(%1918, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.63 : Float(119:32, 12:3808, 7:4, 4:1) = aten::slice(%1919, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1921 : Long() = aten::sub(%max_rel_len.11, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %1922 : int = aten::Int(%1921), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1923 : int[] = prim::ListConstruct(%1910, %1911, %1912, %1922), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %positional_attn.64 : Float(119:32, 12:3808, 4:7, 7:1) = aten::reshape(%positional_attn.63, %1923), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.65 : Float(119:32, 12:3808, 4:7, 4:1) = aten::slice(%positional_attn.64, %66, %79, %1879, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.66 : Float(119:32, 12:3808, 4:7, 4:1) = aten::mul_(%positional_attn.65, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:498:0
  %1927 : int = aten::size(%token_type_mat.15, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1928 : int = aten::size(%token_type_mat.15, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1929 : int = aten::size(%token_type_mat.15, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.11 : Float(12:64, 64:1) = aten::mul(%1870, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
  %1931 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_s_bias.11, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
  %1932 : Tensor[] = prim::ListConstruct(%1931, %1869), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1933 : Float(119:8, 12:952, 4:2, 2:1) = aten::einsum(%67, %1932), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1934 : Bool(119:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1935 : Bool(119:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1934, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1936 : int = aten::size(%q_head.22, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1937 : int[] = prim::ListConstruct(%1927, %1936, %1928, %1929), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %token_type_mat.17 : Bool(119:32, 12:0, 4:8, 4:2) = aten::expand(%1935, %1937, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1939 : Tensor[] = aten::split(%1933, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:371:0
  %diff_token_type.11 : Float(119:8, 12:952, 4:2, 1:1), %same_token_type.11 : Float(119:8, 12:952, 4:2, 1:1) = prim::ListUnpack(%1939), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1942 : int = aten::size(%token_type_mat.17, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1943 : int = aten::size(%token_type_mat.17, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1944 : int = aten::size(%token_type_mat.17, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1945 : int = aten::size(%token_type_mat.17, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1946 : int[] = prim::ListConstruct(%1942, %1943, %1944, %1945), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1947 : Float(119:8, 12:952, 4:2, 4:0) = aten::expand(%same_token_type.11, %1946, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1948 : int = aten::size(%token_type_mat.17, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1949 : int = aten::size(%token_type_mat.17, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1950 : int = aten::size(%token_type_mat.17, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1951 : int = aten::size(%token_type_mat.17, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1952 : int[] = prim::ListConstruct(%1948, %1949, %1950, %1951), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %1953 : Float(119:8, 12:952, 4:2, 4:0) = aten::expand(%diff_token_type.11, %1952, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.21 : Float(119:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.17, %1947, %1953), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.22 : Float(119:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.21, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:522:0
  %1956 : Float(119:192, 12:16, 4:4, 4:1) = aten::add(%content_score.11, %positional_attn.66, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.31 : Float(119:192, 12:16, 4:4, 4:1) = aten::add(%1956, %token_type_attn.22, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.32 : Float(119:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.31, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
  %1959 : Float(119:4, 4:1) = aten::slice(%attention_mask, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1960 : Float(119:4, 1:4, 4:1) = aten::unsqueeze(%1959, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1961 : Float(119:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1960, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1962 : Float(119:4, 1:4, 1:4, 4:1) = aten::to(%1961, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1963 : Float(119:4, 1:4, 1:4, 4:1) = aten::rsub(%1962, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/tensor.py:396:0
  %1964 : Float(119:4, 1:4, 1:4, 4:1) = aten::mul(%1963, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.33 : Float(119:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.32, %1964, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %input.98 : Float(119:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.33, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
  %1967 : Float(119:192, 12:16, 4:4, 4:1) = aten::dropout(%input.98, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1968 : Tensor[] = prim::ListConstruct(%1967, %1898), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %attn_vec.11 : Float(119:3072, 4:64, 12:256, 64:1) = aten::einsum(%69, %1968), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1970 : int[] = prim::ListConstruct(%1877, %1878, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention
  %input.99 : Float(119:3072, 4:768, 768:1) = aten::reshape(%attn_vec.11, %1970), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
  %1972 : Tensor = prim::GetAttr[name="bias"](%1868)
  %1973 : Tensor = prim::GetAttr[name="weight"](%1868)
  %1974 : Float(768:1, 768:768) = aten::t(%1973), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.53 : Float(119:3072, 4:768, 768:1) = aten::matmul(%input.99, %1974), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.100 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.53, %1972, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.11 : Float(119:3072, 4:768, 768:1) = aten::dropout(%input.100, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.101 : Float(119:3072, 4:768, 768:1) = aten::add(%query.10, %attn_out.11, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
  %1979 : Tensor = prim::GetAttr[name="bias"](%1867)
  %1980 : Tensor = prim::GetAttr[name="weight"](%1867)
  %1981 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm
  %input.102 : Float(119:3072, 4:768, 768:1) = aten::layer_norm(%input.101, %1981, %1980, %1979, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.attention/__module.funnel.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1983 : __torch__.torch.nn.modules.normalization.___torch_mangle_2569.LayerNorm = prim::GetAttr[name="layer_norm"](%1865)
  %1984 : __torch__.torch.nn.modules.linear.___torch_mangle_2567.Linear = prim::GetAttr[name="linear_2"](%1865)
  %1985 : __torch__.torch.nn.modules.linear.___torch_mangle_2565.Linear = prim::GetAttr[name="linear_1"](%1865)
  %1986 : Tensor = prim::GetAttr[name="bias"](%1985)
  %1987 : Tensor = prim::GetAttr[name="weight"](%1985)
  %1988 : Float(768:1, 3072:768) = aten::t(%1987), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.54 : Float(119:12288, 4:3072, 3072:1) = aten::matmul(%input.102, %1988), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.11 : Float(119:12288, 4:3072, 3072:1) = aten::add_(%output.54, %1986, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1991 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%x.11, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1992 : Float(119:12288, 4:3072, 3072:1) = aten::pow(%x.11, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1993 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1992, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1994 : Float(119:12288, 4:3072, 3072:1) = aten::add(%x.11, %1993, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1995 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1994, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1996 : Float(119:12288, 4:3072, 3072:1) = aten::tanh(%1995), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1997 : Float(119:12288, 4:3072, 3072:1) = aten::add(%1996, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.103 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%1991, %1997), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.104 : Float(119:12288, 4:3072, 3072:1) = aten::dropout(%input.103, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2000 : Tensor = prim::GetAttr[name="bias"](%1984)
  %2001 : Tensor = prim::GetAttr[name="weight"](%1984)
  %2002 : Float(3072:1, 768:3072) = aten::t(%2001), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.55 : Float(119:3072, 4:768, 768:1) = aten::matmul(%input.104, %2002), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.105 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.55, %2000, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.11 : Float(119:3072, 4:768, 768:1) = aten::dropout(%input.105, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.106 : Float(119:3072, 4:768, 768:1) = aten::add(%input.102, %h.11, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
  %2007 : Tensor = prim::GetAttr[name="bias"](%1983)
  %2008 : Tensor = prim::GetAttr[name="weight"](%1983)
  %2009 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm
  %query : Float(119:3072, 4:768, 768:1) = aten::layer_norm(%input.106, %2009, %2008, %2007, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.2/__module.funnel.encoder.blocks.2.2.ffn/__module.funnel.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2011 : __torch__.transformers.modeling_funnel.___torch_mangle_2585.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%97)
  %2012 : __torch__.transformers.modeling_funnel.___torch_mangle_2579.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%97)
  %2013 : __torch__.torch.nn.modules.normalization.___torch_mangle_2578.LayerNorm = prim::GetAttr[name="layer_norm"](%2012)
  %2014 : __torch__.torch.nn.modules.linear.___torch_mangle_2577.Linear = prim::GetAttr[name="post_proj"](%2012)
  %2015 : Tensor = prim::GetAttr[name="seg_embed"](%2012)
  %2016 : Tensor = prim::GetAttr[name="r_s_bias"](%2012)
  %2017 : Tensor = prim::GetAttr[name="r_kernel"](%2012)
  %2018 : Tensor = prim::GetAttr[name="r_r_bias"](%2012)
  %2019 : Tensor = prim::GetAttr[name="r_w_bias"](%2012)
  %2020 : __torch__.torch.nn.modules.linear.___torch_mangle_2576.Linear = prim::GetAttr[name="v_head"](%2012)
  %2021 : __torch__.torch.nn.modules.linear.___torch_mangle_2575.Linear = prim::GetAttr[name="k_head"](%2012)
  %2022 : __torch__.torch.nn.modules.linear.___torch_mangle_2574.Linear = prim::GetAttr[name="q_head"](%2012)
  %2023 : int = aten::size(%query, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2024 : int = aten::size(%query, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2025 : int = aten::size(%query, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
  %2026 : Tensor = prim::GetAttr[name="weight"](%2022)
  %2027 : Float(768:1, 768:768) = aten::t(%2026), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2028 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query, %2027), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2029 : int[] = prim::ListConstruct(%2023, %2024, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %q_head.23 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%2028, %2029), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
  %2031 : Tensor = prim::GetAttr[name="bias"](%2021)
  %2032 : Tensor = prim::GetAttr[name="weight"](%2021)
  %2033 : Float(768:1, 768:768) = aten::t(%2032), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.56 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query, %2033), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %2035 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.56, %2031, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
  %2036 : int[] = prim::ListConstruct(%2023, %2025, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2037 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%2035, %2036), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
  %2038 : Tensor = prim::GetAttr[name="bias"](%2020)
  %2039 : Tensor = prim::GetAttr[name="weight"](%2020)
  %2040 : Float(768:1, 768:768) = aten::t(%2039), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.57 : Float(119:3072, 4:768, 768:1) = aten::matmul(%query, %2040), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %2042 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.57, %2038, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
  %2043 : int[] = prim::ListConstruct(%2023, %2025, %60, %61), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2044 : Float(119:3072, 4:768, 12:64, 64:1) = aten::view(%2042, %2043), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
  %q_head : Float(119:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.23, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias : Float(12:64, 64:1) = aten::mul(%2019, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
  %2047 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %r_w_bias, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
  %2048 : Tensor[] = prim::ListConstruct(%2047, %2037), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %content_score : Float(119:192, 12:16, 4:4, 4:1) = aten::einsum(%63, %2048), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %v : Float(12:64, 64:1) = aten::mul(%2018, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
  %2051 : Tensor[] = prim::ListConstruct(%260, %2017), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2052 : Float(8:768, 12:64, 64:1) = aten::einsum(%64, %2051), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2053 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %v, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
  %2054 : Tensor[] = prim::ListConstruct(%2053, %2052), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.67 : Float(119:32, 12:3808, 4:8, 8:1) = aten::einsum(%65, %2054), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2056 : int = aten::size(%positional_attn.67, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2057 : int = aten::size(%positional_attn.67, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2058 : int = aten::size(%positional_attn.67, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2059 : int = aten::size(%positional_attn.67, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len : Long() = prim::NumToTensor(%2059), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2061 : int[] = prim::ListConstruct(%2056, %2057, %2059, %2058), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.68 : Float(119:32, 12:3808, 8:4, 4:1) = aten::reshape(%positional_attn.67, %2061), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:428:0
  %2063 : Float(119:32, 12:3808, 8:4, 4:1) = aten::slice(%positional_attn.68, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2064 : Float(119:32, 12:3808, 8:4, 4:1) = aten::slice(%2063, %78, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2065 : Float(119:32, 12:3808, 7:4, 4:1) = aten::slice(%2064, %48, %78, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.69 : Float(119:32, 12:3808, 7:4, 4:1) = aten::slice(%2065, %66, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2067 : Long() = aten::sub(%max_rel_len, %43, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %2068 : int = aten::Int(%2067), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2069 : int[] = prim::ListConstruct(%2056, %2057, %2058, %2068), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %positional_attn.70 : Float(119:32, 12:3808, 4:7, 7:1) = aten::reshape(%positional_attn.69, %2069), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.71 : Float(119:32, 12:3808, 4:7, 4:1) = aten::slice(%positional_attn.70, %66, %79, %2025, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn : Float(119:32, 12:3808, 4:7, 4:1) = aten::mul_(%positional_attn.71, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:498:0
  %2073 : int = aten::size(%token_type_mat.15, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2074 : int = aten::size(%token_type_mat.15, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2075 : int = aten::size(%token_type_mat.15, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias : Float(12:64, 64:1) = aten::mul(%2016, %62), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
  %2077 : Float(119:3072, 4:768, 12:64, 64:1) = aten::add(%q_head, %r_s_bias, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
  %2078 : Tensor[] = prim::ListConstruct(%2077, %2015), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2079 : Float(119:8, 12:952, 4:2, 2:1) = aten::einsum(%67, %2078), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2080 : Bool(119:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2081 : Bool(119:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%2080, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2082 : int = aten::size(%q_head, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2083 : int[] = prim::ListConstruct(%2073, %2082, %2074, %2075), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %token_type_mat : Bool(119:32, 12:0, 4:8, 4:2) = aten::expand(%2081, %2083, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2085 : Tensor[] = aten::split(%2079, %78, %70), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:371:0
  %diff_token_type : Float(119:8, 12:952, 4:2, 1:1), %same_token_type : Float(119:8, 12:952, 4:2, 1:1) = prim::ListUnpack(%2085), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2088 : int = aten::size(%token_type_mat, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2089 : int = aten::size(%token_type_mat, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2090 : int = aten::size(%token_type_mat, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2091 : int = aten::size(%token_type_mat, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2092 : int[] = prim::ListConstruct(%2088, %2089, %2090, %2091), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2093 : Float(119:8, 12:952, 4:2, 4:0) = aten::expand(%same_token_type, %2092, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2094 : int = aten::size(%token_type_mat, %79), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2095 : int = aten::size(%token_type_mat, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2096 : int = aten::size(%token_type_mat, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2097 : int = aten::size(%token_type_mat, %66), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2098 : int[] = prim::ListConstruct(%2094, %2095, %2096, %2097), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %2099 : Float(119:8, 12:952, 4:2, 4:0) = aten::expand(%diff_token_type, %2098, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.23 : Float(119:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat, %2093, %2099), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn : Float(119:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.23, %cls_mask), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:522:0
  %2102 : Float(119:192, 12:16, 4:4, 4:1) = aten::add(%content_score, %positional_attn, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.34 : Float(119:192, 12:16, 4:4, 4:1) = aten::add(%2102, %token_type_attn, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.35 : Float(119:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.34, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
  %2105 : Float(119:4, 4:1) = aten::slice(%attention_mask, %79, %79, %45, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2106 : Float(119:4, 1:4, 4:1) = aten::unsqueeze(%2105, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2107 : Float(119:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%2106, %48), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2108 : Float(119:4, 1:4, 1:4, 4:1) = aten::to(%2107, %39, %75, %75, %42), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2109 : Float(119:4, 1:4, 1:4, 4:1) = aten::rsub(%2108, %78, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/tensor.py:396:0
  %2110 : Float(119:4, 1:4, 1:4, 4:1) = aten::mul(%2109, %68), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score : Float(119:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.35, %2110, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %input.107 : Float(119:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score, %70, %39), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
  %2113 : Float(119:192, 12:16, 4:4, 4:1) = aten::dropout(%input.107, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %2114 : Tensor[] = prim::ListConstruct(%2113, %2044), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %attn_vec : Float(119:3072, 4:64, 12:256, 64:1) = aten::einsum(%69, %2114), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2116 : int[] = prim::ListConstruct(%2023, %2024, %73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention
  %input.108 : Float(119:3072, 4:768, 768:1) = aten::reshape(%attn_vec, %2116), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
  %2118 : Tensor = prim::GetAttr[name="bias"](%2014)
  %2119 : Tensor = prim::GetAttr[name="weight"](%2014)
  %2120 : Float(768:1, 768:768) = aten::t(%2119), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(119:3072, 4:768, 768:1) = aten::matmul(%input.108, %2120), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.109 : Float(119:3072, 4:768, 768:1) = aten::add_(%output.58, %2118, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out : Float(119:3072, 4:768, 768:1) = aten::dropout(%input.109, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.110 : Float(119:3072, 4:768, 768:1) = aten::add(%query, %attn_out, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
  %2125 : Tensor = prim::GetAttr[name="bias"](%2013)
  %2126 : Tensor = prim::GetAttr[name="weight"](%2013)
  %2127 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm
  %input.111 : Float(119:3072, 4:768, 768:1) = aten::layer_norm(%input.110, %2127, %2126, %2125, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.attention/__module.funnel.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %2129 : __torch__.torch.nn.modules.normalization.___torch_mangle_2584.LayerNorm = prim::GetAttr[name="layer_norm"](%2011)
  %2130 : __torch__.torch.nn.modules.linear.___torch_mangle_2582.Linear = prim::GetAttr[name="linear_2"](%2011)
  %2131 : __torch__.torch.nn.modules.linear.___torch_mangle_2580.Linear = prim::GetAttr[name="linear_1"](%2011)
  %2132 : Tensor = prim::GetAttr[name="bias"](%2131)
  %2133 : Tensor = prim::GetAttr[name="weight"](%2131)
  %2134 : Float(768:1, 3072:768) = aten::t(%2133), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.59 : Float(119:12288, 4:3072, 3072:1) = aten::matmul(%input.111, %2134), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x : Float(119:12288, 4:3072, 3072:1) = aten::add_(%output.59, %2132, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2137 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%x, %54), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2138 : Float(119:12288, 4:3072, 3072:1) = aten::pow(%x, %55), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2139 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%2138, %56), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2140 : Float(119:12288, 4:3072, 3072:1) = aten::add(%x, %2139, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2141 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%2140, %57), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2142 : Float(119:12288, 4:3072, 3072:1) = aten::tanh(%2141), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2143 : Float(119:12288, 4:3072, 3072:1) = aten::add(%2142, %58, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.112 : Float(119:12288, 4:3072, 3072:1) = aten::mul(%2137, %2143), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.113 : Float(119:12288, 4:3072, 3072:1) = aten::dropout(%input.112, %59, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2146 : Tensor = prim::GetAttr[name="bias"](%2130)
  %2147 : Tensor = prim::GetAttr[name="weight"](%2130)
  %2148 : Float(3072:1, 768:3072) = aten::t(%2147), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output : Float(119:3072, 4:768, 768:1) = aten::matmul(%input.113, %2148), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.114 : Float(119:3072, 4:768, 768:1) = aten::add_(%output, %2146, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h : Float(119:3072, 4:768, 768:1) = aten::dropout(%input.114, %74, %75), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(119:3072, 4:768, 768:1) = aten::add(%input.111, %h, %78), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
  %2153 : Tensor = prim::GetAttr[name="bias"](%2129)
  %2154 : Tensor = prim::GetAttr[name="weight"](%2129)
  %2155 : int[] = prim::ListConstruct(%73), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm
  %last_hidden_state : Float(119:3072, 4:768, 768:1) = aten::layer_norm(%input.115, %2155, %2154, %2153, %72, %71), scope: __module.funnel/__module.funnel.encoder/__module.funnel.encoder.blocks.2.3/__module.funnel.encoder.blocks.2.3.ffn/__module.funnel.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %24 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:1360:0
  %25 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:1360:0
  %26 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_funnel.py:1360:0
  %27 : int = prim::Constant[value=1]() # transformers/modeling_funnel.py:1360:0
  %28 : Float(119:3072, 4:768, 768:1) = aten::slice(%last_hidden_state, %24, %25, %26, %27) # transformers/modeling_funnel.py:1360:0
  %29 : int = prim::Constant[value=1]() # transformers/modeling_funnel.py:1360:0
  %30 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:1360:0
  %input.116 : Float(119:3072, 768:1) = aten::select(%28, %29, %30) # transformers/modeling_funnel.py:1360:0
  %2157 : float = prim::Constant[value=0.10000000000000001](), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %2158 : bool = prim::Constant[value=0](), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %2159 : int = prim::Constant[value=1](), scope: __module.classifier/__module.classifier.linear_hidden # torch/nn/functional.py:1674:0
  %2160 : __torch__.torch.nn.modules.linear.___torch_mangle_2593.Linear = prim::GetAttr[name="linear_out"](%3)
  %2161 : __torch__.torch.nn.modules.linear.___torch_mangle_2591.Linear = prim::GetAttr[name="linear_hidden"](%3)
  %2162 : Tensor = prim::GetAttr[name="bias"](%2161)
  %2163 : Tensor = prim::GetAttr[name="weight"](%2161)
  %2164 : Float(768:1, 768:768) = aten::t(%2163), scope: __module.classifier/__module.classifier.linear_hidden # torch/nn/functional.py:1674:0
  %hidden : Float(119:768, 768:1) = aten::addmm(%2162, %input.116, %2164, %2159, %2159), scope: __module.classifier/__module.classifier.linear_hidden # torch/nn/functional.py:1674:0
  %input.117 : Float(119:768, 768:1) = aten::tanh(%hidden), scope: __module.classifier # transformers/modeling_funnel.py:791:0
  %input : Float(119:768, 768:1) = aten::dropout(%input.117, %2157, %2158), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
  %2168 : Tensor = prim::GetAttr[name="bias"](%2160)
  %2169 : Tensor = prim::GetAttr[name="weight"](%2160)
  %2170 : Float(768:1, 1:768) = aten::t(%2169), scope: __module.classifier/__module.classifier.linear_out # torch/nn/functional.py:1674:0
  %logits : Float(119:1, 1:1) = aten::addmm(%2168, %input, %2170, %2159, %2159), scope: __module.classifier/__module.classifier.linear_out # torch/nn/functional.py:1674:0
  %33 : int = prim::Constant[value=-1]() # transformers/modeling_funnel.py:1362:0
  %34 : int[] = prim::ListConstruct(%33, %8)
  %35 : Float(17:7, 7:1) = aten::view(%logits, %34) # transformers/modeling_funnel.py:1362:0
  %36 : (Float(17:7, 7:1)) = prim::TupleConstruct(%35)
  return (%36)
