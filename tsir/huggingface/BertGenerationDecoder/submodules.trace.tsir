BertGenerationDecoder(
  (bert): BertGenerationEncoder(
    (embeddings): BertGenerationEmbeddings(
      (word_embeddings): Embedding(50358, 1024, padding_idx=0)
      (position_embeddings): Embedding(512, 1024)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (12): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (13): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (14): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (15): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (16): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (17): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (18): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (19): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (20): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (21): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (22): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (23): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (lm_head): BertGenerationOnlyLMHead(
    (decoder): Linear(in_features=1024, out_features=50358, bias=True)
  )
)

BertGenerationDecoder._actual_script_module
BertGenerationDecoder.forward
  graph(%self.1 : __torch__.transformers.modeling_bert_generation.BertGenerationDecoder,
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %6611 : __torch__.transformers.modeling_bert_generation.BertGenerationOnlyLMHead = prim::GetAttr[name="lm_head"](%self.1)
    %6606 : __torch__.transformers.modeling_bert_generation.BertGenerationEncoder = prim::GetAttr[name="bert"](%self.1)
    %7029 : Tensor = prim::CallMethod[name="forward"](%6606, %input_ids, %attention_mask.1)
    %7030 : Tensor = prim::CallMethod[name="forward"](%6611, %7029)
    %5359 : (Float(17:654654, 13:50358, 50358:1)) = prim::TupleConstruct(%7030)
    return (%5359)

BertGenerationDecoder.bert
BertGenerationEncoder._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_bert_generation.BertGenerationEncoder,
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %1 : __torch__.transformers.modeling_bert.BertEncoder = prim::GetAttr[name="encoder"](%self.2)
    %2 : __torch__.transformers.modeling_bert_generation.BertGenerationEmbeddings = prim::GetAttr[name="embeddings"](%self.2)
    %10 : int = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %11 : int = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %12 : int = prim::Constant[value=9223372036854775807](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %13 : int = prim::Constant[value=1](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %14 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %10, %11, %12, %13), scope: __module.bert # transformers/modeling_utils.py:244:0
    %16 : int = prim::Constant[value=1](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %17 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%14, %16), scope: __module.bert # transformers/modeling_utils.py:244:0
    %18 : int = prim::Constant[value=2](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %19 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%17, %18), scope: __module.bert # transformers/modeling_utils.py:244:0
    %20 : int = prim::Constant[value=3](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %21 : int = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %22 : int = prim::Constant[value=9223372036854775807](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %23 : int = prim::Constant[value=1](), scope: __module.bert # transformers/modeling_utils.py:244:0
    %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%19, %20, %21, %22, %23), scope: __module.bert # transformers/modeling_utils.py:244:0
    %25 : int = prim::Constant[value=6](), scope: __module.bert # transformers/modeling_utils.py:257:0
    %26 : bool = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_utils.py:257:0
    %27 : bool = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_utils.py:257:0
    %28 : None = prim::Constant(), scope: __module.bert
    %29 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %25, %26, %27, %28), scope: __module.bert # transformers/modeling_utils.py:257:0
    %30 : float = prim::Constant[value=1.](), scope: __module.bert # torch/tensor.py:396:0
    %31 : int = prim::Constant[value=1](), scope: __module.bert # torch/tensor.py:396:0
    %32 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%29, %30, %31), scope: __module.bert # torch/tensor.py:396:0
    %33 : Double() = prim::Constant[value={-10000}](), scope: __module.bert # transformers/modeling_utils.py:258:0
    %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%32, %33), scope: __module.bert # transformers/modeling_utils.py:258:0
    %37 : Tensor = prim::CallMethod[name="forward"](%2, %input_ids)
    %38 : Tensor = prim::CallMethod[name="forward"](%1, %37, %attention_mask)
    return (%38)

BertGenerationDecoder.lm_head
BertGenerationOnlyLMHead._actual_script_module
  graph(%self.417 : __torch__.transformers.modeling_bert_generation.BertGenerationOnlyLMHead,
        %4 : Float(17:13312, 13:1024, 1024:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.417)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="decoder"](%self.417)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1, %4)
    return (%5)

BertGenerationEncoder.embeddings
BertGenerationEmbeddings._actual_script_module
  graph(%self.3 : __torch__.transformers.modeling_bert_generation.BertGenerationEmbeddings,
        %input_ids : Long(17:13, 13:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.3)
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.3)
    %4 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="position_embeddings"](%self.3)
    %5 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embeddings"](%self.3)
    %6 : Tensor = prim::GetAttr[name="position_ids"](%self.3)
    %10 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:149:0
    %11 : int = aten::size(%input_ids, %10), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:149:0
    %seq_length : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.embeddings
    %13 : int = aten::Int(%seq_length), scope: __module.bert/__module.bert.embeddings
    %14 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %15 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %16 : int = prim::Constant[value=9223372036854775807](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %17 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %18 : Long(1:512, 512:1) = aten::slice(%6, %14, %15, %16, %17), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %19 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %20 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %21 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %input.1 : Long(1:512, 13:1) = aten::slice(%18, %19, %20, %13, %21), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %input_ids)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %input.1)
    %25 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:162:0
    %input.2 : Float(17:13312, 13:1024, 1024:1) = aten::add(%29, %30, %25), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:162:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.2)
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %31)
    return (%32)

BertGenerationEncoder.encoder
BertEncoder._actual_script_module
  graph(%self.8 : __torch__.transformers.modeling_bert.BertEncoder,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %4 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="23"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %6 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="22"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %8 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="21"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %10 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="20"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %12 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="19"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %14 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="18"](%13)
    %15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %16 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="17"](%15)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %18 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="16"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %20 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="15"](%19)
    %21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %22 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="14"](%21)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %24 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="13"](%23)
    %25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %26 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="12"](%25)
    %27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %28 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="11"](%27)
    %29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %30 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="10"](%29)
    %31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %32 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="9"](%31)
    %33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %34 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="8"](%33)
    %35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %36 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="7"](%35)
    %37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %38 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="6"](%37)
    %39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %40 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="5"](%39)
    %41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %42 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="4"](%41)
    %43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %44 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="3"](%43)
    %45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %46 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="2"](%45)
    %47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %48 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="1"](%47)
    %49 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %50 : __torch__.transformers.modeling_bert.BertLayer = prim::GetAttr[name="0"](%49)
    %75 : Tensor = prim::CallMethod[name="forward"](%50, %1, %attention_mask)
    %76 : Tensor = prim::CallMethod[name="forward"](%48, %75, %attention_mask)
    %77 : Tensor = prim::CallMethod[name="forward"](%46, %76, %attention_mask)
    %78 : Tensor = prim::CallMethod[name="forward"](%44, %77, %attention_mask)
    %79 : Tensor = prim::CallMethod[name="forward"](%42, %78, %attention_mask)
    %80 : Tensor = prim::CallMethod[name="forward"](%40, %79, %attention_mask)
    %81 : Tensor = prim::CallMethod[name="forward"](%38, %80, %attention_mask)
    %82 : Tensor = prim::CallMethod[name="forward"](%36, %81, %attention_mask)
    %83 : Tensor = prim::CallMethod[name="forward"](%34, %82, %attention_mask)
    %84 : Tensor = prim::CallMethod[name="forward"](%32, %83, %attention_mask)
    %85 : Tensor = prim::CallMethod[name="forward"](%30, %84, %attention_mask)
    %86 : Tensor = prim::CallMethod[name="forward"](%28, %85, %attention_mask)
    %87 : Tensor = prim::CallMethod[name="forward"](%26, %86, %attention_mask)
    %88 : Tensor = prim::CallMethod[name="forward"](%24, %87, %attention_mask)
    %89 : Tensor = prim::CallMethod[name="forward"](%22, %88, %attention_mask)
    %90 : Tensor = prim::CallMethod[name="forward"](%20, %89, %attention_mask)
    %91 : Tensor = prim::CallMethod[name="forward"](%18, %90, %attention_mask)
    %92 : Tensor = prim::CallMethod[name="forward"](%16, %91, %attention_mask)
    %93 : Tensor = prim::CallMethod[name="forward"](%14, %92, %attention_mask)
    %94 : Tensor = prim::CallMethod[name="forward"](%12, %93, %attention_mask)
    %95 : Tensor = prim::CallMethod[name="forward"](%10, %94, %attention_mask)
    %96 : Tensor = prim::CallMethod[name="forward"](%8, %95, %attention_mask)
    %97 : Tensor = prim::CallMethod[name="forward"](%6, %96, %attention_mask)
    %98 : Tensor = prim::CallMethod[name="forward"](%4, %97, %attention_mask)
    return (%98)

BertGenerationEmbeddings.LayerNorm
LayerNorm._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.2 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.6)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.6)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %input.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.2, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.3)

BertGenerationEmbeddings.dropout
Dropout._actual_script_module
  graph(%self.7 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
    %input.4 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
    return (%input.4)

BertGenerationEmbeddings.position_embeddings
Embedding._actual_script_module
  graph(%self.5 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.1 : Long(1:512, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.5)
    %3 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %position_embeddings : Float(1:13312, 13:1024, 1024:1) = aten::embedding(%2, %input.1, %3, %4, %5), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    return (%position_embeddings)

BertGenerationEmbeddings.word_embeddings
Embedding._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %3 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %inputs_embeds : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%2, %input_ids, %3, %4, %5), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    return (%inputs_embeds)

ModuleList.*
  module had no methods with graph attrs.

BertLayer._actual_script_module
  graph(%self.9 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.9)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.9)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.9)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.10 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.10)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.10)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.20 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.20)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.11 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
    return (%input.11)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.22 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.22)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.22)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.22)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output # transformers/modeling_bert.py:371:0
    %input.13 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.13)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.16 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.16)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.16)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.16)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output # transformers/modeling_bert.py:295:0
    %input.9 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.9)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.11)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.11)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.11)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.11)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %x.2 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %query_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.2, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %x.4 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %key_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.4, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %x.6 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %value_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.6, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.1, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.1, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:262:0
    %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %input.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.6)
    %context_layer.1 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.1), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.1, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.2 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.2, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.2, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
    %input.7 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.2, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
    return (%input.7)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.15 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.6 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.6, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.1)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
    %output.2 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
    %x.3 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.2, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.3)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.12)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
    %output.1 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
    %x.1 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.1, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.1)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
    %output.3 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
    %x.5 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.3, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.5)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.9 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.19)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.19)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.9, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.17 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.17)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.17)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
    %output.4 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
    %input.8 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.4, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.8)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.1 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.1)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.21)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.21)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.5 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.5, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.10)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.25 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.13 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.25)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.25)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.14 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.13, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.14)

BertOutput.dense
Linear._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.23)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.23)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
    %output.6 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
    %input.12 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.6, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
    return (%input.12)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.2 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.2)

BertLayer._actual_script_module
  graph(%self.26 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.26)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.26)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.26)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.27 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.27)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.27)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.37 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.37)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.21 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
    return (%input.21)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.39 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.39)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.39)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.39)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output # transformers/modeling_bert.py:371:0
    %input.23 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.23)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.33 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.33)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.33)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.33)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output # transformers/modeling_bert.py:295:0
    %input.19 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.19)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.28 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.28)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.28)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.28)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.28)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %x.8 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %query_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.8, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %x.10 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %key_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.10, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %x.12 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %value_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.12, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.2, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.3, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:262:0
    %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %input.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.16)
    %context_layer.3 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.2), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.3, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.4 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.4, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.4, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
    %input.17 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.4, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:279:0
    return (%input.17)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.16 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.16, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.2)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.30)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.30)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
    %output.8 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
    %x.9 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.8, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.9)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
    %output.7 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
    %x.7 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.7, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.7)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.31 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.31)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.31)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
    %output.9 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
    %x.11 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.9, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.11)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.19 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.36)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.19, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
    %output.10 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
    %input.18 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.10, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.18)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.3 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.3)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.11 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.20 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.11, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.20)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.23 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.42)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.24 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.23, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.24)

BertOutput.dense
Linear._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.40)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.40)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
    %output.12 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
    %input.22 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.12, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
    return (%input.22)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.41 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.4 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.4)

BertLayer._actual_script_module
  graph(%self.43 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.43)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.43)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.43)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.44 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.44)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.44)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.54 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.54)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.31 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
    return (%input.31)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.56 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.56)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.56)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.56)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output # transformers/modeling_bert.py:371:0
    %input.33 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.33)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.50 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.50)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.50)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.50)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output # transformers/modeling_bert.py:295:0
    %input.29 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.29)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.45 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.45)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.45)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.45)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.45)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %x.14 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %query_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.14, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %x.16 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %key_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.16, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %x.18 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %value_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.18, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.3, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.5, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:262:0
    %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %input.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.26)
    %context_layer.5 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.5, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.6 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.6, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.6, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
    %input.27 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.6, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:279:0
    return (%input.27)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.26 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.26, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.3)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.47 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.47)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.47)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
    %output.14 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
    %x.15 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.14, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.15)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.46)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.46)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
    %output.13 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
    %x.13 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.13, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.13)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
    %output.15 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
    %x.17 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.15, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.17)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.29 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.53)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.53)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.29, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.51)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.51)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
    %output.16 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
    %input.28 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.16, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.28)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.5 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.5)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.55 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.55)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.55)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.17 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.30 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.17, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.30)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.33 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.59)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.34 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.33, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.34)

BertOutput.dense
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.57)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
    %output.18 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
    %input.32 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.18, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
    return (%input.32)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.6 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.6)

BertLayer._actual_script_module
  graph(%self.60 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.60)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.60)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.60)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.61 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.61)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.61)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.71 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.71)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.41 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
    return (%input.41)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.73 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.73)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.73)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.73)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output # transformers/modeling_bert.py:371:0
    %input.43 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.43)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.67 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.67)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.67)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.67)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output # transformers/modeling_bert.py:295:0
    %input.39 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.39)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.62 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.62)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.62)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.62)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.62)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %x.20 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %query_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.20, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %x.22 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %key_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.22, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %x.24 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %value_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.24, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.4, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.7, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:262:0
    %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %input.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.36)
    %context_layer.7 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.7, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.8 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.8, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.8, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
    %input.37 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.8, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:279:0
    return (%input.37)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.36 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.36, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.4)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
    %output.20 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
    %x.21 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.20, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.21)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.63)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.63)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
    %output.19 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
    %x.19 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.19, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.19)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
    %output.21 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
    %x.23 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.21, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.23)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.39 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.39, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.68)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.68)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
    %output.22 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
    %input.38 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.22, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.38)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.7 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.7)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.72 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.72)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.72)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
    %output.23 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
    %input.40 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.23, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.40)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.43 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.44 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.43, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.44)

BertOutput.dense
Linear._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.74)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.74)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
    %output.24 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
    %input.42 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.24, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
    return (%input.42)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.75 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.8 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.8)

BertLayer._actual_script_module
  graph(%self.77 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.77)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.77)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.77)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.78 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.78)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.78)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.88 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.88)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.51 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
    return (%input.51)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.90 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.90)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.90)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.90)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output # transformers/modeling_bert.py:371:0
    %input.53 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.53)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.84 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.84)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.84)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.84)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output # transformers/modeling_bert.py:295:0
    %input.49 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.49)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.79 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.79)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.79)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.79)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.79)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %x.26 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %query_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.26, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %x.28 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %key_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.28, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %x.30 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %value_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.30, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.5, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.9, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:262:0
    %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %input.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.46)
    %context_layer.9 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.9, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.10 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.10, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.10, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
    %input.47 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.10, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:279:0
    return (%input.47)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.46 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.46, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.5)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.81 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.81)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.81)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
    %output.26 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
    %x.27 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.26, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.27)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
    %output.25 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
    %x.25 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.25, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.25)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
    %output.27 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
    %x.29 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.27, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.29)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.49 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.49, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.85)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.85)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
    %output.28 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
    %input.48 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.28, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.48)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.86 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.9 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.9)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
    %output.29 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
    %input.50 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.29, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.50)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.53 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.54 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.53, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.54)

BertOutput.dense
Linear._actual_script_module
  graph(%self.91 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.91)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.91)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
    %output.30 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
    %input.52 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.30, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
    return (%input.52)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.10 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.10)

BertLayer._actual_script_module
  graph(%self.94 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.94)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.94)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.94)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.95 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.95)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.95)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.105 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.105)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.61 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
    return (%input.61)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.107 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.107)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.107)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.107)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output # transformers/modeling_bert.py:371:0
    %input.63 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.63)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.101 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.101)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.101)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.101)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output # transformers/modeling_bert.py:295:0
    %input.59 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.59)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.96 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.96)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.96)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.96)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.96)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %x.32 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %query_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.32, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %x.34 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %key_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.34, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %x.36 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %value_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.36, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.6, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.11, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:262:0
    %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %input.56 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.56)
    %context_layer.11 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.11, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.12 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.12, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.12, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
    %input.57 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.12, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:279:0
    return (%input.57)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.56 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.56, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.6)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.98)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.98)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
    %output.32 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
    %x.33 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.32, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.33)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.97 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.97)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.97)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
    %output.31 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
    %x.31 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.31, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.31)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.99)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.99)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
    %output.33 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
    %x.35 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.33, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.35)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.59 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.59, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.102)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
    %output.34 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
    %input.58 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.34, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.58)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.103 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.11 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.11)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.106 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.106)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.106)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
    %output.35 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
    %input.60 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.35, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.60)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.63 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.64 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.63, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.64)

BertOutput.dense
Linear._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
    %output.36 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
    %input.62 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.36, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
    return (%input.62)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.12 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.12)

BertLayer._actual_script_module
  graph(%self.111 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.111)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.111)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.111)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.112 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.112)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.112)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.122 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.122)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.71 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
    return (%input.71)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.124 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.124)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.124)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.124)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output # transformers/modeling_bert.py:371:0
    %input.73 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.73)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.118 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.118)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.118)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.118)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output # transformers/modeling_bert.py:295:0
    %input.69 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.69)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.113 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.113)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.113)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.113)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.113)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %x.38 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %query_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.38, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %x.40 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %key_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.40, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %x.42 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %value_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.42, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.7, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.13, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:262:0
    %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.66)
    %context_layer.13 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.13, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.14 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.14, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.14, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
    %input.67 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.14, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:279:0
    return (%input.67)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.117 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.66 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.66, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.7)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.115 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.115)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.115)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
    %output.38 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
    %x.39 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.38, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.39)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
    %output.37 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
    %x.37 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.37, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.37)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.116 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.116)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.116)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
    %output.39 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
    %x.41 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.39, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.41)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.121 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.121)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.121)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.7 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.69, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.7)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.119)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.119)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
    %output.40 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
    %input.68 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.40, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.68)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.13 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.13)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
    %output.41 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
    %input.70 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.41, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.70)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.127 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.73 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.127)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.127)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.74 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.73, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.74)

BertOutput.dense
Linear._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.125)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.125)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
    %output.42 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
    %input.72 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.42, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
    return (%input.72)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.126 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.14 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.14)

BertLayer._actual_script_module
  graph(%self.128 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.128)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.128)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.128)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.129 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.129)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.129)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.139 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.139)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.81 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
    return (%input.81)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.141 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.141)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.141)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.141)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output # transformers/modeling_bert.py:371:0
    %input.83 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.83)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.135 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.135)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.135)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.135)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output # transformers/modeling_bert.py:295:0
    %input.79 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.79)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.130 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.130)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.130)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.130)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.130)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %x.44 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %query_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.44, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %x.46 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %key_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.46, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %x.48 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %value_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.48, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.8, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.15, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:262:0
    %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.76)
    %context_layer.15 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.15, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.16 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.16, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.16, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
    %input.77 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.16, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:279:0
    return (%input.77)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.76 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.8)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.132 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.132)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.132)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
    %output.44 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
    %x.45 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.44, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.45)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.131 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.131)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.131)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
    %output.43 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
    %x.43 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.43, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.43)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.133 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.133)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.133)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
    %output.45 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
    %x.47 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.45, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.47)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.138 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.79 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.138)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.138)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.8 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.79, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.8)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.136 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.136)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.136)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
    %output.46 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
    %input.78 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.46, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.78)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.137 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.15 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.15)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.140)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.140)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
    %output.47 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
    %input.80 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.47, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.80)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.144 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.83 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.144)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.144)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.84 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.83, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.84)

BertOutput.dense
Linear._actual_script_module
  graph(%self.142 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.142)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.142)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
    %output.48 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
    %input.82 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.48, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
    return (%input.82)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.143 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.16 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.16)

BertLayer._actual_script_module
  graph(%self.145 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.145)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.145)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.145)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.146 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.146)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.146)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.156 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.156)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.91 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
    return (%input.91)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.158 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.158)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.158)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.158)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output # transformers/modeling_bert.py:371:0
    %input.93 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.93)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.152 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.152)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.152)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.152)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output # transformers/modeling_bert.py:295:0
    %input.89 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.89)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.147 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.147)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.147)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.147)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.147)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %x.50 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %query_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.50, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %x.52 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %key_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.52, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %x.54 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %value_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.54, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.9, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.17, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:262:0
    %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %input.86 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.86)
    %context_layer.17 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.17, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.18 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.18, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.18, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
    %input.87 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.18, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:279:0
    return (%input.87)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.151 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.86 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.86, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.9)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.149 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.149)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.149)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
    %output.50 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
    %x.51 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.50, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.51)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.148 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.148)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.148)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
    %output.49 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
    %x.49 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.49, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.49)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.150 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.150)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.150)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
    %output.51 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
    %x.53 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.51, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.53)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.89 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.155)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.155)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.89, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.153 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.153)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.153)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
    %output.52 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
    %input.88 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.52, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.88)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.154 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.17 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.17)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.157 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.157)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.157)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
    %output.53 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
    %input.90 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.53, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.90)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.161 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.93 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.161)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.161)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.94 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.93, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.94)

BertOutput.dense
Linear._actual_script_module
  graph(%self.159 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.159)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.159)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
    %output.54 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
    %input.92 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.54, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
    return (%input.92)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.160 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.18 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.18)

BertLayer._actual_script_module
  graph(%self.162 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.162)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.162)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.162)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.163 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.163)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.163)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.173 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.173)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.101 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
    return (%input.101)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.175 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.175)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.175)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.175)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output # transformers/modeling_bert.py:371:0
    %input.103 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.103)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.169 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.169)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.169)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.169)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output # transformers/modeling_bert.py:295:0
    %input.99 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.99)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.164 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.164)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.164)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.164)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.164)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %x.56 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %query_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.56, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %x.58 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %key_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.58, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %x.60 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %value_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.60, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.10, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.19, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:262:0
    %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %input.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.96)
    %context_layer.19 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.19, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.20 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.20, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.20, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
    %input.97 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.20, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:279:0
    return (%input.97)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.168 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.96 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.96, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.10)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.166 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.166)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.166)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
    %output.56 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
    %x.57 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.56, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.57)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.165 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.165)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.165)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
    %output.55 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
    %x.55 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.55, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.55)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.167)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.167)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
    %output.57 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
    %x.59 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.57, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.59)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.172 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.99 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.172)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.172)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.10 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.99, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.10)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.170 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.170)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.170)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
    %output.58 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
    %input.98 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.58, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.98)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.171 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.19 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.19)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.174 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.174)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.174)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
    %output.59 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
    %input.100 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.59, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.100)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.103 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.178)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.178)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.104 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.103, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.104)

BertOutput.dense
Linear._actual_script_module
  graph(%self.176 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.176)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.176)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
    %output.60 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
    %input.102 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.60, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
    return (%input.102)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.177 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.20 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.20)

BertLayer._actual_script_module
  graph(%self.179 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.179)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.179)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.179)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.180 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.180)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.180)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.190 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.190)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.111 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
    return (%input.111)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.192 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.192)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.192)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.192)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output # transformers/modeling_bert.py:371:0
    %input.113 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.113)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.186 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.186)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.186)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.186)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output # transformers/modeling_bert.py:295:0
    %input.109 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.109)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.181 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.181)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.181)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.181)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.181)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %x.62 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %query_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.62, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %x.64 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %key_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.64, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %x.66 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %value_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.66, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.11, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.21, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:262:0
    %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %input.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.106)
    %context_layer.21 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.21, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.22 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.22, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.22, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
    %input.107 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.22, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:279:0
    return (%input.107)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.185 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.106 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.106, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.11)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.183 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.183)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.183)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
    %output.62 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
    %x.63 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.62, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.63)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.182 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.182)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.182)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
    %output.61 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
    %x.61 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.61, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.61)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.184 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.184)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.184)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
    %output.63 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
    %x.65 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.63, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.65)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.109 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.189)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.189)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.11 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.109, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.11)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.187 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.187)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.187)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
    %output.64 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
    %input.108 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.64, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.108)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.188 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.21 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.21)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.191 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.191)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.191)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
    %output.65 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
    %input.110 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.65, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.110)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.195 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.113 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.195)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.195)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.114 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.113, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.114)

BertOutput.dense
Linear._actual_script_module
  graph(%self.193 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.193)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.193)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
    %output.66 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
    %input.112 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.66, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
    return (%input.112)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.194 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.22 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.22)

BertLayer._actual_script_module
  graph(%self.196 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.196)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.196)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.196)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.197 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.197)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.197)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.207 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.207)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.121 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
    return (%input.121)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.209 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.209)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.209)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.209)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output # transformers/modeling_bert.py:371:0
    %input.123 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.123)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.203 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.203)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.203)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.203)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output # transformers/modeling_bert.py:295:0
    %input.119 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.119)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.198 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.198)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.198)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.198)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.198)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %x.68 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %query_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.68, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %x.70 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %key_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.70, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %x.72 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %value_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.72, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.12, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.12, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.24 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.23, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:262:0
    %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.24, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %input.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.116)
    %context_layer.23 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.23, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.24 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.24, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.24, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
    %input.117 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.24, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:279:0
    return (%input.117)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.202 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.116 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.116, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.12)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.200 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.200)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.200)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
    %output.68 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
    %x.69 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.68, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.69)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.199 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.199)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.199)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
    %output.67 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
    %x.67 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.67, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.67)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.201 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.201)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.201)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
    %output.69 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
    %x.71 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.69, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.71)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.206 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.119 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.206)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.206)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.12 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.119, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.12)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.204 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.204)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.204)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
    %output.70 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
    %input.118 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.70, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.118)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.205 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.23 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.23)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.208 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.208)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.208)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
    %output.71 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
    %input.120 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.71, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.120)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.212 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.123 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.212)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.212)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.124 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.123, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.124)

BertOutput.dense
Linear._actual_script_module
  graph(%self.210 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.210)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.210)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
    %output.72 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
    %input.122 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.72, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
    return (%input.122)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.211 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.24 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.24)

BertLayer._actual_script_module
  graph(%self.213 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.213)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.213)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.213)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.214 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.214)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.214)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.224 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.224)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.131 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate # torch/nn/functional.py:1369:0
    return (%input.131)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.226 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.226)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.226)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.226)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output # transformers/modeling_bert.py:371:0
    %input.133 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.133)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.220 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.220)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.220)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.220)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output # transformers/modeling_bert.py:295:0
    %input.129 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.129)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.215 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.215)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.215)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.215)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.215)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %x.74 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %query_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.74, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %x.76 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %key_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.76, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %x.78 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %value_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.78, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.13, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.13, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.25, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:262:0
    %input.125 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.26, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %input.126 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.125, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.126)
    %context_layer.25 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.25, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.26 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.26, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.26, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
    %input.127 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.26, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:279:0
    return (%input.127)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.219 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.126 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.126, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.13)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.217 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.217)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.217)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.key # torch/nn/functional.py:1676:0
    %output.74 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.key # torch/nn/functional.py:1678:0
    %x.75 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.74, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.75)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.216 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.216)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.216)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.query # torch/nn/functional.py:1676:0
    %output.73 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.query # torch/nn/functional.py:1678:0
    %x.73 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.73, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.73)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.218 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.218)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.218)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.value # torch/nn/functional.py:1676:0
    %output.75 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.value # torch/nn/functional.py:1678:0
    %x.77 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.75, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.77)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.223 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.129 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.223)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.223)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.13 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.129, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.13)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.221 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.221)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.221)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1676:0
    %output.76 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1678:0
    %input.128 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.76, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.128)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.222 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.25 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.25)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.225 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.225)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.225)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate/__module.bert.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1676:0
    %output.77 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate/__module.bert.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate/__module.bert.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1678:0
    %input.130 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.77, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate/__module.bert.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.130)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.229 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.133 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.229)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.229)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.134 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.133, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.134)

BertOutput.dense
Linear._actual_script_module
  graph(%self.227 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.227)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.227)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dense # torch/nn/functional.py:1676:0
    %output.78 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dense # torch/nn/functional.py:1678:0
    %input.132 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.78, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dense # torch/nn/functional.py:1678:0
    return (%input.132)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.228 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.26 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.26)

BertLayer._actual_script_module
  graph(%self.230 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.230)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.230)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.230)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.231 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.231)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.231)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.241 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.241)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.141 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate # torch/nn/functional.py:1369:0
    return (%input.141)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.243 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.243)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.243)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.243)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output # transformers/modeling_bert.py:371:0
    %input.143 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.143)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.237 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.237)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.237)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.237)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output # transformers/modeling_bert.py:295:0
    %input.139 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.139)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.232 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.232)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.232)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.232)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.232)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %x.80 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %query_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.80, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %x.82 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %key_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.82, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %x.84 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %value_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.84, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.14, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.27 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.14, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.28 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.27, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:262:0
    %input.135 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.28, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %input.136 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.135, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.136)
    %context_layer.27 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.27, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.28 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.28, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.28, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
    %input.137 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.28, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:279:0
    return (%input.137)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.236 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.136 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.136, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.14)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.234 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.234)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.234)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.key # torch/nn/functional.py:1676:0
    %output.80 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.key # torch/nn/functional.py:1678:0
    %x.81 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.80, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.81)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.233 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.233)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.233)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.query # torch/nn/functional.py:1676:0
    %output.79 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.query # torch/nn/functional.py:1678:0
    %x.79 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.79, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.79)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.235 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.235)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.235)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.value # torch/nn/functional.py:1676:0
    %output.81 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.value # torch/nn/functional.py:1678:0
    %x.83 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.81, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.83)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.240 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.139 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.240)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.240)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.14 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.139, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.14)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.238 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.238)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.238)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1676:0
    %output.82 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1678:0
    %input.138 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.82, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.138)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.239 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.27 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.27)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.242 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.242)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.242)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate/__module.bert.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1676:0
    %output.83 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate/__module.bert.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate/__module.bert.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1678:0
    %input.140 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.83, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate/__module.bert.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.140)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.246 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.143 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.246)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.246)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.144 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.143, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.144)

BertOutput.dense
Linear._actual_script_module
  graph(%self.244 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.244)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.244)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dense # torch/nn/functional.py:1676:0
    %output.84 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dense # torch/nn/functional.py:1678:0
    %input.142 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.84, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dense # torch/nn/functional.py:1678:0
    return (%input.142)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.245 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.28 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.28)

BertLayer._actual_script_module
  graph(%self.247 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.247)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.247)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.247)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.248 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.248)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.248)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.258 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.258)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.151 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate # torch/nn/functional.py:1369:0
    return (%input.151)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.260 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.260)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.260)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.260)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output # transformers/modeling_bert.py:371:0
    %input.153 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.153)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.254 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.254)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.254)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.254)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output # transformers/modeling_bert.py:295:0
    %input.149 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.149)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.249 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.249)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.249)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.249)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.249)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %x.86 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %query_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.86, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %x.88 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %key_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.88, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %x.90 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %value_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.90, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.15, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.29 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.15, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.29, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:262:0
    %input.145 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.30, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %input.146 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.145, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.146)
    %context_layer.29 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.29, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.30 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.30, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.30, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
    %input.147 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.30, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:279:0
    return (%input.147)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.253 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.146 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.146, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.15)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.251 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.251)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.251)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.key # torch/nn/functional.py:1676:0
    %output.86 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.key # torch/nn/functional.py:1678:0
    %x.87 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.86, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.87)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.250 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.250)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.250)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.query # torch/nn/functional.py:1676:0
    %output.85 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.query # torch/nn/functional.py:1678:0
    %x.85 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.85, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.85)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.252 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.252)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.252)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.value # torch/nn/functional.py:1676:0
    %output.87 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.value # torch/nn/functional.py:1678:0
    %x.89 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.87, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.89)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.257 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.149 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.257)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.257)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.15 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.149, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.15)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.255 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.255)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.255)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1676:0
    %output.88 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1678:0
    %input.148 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.88, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.148)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.256 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.29 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.29)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.259 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.259)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.259)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate/__module.bert.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1676:0
    %output.89 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate/__module.bert.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate/__module.bert.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1678:0
    %input.150 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.89, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate/__module.bert.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.150)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.263 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.153 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.263)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.263)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.154 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.153, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.154)

BertOutput.dense
Linear._actual_script_module
  graph(%self.261 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.261)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.261)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dense # torch/nn/functional.py:1676:0
    %output.90 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dense # torch/nn/functional.py:1678:0
    %input.152 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.90, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dense # torch/nn/functional.py:1678:0
    return (%input.152)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.262 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.30 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.30)

BertLayer._actual_script_module
  graph(%self.264 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.264)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.264)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.264)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.265 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.265)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.265)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.275 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.275)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.161 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate # torch/nn/functional.py:1369:0
    return (%input.161)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.277 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.277)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.277)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.277)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output # transformers/modeling_bert.py:371:0
    %input.163 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.163)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.271 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.271)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.271)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.271)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output # transformers/modeling_bert.py:295:0
    %input.159 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.159)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.266 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.266)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.266)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.266)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.266)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %x.92 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %query_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.92, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %x.94 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %key_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.94, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %x.96 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %value_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.96, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.16, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.16, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.32 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.31, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:262:0
    %input.155 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.32, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.155, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.156)
    %context_layer.31 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.31, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.32 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.32, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.32, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
    %input.157 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.32, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:279:0
    return (%input.157)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.270 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.156 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.156, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.16)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.268 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.268)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.268)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.key # torch/nn/functional.py:1676:0
    %output.92 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.key # torch/nn/functional.py:1678:0
    %x.93 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.92, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.93)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.267 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.267)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.267)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.query # torch/nn/functional.py:1676:0
    %output.91 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.query # torch/nn/functional.py:1678:0
    %x.91 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.91, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.91)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.269 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.269)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.269)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.value # torch/nn/functional.py:1676:0
    %output.93 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.value # torch/nn/functional.py:1678:0
    %x.95 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.93, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.95)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.274 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.159 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.274)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.274)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.16 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.159, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.16)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.272 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.272)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.272)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1676:0
    %output.94 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1678:0
    %input.158 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.94, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.158)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.273 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.31 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.31)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.276 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.276)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.276)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate/__module.bert.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1676:0
    %output.95 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate/__module.bert.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate/__module.bert.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1678:0
    %input.160 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.95, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate/__module.bert.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.160)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.280 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.163 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.280)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.280)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.164 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.163, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.164)

BertOutput.dense
Linear._actual_script_module
  graph(%self.278 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.278)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.278)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dense # torch/nn/functional.py:1676:0
    %output.96 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dense # torch/nn/functional.py:1678:0
    %input.162 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.96, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dense # torch/nn/functional.py:1678:0
    return (%input.162)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.279 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.32 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.32)

BertLayer._actual_script_module
  graph(%self.281 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.281)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.281)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.281)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.282 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.282)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.282)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.292 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.292)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.171 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate # torch/nn/functional.py:1369:0
    return (%input.171)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.294 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.294)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.294)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.294)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output # transformers/modeling_bert.py:371:0
    %input.173 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.173)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.288 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.288)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.288)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.288)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output # transformers/modeling_bert.py:295:0
    %input.169 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.169)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.283 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.283)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.283)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.283)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.283)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %x.98 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %query_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.98, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %x.100 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %key_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.100, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %x.102 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %value_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.102, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.17, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.33 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.17, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.34 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.33, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:262:0
    %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.34, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.166)
    %context_layer.33 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.33, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.34 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.34, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.34, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
    %input.167 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.34, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:279:0
    return (%input.167)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.287 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.166 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.17)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.285 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.285)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.285)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.key # torch/nn/functional.py:1676:0
    %output.98 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.key # torch/nn/functional.py:1678:0
    %x.99 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.98, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.99)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.284 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.284)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.284)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.query # torch/nn/functional.py:1676:0
    %output.97 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.query # torch/nn/functional.py:1678:0
    %x.97 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.97, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.97)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.286 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.286)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.286)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.value # torch/nn/functional.py:1676:0
    %output.99 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.value # torch/nn/functional.py:1678:0
    %x.101 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.99, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.101)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.291 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.169 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.291)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.291)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.17 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.169, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.17)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.289 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.289)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.289)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1676:0
    %output.100 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1678:0
    %input.168 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.100, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.168)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.290 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.33 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.33)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.293 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.293)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.293)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate/__module.bert.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1676:0
    %output.101 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate/__module.bert.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate/__module.bert.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1678:0
    %input.170 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.101, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate/__module.bert.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.170)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.297 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.173 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.297)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.297)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.174 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.173, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.174)

BertOutput.dense
Linear._actual_script_module
  graph(%self.295 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.295)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.295)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dense # torch/nn/functional.py:1676:0
    %output.102 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dense # torch/nn/functional.py:1678:0
    %input.172 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.102, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dense # torch/nn/functional.py:1678:0
    return (%input.172)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.296 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.34 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.34)

BertLayer._actual_script_module
  graph(%self.298 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.298)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.298)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.298)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.299 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.299)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.299)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.309 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.309)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.181 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate # torch/nn/functional.py:1369:0
    return (%input.181)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.311 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.311)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.311)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.311)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output # transformers/modeling_bert.py:371:0
    %input.183 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.183)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.305 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.305)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.305)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.305)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output # transformers/modeling_bert.py:295:0
    %input.179 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.179)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.300 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.300)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.300)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.300)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.300)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %x.104 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %query_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.104, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %x.106 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %key_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.106, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %x.108 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %value_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.108, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.18, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.18, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.35, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:262:0
    %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.36, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %input.176 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.175, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.176)
    %context_layer.35 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.35, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.36 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.36, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.36, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
    %input.177 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.36, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:279:0
    return (%input.177)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.304 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.176 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.176, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.18)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.302 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.302)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.302)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.key # torch/nn/functional.py:1676:0
    %output.104 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.key # torch/nn/functional.py:1678:0
    %x.105 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.104, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.105)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.301 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.301)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.301)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.query # torch/nn/functional.py:1676:0
    %output.103 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.query # torch/nn/functional.py:1678:0
    %x.103 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.103, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.103)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.303 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.303)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.303)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.value # torch/nn/functional.py:1676:0
    %output.105 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.value # torch/nn/functional.py:1678:0
    %x.107 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.105, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.107)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.308 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.179 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.308)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.308)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.18 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.179, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.18)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.306 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.306)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.306)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1676:0
    %output.106 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1678:0
    %input.178 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.106, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.178)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.307 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.35 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.35)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.310 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.310)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.310)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate/__module.bert.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1676:0
    %output.107 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate/__module.bert.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate/__module.bert.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1678:0
    %input.180 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.107, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate/__module.bert.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.180)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.314 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.183 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.314)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.314)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.184 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.183, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.184)

BertOutput.dense
Linear._actual_script_module
  graph(%self.312 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.312)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.312)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dense # torch/nn/functional.py:1676:0
    %output.108 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dense # torch/nn/functional.py:1678:0
    %input.182 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.108, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dense # torch/nn/functional.py:1678:0
    return (%input.182)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.313 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.36 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.36)

BertLayer._actual_script_module
  graph(%self.315 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.315)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.315)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.315)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.316 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.316)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.316)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.326 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.326)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.191 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate # torch/nn/functional.py:1369:0
    return (%input.191)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.328 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.328)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.328)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.328)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output # transformers/modeling_bert.py:371:0
    %input.193 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.193)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.322 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.322)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.322)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.322)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output # transformers/modeling_bert.py:295:0
    %input.189 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.189)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.317 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.317)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.317)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.317)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.317)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %x.110 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %query_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.110, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %x.112 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %key_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.112, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %x.114 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %value_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.114, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.19, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.37 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.19, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.38 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.37, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:262:0
    %input.185 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.38, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %input.186 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.185, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.186)
    %context_layer.37 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.37, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.38 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.38, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.38, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
    %input.187 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.38, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:279:0
    return (%input.187)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.321 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.186 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.186, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.19)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.319 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.319)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.319)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.key # torch/nn/functional.py:1676:0
    %output.110 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.key # torch/nn/functional.py:1678:0
    %x.111 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.110, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.111)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.318 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.318)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.318)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.query # torch/nn/functional.py:1676:0
    %output.109 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.query # torch/nn/functional.py:1678:0
    %x.109 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.109, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.109)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.320 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.320)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.320)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.value # torch/nn/functional.py:1676:0
    %output.111 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.value # torch/nn/functional.py:1678:0
    %x.113 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.111, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.113)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.325 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.189 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.325)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.325)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.19 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.189, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.19)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.323 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.323)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.323)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1676:0
    %output.112 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1678:0
    %input.188 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.112, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.188)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.324 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.37 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.37)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.327 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.327)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.327)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate/__module.bert.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1676:0
    %output.113 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate/__module.bert.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate/__module.bert.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1678:0
    %input.190 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.113, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate/__module.bert.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.190)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.331 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.193 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.331)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.331)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.194 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.193, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.194)

BertOutput.dense
Linear._actual_script_module
  graph(%self.329 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.329)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.329)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dense # torch/nn/functional.py:1676:0
    %output.114 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dense # torch/nn/functional.py:1678:0
    %input.192 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.114, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dense # torch/nn/functional.py:1678:0
    return (%input.192)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.330 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.38 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.38)

BertLayer._actual_script_module
  graph(%self.332 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.332)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.332)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.332)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.333 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.333)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.333)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.343 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.343)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.201 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate # torch/nn/functional.py:1369:0
    return (%input.201)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.345 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.345)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.345)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.345)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output # transformers/modeling_bert.py:371:0
    %input.203 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.203)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.339 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.339)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.339)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.339)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output # transformers/modeling_bert.py:295:0
    %input.199 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.199)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.334 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.334)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.334)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.334)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.334)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %x.116 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %query_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.116, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %x.118 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %key_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.118, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %x.120 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %value_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.120, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.20, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.20, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.39, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:262:0
    %input.195 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.40, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %input.196 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.195, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.196)
    %context_layer.39 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.39, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.40 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.40, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.40, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
    %input.197 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.40, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:279:0
    return (%input.197)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.338 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.196 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.196, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.20)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.336 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.336)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.336)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.key # torch/nn/functional.py:1676:0
    %output.116 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.key # torch/nn/functional.py:1678:0
    %x.117 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.116, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.117)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.335 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.335)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.335)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.query # torch/nn/functional.py:1676:0
    %output.115 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.query # torch/nn/functional.py:1678:0
    %x.115 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.115, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.115)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.337 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.337)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.337)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.value # torch/nn/functional.py:1676:0
    %output.117 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.value # torch/nn/functional.py:1678:0
    %x.119 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.117, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.119)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.342 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.199 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.342)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.342)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.20 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.199, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.20)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.340 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.340)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.340)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1676:0
    %output.118 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1678:0
    %input.198 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.118, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.198)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.341 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.39 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.39)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.344 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.344)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.344)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate/__module.bert.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1676:0
    %output.119 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate/__module.bert.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate/__module.bert.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1678:0
    %input.200 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.119, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate/__module.bert.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.200)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.348 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.203 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.348)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.348)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.204 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.203, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.204)

BertOutput.dense
Linear._actual_script_module
  graph(%self.346 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.346)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.346)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dense # torch/nn/functional.py:1676:0
    %output.120 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dense # torch/nn/functional.py:1678:0
    %input.202 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.120, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dense # torch/nn/functional.py:1678:0
    return (%input.202)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.347 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.40 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.40)

BertLayer._actual_script_module
  graph(%self.349 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.349)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.349)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.349)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.350 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.350)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.350)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.360 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.360)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.211 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate # torch/nn/functional.py:1369:0
    return (%input.211)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.362 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.362)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.362)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.362)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output # transformers/modeling_bert.py:371:0
    %input.213 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.213)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.356 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.356)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.356)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.356)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output # transformers/modeling_bert.py:295:0
    %input.209 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.209)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.351 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.351)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.351)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.351)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.351)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %x.122 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %query_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.122, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %x.124 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %key_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.124, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %x.126 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %value_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.126, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.21, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.41 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.21, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.42 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.41, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:262:0
    %input.205 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.42, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %input.206 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.205, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.206)
    %context_layer.41 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.41, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.42 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.42, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.42, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
    %input.207 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.42, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:279:0
    return (%input.207)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.355 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.206 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.206, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.21)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.353 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.353)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.353)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.key # torch/nn/functional.py:1676:0
    %output.122 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.key # torch/nn/functional.py:1678:0
    %x.123 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.122, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.123)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.352 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.352)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.352)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.query # torch/nn/functional.py:1676:0
    %output.121 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.query # torch/nn/functional.py:1678:0
    %x.121 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.121, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.121)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.354 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.354)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.354)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.value # torch/nn/functional.py:1676:0
    %output.123 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.value # torch/nn/functional.py:1678:0
    %x.125 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.123, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.125)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.359 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.209 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.359)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.359)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.21 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.209, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.21)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.357 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.357)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.357)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1676:0
    %output.124 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1678:0
    %input.208 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.124, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.208)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.358 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.41 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.41)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.361 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.361)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.361)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate/__module.bert.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1676:0
    %output.125 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate/__module.bert.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate/__module.bert.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1678:0
    %input.210 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.125, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate/__module.bert.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.210)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.365 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.213 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.365)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.365)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.214 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.213, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.214)

BertOutput.dense
Linear._actual_script_module
  graph(%self.363 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.363)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.363)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dense # torch/nn/functional.py:1676:0
    %output.126 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dense # torch/nn/functional.py:1678:0
    %input.212 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.126, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dense # torch/nn/functional.py:1678:0
    return (%input.212)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.364 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.42 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.42)

BertLayer._actual_script_module
  graph(%self.366 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.366)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.366)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.366)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.367 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.367)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.367)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.377 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.377)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.221 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate # torch/nn/functional.py:1369:0
    return (%input.221)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.379 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.379)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.379)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.379)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output # transformers/modeling_bert.py:371:0
    %input.223 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.223)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.373 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.373)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.373)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.373)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output # transformers/modeling_bert.py:295:0
    %input.219 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.219)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.368 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.368)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.368)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.368)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.368)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %x.128 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %query_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.128, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %x.130 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %key_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.130, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %x.132 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %value_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.132, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.22, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.43 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.22, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.44 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.43, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:262:0
    %input.215 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.44, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %input.216 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.215, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.216)
    %context_layer.43 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.43, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.44 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.44, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.44, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
    %input.217 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.44, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:279:0
    return (%input.217)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.372 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.216 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.216, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.22)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.370 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.370)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.370)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.key # torch/nn/functional.py:1676:0
    %output.128 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.key # torch/nn/functional.py:1678:0
    %x.129 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.128, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.129)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.369 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.369)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.369)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.query # torch/nn/functional.py:1676:0
    %output.127 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.query # torch/nn/functional.py:1678:0
    %x.127 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.127, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.127)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.371 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.371)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.371)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.value # torch/nn/functional.py:1676:0
    %output.129 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.value # torch/nn/functional.py:1678:0
    %x.131 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.129, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.131)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.376 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.219 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.376)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.376)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.22 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.219, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.22)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.374 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.374)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.374)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1676:0
    %output.130 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1678:0
    %input.218 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.130, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.218)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.375 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.43 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.43)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.378 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.378)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.378)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate/__module.bert.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1676:0
    %output.131 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate/__module.bert.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate/__module.bert.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1678:0
    %input.220 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.131, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate/__module.bert.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.220)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.382 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.223 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.382)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.382)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.224 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.223, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.224)

BertOutput.dense
Linear._actual_script_module
  graph(%self.380 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.380)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.380)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dense # torch/nn/functional.py:1676:0
    %output.132 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dense # torch/nn/functional.py:1678:0
    %input.222 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.132, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dense # torch/nn/functional.py:1678:0
    return (%input.222)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.381 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.44 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.44)

BertLayer._actual_script_module
  graph(%self.383 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.383)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.383)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.383)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.384 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.384)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.384)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.394 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.394)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.231 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate # torch/nn/functional.py:1369:0
    return (%input.231)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.396 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.396)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.396)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.396)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output # transformers/modeling_bert.py:371:0
    %input.233 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.233)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.390 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.390)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.390)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.390)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output # transformers/modeling_bert.py:295:0
    %input.229 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.229)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.385 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.385)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.385)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.385)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.385)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %x.134 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %query_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.134, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %x.136 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %key_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.136, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %x.138 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %value_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.138, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.23, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.23, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.45, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:262:0
    %input.225 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.46, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %input.226 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.225, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.226)
    %context_layer.45 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.45, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
    %context_layer.46 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer.46, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer.46, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
    %input.227 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.46, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:279:0
    return (%input.227)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.389 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.226 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.226, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.23)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.387 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.387)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.387)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.key # torch/nn/functional.py:1676:0
    %output.134 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.key # torch/nn/functional.py:1678:0
    %x.135 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.134, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.135)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.386 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.386)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.386)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.query # torch/nn/functional.py:1676:0
    %output.133 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.query # torch/nn/functional.py:1678:0
    %x.133 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.133, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.133)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.388 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.388)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.388)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.value # torch/nn/functional.py:1676:0
    %output.135 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.value # torch/nn/functional.py:1678:0
    %x.137 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.135, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.137)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.393 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.229 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.393)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.393)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.23 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.229, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.23)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.391 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.391)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.391)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1676:0
    %output.136 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1678:0
    %input.228 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.136, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.228)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.392 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.45 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.45)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.395 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.395)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.395)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate/__module.bert.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1676:0
    %output.137 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate/__module.bert.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate/__module.bert.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1678:0
    %input.230 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.137, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate/__module.bert.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.230)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.399 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.233 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.399)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.399)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.234 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.233, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.234)

BertOutput.dense
Linear._actual_script_module
  graph(%self.397 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.397)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.397)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dense # torch/nn/functional.py:1676:0
    %output.138 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dense # torch/nn/functional.py:1678:0
    %input.232 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.138, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dense # torch/nn/functional.py:1678:0
    return (%input.232)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.398 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.46 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.46)

BertLayer._actual_script_module
  graph(%self.400 : __torch__.transformers.modeling_bert.BertLayer,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertOutput = prim::GetAttr[name="output"](%self.400)
    %4 : __torch__.transformers.modeling_bert.BertIntermediate = prim::GetAttr[name="intermediate"](%self.400)
    %5 : __torch__.transformers.modeling_bert.BertAttention = prim::GetAttr[name="attention"](%self.400)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

BertLayer.attention
BertAttention._actual_script_module
  graph(%self.401 : __torch__.transformers.modeling_bert.BertAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_bert.BertSelfOutput = prim::GetAttr[name="output"](%self.401)
    %4 : __torch__.transformers.modeling_bert.BertSelfAttention = prim::GetAttr[name="self"](%self.401)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

BertLayer.intermediate
BertIntermediate._actual_script_module
  graph(%self.411 : __torch__.transformers.modeling_bert.BertIntermediate,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.411)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.241 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate # torch/nn/functional.py:1369:0
    return (%input.241)

BertLayer.output
BertOutput._actual_script_module
  graph(%self.413 : __torch__.transformers.modeling_bert.BertOutput,
        %1 : Float(17:53248, 13:4096, 4096:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.413)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.413)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.413)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output # transformers/modeling_bert.py:371:0
    %input.243 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output # transformers/modeling_bert.py:371:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.243)
    return (%13)

BertAttention.output
BertSelfOutput._actual_script_module
  graph(%self.407 : __torch__.transformers.modeling_bert.BertSelfOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.407)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.407)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.407)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output # transformers/modeling_bert.py:295:0
    %input.239 : Float(17:13312, 13:1024, 1024:1) = aten::add(%12, %2, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output # transformers/modeling_bert.py:295:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.239)
    return (%13)

BertAttention.self
BertSelfAttention._actual_script_module
  graph(%self.402 : __torch__.transformers.modeling_bert.BertSelfAttention,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.402)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.402)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.402)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.402)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %11 : int = aten::size(%111, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %13 : int = aten::Int(%12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %15 : int = aten::size(%111, %14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %17 : int = aten::Int(%16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %21 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %22 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %x.140 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%111, %23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %25 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %26 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %27 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %28 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %query_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.140, %29), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %31 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %32 : int = aten::size(%112, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %34 : int = aten::Int(%33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %36 : int = aten::size(%112, %35), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %38 : int = aten::Int(%37), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %42 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %43 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %x.142 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%112, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %46 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %47 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %48 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %49 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %key_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.142, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %52 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %53 : int = aten::size(%113, %52), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %55 : int = aten::Int(%54), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %57 : int = aten::size(%113, %56), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %59 : int = aten::Int(%58), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %63 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %64 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %x : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%113, %65), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
    %67 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %68 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %69 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %70 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %value_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x, %71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
    %73 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:258:0
    %74 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:258:0
    %75 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer, %73, %74), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:258:0
    %attention_scores.47 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer, %75), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:258:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:259:0
    %attention_scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.47, %77), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:259:0
    %79 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:262:0
    %input.235 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %79), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:262:0
    %81 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %input.236 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.235, %81, %82), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.236)
    %context_layer.47 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%114, %value_layer), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:275:0
    %86 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
    %87 : int = prim::Constant[value=2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
    %88 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
    %89 : int = prim::Constant[value=3](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %91 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.47, %90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
    %92 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
    %context_layer : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%91, %92), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
    %94 : int = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:278:0
    %95 : int = aten::size(%context_layer, %94), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:278:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %97 : int = aten::Int(%96), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:278:0
    %99 : int = aten::size(%context_layer, %98), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:278:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %101 : int = aten::Int(%100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %108 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:279:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
    %input.237 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer, %109), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:279:0
    return (%input.237)

BertSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.406 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.236 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.236, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs)

BertSelfAttention.key
Linear._actual_script_module
  graph(%self.404 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.404)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.404)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.key # torch/nn/functional.py:1676:0
    %output.140 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.key # torch/nn/functional.py:1678:0
    %x.141 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.140, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.141)

BertSelfAttention.query
Linear._actual_script_module
  graph(%self.403 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.403)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.403)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.query # torch/nn/functional.py:1676:0
    %output.139 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.query # torch/nn/functional.py:1678:0
    %x.139 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.139, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.139)

BertSelfAttention.value
Linear._actual_script_module
  graph(%self.405 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.405)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.405)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.value # torch/nn/functional.py:1676:0
    %output.141 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.value # torch/nn/functional.py:1678:0
    %x.143 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.141, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.143)

BertSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.410 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.239 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.410)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.410)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.239, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor)

BertSelfOutput.dense
Linear._actual_script_module
  graph(%self.408 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.408)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.408)
    %4 : Float(1024:1, 1024:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1676:0
    %output.142 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1678:0
    %input.238 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.142, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.238)

BertSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.409 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.47 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.47)

BertIntermediate.dense
Linear._actual_script_module
  graph(%self.412 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.412)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.412)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate/__module.bert.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1676:0
    %output.143 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate/__module.bert.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate/__module.bert.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1678:0
    %input.240 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.143, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate/__module.bert.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.240)

BertOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.416 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.243 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.416)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.416)
    %4 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.LayerNorm # torch/nn/functional.py:2048:0
    %input : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.243, %5, %3, %2, %6, %7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input)

BertOutput.dense
Linear._actual_script_module
  graph(%self.414 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:53248, 13:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.414)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.414)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dense # torch/nn/functional.py:1676:0
    %output.144 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dense # torch/nn/functional.py:1678:0
    %input.242 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.144, %2, %6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dense # torch/nn/functional.py:1678:0
    return (%input.242)

BertOutput.dropout
Dropout._actual_script_module
  graph(%self.415 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dropout # torch/nn/functional.py:973:0
    %hidden_states : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states)

BertGenerationOnlyLMHead.decoder
Linear._actual_script_module
  graph(%self : __torch__.torch.nn.modules.linear.Linear,
        %bias : Float(50358:1),
        %2 : Float(17:13312, 13:1024, 1024:1)):
    %3 : Tensor = prim::GetAttr[name="weight"](%self)
    %4 : Float(1024:1, 50358:1024) = aten::t(%3), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1676:0
    %output : Float(17:654654, 13:50358, 50358:1) = aten::matmul(%2, %4), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1678:0
    %7 : Float(17:654654, 13:50358, 50358:1) = aten::add_(%output, %bias, %6), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1678:0
    return (%7)

