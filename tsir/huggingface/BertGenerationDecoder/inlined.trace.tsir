graph(%self.1 : __torch__.transformers.modeling_bert_generation.BertGenerationDecoder,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_bert_generation.BertGenerationOnlyLMHead = prim::GetAttr[name="lm_head"](%self.1)
  %4 : __torch__.transformers.modeling_bert_generation.BertGenerationEncoder = prim::GetAttr[name="bert"](%self.1)
  %8 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %9 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %10 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %11 : int = prim::Constant[value=16](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %12 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %13 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %14 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %15 : int = prim::Constant[value=1024](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %16 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %17 : Double() = prim::Constant[value={-10000}](), scope: __module.bert # transformers/modeling_utils.py:258:0
  %18 : float = prim::Constant[value=1.](), scope: __module.bert # torch/tensor.py:396:0
  %19 : None = prim::Constant(), scope: __module.bert
  %20 : bool = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_utils.py:257:0
  %21 : int = prim::Constant[value=6](), scope: __module.bert # transformers/modeling_utils.py:257:0
  %22 : int = prim::Constant[value=3](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %23 : int = prim::Constant[value=2](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %24 : int = prim::Constant[value=1](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %25 : int = prim::Constant[value=9223372036854775807](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %26 : int = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %27 : __torch__.transformers.modeling_bert.___torch_mangle_6988.BertEncoder = prim::GetAttr[name="encoder"](%4)
  %28 : __torch__.transformers.modeling_bert_generation.BertGenerationEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %29 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %26, %26, %25, %24), scope: __module.bert # transformers/modeling_utils.py:244:0
  %30 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%29, %24), scope: __module.bert # transformers/modeling_utils.py:244:0
  %31 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%30, %23), scope: __module.bert # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%31, %22, %26, %25, %24), scope: __module.bert # transformers/modeling_utils.py:244:0
  %33 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %21, %20, %20, %19), scope: __module.bert # transformers/modeling_utils.py:257:0
  %34 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%33, %18, %24), scope: __module.bert # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%34, %17), scope: __module.bert # transformers/modeling_utils.py:258:0
  %36 : __torch__.torch.nn.modules.normalization.___torch_mangle_6577.LayerNorm = prim::GetAttr[name="LayerNorm"](%28)
  %37 : __torch__.torch.nn.modules.sparse.___torch_mangle_6576.Embedding = prim::GetAttr[name="position_embeddings"](%28)
  %38 : __torch__.torch.nn.modules.sparse.___torch_mangle_6575.Embedding = prim::GetAttr[name="word_embeddings"](%28)
  %39 : Tensor = prim::GetAttr[name="position_ids"](%28)
  %40 : int = aten::size(%input_ids, %24), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:149:0
  %41 : Long(1:512, 512:1) = aten::slice(%39, %26, %26, %25, %24), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%41, %24, %26, %40, %24), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:156:0
  %43 : Tensor = prim::GetAttr[name="weight"](%38)
  %inputs_embeds : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%43, %input_ids, %26, %20, %20), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %45 : Tensor = prim::GetAttr[name="weight"](%37)
  %position_embeddings : Float(1:13312, 13:1024, 1024:1) = aten::embedding(%45, %input.1, %12, %20, %20), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %input.2 : Float(17:13312, 13:1024, 1024:1) = aten::add(%inputs_embeds, %position_embeddings, %24), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert_generation.py:162:0
  %48 : Tensor = prim::GetAttr[name="bias"](%36)
  %49 : Tensor = prim::GetAttr[name="weight"](%36)
  %50 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm
  %input.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.2, %50, %49, %48, %14, %13), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.4 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.3, %16, %20), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %53 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %54 : __torch__.transformers.modeling_bert.___torch_mangle_6986.BertLayer = prim::GetAttr[name="23"](%53)
  %55 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %56 : __torch__.transformers.modeling_bert.___torch_mangle_6969.BertLayer = prim::GetAttr[name="22"](%55)
  %57 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %58 : __torch__.transformers.modeling_bert.___torch_mangle_6952.BertLayer = prim::GetAttr[name="21"](%57)
  %59 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %60 : __torch__.transformers.modeling_bert.___torch_mangle_6935.BertLayer = prim::GetAttr[name="20"](%59)
  %61 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %62 : __torch__.transformers.modeling_bert.___torch_mangle_6918.BertLayer = prim::GetAttr[name="19"](%61)
  %63 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %64 : __torch__.transformers.modeling_bert.___torch_mangle_6901.BertLayer = prim::GetAttr[name="18"](%63)
  %65 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %66 : __torch__.transformers.modeling_bert.___torch_mangle_6884.BertLayer = prim::GetAttr[name="17"](%65)
  %67 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %68 : __torch__.transformers.modeling_bert.___torch_mangle_6867.BertLayer = prim::GetAttr[name="16"](%67)
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %70 : __torch__.transformers.modeling_bert.___torch_mangle_6850.BertLayer = prim::GetAttr[name="15"](%69)
  %71 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %72 : __torch__.transformers.modeling_bert.___torch_mangle_6833.BertLayer = prim::GetAttr[name="14"](%71)
  %73 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %74 : __torch__.transformers.modeling_bert.___torch_mangle_6816.BertLayer = prim::GetAttr[name="13"](%73)
  %75 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %76 : __torch__.transformers.modeling_bert.___torch_mangle_6799.BertLayer = prim::GetAttr[name="12"](%75)
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %78 : __torch__.transformers.modeling_bert.___torch_mangle_6782.BertLayer = prim::GetAttr[name="11"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %80 : __torch__.transformers.modeling_bert.___torch_mangle_6765.BertLayer = prim::GetAttr[name="10"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %82 : __torch__.transformers.modeling_bert.___torch_mangle_6748.BertLayer = prim::GetAttr[name="9"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %84 : __torch__.transformers.modeling_bert.___torch_mangle_6731.BertLayer = prim::GetAttr[name="8"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %86 : __torch__.transformers.modeling_bert.___torch_mangle_6714.BertLayer = prim::GetAttr[name="7"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %88 : __torch__.transformers.modeling_bert.___torch_mangle_6697.BertLayer = prim::GetAttr[name="6"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %90 : __torch__.transformers.modeling_bert.___torch_mangle_6680.BertLayer = prim::GetAttr[name="5"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %92 : __torch__.transformers.modeling_bert.___torch_mangle_6663.BertLayer = prim::GetAttr[name="4"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %94 : __torch__.transformers.modeling_bert.___torch_mangle_6646.BertLayer = prim::GetAttr[name="3"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %96 : __torch__.transformers.modeling_bert.___torch_mangle_6629.BertLayer = prim::GetAttr[name="2"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %98 : __torch__.transformers.modeling_bert.___torch_mangle_6612.BertLayer = prim::GetAttr[name="1"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_6987.ModuleList = prim::GetAttr[name="layer"](%27)
  %100 : __torch__.transformers.modeling_bert.___torch_mangle_6595.BertLayer = prim::GetAttr[name="0"](%99)
  %101 : __torch__.transformers.modeling_bert.___torch_mangle_6594.BertOutput = prim::GetAttr[name="output"](%100)
  %102 : __torch__.transformers.modeling_bert.___torch_mangle_6590.BertIntermediate = prim::GetAttr[name="intermediate"](%100)
  %103 : __torch__.transformers.modeling_bert.___torch_mangle_6588.BertAttention = prim::GetAttr[name="attention"](%100)
  %104 : __torch__.transformers.modeling_bert.___torch_mangle_6587.BertSelfOutput = prim::GetAttr[name="output"](%103)
  %105 : __torch__.transformers.modeling_bert.___torch_mangle_6583.BertSelfAttention = prim::GetAttr[name="self"](%103)
  %106 : __torch__.torch.nn.modules.linear.___torch_mangle_6581.Linear = prim::GetAttr[name="value"](%105)
  %107 : __torch__.torch.nn.modules.linear.___torch_mangle_6580.Linear = prim::GetAttr[name="key"](%105)
  %108 : __torch__.torch.nn.modules.linear.___torch_mangle_6579.Linear = prim::GetAttr[name="query"](%105)
  %109 : Tensor = prim::GetAttr[name="bias"](%108)
  %110 : Tensor = prim::GetAttr[name="weight"](%108)
  %111 : Float(1024:1, 1024:1024) = aten::t(%110), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.4, %111), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.1, %109, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %114 : Tensor = prim::GetAttr[name="bias"](%107)
  %115 : Tensor = prim::GetAttr[name="weight"](%107)
  %116 : Float(1024:1, 1024:1024) = aten::t(%115), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.4, %116), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.2, %114, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %119 : Tensor = prim::GetAttr[name="bias"](%106)
  %120 : Tensor = prim::GetAttr[name="weight"](%106)
  %121 : Float(1024:1, 1024:1024) = aten::t(%120), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.4, %121), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.3, %119, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %124 : int = aten::size(%x.1, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %125 : int = aten::size(%x.1, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %126 : int[] = prim::ListConstruct(%124, %125, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.2 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.1, %126), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %128 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.2, %128), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %130 : int = aten::size(%x.3, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %131 : int = aten::size(%x.3, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %132 : int[] = prim::ListConstruct(%130, %131, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.4 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.3, %132), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %134 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.4, %134), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %136 : int = aten::size(%x.5, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %137 : int = aten::size(%x.5, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %138 : int[] = prim::ListConstruct(%136, %137, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.6 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.5, %138), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %140 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.6, %140), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %142 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.1, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %142), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.1, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:262:0
  %input.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.6, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:275:0
  %149 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %150 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.1, %149), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.2 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%150, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %152 : int = aten::size(%context_layer.2, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %153 : int = aten::size(%context_layer.2, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %154 : int[] = prim::ListConstruct(%152, %153, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %input.7 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.2, %154), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
  %156 : __torch__.torch.nn.modules.normalization.___torch_mangle_6585.LayerNorm = prim::GetAttr[name="LayerNorm"](%104)
  %157 : __torch__.torch.nn.modules.linear.___torch_mangle_6584.Linear = prim::GetAttr[name="dense"](%104)
  %158 : Tensor = prim::GetAttr[name="bias"](%157)
  %159 : Tensor = prim::GetAttr[name="weight"](%157)
  %160 : Float(1024:1, 1024:1024) = aten::t(%159), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.7, %160), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.8 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.4, %158, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.8, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.1, %input.4, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output # transformers/modeling_bert.py:295:0
  %165 : Tensor = prim::GetAttr[name="bias"](%156)
  %166 : Tensor = prim::GetAttr[name="weight"](%156)
  %167 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.9, %167, %166, %165, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %169 : __torch__.torch.nn.modules.linear.___torch_mangle_6589.Linear = prim::GetAttr[name="dense"](%102)
  %170 : Tensor = prim::GetAttr[name="bias"](%169)
  %171 : Tensor = prim::GetAttr[name="weight"](%169)
  %172 : Float(1024:1, 4096:1024) = aten::t(%171), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.1, %172), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.5, %170, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.11 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %176 : __torch__.torch.nn.modules.normalization.___torch_mangle_6592.LayerNorm = prim::GetAttr[name="LayerNorm"](%101)
  %177 : __torch__.torch.nn.modules.linear.___torch_mangle_6591.Linear = prim::GetAttr[name="dense"](%101)
  %178 : Tensor = prim::GetAttr[name="bias"](%177)
  %179 : Tensor = prim::GetAttr[name="weight"](%177)
  %180 : Float(4096:1, 1024:4096) = aten::t(%179), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.11, %180), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.12 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.6, %178, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.12, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.13 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.2, %input_tensor.1, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output # transformers/modeling_bert.py:371:0
  %185 : Tensor = prim::GetAttr[name="bias"](%176)
  %186 : Tensor = prim::GetAttr[name="weight"](%176)
  %187 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm
  %input.14 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.13, %187, %186, %185, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %189 : __torch__.transformers.modeling_bert.___torch_mangle_6611.BertOutput = prim::GetAttr[name="output"](%98)
  %190 : __torch__.transformers.modeling_bert.___torch_mangle_6607.BertIntermediate = prim::GetAttr[name="intermediate"](%98)
  %191 : __torch__.transformers.modeling_bert.___torch_mangle_6605.BertAttention = prim::GetAttr[name="attention"](%98)
  %192 : __torch__.transformers.modeling_bert.___torch_mangle_6604.BertSelfOutput = prim::GetAttr[name="output"](%191)
  %193 : __torch__.transformers.modeling_bert.___torch_mangle_6600.BertSelfAttention = prim::GetAttr[name="self"](%191)
  %194 : __torch__.torch.nn.modules.linear.___torch_mangle_6598.Linear = prim::GetAttr[name="value"](%193)
  %195 : __torch__.torch.nn.modules.linear.___torch_mangle_6597.Linear = prim::GetAttr[name="key"](%193)
  %196 : __torch__.torch.nn.modules.linear.___torch_mangle_6596.Linear = prim::GetAttr[name="query"](%193)
  %197 : Tensor = prim::GetAttr[name="bias"](%196)
  %198 : Tensor = prim::GetAttr[name="weight"](%196)
  %199 : Float(1024:1, 1024:1024) = aten::t(%198), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.14, %199), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.7, %197, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %202 : Tensor = prim::GetAttr[name="bias"](%195)
  %203 : Tensor = prim::GetAttr[name="weight"](%195)
  %204 : Float(1024:1, 1024:1024) = aten::t(%203), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.14, %204), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.8, %202, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %207 : Tensor = prim::GetAttr[name="bias"](%194)
  %208 : Tensor = prim::GetAttr[name="weight"](%194)
  %209 : Float(1024:1, 1024:1024) = aten::t(%208), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.14, %209), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.9, %207, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %212 : int = aten::size(%x.7, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %213 : int = aten::size(%x.7, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %214 : int[] = prim::ListConstruct(%212, %213, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.8 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.7, %214), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %216 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.8, %216), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %218 : int = aten::size(%x.9, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %219 : int = aten::size(%x.9, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %220 : int[] = prim::ListConstruct(%218, %219, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.10 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.9, %220), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %222 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.10, %222), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %224 : int = aten::size(%x.11, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %225 : int = aten::size(%x.11, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %226 : int[] = prim::ListConstruct(%224, %225, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.12 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.11, %226), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %228 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.12, %228), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %230 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.2, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %230), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.3, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:259:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:262:0
  %input.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.16, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:275:0
  %237 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %238 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.3, %237), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.4 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%238, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %240 : int = aten::size(%context_layer.4, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %241 : int = aten::size(%context_layer.4, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %242 : int[] = prim::ListConstruct(%240, %241, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %input.17 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.4, %242), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:279:0
  %244 : __torch__.torch.nn.modules.normalization.___torch_mangle_6602.LayerNorm = prim::GetAttr[name="LayerNorm"](%192)
  %245 : __torch__.torch.nn.modules.linear.___torch_mangle_6601.Linear = prim::GetAttr[name="dense"](%192)
  %246 : Tensor = prim::GetAttr[name="bias"](%245)
  %247 : Tensor = prim::GetAttr[name="weight"](%245)
  %248 : Float(1024:1, 1024:1024) = aten::t(%247), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.17, %248), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.18 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.10, %246, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.18, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.19 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.3, %input.14, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output # transformers/modeling_bert.py:295:0
  %253 : Tensor = prim::GetAttr[name="bias"](%244)
  %254 : Tensor = prim::GetAttr[name="weight"](%244)
  %255 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.19, %255, %254, %253, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %257 : __torch__.torch.nn.modules.linear.___torch_mangle_6606.Linear = prim::GetAttr[name="dense"](%190)
  %258 : Tensor = prim::GetAttr[name="bias"](%257)
  %259 : Tensor = prim::GetAttr[name="weight"](%257)
  %260 : Float(1024:1, 4096:1024) = aten::t(%259), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.2, %260), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.20 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.11, %258, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.21 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %264 : __torch__.torch.nn.modules.normalization.___torch_mangle_6609.LayerNorm = prim::GetAttr[name="LayerNorm"](%189)
  %265 : __torch__.torch.nn.modules.linear.___torch_mangle_6608.Linear = prim::GetAttr[name="dense"](%189)
  %266 : Tensor = prim::GetAttr[name="bias"](%265)
  %267 : Tensor = prim::GetAttr[name="weight"](%265)
  %268 : Float(4096:1, 1024:4096) = aten::t(%267), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.21, %268), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.22 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.12, %266, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.22, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.4, %input_tensor.2, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output # transformers/modeling_bert.py:371:0
  %273 : Tensor = prim::GetAttr[name="bias"](%264)
  %274 : Tensor = prim::GetAttr[name="weight"](%264)
  %275 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm
  %input.24 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.23, %275, %274, %273, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %277 : __torch__.transformers.modeling_bert.___torch_mangle_6628.BertOutput = prim::GetAttr[name="output"](%96)
  %278 : __torch__.transformers.modeling_bert.___torch_mangle_6624.BertIntermediate = prim::GetAttr[name="intermediate"](%96)
  %279 : __torch__.transformers.modeling_bert.___torch_mangle_6622.BertAttention = prim::GetAttr[name="attention"](%96)
  %280 : __torch__.transformers.modeling_bert.___torch_mangle_6621.BertSelfOutput = prim::GetAttr[name="output"](%279)
  %281 : __torch__.transformers.modeling_bert.___torch_mangle_6617.BertSelfAttention = prim::GetAttr[name="self"](%279)
  %282 : __torch__.torch.nn.modules.linear.___torch_mangle_6615.Linear = prim::GetAttr[name="value"](%281)
  %283 : __torch__.torch.nn.modules.linear.___torch_mangle_6614.Linear = prim::GetAttr[name="key"](%281)
  %284 : __torch__.torch.nn.modules.linear.___torch_mangle_6613.Linear = prim::GetAttr[name="query"](%281)
  %285 : Tensor = prim::GetAttr[name="bias"](%284)
  %286 : Tensor = prim::GetAttr[name="weight"](%284)
  %287 : Float(1024:1, 1024:1024) = aten::t(%286), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.24, %287), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.13, %285, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %290 : Tensor = prim::GetAttr[name="bias"](%283)
  %291 : Tensor = prim::GetAttr[name="weight"](%283)
  %292 : Float(1024:1, 1024:1024) = aten::t(%291), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.24, %292), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.14, %290, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %295 : Tensor = prim::GetAttr[name="bias"](%282)
  %296 : Tensor = prim::GetAttr[name="weight"](%282)
  %297 : Float(1024:1, 1024:1024) = aten::t(%296), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.24, %297), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.15, %295, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %300 : int = aten::size(%x.13, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %301 : int = aten::size(%x.13, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %302 : int[] = prim::ListConstruct(%300, %301, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.14 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.13, %302), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %304 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.14, %304), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %306 : int = aten::size(%x.15, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %307 : int = aten::size(%x.15, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %308 : int[] = prim::ListConstruct(%306, %307, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.16 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.15, %308), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %310 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.16, %310), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %312 : int = aten::size(%x.17, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %313 : int = aten::size(%x.17, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %314 : int[] = prim::ListConstruct(%312, %313, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.18 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.17, %314), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %316 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.18, %316), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %318 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.3, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %318), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.5, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:259:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:262:0
  %input.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.26, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:275:0
  %325 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %326 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.5, %325), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.6 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%326, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %328 : int = aten::size(%context_layer.6, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %329 : int = aten::size(%context_layer.6, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %330 : int[] = prim::ListConstruct(%328, %329, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %input.27 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.6, %330), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:279:0
  %332 : __torch__.torch.nn.modules.normalization.___torch_mangle_6619.LayerNorm = prim::GetAttr[name="LayerNorm"](%280)
  %333 : __torch__.torch.nn.modules.linear.___torch_mangle_6618.Linear = prim::GetAttr[name="dense"](%280)
  %334 : Tensor = prim::GetAttr[name="bias"](%333)
  %335 : Tensor = prim::GetAttr[name="weight"](%333)
  %336 : Float(1024:1, 1024:1024) = aten::t(%335), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.27, %336), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.28 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.16, %334, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.28, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.29 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.5, %input.24, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output # transformers/modeling_bert.py:295:0
  %341 : Tensor = prim::GetAttr[name="bias"](%332)
  %342 : Tensor = prim::GetAttr[name="weight"](%332)
  %343 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.29, %343, %342, %341, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %345 : __torch__.torch.nn.modules.linear.___torch_mangle_6623.Linear = prim::GetAttr[name="dense"](%278)
  %346 : Tensor = prim::GetAttr[name="bias"](%345)
  %347 : Tensor = prim::GetAttr[name="weight"](%345)
  %348 : Float(1024:1, 4096:1024) = aten::t(%347), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.3, %348), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.30 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.17, %346, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.31 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.30), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %352 : __torch__.torch.nn.modules.normalization.___torch_mangle_6626.LayerNorm = prim::GetAttr[name="LayerNorm"](%277)
  %353 : __torch__.torch.nn.modules.linear.___torch_mangle_6625.Linear = prim::GetAttr[name="dense"](%277)
  %354 : Tensor = prim::GetAttr[name="bias"](%353)
  %355 : Tensor = prim::GetAttr[name="weight"](%353)
  %356 : Float(4096:1, 1024:4096) = aten::t(%355), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.31, %356), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.32 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.18, %354, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.32, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.6, %input_tensor.3, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output # transformers/modeling_bert.py:371:0
  %361 : Tensor = prim::GetAttr[name="bias"](%352)
  %362 : Tensor = prim::GetAttr[name="weight"](%352)
  %363 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm
  %input.34 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.33, %363, %362, %361, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %365 : __torch__.transformers.modeling_bert.___torch_mangle_6645.BertOutput = prim::GetAttr[name="output"](%94)
  %366 : __torch__.transformers.modeling_bert.___torch_mangle_6641.BertIntermediate = prim::GetAttr[name="intermediate"](%94)
  %367 : __torch__.transformers.modeling_bert.___torch_mangle_6639.BertAttention = prim::GetAttr[name="attention"](%94)
  %368 : __torch__.transformers.modeling_bert.___torch_mangle_6638.BertSelfOutput = prim::GetAttr[name="output"](%367)
  %369 : __torch__.transformers.modeling_bert.___torch_mangle_6634.BertSelfAttention = prim::GetAttr[name="self"](%367)
  %370 : __torch__.torch.nn.modules.linear.___torch_mangle_6632.Linear = prim::GetAttr[name="value"](%369)
  %371 : __torch__.torch.nn.modules.linear.___torch_mangle_6631.Linear = prim::GetAttr[name="key"](%369)
  %372 : __torch__.torch.nn.modules.linear.___torch_mangle_6630.Linear = prim::GetAttr[name="query"](%369)
  %373 : Tensor = prim::GetAttr[name="bias"](%372)
  %374 : Tensor = prim::GetAttr[name="weight"](%372)
  %375 : Float(1024:1, 1024:1024) = aten::t(%374), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.34, %375), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.19, %373, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %378 : Tensor = prim::GetAttr[name="bias"](%371)
  %379 : Tensor = prim::GetAttr[name="weight"](%371)
  %380 : Float(1024:1, 1024:1024) = aten::t(%379), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.34, %380), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.20, %378, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %383 : Tensor = prim::GetAttr[name="bias"](%370)
  %384 : Tensor = prim::GetAttr[name="weight"](%370)
  %385 : Float(1024:1, 1024:1024) = aten::t(%384), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.34, %385), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.21, %383, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %388 : int = aten::size(%x.19, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %389 : int = aten::size(%x.19, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %390 : int[] = prim::ListConstruct(%388, %389, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.20 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.19, %390), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %392 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.20, %392), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %394 : int = aten::size(%x.21, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %395 : int = aten::size(%x.21, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %396 : int[] = prim::ListConstruct(%394, %395, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.22 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.21, %396), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %398 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.22, %398), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %400 : int = aten::size(%x.23, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %401 : int = aten::size(%x.23, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %402 : int[] = prim::ListConstruct(%400, %401, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.24 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.23, %402), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %404 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.24, %404), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %406 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.4, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %406), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.7, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:259:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:262:0
  %input.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.36, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:275:0
  %413 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %414 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.7, %413), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.8 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%414, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %416 : int = aten::size(%context_layer.8, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %417 : int = aten::size(%context_layer.8, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %418 : int[] = prim::ListConstruct(%416, %417, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %input.37 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.8, %418), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:279:0
  %420 : __torch__.torch.nn.modules.normalization.___torch_mangle_6636.LayerNorm = prim::GetAttr[name="LayerNorm"](%368)
  %421 : __torch__.torch.nn.modules.linear.___torch_mangle_6635.Linear = prim::GetAttr[name="dense"](%368)
  %422 : Tensor = prim::GetAttr[name="bias"](%421)
  %423 : Tensor = prim::GetAttr[name="weight"](%421)
  %424 : Float(1024:1, 1024:1024) = aten::t(%423), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.37, %424), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.38 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.22, %422, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.38, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.39 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.7, %input.34, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output # transformers/modeling_bert.py:295:0
  %429 : Tensor = prim::GetAttr[name="bias"](%420)
  %430 : Tensor = prim::GetAttr[name="weight"](%420)
  %431 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.39, %431, %430, %429, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %433 : __torch__.torch.nn.modules.linear.___torch_mangle_6640.Linear = prim::GetAttr[name="dense"](%366)
  %434 : Tensor = prim::GetAttr[name="bias"](%433)
  %435 : Tensor = prim::GetAttr[name="weight"](%433)
  %436 : Float(1024:1, 4096:1024) = aten::t(%435), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.4, %436), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.40 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.23, %434, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.41 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.40), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %440 : __torch__.torch.nn.modules.normalization.___torch_mangle_6643.LayerNorm = prim::GetAttr[name="LayerNorm"](%365)
  %441 : __torch__.torch.nn.modules.linear.___torch_mangle_6642.Linear = prim::GetAttr[name="dense"](%365)
  %442 : Tensor = prim::GetAttr[name="bias"](%441)
  %443 : Tensor = prim::GetAttr[name="weight"](%441)
  %444 : Float(4096:1, 1024:4096) = aten::t(%443), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.41, %444), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.42 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.24, %442, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.42, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.43 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.8, %input_tensor.4, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output # transformers/modeling_bert.py:371:0
  %449 : Tensor = prim::GetAttr[name="bias"](%440)
  %450 : Tensor = prim::GetAttr[name="weight"](%440)
  %451 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm
  %input.44 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.43, %451, %450, %449, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %453 : __torch__.transformers.modeling_bert.___torch_mangle_6662.BertOutput = prim::GetAttr[name="output"](%92)
  %454 : __torch__.transformers.modeling_bert.___torch_mangle_6658.BertIntermediate = prim::GetAttr[name="intermediate"](%92)
  %455 : __torch__.transformers.modeling_bert.___torch_mangle_6656.BertAttention = prim::GetAttr[name="attention"](%92)
  %456 : __torch__.transformers.modeling_bert.___torch_mangle_6655.BertSelfOutput = prim::GetAttr[name="output"](%455)
  %457 : __torch__.transformers.modeling_bert.___torch_mangle_6651.BertSelfAttention = prim::GetAttr[name="self"](%455)
  %458 : __torch__.torch.nn.modules.linear.___torch_mangle_6649.Linear = prim::GetAttr[name="value"](%457)
  %459 : __torch__.torch.nn.modules.linear.___torch_mangle_6648.Linear = prim::GetAttr[name="key"](%457)
  %460 : __torch__.torch.nn.modules.linear.___torch_mangle_6647.Linear = prim::GetAttr[name="query"](%457)
  %461 : Tensor = prim::GetAttr[name="bias"](%460)
  %462 : Tensor = prim::GetAttr[name="weight"](%460)
  %463 : Float(1024:1, 1024:1024) = aten::t(%462), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.44, %463), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.25, %461, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %466 : Tensor = prim::GetAttr[name="bias"](%459)
  %467 : Tensor = prim::GetAttr[name="weight"](%459)
  %468 : Float(1024:1, 1024:1024) = aten::t(%467), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.44, %468), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.26, %466, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %471 : Tensor = prim::GetAttr[name="bias"](%458)
  %472 : Tensor = prim::GetAttr[name="weight"](%458)
  %473 : Float(1024:1, 1024:1024) = aten::t(%472), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.44, %473), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.27, %471, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %476 : int = aten::size(%x.25, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %477 : int = aten::size(%x.25, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %478 : int[] = prim::ListConstruct(%476, %477, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.26 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.25, %478), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %480 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.26, %480), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %482 : int = aten::size(%x.27, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %483 : int = aten::size(%x.27, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %484 : int[] = prim::ListConstruct(%482, %483, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.28 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.27, %484), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %486 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.28, %486), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %488 : int = aten::size(%x.29, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %489 : int = aten::size(%x.29, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %490 : int[] = prim::ListConstruct(%488, %489, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.30 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.29, %490), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %492 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.30, %492), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %494 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.5, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %494), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.9, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:259:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:262:0
  %input.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.46, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:275:0
  %501 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %502 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.9, %501), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.10 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%502, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %504 : int = aten::size(%context_layer.10, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %505 : int = aten::size(%context_layer.10, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %506 : int[] = prim::ListConstruct(%504, %505, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %input.47 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.10, %506), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:279:0
  %508 : __torch__.torch.nn.modules.normalization.___torch_mangle_6653.LayerNorm = prim::GetAttr[name="LayerNorm"](%456)
  %509 : __torch__.torch.nn.modules.linear.___torch_mangle_6652.Linear = prim::GetAttr[name="dense"](%456)
  %510 : Tensor = prim::GetAttr[name="bias"](%509)
  %511 : Tensor = prim::GetAttr[name="weight"](%509)
  %512 : Float(1024:1, 1024:1024) = aten::t(%511), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.47, %512), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.48 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.28, %510, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.48, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.49 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.9, %input.44, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output # transformers/modeling_bert.py:295:0
  %517 : Tensor = prim::GetAttr[name="bias"](%508)
  %518 : Tensor = prim::GetAttr[name="weight"](%508)
  %519 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.49, %519, %518, %517, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %521 : __torch__.torch.nn.modules.linear.___torch_mangle_6657.Linear = prim::GetAttr[name="dense"](%454)
  %522 : Tensor = prim::GetAttr[name="bias"](%521)
  %523 : Tensor = prim::GetAttr[name="weight"](%521)
  %524 : Float(1024:1, 4096:1024) = aten::t(%523), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.5, %524), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.50 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.29, %522, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.51 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %528 : __torch__.torch.nn.modules.normalization.___torch_mangle_6660.LayerNorm = prim::GetAttr[name="LayerNorm"](%453)
  %529 : __torch__.torch.nn.modules.linear.___torch_mangle_6659.Linear = prim::GetAttr[name="dense"](%453)
  %530 : Tensor = prim::GetAttr[name="bias"](%529)
  %531 : Tensor = prim::GetAttr[name="weight"](%529)
  %532 : Float(4096:1, 1024:4096) = aten::t(%531), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.51, %532), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.52 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.30, %530, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.52, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.53 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.10, %input_tensor.5, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output # transformers/modeling_bert.py:371:0
  %537 : Tensor = prim::GetAttr[name="bias"](%528)
  %538 : Tensor = prim::GetAttr[name="weight"](%528)
  %539 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm
  %input.54 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.53, %539, %538, %537, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %541 : __torch__.transformers.modeling_bert.___torch_mangle_6679.BertOutput = prim::GetAttr[name="output"](%90)
  %542 : __torch__.transformers.modeling_bert.___torch_mangle_6675.BertIntermediate = prim::GetAttr[name="intermediate"](%90)
  %543 : __torch__.transformers.modeling_bert.___torch_mangle_6673.BertAttention = prim::GetAttr[name="attention"](%90)
  %544 : __torch__.transformers.modeling_bert.___torch_mangle_6672.BertSelfOutput = prim::GetAttr[name="output"](%543)
  %545 : __torch__.transformers.modeling_bert.___torch_mangle_6668.BertSelfAttention = prim::GetAttr[name="self"](%543)
  %546 : __torch__.torch.nn.modules.linear.___torch_mangle_6666.Linear = prim::GetAttr[name="value"](%545)
  %547 : __torch__.torch.nn.modules.linear.___torch_mangle_6665.Linear = prim::GetAttr[name="key"](%545)
  %548 : __torch__.torch.nn.modules.linear.___torch_mangle_6664.Linear = prim::GetAttr[name="query"](%545)
  %549 : Tensor = prim::GetAttr[name="bias"](%548)
  %550 : Tensor = prim::GetAttr[name="weight"](%548)
  %551 : Float(1024:1, 1024:1024) = aten::t(%550), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.54, %551), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.31, %549, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %554 : Tensor = prim::GetAttr[name="bias"](%547)
  %555 : Tensor = prim::GetAttr[name="weight"](%547)
  %556 : Float(1024:1, 1024:1024) = aten::t(%555), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.54, %556), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.32, %554, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %559 : Tensor = prim::GetAttr[name="bias"](%546)
  %560 : Tensor = prim::GetAttr[name="weight"](%546)
  %561 : Float(1024:1, 1024:1024) = aten::t(%560), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.54, %561), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.33, %559, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %564 : int = aten::size(%x.31, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %565 : int = aten::size(%x.31, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %566 : int[] = prim::ListConstruct(%564, %565, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.32 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.31, %566), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %568 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.32, %568), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %570 : int = aten::size(%x.33, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %571 : int = aten::size(%x.33, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %572 : int[] = prim::ListConstruct(%570, %571, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.34 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.33, %572), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %574 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.34, %574), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %576 : int = aten::size(%x.35, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %577 : int = aten::size(%x.35, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %578 : int[] = prim::ListConstruct(%576, %577, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.36 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.35, %578), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %580 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.36, %580), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %582 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.6, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %582), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.11, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:259:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:262:0
  %input.56 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.56, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:275:0
  %589 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %590 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.11, %589), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.12 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%590, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %592 : int = aten::size(%context_layer.12, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %593 : int = aten::size(%context_layer.12, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %594 : int[] = prim::ListConstruct(%592, %593, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %input.57 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.12, %594), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:279:0
  %596 : __torch__.torch.nn.modules.normalization.___torch_mangle_6670.LayerNorm = prim::GetAttr[name="LayerNorm"](%544)
  %597 : __torch__.torch.nn.modules.linear.___torch_mangle_6669.Linear = prim::GetAttr[name="dense"](%544)
  %598 : Tensor = prim::GetAttr[name="bias"](%597)
  %599 : Tensor = prim::GetAttr[name="weight"](%597)
  %600 : Float(1024:1, 1024:1024) = aten::t(%599), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.57, %600), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.58 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.34, %598, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.58, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.59 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.11, %input.54, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output # transformers/modeling_bert.py:295:0
  %605 : Tensor = prim::GetAttr[name="bias"](%596)
  %606 : Tensor = prim::GetAttr[name="weight"](%596)
  %607 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.59, %607, %606, %605, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %609 : __torch__.torch.nn.modules.linear.___torch_mangle_6674.Linear = prim::GetAttr[name="dense"](%542)
  %610 : Tensor = prim::GetAttr[name="bias"](%609)
  %611 : Tensor = prim::GetAttr[name="weight"](%609)
  %612 : Float(1024:1, 4096:1024) = aten::t(%611), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.6, %612), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.60 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.35, %610, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.61 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.60), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %616 : __torch__.torch.nn.modules.normalization.___torch_mangle_6677.LayerNorm = prim::GetAttr[name="LayerNorm"](%541)
  %617 : __torch__.torch.nn.modules.linear.___torch_mangle_6676.Linear = prim::GetAttr[name="dense"](%541)
  %618 : Tensor = prim::GetAttr[name="bias"](%617)
  %619 : Tensor = prim::GetAttr[name="weight"](%617)
  %620 : Float(4096:1, 1024:4096) = aten::t(%619), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.61, %620), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.62 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.36, %618, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.62, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.63 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.12, %input_tensor.6, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output # transformers/modeling_bert.py:371:0
  %625 : Tensor = prim::GetAttr[name="bias"](%616)
  %626 : Tensor = prim::GetAttr[name="weight"](%616)
  %627 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm
  %input.64 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.63, %627, %626, %625, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %629 : __torch__.transformers.modeling_bert.___torch_mangle_6696.BertOutput = prim::GetAttr[name="output"](%88)
  %630 : __torch__.transformers.modeling_bert.___torch_mangle_6692.BertIntermediate = prim::GetAttr[name="intermediate"](%88)
  %631 : __torch__.transformers.modeling_bert.___torch_mangle_6690.BertAttention = prim::GetAttr[name="attention"](%88)
  %632 : __torch__.transformers.modeling_bert.___torch_mangle_6689.BertSelfOutput = prim::GetAttr[name="output"](%631)
  %633 : __torch__.transformers.modeling_bert.___torch_mangle_6685.BertSelfAttention = prim::GetAttr[name="self"](%631)
  %634 : __torch__.torch.nn.modules.linear.___torch_mangle_6683.Linear = prim::GetAttr[name="value"](%633)
  %635 : __torch__.torch.nn.modules.linear.___torch_mangle_6682.Linear = prim::GetAttr[name="key"](%633)
  %636 : __torch__.torch.nn.modules.linear.___torch_mangle_6681.Linear = prim::GetAttr[name="query"](%633)
  %637 : Tensor = prim::GetAttr[name="bias"](%636)
  %638 : Tensor = prim::GetAttr[name="weight"](%636)
  %639 : Float(1024:1, 1024:1024) = aten::t(%638), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.64, %639), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.37, %637, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %642 : Tensor = prim::GetAttr[name="bias"](%635)
  %643 : Tensor = prim::GetAttr[name="weight"](%635)
  %644 : Float(1024:1, 1024:1024) = aten::t(%643), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.64, %644), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.38, %642, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %647 : Tensor = prim::GetAttr[name="bias"](%634)
  %648 : Tensor = prim::GetAttr[name="weight"](%634)
  %649 : Float(1024:1, 1024:1024) = aten::t(%648), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.64, %649), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.39, %647, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %652 : int = aten::size(%x.37, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %653 : int = aten::size(%x.37, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %654 : int[] = prim::ListConstruct(%652, %653, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.38 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.37, %654), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %656 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.38, %656), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %658 : int = aten::size(%x.39, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %659 : int = aten::size(%x.39, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %660 : int[] = prim::ListConstruct(%658, %659, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.40 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.39, %660), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %662 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.40, %662), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %664 : int = aten::size(%x.41, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %665 : int = aten::size(%x.41, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %666 : int[] = prim::ListConstruct(%664, %665, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.42 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.41, %666), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %668 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.42, %668), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %670 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.7, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %670), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.13, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:259:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:262:0
  %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.66, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:275:0
  %677 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %678 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.13, %677), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.14 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%678, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %680 : int = aten::size(%context_layer.14, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %681 : int = aten::size(%context_layer.14, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %682 : int[] = prim::ListConstruct(%680, %681, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %input.67 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.14, %682), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:279:0
  %684 : __torch__.torch.nn.modules.normalization.___torch_mangle_6687.LayerNorm = prim::GetAttr[name="LayerNorm"](%632)
  %685 : __torch__.torch.nn.modules.linear.___torch_mangle_6686.Linear = prim::GetAttr[name="dense"](%632)
  %686 : Tensor = prim::GetAttr[name="bias"](%685)
  %687 : Tensor = prim::GetAttr[name="weight"](%685)
  %688 : Float(1024:1, 1024:1024) = aten::t(%687), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.67, %688), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.68 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.40, %686, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.68, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.13, %input.64, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output # transformers/modeling_bert.py:295:0
  %693 : Tensor = prim::GetAttr[name="bias"](%684)
  %694 : Tensor = prim::GetAttr[name="weight"](%684)
  %695 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.69, %695, %694, %693, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %697 : __torch__.torch.nn.modules.linear.___torch_mangle_6691.Linear = prim::GetAttr[name="dense"](%630)
  %698 : Tensor = prim::GetAttr[name="bias"](%697)
  %699 : Tensor = prim::GetAttr[name="weight"](%697)
  %700 : Float(1024:1, 4096:1024) = aten::t(%699), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.7, %700), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.70 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.41, %698, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.71 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.70), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %704 : __torch__.torch.nn.modules.normalization.___torch_mangle_6694.LayerNorm = prim::GetAttr[name="LayerNorm"](%629)
  %705 : __torch__.torch.nn.modules.linear.___torch_mangle_6693.Linear = prim::GetAttr[name="dense"](%629)
  %706 : Tensor = prim::GetAttr[name="bias"](%705)
  %707 : Tensor = prim::GetAttr[name="weight"](%705)
  %708 : Float(4096:1, 1024:4096) = aten::t(%707), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.71, %708), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.72 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.42, %706, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.72, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.14, %input_tensor.7, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output # transformers/modeling_bert.py:371:0
  %713 : Tensor = prim::GetAttr[name="bias"](%704)
  %714 : Tensor = prim::GetAttr[name="weight"](%704)
  %715 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm
  %input.74 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.73, %715, %714, %713, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %717 : __torch__.transformers.modeling_bert.___torch_mangle_6713.BertOutput = prim::GetAttr[name="output"](%86)
  %718 : __torch__.transformers.modeling_bert.___torch_mangle_6709.BertIntermediate = prim::GetAttr[name="intermediate"](%86)
  %719 : __torch__.transformers.modeling_bert.___torch_mangle_6707.BertAttention = prim::GetAttr[name="attention"](%86)
  %720 : __torch__.transformers.modeling_bert.___torch_mangle_6706.BertSelfOutput = prim::GetAttr[name="output"](%719)
  %721 : __torch__.transformers.modeling_bert.___torch_mangle_6702.BertSelfAttention = prim::GetAttr[name="self"](%719)
  %722 : __torch__.torch.nn.modules.linear.___torch_mangle_6700.Linear = prim::GetAttr[name="value"](%721)
  %723 : __torch__.torch.nn.modules.linear.___torch_mangle_6699.Linear = prim::GetAttr[name="key"](%721)
  %724 : __torch__.torch.nn.modules.linear.___torch_mangle_6698.Linear = prim::GetAttr[name="query"](%721)
  %725 : Tensor = prim::GetAttr[name="bias"](%724)
  %726 : Tensor = prim::GetAttr[name="weight"](%724)
  %727 : Float(1024:1, 1024:1024) = aten::t(%726), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.74, %727), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.43, %725, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %730 : Tensor = prim::GetAttr[name="bias"](%723)
  %731 : Tensor = prim::GetAttr[name="weight"](%723)
  %732 : Float(1024:1, 1024:1024) = aten::t(%731), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.74, %732), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.44, %730, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %735 : Tensor = prim::GetAttr[name="bias"](%722)
  %736 : Tensor = prim::GetAttr[name="weight"](%722)
  %737 : Float(1024:1, 1024:1024) = aten::t(%736), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.74, %737), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.45, %735, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %740 : int = aten::size(%x.43, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %741 : int = aten::size(%x.43, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %742 : int[] = prim::ListConstruct(%740, %741, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.44 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.43, %742), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %744 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.44, %744), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %746 : int = aten::size(%x.45, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %747 : int = aten::size(%x.45, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %748 : int[] = prim::ListConstruct(%746, %747, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.46 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.45, %748), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %750 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.46, %750), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %752 : int = aten::size(%x.47, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %753 : int = aten::size(%x.47, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %754 : int[] = prim::ListConstruct(%752, %753, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.48 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.47, %754), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %756 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.48, %756), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %758 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.8, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %758), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.15, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:259:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:262:0
  %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:275:0
  %765 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %766 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.15, %765), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.16 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%766, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %768 : int = aten::size(%context_layer.16, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %769 : int = aten::size(%context_layer.16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %770 : int[] = prim::ListConstruct(%768, %769, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %input.77 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.16, %770), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:279:0
  %772 : __torch__.torch.nn.modules.normalization.___torch_mangle_6704.LayerNorm = prim::GetAttr[name="LayerNorm"](%720)
  %773 : __torch__.torch.nn.modules.linear.___torch_mangle_6703.Linear = prim::GetAttr[name="dense"](%720)
  %774 : Tensor = prim::GetAttr[name="bias"](%773)
  %775 : Tensor = prim::GetAttr[name="weight"](%773)
  %776 : Float(1024:1, 1024:1024) = aten::t(%775), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.77, %776), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.78 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.46, %774, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.78, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.79 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.15, %input.74, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output # transformers/modeling_bert.py:295:0
  %781 : Tensor = prim::GetAttr[name="bias"](%772)
  %782 : Tensor = prim::GetAttr[name="weight"](%772)
  %783 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.79, %783, %782, %781, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %785 : __torch__.torch.nn.modules.linear.___torch_mangle_6708.Linear = prim::GetAttr[name="dense"](%718)
  %786 : Tensor = prim::GetAttr[name="bias"](%785)
  %787 : Tensor = prim::GetAttr[name="weight"](%785)
  %788 : Float(1024:1, 4096:1024) = aten::t(%787), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.8, %788), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.80 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.47, %786, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.81 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.80), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %792 : __torch__.torch.nn.modules.normalization.___torch_mangle_6711.LayerNorm = prim::GetAttr[name="LayerNorm"](%717)
  %793 : __torch__.torch.nn.modules.linear.___torch_mangle_6710.Linear = prim::GetAttr[name="dense"](%717)
  %794 : Tensor = prim::GetAttr[name="bias"](%793)
  %795 : Tensor = prim::GetAttr[name="weight"](%793)
  %796 : Float(4096:1, 1024:4096) = aten::t(%795), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.81, %796), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.82 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.48, %794, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.82, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.83 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.16, %input_tensor.8, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output # transformers/modeling_bert.py:371:0
  %801 : Tensor = prim::GetAttr[name="bias"](%792)
  %802 : Tensor = prim::GetAttr[name="weight"](%792)
  %803 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm
  %input.84 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.83, %803, %802, %801, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %805 : __torch__.transformers.modeling_bert.___torch_mangle_6730.BertOutput = prim::GetAttr[name="output"](%84)
  %806 : __torch__.transformers.modeling_bert.___torch_mangle_6726.BertIntermediate = prim::GetAttr[name="intermediate"](%84)
  %807 : __torch__.transformers.modeling_bert.___torch_mangle_6724.BertAttention = prim::GetAttr[name="attention"](%84)
  %808 : __torch__.transformers.modeling_bert.___torch_mangle_6723.BertSelfOutput = prim::GetAttr[name="output"](%807)
  %809 : __torch__.transformers.modeling_bert.___torch_mangle_6719.BertSelfAttention = prim::GetAttr[name="self"](%807)
  %810 : __torch__.torch.nn.modules.linear.___torch_mangle_6717.Linear = prim::GetAttr[name="value"](%809)
  %811 : __torch__.torch.nn.modules.linear.___torch_mangle_6716.Linear = prim::GetAttr[name="key"](%809)
  %812 : __torch__.torch.nn.modules.linear.___torch_mangle_6715.Linear = prim::GetAttr[name="query"](%809)
  %813 : Tensor = prim::GetAttr[name="bias"](%812)
  %814 : Tensor = prim::GetAttr[name="weight"](%812)
  %815 : Float(1024:1, 1024:1024) = aten::t(%814), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.84, %815), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.49, %813, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %818 : Tensor = prim::GetAttr[name="bias"](%811)
  %819 : Tensor = prim::GetAttr[name="weight"](%811)
  %820 : Float(1024:1, 1024:1024) = aten::t(%819), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.84, %820), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.50, %818, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %823 : Tensor = prim::GetAttr[name="bias"](%810)
  %824 : Tensor = prim::GetAttr[name="weight"](%810)
  %825 : Float(1024:1, 1024:1024) = aten::t(%824), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.84, %825), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.51, %823, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %828 : int = aten::size(%x.49, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %829 : int = aten::size(%x.49, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %830 : int[] = prim::ListConstruct(%828, %829, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.50 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.49, %830), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %832 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.50, %832), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %834 : int = aten::size(%x.51, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %835 : int = aten::size(%x.51, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %836 : int[] = prim::ListConstruct(%834, %835, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.52 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.51, %836), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %838 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.52, %838), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %840 : int = aten::size(%x.53, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %841 : int = aten::size(%x.53, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %842 : int[] = prim::ListConstruct(%840, %841, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.54 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.53, %842), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %844 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.54, %844), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %846 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.9, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %846), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.17, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:259:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:262:0
  %input.86 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.86, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:275:0
  %853 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %854 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.17, %853), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.18 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%854, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %856 : int = aten::size(%context_layer.18, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %857 : int = aten::size(%context_layer.18, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %858 : int[] = prim::ListConstruct(%856, %857, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %input.87 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.18, %858), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:279:0
  %860 : __torch__.torch.nn.modules.normalization.___torch_mangle_6721.LayerNorm = prim::GetAttr[name="LayerNorm"](%808)
  %861 : __torch__.torch.nn.modules.linear.___torch_mangle_6720.Linear = prim::GetAttr[name="dense"](%808)
  %862 : Tensor = prim::GetAttr[name="bias"](%861)
  %863 : Tensor = prim::GetAttr[name="weight"](%861)
  %864 : Float(1024:1, 1024:1024) = aten::t(%863), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.87, %864), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.88 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.52, %862, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.88, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.89 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.17, %input.84, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output # transformers/modeling_bert.py:295:0
  %869 : Tensor = prim::GetAttr[name="bias"](%860)
  %870 : Tensor = prim::GetAttr[name="weight"](%860)
  %871 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.89, %871, %870, %869, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %873 : __torch__.torch.nn.modules.linear.___torch_mangle_6725.Linear = prim::GetAttr[name="dense"](%806)
  %874 : Tensor = prim::GetAttr[name="bias"](%873)
  %875 : Tensor = prim::GetAttr[name="weight"](%873)
  %876 : Float(1024:1, 4096:1024) = aten::t(%875), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.9, %876), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.90 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.53, %874, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.91 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.90), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %880 : __torch__.torch.nn.modules.normalization.___torch_mangle_6728.LayerNorm = prim::GetAttr[name="LayerNorm"](%805)
  %881 : __torch__.torch.nn.modules.linear.___torch_mangle_6727.Linear = prim::GetAttr[name="dense"](%805)
  %882 : Tensor = prim::GetAttr[name="bias"](%881)
  %883 : Tensor = prim::GetAttr[name="weight"](%881)
  %884 : Float(4096:1, 1024:4096) = aten::t(%883), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.91, %884), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.92 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.54, %882, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.92, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.93 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.18, %input_tensor.9, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output # transformers/modeling_bert.py:371:0
  %889 : Tensor = prim::GetAttr[name="bias"](%880)
  %890 : Tensor = prim::GetAttr[name="weight"](%880)
  %891 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm
  %input.94 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.93, %891, %890, %889, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %893 : __torch__.transformers.modeling_bert.___torch_mangle_6747.BertOutput = prim::GetAttr[name="output"](%82)
  %894 : __torch__.transformers.modeling_bert.___torch_mangle_6743.BertIntermediate = prim::GetAttr[name="intermediate"](%82)
  %895 : __torch__.transformers.modeling_bert.___torch_mangle_6741.BertAttention = prim::GetAttr[name="attention"](%82)
  %896 : __torch__.transformers.modeling_bert.___torch_mangle_6740.BertSelfOutput = prim::GetAttr[name="output"](%895)
  %897 : __torch__.transformers.modeling_bert.___torch_mangle_6736.BertSelfAttention = prim::GetAttr[name="self"](%895)
  %898 : __torch__.torch.nn.modules.linear.___torch_mangle_6734.Linear = prim::GetAttr[name="value"](%897)
  %899 : __torch__.torch.nn.modules.linear.___torch_mangle_6733.Linear = prim::GetAttr[name="key"](%897)
  %900 : __torch__.torch.nn.modules.linear.___torch_mangle_6732.Linear = prim::GetAttr[name="query"](%897)
  %901 : Tensor = prim::GetAttr[name="bias"](%900)
  %902 : Tensor = prim::GetAttr[name="weight"](%900)
  %903 : Float(1024:1, 1024:1024) = aten::t(%902), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.94, %903), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.55, %901, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %906 : Tensor = prim::GetAttr[name="bias"](%899)
  %907 : Tensor = prim::GetAttr[name="weight"](%899)
  %908 : Float(1024:1, 1024:1024) = aten::t(%907), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.94, %908), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.56, %906, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %911 : Tensor = prim::GetAttr[name="bias"](%898)
  %912 : Tensor = prim::GetAttr[name="weight"](%898)
  %913 : Float(1024:1, 1024:1024) = aten::t(%912), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.94, %913), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.57, %911, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %916 : int = aten::size(%x.55, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %917 : int = aten::size(%x.55, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %918 : int[] = prim::ListConstruct(%916, %917, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.56 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.55, %918), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %920 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.56, %920), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %922 : int = aten::size(%x.57, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %923 : int = aten::size(%x.57, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %924 : int[] = prim::ListConstruct(%922, %923, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.58 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.57, %924), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %926 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.58, %926), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %928 : int = aten::size(%x.59, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %929 : int = aten::size(%x.59, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %930 : int[] = prim::ListConstruct(%928, %929, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.60 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.59, %930), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %932 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.60, %932), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %934 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.10, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %934), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.19, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:259:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:262:0
  %input.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.96, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:275:0
  %941 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %942 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.19, %941), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.20 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%942, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %944 : int = aten::size(%context_layer.20, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %945 : int = aten::size(%context_layer.20, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %946 : int[] = prim::ListConstruct(%944, %945, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %input.97 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.20, %946), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:279:0
  %948 : __torch__.torch.nn.modules.normalization.___torch_mangle_6738.LayerNorm = prim::GetAttr[name="LayerNorm"](%896)
  %949 : __torch__.torch.nn.modules.linear.___torch_mangle_6737.Linear = prim::GetAttr[name="dense"](%896)
  %950 : Tensor = prim::GetAttr[name="bias"](%949)
  %951 : Tensor = prim::GetAttr[name="weight"](%949)
  %952 : Float(1024:1, 1024:1024) = aten::t(%951), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.97, %952), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.98 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.58, %950, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.98, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.99 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.19, %input.94, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output # transformers/modeling_bert.py:295:0
  %957 : Tensor = prim::GetAttr[name="bias"](%948)
  %958 : Tensor = prim::GetAttr[name="weight"](%948)
  %959 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.99, %959, %958, %957, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %961 : __torch__.torch.nn.modules.linear.___torch_mangle_6742.Linear = prim::GetAttr[name="dense"](%894)
  %962 : Tensor = prim::GetAttr[name="bias"](%961)
  %963 : Tensor = prim::GetAttr[name="weight"](%961)
  %964 : Float(1024:1, 4096:1024) = aten::t(%963), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.10, %964), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.100 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.59, %962, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.101 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %968 : __torch__.torch.nn.modules.normalization.___torch_mangle_6745.LayerNorm = prim::GetAttr[name="LayerNorm"](%893)
  %969 : __torch__.torch.nn.modules.linear.___torch_mangle_6744.Linear = prim::GetAttr[name="dense"](%893)
  %970 : Tensor = prim::GetAttr[name="bias"](%969)
  %971 : Tensor = prim::GetAttr[name="weight"](%969)
  %972 : Float(4096:1, 1024:4096) = aten::t(%971), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.101, %972), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.102 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.60, %970, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.102, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.103 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.20, %input_tensor.10, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output # transformers/modeling_bert.py:371:0
  %977 : Tensor = prim::GetAttr[name="bias"](%968)
  %978 : Tensor = prim::GetAttr[name="weight"](%968)
  %979 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm
  %input.104 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.103, %979, %978, %977, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %981 : __torch__.transformers.modeling_bert.___torch_mangle_6764.BertOutput = prim::GetAttr[name="output"](%80)
  %982 : __torch__.transformers.modeling_bert.___torch_mangle_6760.BertIntermediate = prim::GetAttr[name="intermediate"](%80)
  %983 : __torch__.transformers.modeling_bert.___torch_mangle_6758.BertAttention = prim::GetAttr[name="attention"](%80)
  %984 : __torch__.transformers.modeling_bert.___torch_mangle_6757.BertSelfOutput = prim::GetAttr[name="output"](%983)
  %985 : __torch__.transformers.modeling_bert.___torch_mangle_6753.BertSelfAttention = prim::GetAttr[name="self"](%983)
  %986 : __torch__.torch.nn.modules.linear.___torch_mangle_6751.Linear = prim::GetAttr[name="value"](%985)
  %987 : __torch__.torch.nn.modules.linear.___torch_mangle_6750.Linear = prim::GetAttr[name="key"](%985)
  %988 : __torch__.torch.nn.modules.linear.___torch_mangle_6749.Linear = prim::GetAttr[name="query"](%985)
  %989 : Tensor = prim::GetAttr[name="bias"](%988)
  %990 : Tensor = prim::GetAttr[name="weight"](%988)
  %991 : Float(1024:1, 1024:1024) = aten::t(%990), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.104, %991), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.61, %989, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %994 : Tensor = prim::GetAttr[name="bias"](%987)
  %995 : Tensor = prim::GetAttr[name="weight"](%987)
  %996 : Float(1024:1, 1024:1024) = aten::t(%995), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.104, %996), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.62, %994, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %999 : Tensor = prim::GetAttr[name="bias"](%986)
  %1000 : Tensor = prim::GetAttr[name="weight"](%986)
  %1001 : Float(1024:1, 1024:1024) = aten::t(%1000), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.104, %1001), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.63, %999, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %1004 : int = aten::size(%x.61, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1005 : int = aten::size(%x.61, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1006 : int[] = prim::ListConstruct(%1004, %1005, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.62 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.61, %1006), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1008 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.62, %1008), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1010 : int = aten::size(%x.63, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1011 : int = aten::size(%x.63, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1012 : int[] = prim::ListConstruct(%1010, %1011, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.64 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.63, %1012), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1014 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.64, %1014), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1016 : int = aten::size(%x.65, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1017 : int = aten::size(%x.65, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1018 : int[] = prim::ListConstruct(%1016, %1017, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.66 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.65, %1018), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1020 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.66, %1020), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1022 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.11, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1022), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.21, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:259:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:262:0
  %input.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.106, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:275:0
  %1029 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %1030 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.21, %1029), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.22 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1030, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %1032 : int = aten::size(%context_layer.22, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1033 : int = aten::size(%context_layer.22, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1034 : int[] = prim::ListConstruct(%1032, %1033, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %input.107 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.22, %1034), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:279:0
  %1036 : __torch__.torch.nn.modules.normalization.___torch_mangle_6755.LayerNorm = prim::GetAttr[name="LayerNorm"](%984)
  %1037 : __torch__.torch.nn.modules.linear.___torch_mangle_6754.Linear = prim::GetAttr[name="dense"](%984)
  %1038 : Tensor = prim::GetAttr[name="bias"](%1037)
  %1039 : Tensor = prim::GetAttr[name="weight"](%1037)
  %1040 : Float(1024:1, 1024:1024) = aten::t(%1039), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.107, %1040), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.108 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.64, %1038, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.108, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.109 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.21, %input.104, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output # transformers/modeling_bert.py:295:0
  %1045 : Tensor = prim::GetAttr[name="bias"](%1036)
  %1046 : Tensor = prim::GetAttr[name="weight"](%1036)
  %1047 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.109, %1047, %1046, %1045, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1049 : __torch__.torch.nn.modules.linear.___torch_mangle_6759.Linear = prim::GetAttr[name="dense"](%982)
  %1050 : Tensor = prim::GetAttr[name="bias"](%1049)
  %1051 : Tensor = prim::GetAttr[name="weight"](%1049)
  %1052 : Float(1024:1, 4096:1024) = aten::t(%1051), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.11, %1052), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.110 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.65, %1050, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.111 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.110), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1056 : __torch__.torch.nn.modules.normalization.___torch_mangle_6762.LayerNorm = prim::GetAttr[name="LayerNorm"](%981)
  %1057 : __torch__.torch.nn.modules.linear.___torch_mangle_6761.Linear = prim::GetAttr[name="dense"](%981)
  %1058 : Tensor = prim::GetAttr[name="bias"](%1057)
  %1059 : Tensor = prim::GetAttr[name="weight"](%1057)
  %1060 : Float(4096:1, 1024:4096) = aten::t(%1059), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.111, %1060), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.112 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.66, %1058, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.112, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.113 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.22, %input_tensor.11, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output # transformers/modeling_bert.py:371:0
  %1065 : Tensor = prim::GetAttr[name="bias"](%1056)
  %1066 : Tensor = prim::GetAttr[name="weight"](%1056)
  %1067 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm
  %input.114 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.113, %1067, %1066, %1065, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1069 : __torch__.transformers.modeling_bert.___torch_mangle_6781.BertOutput = prim::GetAttr[name="output"](%78)
  %1070 : __torch__.transformers.modeling_bert.___torch_mangle_6777.BertIntermediate = prim::GetAttr[name="intermediate"](%78)
  %1071 : __torch__.transformers.modeling_bert.___torch_mangle_6775.BertAttention = prim::GetAttr[name="attention"](%78)
  %1072 : __torch__.transformers.modeling_bert.___torch_mangle_6774.BertSelfOutput = prim::GetAttr[name="output"](%1071)
  %1073 : __torch__.transformers.modeling_bert.___torch_mangle_6770.BertSelfAttention = prim::GetAttr[name="self"](%1071)
  %1074 : __torch__.torch.nn.modules.linear.___torch_mangle_6768.Linear = prim::GetAttr[name="value"](%1073)
  %1075 : __torch__.torch.nn.modules.linear.___torch_mangle_6767.Linear = prim::GetAttr[name="key"](%1073)
  %1076 : __torch__.torch.nn.modules.linear.___torch_mangle_6766.Linear = prim::GetAttr[name="query"](%1073)
  %1077 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1078 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1079 : Float(1024:1, 1024:1024) = aten::t(%1078), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.114, %1079), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.67, %1077, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1082 : Tensor = prim::GetAttr[name="bias"](%1075)
  %1083 : Tensor = prim::GetAttr[name="weight"](%1075)
  %1084 : Float(1024:1, 1024:1024) = aten::t(%1083), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.114, %1084), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.68, %1082, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1087 : Tensor = prim::GetAttr[name="bias"](%1074)
  %1088 : Tensor = prim::GetAttr[name="weight"](%1074)
  %1089 : Float(1024:1, 1024:1024) = aten::t(%1088), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.114, %1089), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.69, %1087, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1092 : int = aten::size(%x.67, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1093 : int = aten::size(%x.67, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1094 : int[] = prim::ListConstruct(%1092, %1093, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.68 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.67, %1094), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1096 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %query_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.68, %1096), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1098 : int = aten::size(%x.69, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1099 : int = aten::size(%x.69, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1100 : int[] = prim::ListConstruct(%1098, %1099, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.70 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.69, %1100), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1102 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %key_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.70, %1102), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1104 : int = aten::size(%x.71, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1105 : int = aten::size(%x.71, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1106 : int[] = prim::ListConstruct(%1104, %1105, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.72 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.71, %1106), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1108 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %value_layer.12 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.72, %1108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1110 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.12, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.12, %1110), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.24 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.23, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:259:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.24, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:262:0
  %input.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.116, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.12, %value_layer.12), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:275:0
  %1117 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %1118 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.23, %1117), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.24 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1118, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %1120 : int = aten::size(%context_layer.24, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1121 : int = aten::size(%context_layer.24, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1122 : int[] = prim::ListConstruct(%1120, %1121, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %input.117 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.24, %1122), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:279:0
  %1124 : __torch__.torch.nn.modules.normalization.___torch_mangle_6772.LayerNorm = prim::GetAttr[name="LayerNorm"](%1072)
  %1125 : __torch__.torch.nn.modules.linear.___torch_mangle_6771.Linear = prim::GetAttr[name="dense"](%1072)
  %1126 : Tensor = prim::GetAttr[name="bias"](%1125)
  %1127 : Tensor = prim::GetAttr[name="weight"](%1125)
  %1128 : Float(1024:1, 1024:1024) = aten::t(%1127), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.117, %1128), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.118 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.70, %1126, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.118, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.119 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.23, %input.114, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output # transformers/modeling_bert.py:295:0
  %1133 : Tensor = prim::GetAttr[name="bias"](%1124)
  %1134 : Tensor = prim::GetAttr[name="weight"](%1124)
  %1135 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm
  %input_tensor.12 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.119, %1135, %1134, %1133, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1137 : __torch__.torch.nn.modules.linear.___torch_mangle_6776.Linear = prim::GetAttr[name="dense"](%1070)
  %1138 : Tensor = prim::GetAttr[name="bias"](%1137)
  %1139 : Tensor = prim::GetAttr[name="weight"](%1137)
  %1140 : Float(1024:1, 4096:1024) = aten::t(%1139), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.12, %1140), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.120 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.71, %1138, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.121 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.120), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1144 : __torch__.torch.nn.modules.normalization.___torch_mangle_6779.LayerNorm = prim::GetAttr[name="LayerNorm"](%1069)
  %1145 : __torch__.torch.nn.modules.linear.___torch_mangle_6778.Linear = prim::GetAttr[name="dense"](%1069)
  %1146 : Tensor = prim::GetAttr[name="bias"](%1145)
  %1147 : Tensor = prim::GetAttr[name="weight"](%1145)
  %1148 : Float(4096:1, 1024:4096) = aten::t(%1147), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.121, %1148), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.122 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.72, %1146, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.24 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.122, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.123 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.24, %input_tensor.12, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output # transformers/modeling_bert.py:371:0
  %1153 : Tensor = prim::GetAttr[name="bias"](%1144)
  %1154 : Tensor = prim::GetAttr[name="weight"](%1144)
  %1155 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm
  %input.124 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.123, %1155, %1154, %1153, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1157 : __torch__.transformers.modeling_bert.___torch_mangle_6798.BertOutput = prim::GetAttr[name="output"](%76)
  %1158 : __torch__.transformers.modeling_bert.___torch_mangle_6794.BertIntermediate = prim::GetAttr[name="intermediate"](%76)
  %1159 : __torch__.transformers.modeling_bert.___torch_mangle_6792.BertAttention = prim::GetAttr[name="attention"](%76)
  %1160 : __torch__.transformers.modeling_bert.___torch_mangle_6791.BertSelfOutput = prim::GetAttr[name="output"](%1159)
  %1161 : __torch__.transformers.modeling_bert.___torch_mangle_6787.BertSelfAttention = prim::GetAttr[name="self"](%1159)
  %1162 : __torch__.torch.nn.modules.linear.___torch_mangle_6785.Linear = prim::GetAttr[name="value"](%1161)
  %1163 : __torch__.torch.nn.modules.linear.___torch_mangle_6784.Linear = prim::GetAttr[name="key"](%1161)
  %1164 : __torch__.torch.nn.modules.linear.___torch_mangle_6783.Linear = prim::GetAttr[name="query"](%1161)
  %1165 : Tensor = prim::GetAttr[name="bias"](%1164)
  %1166 : Tensor = prim::GetAttr[name="weight"](%1164)
  %1167 : Float(1024:1, 1024:1024) = aten::t(%1166), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.query # torch/nn/functional.py:1676:0
  %output.73 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.124, %1167), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.query # torch/nn/functional.py:1676:0
  %x.73 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.73, %1165, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.query # torch/nn/functional.py:1678:0
  %1170 : Tensor = prim::GetAttr[name="bias"](%1163)
  %1171 : Tensor = prim::GetAttr[name="weight"](%1163)
  %1172 : Float(1024:1, 1024:1024) = aten::t(%1171), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.key # torch/nn/functional.py:1676:0
  %output.74 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.124, %1172), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.key # torch/nn/functional.py:1676:0
  %x.75 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.74, %1170, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.key # torch/nn/functional.py:1678:0
  %1175 : Tensor = prim::GetAttr[name="bias"](%1162)
  %1176 : Tensor = prim::GetAttr[name="weight"](%1162)
  %1177 : Float(1024:1, 1024:1024) = aten::t(%1176), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.value # torch/nn/functional.py:1676:0
  %output.75 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.124, %1177), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.value # torch/nn/functional.py:1676:0
  %x.77 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.75, %1175, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.value # torch/nn/functional.py:1678:0
  %1180 : int = aten::size(%x.73, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1181 : int = aten::size(%x.73, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1182 : int[] = prim::ListConstruct(%1180, %1181, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
  %x.74 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.73, %1182), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
  %1184 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
  %query_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.74, %1184), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
  %1186 : int = aten::size(%x.75, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1187 : int = aten::size(%x.75, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1188 : int[] = prim::ListConstruct(%1186, %1187, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
  %x.76 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.75, %1188), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
  %1190 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
  %key_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.76, %1190), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
  %1192 : int = aten::size(%x.77, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1193 : int = aten::size(%x.77, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:227:0
  %1194 : int[] = prim::ListConstruct(%1192, %1193, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
  %x.78 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.77, %1194), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:228:0
  %1196 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
  %value_layer.13 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.78, %1196), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:229:0
  %1198 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.13, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.13, %1198), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.25, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:259:0
  %input.125 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.26, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:262:0
  %input.126 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.125, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.126, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self/__module.bert.encoder.layer.12.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.25 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.13, %value_layer.13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:275:0
  %1205 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
  %1206 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.25, %1205), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.26 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1206, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:277:0
  %1208 : int = aten::size(%context_layer.26, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:278:0
  %1209 : int = aten::size(%context_layer.26, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:278:0
  %1210 : int[] = prim::ListConstruct(%1208, %1209, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self
  %input.127 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.26, %1210), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.self # transformers/modeling_bert.py:279:0
  %1212 : __torch__.torch.nn.modules.normalization.___torch_mangle_6789.LayerNorm = prim::GetAttr[name="LayerNorm"](%1160)
  %1213 : __torch__.torch.nn.modules.linear.___torch_mangle_6788.Linear = prim::GetAttr[name="dense"](%1160)
  %1214 : Tensor = prim::GetAttr[name="bias"](%1213)
  %1215 : Tensor = prim::GetAttr[name="weight"](%1213)
  %1216 : Float(1024:1, 1024:1024) = aten::t(%1215), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1676:0
  %output.76 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.127, %1216), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1676:0
  %input.128 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.76, %1214, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.25 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.128, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.dropout # torch/nn/functional.py:973:0
  %input.129 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.25, %input.124, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output # transformers/modeling_bert.py:295:0
  %1221 : Tensor = prim::GetAttr[name="bias"](%1212)
  %1222 : Tensor = prim::GetAttr[name="weight"](%1212)
  %1223 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.LayerNorm
  %input_tensor.13 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.129, %1223, %1222, %1221, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.attention/__module.bert.encoder.layer.12.attention.output/__module.bert.encoder.layer.12.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1225 : __torch__.torch.nn.modules.linear.___torch_mangle_6793.Linear = prim::GetAttr[name="dense"](%1158)
  %1226 : Tensor = prim::GetAttr[name="bias"](%1225)
  %1227 : Tensor = prim::GetAttr[name="weight"](%1225)
  %1228 : Float(1024:1, 4096:1024) = aten::t(%1227), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate/__module.bert.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1676:0
  %output.77 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.13, %1228), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate/__module.bert.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1676:0
  %input.130 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.77, %1226, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate/__module.bert.encoder.layer.12.intermediate.dense # torch/nn/functional.py:1678:0
  %input.131 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.130), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.intermediate # torch/nn/functional.py:1369:0
  %1232 : __torch__.torch.nn.modules.normalization.___torch_mangle_6796.LayerNorm = prim::GetAttr[name="LayerNorm"](%1157)
  %1233 : __torch__.torch.nn.modules.linear.___torch_mangle_6795.Linear = prim::GetAttr[name="dense"](%1157)
  %1234 : Tensor = prim::GetAttr[name="bias"](%1233)
  %1235 : Tensor = prim::GetAttr[name="weight"](%1233)
  %1236 : Float(4096:1, 1024:4096) = aten::t(%1235), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dense # torch/nn/functional.py:1676:0
  %output.78 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.131, %1236), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dense # torch/nn/functional.py:1676:0
  %input.132 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.78, %1234, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.26 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.132, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.dropout # torch/nn/functional.py:973:0
  %input.133 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.26, %input_tensor.13, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output # transformers/modeling_bert.py:371:0
  %1241 : Tensor = prim::GetAttr[name="bias"](%1232)
  %1242 : Tensor = prim::GetAttr[name="weight"](%1232)
  %1243 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.LayerNorm
  %input.134 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.133, %1243, %1242, %1241, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.12/__module.bert.encoder.layer.12.output/__module.bert.encoder.layer.12.output.LayerNorm # torch/nn/functional.py:2048:0
  %1245 : __torch__.transformers.modeling_bert.___torch_mangle_6815.BertOutput = prim::GetAttr[name="output"](%74)
  %1246 : __torch__.transformers.modeling_bert.___torch_mangle_6811.BertIntermediate = prim::GetAttr[name="intermediate"](%74)
  %1247 : __torch__.transformers.modeling_bert.___torch_mangle_6809.BertAttention = prim::GetAttr[name="attention"](%74)
  %1248 : __torch__.transformers.modeling_bert.___torch_mangle_6808.BertSelfOutput = prim::GetAttr[name="output"](%1247)
  %1249 : __torch__.transformers.modeling_bert.___torch_mangle_6804.BertSelfAttention = prim::GetAttr[name="self"](%1247)
  %1250 : __torch__.torch.nn.modules.linear.___torch_mangle_6802.Linear = prim::GetAttr[name="value"](%1249)
  %1251 : __torch__.torch.nn.modules.linear.___torch_mangle_6801.Linear = prim::GetAttr[name="key"](%1249)
  %1252 : __torch__.torch.nn.modules.linear.___torch_mangle_6800.Linear = prim::GetAttr[name="query"](%1249)
  %1253 : Tensor = prim::GetAttr[name="bias"](%1252)
  %1254 : Tensor = prim::GetAttr[name="weight"](%1252)
  %1255 : Float(1024:1, 1024:1024) = aten::t(%1254), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.query # torch/nn/functional.py:1676:0
  %output.79 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.134, %1255), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.query # torch/nn/functional.py:1676:0
  %x.79 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.79, %1253, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.query # torch/nn/functional.py:1678:0
  %1258 : Tensor = prim::GetAttr[name="bias"](%1251)
  %1259 : Tensor = prim::GetAttr[name="weight"](%1251)
  %1260 : Float(1024:1, 1024:1024) = aten::t(%1259), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.key # torch/nn/functional.py:1676:0
  %output.80 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.134, %1260), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.key # torch/nn/functional.py:1676:0
  %x.81 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.80, %1258, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.key # torch/nn/functional.py:1678:0
  %1263 : Tensor = prim::GetAttr[name="bias"](%1250)
  %1264 : Tensor = prim::GetAttr[name="weight"](%1250)
  %1265 : Float(1024:1, 1024:1024) = aten::t(%1264), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.value # torch/nn/functional.py:1676:0
  %output.81 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.134, %1265), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.value # torch/nn/functional.py:1676:0
  %x.83 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.81, %1263, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.value # torch/nn/functional.py:1678:0
  %1268 : int = aten::size(%x.79, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1269 : int = aten::size(%x.79, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1270 : int[] = prim::ListConstruct(%1268, %1269, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
  %x.80 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.79, %1270), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
  %1272 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
  %query_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.80, %1272), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
  %1274 : int = aten::size(%x.81, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1275 : int = aten::size(%x.81, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1276 : int[] = prim::ListConstruct(%1274, %1275, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
  %x.82 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.81, %1276), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
  %1278 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
  %key_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.82, %1278), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
  %1280 : int = aten::size(%x.83, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1281 : int = aten::size(%x.83, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:227:0
  %1282 : int[] = prim::ListConstruct(%1280, %1281, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
  %x.84 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.83, %1282), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:228:0
  %1284 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
  %value_layer.14 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.84, %1284), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:229:0
  %1286 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.14, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.27 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.14, %1286), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.28 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.27, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:259:0
  %input.135 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.28, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:262:0
  %input.136 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.135, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.136, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self/__module.bert.encoder.layer.13.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.27 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.14, %value_layer.14), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:275:0
  %1293 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
  %1294 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.27, %1293), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.28 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1294, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:277:0
  %1296 : int = aten::size(%context_layer.28, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:278:0
  %1297 : int = aten::size(%context_layer.28, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:278:0
  %1298 : int[] = prim::ListConstruct(%1296, %1297, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self
  %input.137 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.28, %1298), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.self # transformers/modeling_bert.py:279:0
  %1300 : __torch__.torch.nn.modules.normalization.___torch_mangle_6806.LayerNorm = prim::GetAttr[name="LayerNorm"](%1248)
  %1301 : __torch__.torch.nn.modules.linear.___torch_mangle_6805.Linear = prim::GetAttr[name="dense"](%1248)
  %1302 : Tensor = prim::GetAttr[name="bias"](%1301)
  %1303 : Tensor = prim::GetAttr[name="weight"](%1301)
  %1304 : Float(1024:1, 1024:1024) = aten::t(%1303), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1676:0
  %output.82 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.137, %1304), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1676:0
  %input.138 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.82, %1302, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.27 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.138, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.dropout # torch/nn/functional.py:973:0
  %input.139 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.27, %input.134, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output # transformers/modeling_bert.py:295:0
  %1309 : Tensor = prim::GetAttr[name="bias"](%1300)
  %1310 : Tensor = prim::GetAttr[name="weight"](%1300)
  %1311 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.LayerNorm
  %input_tensor.14 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.139, %1311, %1310, %1309, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.attention/__module.bert.encoder.layer.13.attention.output/__module.bert.encoder.layer.13.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1313 : __torch__.torch.nn.modules.linear.___torch_mangle_6810.Linear = prim::GetAttr[name="dense"](%1246)
  %1314 : Tensor = prim::GetAttr[name="bias"](%1313)
  %1315 : Tensor = prim::GetAttr[name="weight"](%1313)
  %1316 : Float(1024:1, 4096:1024) = aten::t(%1315), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate/__module.bert.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1676:0
  %output.83 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.14, %1316), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate/__module.bert.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1676:0
  %input.140 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.83, %1314, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate/__module.bert.encoder.layer.13.intermediate.dense # torch/nn/functional.py:1678:0
  %input.141 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.140), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.intermediate # torch/nn/functional.py:1369:0
  %1320 : __torch__.torch.nn.modules.normalization.___torch_mangle_6813.LayerNorm = prim::GetAttr[name="LayerNorm"](%1245)
  %1321 : __torch__.torch.nn.modules.linear.___torch_mangle_6812.Linear = prim::GetAttr[name="dense"](%1245)
  %1322 : Tensor = prim::GetAttr[name="bias"](%1321)
  %1323 : Tensor = prim::GetAttr[name="weight"](%1321)
  %1324 : Float(4096:1, 1024:4096) = aten::t(%1323), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dense # torch/nn/functional.py:1676:0
  %output.84 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.141, %1324), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dense # torch/nn/functional.py:1676:0
  %input.142 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.84, %1322, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.28 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.142, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.dropout # torch/nn/functional.py:973:0
  %input.143 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.28, %input_tensor.14, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output # transformers/modeling_bert.py:371:0
  %1329 : Tensor = prim::GetAttr[name="bias"](%1320)
  %1330 : Tensor = prim::GetAttr[name="weight"](%1320)
  %1331 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.LayerNorm
  %input.144 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.143, %1331, %1330, %1329, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.13/__module.bert.encoder.layer.13.output/__module.bert.encoder.layer.13.output.LayerNorm # torch/nn/functional.py:2048:0
  %1333 : __torch__.transformers.modeling_bert.___torch_mangle_6832.BertOutput = prim::GetAttr[name="output"](%72)
  %1334 : __torch__.transformers.modeling_bert.___torch_mangle_6828.BertIntermediate = prim::GetAttr[name="intermediate"](%72)
  %1335 : __torch__.transformers.modeling_bert.___torch_mangle_6826.BertAttention = prim::GetAttr[name="attention"](%72)
  %1336 : __torch__.transformers.modeling_bert.___torch_mangle_6825.BertSelfOutput = prim::GetAttr[name="output"](%1335)
  %1337 : __torch__.transformers.modeling_bert.___torch_mangle_6821.BertSelfAttention = prim::GetAttr[name="self"](%1335)
  %1338 : __torch__.torch.nn.modules.linear.___torch_mangle_6819.Linear = prim::GetAttr[name="value"](%1337)
  %1339 : __torch__.torch.nn.modules.linear.___torch_mangle_6818.Linear = prim::GetAttr[name="key"](%1337)
  %1340 : __torch__.torch.nn.modules.linear.___torch_mangle_6817.Linear = prim::GetAttr[name="query"](%1337)
  %1341 : Tensor = prim::GetAttr[name="bias"](%1340)
  %1342 : Tensor = prim::GetAttr[name="weight"](%1340)
  %1343 : Float(1024:1, 1024:1024) = aten::t(%1342), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.query # torch/nn/functional.py:1676:0
  %output.85 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.144, %1343), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.query # torch/nn/functional.py:1676:0
  %x.85 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.85, %1341, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.query # torch/nn/functional.py:1678:0
  %1346 : Tensor = prim::GetAttr[name="bias"](%1339)
  %1347 : Tensor = prim::GetAttr[name="weight"](%1339)
  %1348 : Float(1024:1, 1024:1024) = aten::t(%1347), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.key # torch/nn/functional.py:1676:0
  %output.86 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.144, %1348), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.key # torch/nn/functional.py:1676:0
  %x.87 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.86, %1346, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.key # torch/nn/functional.py:1678:0
  %1351 : Tensor = prim::GetAttr[name="bias"](%1338)
  %1352 : Tensor = prim::GetAttr[name="weight"](%1338)
  %1353 : Float(1024:1, 1024:1024) = aten::t(%1352), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.value # torch/nn/functional.py:1676:0
  %output.87 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.144, %1353), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.value # torch/nn/functional.py:1676:0
  %x.89 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.87, %1351, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.value # torch/nn/functional.py:1678:0
  %1356 : int = aten::size(%x.85, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1357 : int = aten::size(%x.85, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1358 : int[] = prim::ListConstruct(%1356, %1357, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
  %x.86 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.85, %1358), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
  %1360 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
  %query_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.86, %1360), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
  %1362 : int = aten::size(%x.87, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1363 : int = aten::size(%x.87, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1364 : int[] = prim::ListConstruct(%1362, %1363, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
  %x.88 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.87, %1364), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
  %1366 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
  %key_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.88, %1366), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
  %1368 : int = aten::size(%x.89, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1369 : int = aten::size(%x.89, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:227:0
  %1370 : int[] = prim::ListConstruct(%1368, %1369, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
  %x.90 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.89, %1370), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:228:0
  %1372 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
  %value_layer.15 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.90, %1372), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:229:0
  %1374 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.15, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.29 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.15, %1374), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.29, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:259:0
  %input.145 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.30, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:262:0
  %input.146 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.145, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.146, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self/__module.bert.encoder.layer.14.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.29 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.15, %value_layer.15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:275:0
  %1381 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
  %1382 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.29, %1381), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.30 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1382, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:277:0
  %1384 : int = aten::size(%context_layer.30, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:278:0
  %1385 : int = aten::size(%context_layer.30, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:278:0
  %1386 : int[] = prim::ListConstruct(%1384, %1385, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self
  %input.147 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.30, %1386), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.self # transformers/modeling_bert.py:279:0
  %1388 : __torch__.torch.nn.modules.normalization.___torch_mangle_6823.LayerNorm = prim::GetAttr[name="LayerNorm"](%1336)
  %1389 : __torch__.torch.nn.modules.linear.___torch_mangle_6822.Linear = prim::GetAttr[name="dense"](%1336)
  %1390 : Tensor = prim::GetAttr[name="bias"](%1389)
  %1391 : Tensor = prim::GetAttr[name="weight"](%1389)
  %1392 : Float(1024:1, 1024:1024) = aten::t(%1391), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1676:0
  %output.88 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.147, %1392), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1676:0
  %input.148 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.88, %1390, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.29 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.148, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.dropout # torch/nn/functional.py:973:0
  %input.149 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.29, %input.144, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output # transformers/modeling_bert.py:295:0
  %1397 : Tensor = prim::GetAttr[name="bias"](%1388)
  %1398 : Tensor = prim::GetAttr[name="weight"](%1388)
  %1399 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.LayerNorm
  %input_tensor.15 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.149, %1399, %1398, %1397, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.attention/__module.bert.encoder.layer.14.attention.output/__module.bert.encoder.layer.14.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1401 : __torch__.torch.nn.modules.linear.___torch_mangle_6827.Linear = prim::GetAttr[name="dense"](%1334)
  %1402 : Tensor = prim::GetAttr[name="bias"](%1401)
  %1403 : Tensor = prim::GetAttr[name="weight"](%1401)
  %1404 : Float(1024:1, 4096:1024) = aten::t(%1403), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate/__module.bert.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1676:0
  %output.89 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.15, %1404), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate/__module.bert.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1676:0
  %input.150 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.89, %1402, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate/__module.bert.encoder.layer.14.intermediate.dense # torch/nn/functional.py:1678:0
  %input.151 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.150), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.intermediate # torch/nn/functional.py:1369:0
  %1408 : __torch__.torch.nn.modules.normalization.___torch_mangle_6830.LayerNorm = prim::GetAttr[name="LayerNorm"](%1333)
  %1409 : __torch__.torch.nn.modules.linear.___torch_mangle_6829.Linear = prim::GetAttr[name="dense"](%1333)
  %1410 : Tensor = prim::GetAttr[name="bias"](%1409)
  %1411 : Tensor = prim::GetAttr[name="weight"](%1409)
  %1412 : Float(4096:1, 1024:4096) = aten::t(%1411), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dense # torch/nn/functional.py:1676:0
  %output.90 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.151, %1412), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dense # torch/nn/functional.py:1676:0
  %input.152 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.90, %1410, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.30 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.152, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.dropout # torch/nn/functional.py:973:0
  %input.153 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.30, %input_tensor.15, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output # transformers/modeling_bert.py:371:0
  %1417 : Tensor = prim::GetAttr[name="bias"](%1408)
  %1418 : Tensor = prim::GetAttr[name="weight"](%1408)
  %1419 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.LayerNorm
  %input.154 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.153, %1419, %1418, %1417, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.14/__module.bert.encoder.layer.14.output/__module.bert.encoder.layer.14.output.LayerNorm # torch/nn/functional.py:2048:0
  %1421 : __torch__.transformers.modeling_bert.___torch_mangle_6849.BertOutput = prim::GetAttr[name="output"](%70)
  %1422 : __torch__.transformers.modeling_bert.___torch_mangle_6845.BertIntermediate = prim::GetAttr[name="intermediate"](%70)
  %1423 : __torch__.transformers.modeling_bert.___torch_mangle_6843.BertAttention = prim::GetAttr[name="attention"](%70)
  %1424 : __torch__.transformers.modeling_bert.___torch_mangle_6842.BertSelfOutput = prim::GetAttr[name="output"](%1423)
  %1425 : __torch__.transformers.modeling_bert.___torch_mangle_6838.BertSelfAttention = prim::GetAttr[name="self"](%1423)
  %1426 : __torch__.torch.nn.modules.linear.___torch_mangle_6836.Linear = prim::GetAttr[name="value"](%1425)
  %1427 : __torch__.torch.nn.modules.linear.___torch_mangle_6835.Linear = prim::GetAttr[name="key"](%1425)
  %1428 : __torch__.torch.nn.modules.linear.___torch_mangle_6834.Linear = prim::GetAttr[name="query"](%1425)
  %1429 : Tensor = prim::GetAttr[name="bias"](%1428)
  %1430 : Tensor = prim::GetAttr[name="weight"](%1428)
  %1431 : Float(1024:1, 1024:1024) = aten::t(%1430), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.query # torch/nn/functional.py:1676:0
  %output.91 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.154, %1431), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.query # torch/nn/functional.py:1676:0
  %x.91 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.91, %1429, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.query # torch/nn/functional.py:1678:0
  %1434 : Tensor = prim::GetAttr[name="bias"](%1427)
  %1435 : Tensor = prim::GetAttr[name="weight"](%1427)
  %1436 : Float(1024:1, 1024:1024) = aten::t(%1435), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.key # torch/nn/functional.py:1676:0
  %output.92 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.154, %1436), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.key # torch/nn/functional.py:1676:0
  %x.93 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.92, %1434, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.key # torch/nn/functional.py:1678:0
  %1439 : Tensor = prim::GetAttr[name="bias"](%1426)
  %1440 : Tensor = prim::GetAttr[name="weight"](%1426)
  %1441 : Float(1024:1, 1024:1024) = aten::t(%1440), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.value # torch/nn/functional.py:1676:0
  %output.93 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.154, %1441), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.value # torch/nn/functional.py:1676:0
  %x.95 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.93, %1439, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.value # torch/nn/functional.py:1678:0
  %1444 : int = aten::size(%x.91, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1445 : int = aten::size(%x.91, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1446 : int[] = prim::ListConstruct(%1444, %1445, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
  %x.92 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.91, %1446), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
  %1448 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
  %query_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.92, %1448), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
  %1450 : int = aten::size(%x.93, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1451 : int = aten::size(%x.93, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1452 : int[] = prim::ListConstruct(%1450, %1451, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
  %x.94 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.93, %1452), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
  %1454 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
  %key_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.94, %1454), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
  %1456 : int = aten::size(%x.95, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1457 : int = aten::size(%x.95, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:227:0
  %1458 : int[] = prim::ListConstruct(%1456, %1457, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
  %x.96 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.95, %1458), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:228:0
  %1460 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
  %value_layer.16 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.96, %1460), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:229:0
  %1462 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.16, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.16, %1462), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.32 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.31, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:259:0
  %input.155 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.32, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:262:0
  %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.155, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.156, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self/__module.bert.encoder.layer.15.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.31 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.16, %value_layer.16), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:275:0
  %1469 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
  %1470 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.31, %1469), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.32 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1470, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:277:0
  %1472 : int = aten::size(%context_layer.32, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:278:0
  %1473 : int = aten::size(%context_layer.32, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:278:0
  %1474 : int[] = prim::ListConstruct(%1472, %1473, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self
  %input.157 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.32, %1474), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.self # transformers/modeling_bert.py:279:0
  %1476 : __torch__.torch.nn.modules.normalization.___torch_mangle_6840.LayerNorm = prim::GetAttr[name="LayerNorm"](%1424)
  %1477 : __torch__.torch.nn.modules.linear.___torch_mangle_6839.Linear = prim::GetAttr[name="dense"](%1424)
  %1478 : Tensor = prim::GetAttr[name="bias"](%1477)
  %1479 : Tensor = prim::GetAttr[name="weight"](%1477)
  %1480 : Float(1024:1, 1024:1024) = aten::t(%1479), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1676:0
  %output.94 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.157, %1480), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1676:0
  %input.158 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.94, %1478, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.31 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.158, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.dropout # torch/nn/functional.py:973:0
  %input.159 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.31, %input.154, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output # transformers/modeling_bert.py:295:0
  %1485 : Tensor = prim::GetAttr[name="bias"](%1476)
  %1486 : Tensor = prim::GetAttr[name="weight"](%1476)
  %1487 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.LayerNorm
  %input_tensor.16 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.159, %1487, %1486, %1485, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.attention/__module.bert.encoder.layer.15.attention.output/__module.bert.encoder.layer.15.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1489 : __torch__.torch.nn.modules.linear.___torch_mangle_6844.Linear = prim::GetAttr[name="dense"](%1422)
  %1490 : Tensor = prim::GetAttr[name="bias"](%1489)
  %1491 : Tensor = prim::GetAttr[name="weight"](%1489)
  %1492 : Float(1024:1, 4096:1024) = aten::t(%1491), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate/__module.bert.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1676:0
  %output.95 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.16, %1492), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate/__module.bert.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1676:0
  %input.160 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.95, %1490, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate/__module.bert.encoder.layer.15.intermediate.dense # torch/nn/functional.py:1678:0
  %input.161 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.160), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.intermediate # torch/nn/functional.py:1369:0
  %1496 : __torch__.torch.nn.modules.normalization.___torch_mangle_6847.LayerNorm = prim::GetAttr[name="LayerNorm"](%1421)
  %1497 : __torch__.torch.nn.modules.linear.___torch_mangle_6846.Linear = prim::GetAttr[name="dense"](%1421)
  %1498 : Tensor = prim::GetAttr[name="bias"](%1497)
  %1499 : Tensor = prim::GetAttr[name="weight"](%1497)
  %1500 : Float(4096:1, 1024:4096) = aten::t(%1499), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dense # torch/nn/functional.py:1676:0
  %output.96 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.161, %1500), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dense # torch/nn/functional.py:1676:0
  %input.162 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.96, %1498, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.32 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.162, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.dropout # torch/nn/functional.py:973:0
  %input.163 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.32, %input_tensor.16, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output # transformers/modeling_bert.py:371:0
  %1505 : Tensor = prim::GetAttr[name="bias"](%1496)
  %1506 : Tensor = prim::GetAttr[name="weight"](%1496)
  %1507 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.LayerNorm
  %input.164 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.163, %1507, %1506, %1505, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.15/__module.bert.encoder.layer.15.output/__module.bert.encoder.layer.15.output.LayerNorm # torch/nn/functional.py:2048:0
  %1509 : __torch__.transformers.modeling_bert.___torch_mangle_6866.BertOutput = prim::GetAttr[name="output"](%68)
  %1510 : __torch__.transformers.modeling_bert.___torch_mangle_6862.BertIntermediate = prim::GetAttr[name="intermediate"](%68)
  %1511 : __torch__.transformers.modeling_bert.___torch_mangle_6860.BertAttention = prim::GetAttr[name="attention"](%68)
  %1512 : __torch__.transformers.modeling_bert.___torch_mangle_6859.BertSelfOutput = prim::GetAttr[name="output"](%1511)
  %1513 : __torch__.transformers.modeling_bert.___torch_mangle_6855.BertSelfAttention = prim::GetAttr[name="self"](%1511)
  %1514 : __torch__.torch.nn.modules.linear.___torch_mangle_6853.Linear = prim::GetAttr[name="value"](%1513)
  %1515 : __torch__.torch.nn.modules.linear.___torch_mangle_6852.Linear = prim::GetAttr[name="key"](%1513)
  %1516 : __torch__.torch.nn.modules.linear.___torch_mangle_6851.Linear = prim::GetAttr[name="query"](%1513)
  %1517 : Tensor = prim::GetAttr[name="bias"](%1516)
  %1518 : Tensor = prim::GetAttr[name="weight"](%1516)
  %1519 : Float(1024:1, 1024:1024) = aten::t(%1518), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.query # torch/nn/functional.py:1676:0
  %output.97 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.164, %1519), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.query # torch/nn/functional.py:1676:0
  %x.97 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.97, %1517, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.query # torch/nn/functional.py:1678:0
  %1522 : Tensor = prim::GetAttr[name="bias"](%1515)
  %1523 : Tensor = prim::GetAttr[name="weight"](%1515)
  %1524 : Float(1024:1, 1024:1024) = aten::t(%1523), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.key # torch/nn/functional.py:1676:0
  %output.98 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.164, %1524), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.key # torch/nn/functional.py:1676:0
  %x.99 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.98, %1522, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.key # torch/nn/functional.py:1678:0
  %1527 : Tensor = prim::GetAttr[name="bias"](%1514)
  %1528 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1529 : Float(1024:1, 1024:1024) = aten::t(%1528), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.value # torch/nn/functional.py:1676:0
  %output.99 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.164, %1529), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.value # torch/nn/functional.py:1676:0
  %x.101 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.99, %1527, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.value # torch/nn/functional.py:1678:0
  %1532 : int = aten::size(%x.97, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1533 : int = aten::size(%x.97, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1534 : int[] = prim::ListConstruct(%1532, %1533, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
  %x.98 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.97, %1534), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
  %1536 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
  %query_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.98, %1536), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
  %1538 : int = aten::size(%x.99, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1539 : int = aten::size(%x.99, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1540 : int[] = prim::ListConstruct(%1538, %1539, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
  %x.100 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.99, %1540), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
  %1542 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
  %key_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.100, %1542), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
  %1544 : int = aten::size(%x.101, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1545 : int = aten::size(%x.101, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:227:0
  %1546 : int[] = prim::ListConstruct(%1544, %1545, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
  %x.102 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.101, %1546), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:228:0
  %1548 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
  %value_layer.17 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.102, %1548), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:229:0
  %1550 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.17, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.33 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.17, %1550), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.34 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.33, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:259:0
  %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.34, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:262:0
  %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self/__module.bert.encoder.layer.16.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.33 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.17, %value_layer.17), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:275:0
  %1557 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
  %1558 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.33, %1557), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.34 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1558, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:277:0
  %1560 : int = aten::size(%context_layer.34, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:278:0
  %1561 : int = aten::size(%context_layer.34, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:278:0
  %1562 : int[] = prim::ListConstruct(%1560, %1561, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self
  %input.167 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.34, %1562), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.self # transformers/modeling_bert.py:279:0
  %1564 : __torch__.torch.nn.modules.normalization.___torch_mangle_6857.LayerNorm = prim::GetAttr[name="LayerNorm"](%1512)
  %1565 : __torch__.torch.nn.modules.linear.___torch_mangle_6856.Linear = prim::GetAttr[name="dense"](%1512)
  %1566 : Tensor = prim::GetAttr[name="bias"](%1565)
  %1567 : Tensor = prim::GetAttr[name="weight"](%1565)
  %1568 : Float(1024:1, 1024:1024) = aten::t(%1567), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1676:0
  %output.100 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.167, %1568), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1676:0
  %input.168 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.100, %1566, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.33 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.168, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.dropout # torch/nn/functional.py:973:0
  %input.169 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.33, %input.164, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output # transformers/modeling_bert.py:295:0
  %1573 : Tensor = prim::GetAttr[name="bias"](%1564)
  %1574 : Tensor = prim::GetAttr[name="weight"](%1564)
  %1575 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.LayerNorm
  %input_tensor.17 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.169, %1575, %1574, %1573, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.attention/__module.bert.encoder.layer.16.attention.output/__module.bert.encoder.layer.16.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1577 : __torch__.torch.nn.modules.linear.___torch_mangle_6861.Linear = prim::GetAttr[name="dense"](%1510)
  %1578 : Tensor = prim::GetAttr[name="bias"](%1577)
  %1579 : Tensor = prim::GetAttr[name="weight"](%1577)
  %1580 : Float(1024:1, 4096:1024) = aten::t(%1579), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate/__module.bert.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1676:0
  %output.101 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.17, %1580), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate/__module.bert.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1676:0
  %input.170 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.101, %1578, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate/__module.bert.encoder.layer.16.intermediate.dense # torch/nn/functional.py:1678:0
  %input.171 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.170), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.intermediate # torch/nn/functional.py:1369:0
  %1584 : __torch__.torch.nn.modules.normalization.___torch_mangle_6864.LayerNorm = prim::GetAttr[name="LayerNorm"](%1509)
  %1585 : __torch__.torch.nn.modules.linear.___torch_mangle_6863.Linear = prim::GetAttr[name="dense"](%1509)
  %1586 : Tensor = prim::GetAttr[name="bias"](%1585)
  %1587 : Tensor = prim::GetAttr[name="weight"](%1585)
  %1588 : Float(4096:1, 1024:4096) = aten::t(%1587), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dense # torch/nn/functional.py:1676:0
  %output.102 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.171, %1588), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dense # torch/nn/functional.py:1676:0
  %input.172 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.102, %1586, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.34 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.172, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.dropout # torch/nn/functional.py:973:0
  %input.173 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.34, %input_tensor.17, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output # transformers/modeling_bert.py:371:0
  %1593 : Tensor = prim::GetAttr[name="bias"](%1584)
  %1594 : Tensor = prim::GetAttr[name="weight"](%1584)
  %1595 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.LayerNorm
  %input.174 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.173, %1595, %1594, %1593, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.16/__module.bert.encoder.layer.16.output/__module.bert.encoder.layer.16.output.LayerNorm # torch/nn/functional.py:2048:0
  %1597 : __torch__.transformers.modeling_bert.___torch_mangle_6883.BertOutput = prim::GetAttr[name="output"](%66)
  %1598 : __torch__.transformers.modeling_bert.___torch_mangle_6879.BertIntermediate = prim::GetAttr[name="intermediate"](%66)
  %1599 : __torch__.transformers.modeling_bert.___torch_mangle_6877.BertAttention = prim::GetAttr[name="attention"](%66)
  %1600 : __torch__.transformers.modeling_bert.___torch_mangle_6876.BertSelfOutput = prim::GetAttr[name="output"](%1599)
  %1601 : __torch__.transformers.modeling_bert.___torch_mangle_6872.BertSelfAttention = prim::GetAttr[name="self"](%1599)
  %1602 : __torch__.torch.nn.modules.linear.___torch_mangle_6870.Linear = prim::GetAttr[name="value"](%1601)
  %1603 : __torch__.torch.nn.modules.linear.___torch_mangle_6869.Linear = prim::GetAttr[name="key"](%1601)
  %1604 : __torch__.torch.nn.modules.linear.___torch_mangle_6868.Linear = prim::GetAttr[name="query"](%1601)
  %1605 : Tensor = prim::GetAttr[name="bias"](%1604)
  %1606 : Tensor = prim::GetAttr[name="weight"](%1604)
  %1607 : Float(1024:1, 1024:1024) = aten::t(%1606), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.query # torch/nn/functional.py:1676:0
  %output.103 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.174, %1607), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.query # torch/nn/functional.py:1676:0
  %x.103 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.103, %1605, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.query # torch/nn/functional.py:1678:0
  %1610 : Tensor = prim::GetAttr[name="bias"](%1603)
  %1611 : Tensor = prim::GetAttr[name="weight"](%1603)
  %1612 : Float(1024:1, 1024:1024) = aten::t(%1611), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.key # torch/nn/functional.py:1676:0
  %output.104 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.174, %1612), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.key # torch/nn/functional.py:1676:0
  %x.105 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.104, %1610, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.key # torch/nn/functional.py:1678:0
  %1615 : Tensor = prim::GetAttr[name="bias"](%1602)
  %1616 : Tensor = prim::GetAttr[name="weight"](%1602)
  %1617 : Float(1024:1, 1024:1024) = aten::t(%1616), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.value # torch/nn/functional.py:1676:0
  %output.105 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.174, %1617), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.value # torch/nn/functional.py:1676:0
  %x.107 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.105, %1615, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.value # torch/nn/functional.py:1678:0
  %1620 : int = aten::size(%x.103, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1621 : int = aten::size(%x.103, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1622 : int[] = prim::ListConstruct(%1620, %1621, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
  %x.104 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.103, %1622), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
  %1624 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
  %query_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.104, %1624), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
  %1626 : int = aten::size(%x.105, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1627 : int = aten::size(%x.105, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1628 : int[] = prim::ListConstruct(%1626, %1627, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
  %x.106 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.105, %1628), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
  %1630 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
  %key_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.106, %1630), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
  %1632 : int = aten::size(%x.107, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1633 : int = aten::size(%x.107, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:227:0
  %1634 : int[] = prim::ListConstruct(%1632, %1633, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
  %x.108 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.107, %1634), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:228:0
  %1636 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
  %value_layer.18 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.108, %1636), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:229:0
  %1638 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.18, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.18, %1638), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.35, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:259:0
  %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.36, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:262:0
  %input.176 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.175, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.176, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self/__module.bert.encoder.layer.17.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.35 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.18, %value_layer.18), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:275:0
  %1645 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
  %1646 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.35, %1645), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.36 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1646, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:277:0
  %1648 : int = aten::size(%context_layer.36, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:278:0
  %1649 : int = aten::size(%context_layer.36, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:278:0
  %1650 : int[] = prim::ListConstruct(%1648, %1649, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self
  %input.177 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.36, %1650), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.self # transformers/modeling_bert.py:279:0
  %1652 : __torch__.torch.nn.modules.normalization.___torch_mangle_6874.LayerNorm = prim::GetAttr[name="LayerNorm"](%1600)
  %1653 : __torch__.torch.nn.modules.linear.___torch_mangle_6873.Linear = prim::GetAttr[name="dense"](%1600)
  %1654 : Tensor = prim::GetAttr[name="bias"](%1653)
  %1655 : Tensor = prim::GetAttr[name="weight"](%1653)
  %1656 : Float(1024:1, 1024:1024) = aten::t(%1655), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1676:0
  %output.106 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.177, %1656), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1676:0
  %input.178 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.106, %1654, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.35 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.178, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.dropout # torch/nn/functional.py:973:0
  %input.179 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.35, %input.174, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output # transformers/modeling_bert.py:295:0
  %1661 : Tensor = prim::GetAttr[name="bias"](%1652)
  %1662 : Tensor = prim::GetAttr[name="weight"](%1652)
  %1663 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.LayerNorm
  %input_tensor.18 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.179, %1663, %1662, %1661, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.attention/__module.bert.encoder.layer.17.attention.output/__module.bert.encoder.layer.17.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1665 : __torch__.torch.nn.modules.linear.___torch_mangle_6878.Linear = prim::GetAttr[name="dense"](%1598)
  %1666 : Tensor = prim::GetAttr[name="bias"](%1665)
  %1667 : Tensor = prim::GetAttr[name="weight"](%1665)
  %1668 : Float(1024:1, 4096:1024) = aten::t(%1667), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate/__module.bert.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1676:0
  %output.107 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.18, %1668), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate/__module.bert.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1676:0
  %input.180 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.107, %1666, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate/__module.bert.encoder.layer.17.intermediate.dense # torch/nn/functional.py:1678:0
  %input.181 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.180), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.intermediate # torch/nn/functional.py:1369:0
  %1672 : __torch__.torch.nn.modules.normalization.___torch_mangle_6881.LayerNorm = prim::GetAttr[name="LayerNorm"](%1597)
  %1673 : __torch__.torch.nn.modules.linear.___torch_mangle_6880.Linear = prim::GetAttr[name="dense"](%1597)
  %1674 : Tensor = prim::GetAttr[name="bias"](%1673)
  %1675 : Tensor = prim::GetAttr[name="weight"](%1673)
  %1676 : Float(4096:1, 1024:4096) = aten::t(%1675), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dense # torch/nn/functional.py:1676:0
  %output.108 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.181, %1676), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dense # torch/nn/functional.py:1676:0
  %input.182 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.108, %1674, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.36 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.182, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.dropout # torch/nn/functional.py:973:0
  %input.183 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.36, %input_tensor.18, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output # transformers/modeling_bert.py:371:0
  %1681 : Tensor = prim::GetAttr[name="bias"](%1672)
  %1682 : Tensor = prim::GetAttr[name="weight"](%1672)
  %1683 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.LayerNorm
  %input.184 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.183, %1683, %1682, %1681, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.17/__module.bert.encoder.layer.17.output/__module.bert.encoder.layer.17.output.LayerNorm # torch/nn/functional.py:2048:0
  %1685 : __torch__.transformers.modeling_bert.___torch_mangle_6900.BertOutput = prim::GetAttr[name="output"](%64)
  %1686 : __torch__.transformers.modeling_bert.___torch_mangle_6896.BertIntermediate = prim::GetAttr[name="intermediate"](%64)
  %1687 : __torch__.transformers.modeling_bert.___torch_mangle_6894.BertAttention = prim::GetAttr[name="attention"](%64)
  %1688 : __torch__.transformers.modeling_bert.___torch_mangle_6893.BertSelfOutput = prim::GetAttr[name="output"](%1687)
  %1689 : __torch__.transformers.modeling_bert.___torch_mangle_6889.BertSelfAttention = prim::GetAttr[name="self"](%1687)
  %1690 : __torch__.torch.nn.modules.linear.___torch_mangle_6887.Linear = prim::GetAttr[name="value"](%1689)
  %1691 : __torch__.torch.nn.modules.linear.___torch_mangle_6886.Linear = prim::GetAttr[name="key"](%1689)
  %1692 : __torch__.torch.nn.modules.linear.___torch_mangle_6885.Linear = prim::GetAttr[name="query"](%1689)
  %1693 : Tensor = prim::GetAttr[name="bias"](%1692)
  %1694 : Tensor = prim::GetAttr[name="weight"](%1692)
  %1695 : Float(1024:1, 1024:1024) = aten::t(%1694), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.query # torch/nn/functional.py:1676:0
  %output.109 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.184, %1695), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.query # torch/nn/functional.py:1676:0
  %x.109 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.109, %1693, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.query # torch/nn/functional.py:1678:0
  %1698 : Tensor = prim::GetAttr[name="bias"](%1691)
  %1699 : Tensor = prim::GetAttr[name="weight"](%1691)
  %1700 : Float(1024:1, 1024:1024) = aten::t(%1699), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.key # torch/nn/functional.py:1676:0
  %output.110 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.184, %1700), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.key # torch/nn/functional.py:1676:0
  %x.111 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.110, %1698, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.key # torch/nn/functional.py:1678:0
  %1703 : Tensor = prim::GetAttr[name="bias"](%1690)
  %1704 : Tensor = prim::GetAttr[name="weight"](%1690)
  %1705 : Float(1024:1, 1024:1024) = aten::t(%1704), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.value # torch/nn/functional.py:1676:0
  %output.111 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.184, %1705), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.value # torch/nn/functional.py:1676:0
  %x.113 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.111, %1703, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.value # torch/nn/functional.py:1678:0
  %1708 : int = aten::size(%x.109, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1709 : int = aten::size(%x.109, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1710 : int[] = prim::ListConstruct(%1708, %1709, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
  %x.110 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.109, %1710), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
  %1712 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
  %query_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.110, %1712), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
  %1714 : int = aten::size(%x.111, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1715 : int = aten::size(%x.111, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1716 : int[] = prim::ListConstruct(%1714, %1715, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
  %x.112 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.111, %1716), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
  %1718 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
  %key_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.112, %1718), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
  %1720 : int = aten::size(%x.113, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1721 : int = aten::size(%x.113, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:227:0
  %1722 : int[] = prim::ListConstruct(%1720, %1721, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
  %x.114 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.113, %1722), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:228:0
  %1724 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
  %value_layer.19 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.114, %1724), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:229:0
  %1726 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.19, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.37 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.19, %1726), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.38 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.37, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:259:0
  %input.185 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.38, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:262:0
  %input.186 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.185, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.186, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self/__module.bert.encoder.layer.18.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.37 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.19, %value_layer.19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:275:0
  %1733 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
  %1734 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.37, %1733), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.38 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1734, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:277:0
  %1736 : int = aten::size(%context_layer.38, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:278:0
  %1737 : int = aten::size(%context_layer.38, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:278:0
  %1738 : int[] = prim::ListConstruct(%1736, %1737, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self
  %input.187 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.38, %1738), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.self # transformers/modeling_bert.py:279:0
  %1740 : __torch__.torch.nn.modules.normalization.___torch_mangle_6891.LayerNorm = prim::GetAttr[name="LayerNorm"](%1688)
  %1741 : __torch__.torch.nn.modules.linear.___torch_mangle_6890.Linear = prim::GetAttr[name="dense"](%1688)
  %1742 : Tensor = prim::GetAttr[name="bias"](%1741)
  %1743 : Tensor = prim::GetAttr[name="weight"](%1741)
  %1744 : Float(1024:1, 1024:1024) = aten::t(%1743), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1676:0
  %output.112 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.187, %1744), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1676:0
  %input.188 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.112, %1742, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.37 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.188, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.dropout # torch/nn/functional.py:973:0
  %input.189 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.37, %input.184, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output # transformers/modeling_bert.py:295:0
  %1749 : Tensor = prim::GetAttr[name="bias"](%1740)
  %1750 : Tensor = prim::GetAttr[name="weight"](%1740)
  %1751 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.LayerNorm
  %input_tensor.19 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.189, %1751, %1750, %1749, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.attention/__module.bert.encoder.layer.18.attention.output/__module.bert.encoder.layer.18.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1753 : __torch__.torch.nn.modules.linear.___torch_mangle_6895.Linear = prim::GetAttr[name="dense"](%1686)
  %1754 : Tensor = prim::GetAttr[name="bias"](%1753)
  %1755 : Tensor = prim::GetAttr[name="weight"](%1753)
  %1756 : Float(1024:1, 4096:1024) = aten::t(%1755), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate/__module.bert.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1676:0
  %output.113 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.19, %1756), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate/__module.bert.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1676:0
  %input.190 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.113, %1754, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate/__module.bert.encoder.layer.18.intermediate.dense # torch/nn/functional.py:1678:0
  %input.191 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.190), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.intermediate # torch/nn/functional.py:1369:0
  %1760 : __torch__.torch.nn.modules.normalization.___torch_mangle_6898.LayerNorm = prim::GetAttr[name="LayerNorm"](%1685)
  %1761 : __torch__.torch.nn.modules.linear.___torch_mangle_6897.Linear = prim::GetAttr[name="dense"](%1685)
  %1762 : Tensor = prim::GetAttr[name="bias"](%1761)
  %1763 : Tensor = prim::GetAttr[name="weight"](%1761)
  %1764 : Float(4096:1, 1024:4096) = aten::t(%1763), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dense # torch/nn/functional.py:1676:0
  %output.114 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.191, %1764), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dense # torch/nn/functional.py:1676:0
  %input.192 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.114, %1762, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.38 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.192, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.dropout # torch/nn/functional.py:973:0
  %input.193 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.38, %input_tensor.19, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output # transformers/modeling_bert.py:371:0
  %1769 : Tensor = prim::GetAttr[name="bias"](%1760)
  %1770 : Tensor = prim::GetAttr[name="weight"](%1760)
  %1771 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.LayerNorm
  %input.194 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.193, %1771, %1770, %1769, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.18/__module.bert.encoder.layer.18.output/__module.bert.encoder.layer.18.output.LayerNorm # torch/nn/functional.py:2048:0
  %1773 : __torch__.transformers.modeling_bert.___torch_mangle_6917.BertOutput = prim::GetAttr[name="output"](%62)
  %1774 : __torch__.transformers.modeling_bert.___torch_mangle_6913.BertIntermediate = prim::GetAttr[name="intermediate"](%62)
  %1775 : __torch__.transformers.modeling_bert.___torch_mangle_6911.BertAttention = prim::GetAttr[name="attention"](%62)
  %1776 : __torch__.transformers.modeling_bert.___torch_mangle_6910.BertSelfOutput = prim::GetAttr[name="output"](%1775)
  %1777 : __torch__.transformers.modeling_bert.___torch_mangle_6906.BertSelfAttention = prim::GetAttr[name="self"](%1775)
  %1778 : __torch__.torch.nn.modules.linear.___torch_mangle_6904.Linear = prim::GetAttr[name="value"](%1777)
  %1779 : __torch__.torch.nn.modules.linear.___torch_mangle_6903.Linear = prim::GetAttr[name="key"](%1777)
  %1780 : __torch__.torch.nn.modules.linear.___torch_mangle_6902.Linear = prim::GetAttr[name="query"](%1777)
  %1781 : Tensor = prim::GetAttr[name="bias"](%1780)
  %1782 : Tensor = prim::GetAttr[name="weight"](%1780)
  %1783 : Float(1024:1, 1024:1024) = aten::t(%1782), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.query # torch/nn/functional.py:1676:0
  %output.115 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.194, %1783), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.query # torch/nn/functional.py:1676:0
  %x.115 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.115, %1781, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.query # torch/nn/functional.py:1678:0
  %1786 : Tensor = prim::GetAttr[name="bias"](%1779)
  %1787 : Tensor = prim::GetAttr[name="weight"](%1779)
  %1788 : Float(1024:1, 1024:1024) = aten::t(%1787), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.key # torch/nn/functional.py:1676:0
  %output.116 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.194, %1788), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.key # torch/nn/functional.py:1676:0
  %x.117 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.116, %1786, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.key # torch/nn/functional.py:1678:0
  %1791 : Tensor = prim::GetAttr[name="bias"](%1778)
  %1792 : Tensor = prim::GetAttr[name="weight"](%1778)
  %1793 : Float(1024:1, 1024:1024) = aten::t(%1792), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.value # torch/nn/functional.py:1676:0
  %output.117 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.194, %1793), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.value # torch/nn/functional.py:1676:0
  %x.119 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.117, %1791, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.value # torch/nn/functional.py:1678:0
  %1796 : int = aten::size(%x.115, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1797 : int = aten::size(%x.115, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1798 : int[] = prim::ListConstruct(%1796, %1797, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
  %x.116 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.115, %1798), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
  %1800 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
  %query_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.116, %1800), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
  %1802 : int = aten::size(%x.117, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1803 : int = aten::size(%x.117, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1804 : int[] = prim::ListConstruct(%1802, %1803, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
  %x.118 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.117, %1804), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
  %1806 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
  %key_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.118, %1806), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
  %1808 : int = aten::size(%x.119, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1809 : int = aten::size(%x.119, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:227:0
  %1810 : int[] = prim::ListConstruct(%1808, %1809, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
  %x.120 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.119, %1810), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:228:0
  %1812 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
  %value_layer.20 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.120, %1812), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:229:0
  %1814 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.20, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.20, %1814), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.39, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:259:0
  %input.195 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.40, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:262:0
  %input.196 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.195, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.196, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self/__module.bert.encoder.layer.19.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.39 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.20, %value_layer.20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:275:0
  %1821 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
  %1822 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.39, %1821), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.40 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1822, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:277:0
  %1824 : int = aten::size(%context_layer.40, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:278:0
  %1825 : int = aten::size(%context_layer.40, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:278:0
  %1826 : int[] = prim::ListConstruct(%1824, %1825, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self
  %input.197 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.40, %1826), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.self # transformers/modeling_bert.py:279:0
  %1828 : __torch__.torch.nn.modules.normalization.___torch_mangle_6908.LayerNorm = prim::GetAttr[name="LayerNorm"](%1776)
  %1829 : __torch__.torch.nn.modules.linear.___torch_mangle_6907.Linear = prim::GetAttr[name="dense"](%1776)
  %1830 : Tensor = prim::GetAttr[name="bias"](%1829)
  %1831 : Tensor = prim::GetAttr[name="weight"](%1829)
  %1832 : Float(1024:1, 1024:1024) = aten::t(%1831), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1676:0
  %output.118 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.197, %1832), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1676:0
  %input.198 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.118, %1830, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.39 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.198, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.dropout # torch/nn/functional.py:973:0
  %input.199 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.39, %input.194, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output # transformers/modeling_bert.py:295:0
  %1837 : Tensor = prim::GetAttr[name="bias"](%1828)
  %1838 : Tensor = prim::GetAttr[name="weight"](%1828)
  %1839 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.LayerNorm
  %input_tensor.20 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.199, %1839, %1838, %1837, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.attention/__module.bert.encoder.layer.19.attention.output/__module.bert.encoder.layer.19.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1841 : __torch__.torch.nn.modules.linear.___torch_mangle_6912.Linear = prim::GetAttr[name="dense"](%1774)
  %1842 : Tensor = prim::GetAttr[name="bias"](%1841)
  %1843 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1844 : Float(1024:1, 4096:1024) = aten::t(%1843), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate/__module.bert.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1676:0
  %output.119 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.20, %1844), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate/__module.bert.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1676:0
  %input.200 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.119, %1842, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate/__module.bert.encoder.layer.19.intermediate.dense # torch/nn/functional.py:1678:0
  %input.201 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.200), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.intermediate # torch/nn/functional.py:1369:0
  %1848 : __torch__.torch.nn.modules.normalization.___torch_mangle_6915.LayerNorm = prim::GetAttr[name="LayerNorm"](%1773)
  %1849 : __torch__.torch.nn.modules.linear.___torch_mangle_6914.Linear = prim::GetAttr[name="dense"](%1773)
  %1850 : Tensor = prim::GetAttr[name="bias"](%1849)
  %1851 : Tensor = prim::GetAttr[name="weight"](%1849)
  %1852 : Float(4096:1, 1024:4096) = aten::t(%1851), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dense # torch/nn/functional.py:1676:0
  %output.120 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.201, %1852), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dense # torch/nn/functional.py:1676:0
  %input.202 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.120, %1850, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.40 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.202, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.dropout # torch/nn/functional.py:973:0
  %input.203 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.40, %input_tensor.20, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output # transformers/modeling_bert.py:371:0
  %1857 : Tensor = prim::GetAttr[name="bias"](%1848)
  %1858 : Tensor = prim::GetAttr[name="weight"](%1848)
  %1859 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.LayerNorm
  %input.204 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.203, %1859, %1858, %1857, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.19/__module.bert.encoder.layer.19.output/__module.bert.encoder.layer.19.output.LayerNorm # torch/nn/functional.py:2048:0
  %1861 : __torch__.transformers.modeling_bert.___torch_mangle_6934.BertOutput = prim::GetAttr[name="output"](%60)
  %1862 : __torch__.transformers.modeling_bert.___torch_mangle_6930.BertIntermediate = prim::GetAttr[name="intermediate"](%60)
  %1863 : __torch__.transformers.modeling_bert.___torch_mangle_6928.BertAttention = prim::GetAttr[name="attention"](%60)
  %1864 : __torch__.transformers.modeling_bert.___torch_mangle_6927.BertSelfOutput = prim::GetAttr[name="output"](%1863)
  %1865 : __torch__.transformers.modeling_bert.___torch_mangle_6923.BertSelfAttention = prim::GetAttr[name="self"](%1863)
  %1866 : __torch__.torch.nn.modules.linear.___torch_mangle_6921.Linear = prim::GetAttr[name="value"](%1865)
  %1867 : __torch__.torch.nn.modules.linear.___torch_mangle_6920.Linear = prim::GetAttr[name="key"](%1865)
  %1868 : __torch__.torch.nn.modules.linear.___torch_mangle_6919.Linear = prim::GetAttr[name="query"](%1865)
  %1869 : Tensor = prim::GetAttr[name="bias"](%1868)
  %1870 : Tensor = prim::GetAttr[name="weight"](%1868)
  %1871 : Float(1024:1, 1024:1024) = aten::t(%1870), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.query # torch/nn/functional.py:1676:0
  %output.121 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.204, %1871), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.query # torch/nn/functional.py:1676:0
  %x.121 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.121, %1869, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.query # torch/nn/functional.py:1678:0
  %1874 : Tensor = prim::GetAttr[name="bias"](%1867)
  %1875 : Tensor = prim::GetAttr[name="weight"](%1867)
  %1876 : Float(1024:1, 1024:1024) = aten::t(%1875), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.key # torch/nn/functional.py:1676:0
  %output.122 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.204, %1876), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.key # torch/nn/functional.py:1676:0
  %x.123 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.122, %1874, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.key # torch/nn/functional.py:1678:0
  %1879 : Tensor = prim::GetAttr[name="bias"](%1866)
  %1880 : Tensor = prim::GetAttr[name="weight"](%1866)
  %1881 : Float(1024:1, 1024:1024) = aten::t(%1880), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.value # torch/nn/functional.py:1676:0
  %output.123 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.204, %1881), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.value # torch/nn/functional.py:1676:0
  %x.125 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.123, %1879, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.value # torch/nn/functional.py:1678:0
  %1884 : int = aten::size(%x.121, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1885 : int = aten::size(%x.121, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1886 : int[] = prim::ListConstruct(%1884, %1885, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
  %x.122 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.121, %1886), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
  %1888 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
  %query_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.122, %1888), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
  %1890 : int = aten::size(%x.123, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1891 : int = aten::size(%x.123, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1892 : int[] = prim::ListConstruct(%1890, %1891, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
  %x.124 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.123, %1892), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
  %1894 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
  %key_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.124, %1894), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
  %1896 : int = aten::size(%x.125, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1897 : int = aten::size(%x.125, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:227:0
  %1898 : int[] = prim::ListConstruct(%1896, %1897, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
  %x.126 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.125, %1898), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:228:0
  %1900 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
  %value_layer.21 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.126, %1900), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:229:0
  %1902 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.21, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.41 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.21, %1902), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.42 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.41, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:259:0
  %input.205 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.42, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:262:0
  %input.206 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.205, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.206, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self/__module.bert.encoder.layer.20.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.41 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.21, %value_layer.21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:275:0
  %1909 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
  %1910 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.41, %1909), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.42 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1910, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:277:0
  %1912 : int = aten::size(%context_layer.42, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:278:0
  %1913 : int = aten::size(%context_layer.42, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:278:0
  %1914 : int[] = prim::ListConstruct(%1912, %1913, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self
  %input.207 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.42, %1914), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.self # transformers/modeling_bert.py:279:0
  %1916 : __torch__.torch.nn.modules.normalization.___torch_mangle_6925.LayerNorm = prim::GetAttr[name="LayerNorm"](%1864)
  %1917 : __torch__.torch.nn.modules.linear.___torch_mangle_6924.Linear = prim::GetAttr[name="dense"](%1864)
  %1918 : Tensor = prim::GetAttr[name="bias"](%1917)
  %1919 : Tensor = prim::GetAttr[name="weight"](%1917)
  %1920 : Float(1024:1, 1024:1024) = aten::t(%1919), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1676:0
  %output.124 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.207, %1920), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1676:0
  %input.208 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.124, %1918, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.41 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.208, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.dropout # torch/nn/functional.py:973:0
  %input.209 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.41, %input.204, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output # transformers/modeling_bert.py:295:0
  %1925 : Tensor = prim::GetAttr[name="bias"](%1916)
  %1926 : Tensor = prim::GetAttr[name="weight"](%1916)
  %1927 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.LayerNorm
  %input_tensor.21 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.209, %1927, %1926, %1925, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.attention/__module.bert.encoder.layer.20.attention.output/__module.bert.encoder.layer.20.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1929 : __torch__.torch.nn.modules.linear.___torch_mangle_6929.Linear = prim::GetAttr[name="dense"](%1862)
  %1930 : Tensor = prim::GetAttr[name="bias"](%1929)
  %1931 : Tensor = prim::GetAttr[name="weight"](%1929)
  %1932 : Float(1024:1, 4096:1024) = aten::t(%1931), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate/__module.bert.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1676:0
  %output.125 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.21, %1932), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate/__module.bert.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1676:0
  %input.210 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.125, %1930, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate/__module.bert.encoder.layer.20.intermediate.dense # torch/nn/functional.py:1678:0
  %input.211 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.210), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.intermediate # torch/nn/functional.py:1369:0
  %1936 : __torch__.torch.nn.modules.normalization.___torch_mangle_6932.LayerNorm = prim::GetAttr[name="LayerNorm"](%1861)
  %1937 : __torch__.torch.nn.modules.linear.___torch_mangle_6931.Linear = prim::GetAttr[name="dense"](%1861)
  %1938 : Tensor = prim::GetAttr[name="bias"](%1937)
  %1939 : Tensor = prim::GetAttr[name="weight"](%1937)
  %1940 : Float(4096:1, 1024:4096) = aten::t(%1939), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dense # torch/nn/functional.py:1676:0
  %output.126 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.211, %1940), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dense # torch/nn/functional.py:1676:0
  %input.212 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.126, %1938, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.42 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.212, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.dropout # torch/nn/functional.py:973:0
  %input.213 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.42, %input_tensor.21, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output # transformers/modeling_bert.py:371:0
  %1945 : Tensor = prim::GetAttr[name="bias"](%1936)
  %1946 : Tensor = prim::GetAttr[name="weight"](%1936)
  %1947 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.LayerNorm
  %input.214 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.213, %1947, %1946, %1945, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.20/__module.bert.encoder.layer.20.output/__module.bert.encoder.layer.20.output.LayerNorm # torch/nn/functional.py:2048:0
  %1949 : __torch__.transformers.modeling_bert.___torch_mangle_6951.BertOutput = prim::GetAttr[name="output"](%58)
  %1950 : __torch__.transformers.modeling_bert.___torch_mangle_6947.BertIntermediate = prim::GetAttr[name="intermediate"](%58)
  %1951 : __torch__.transformers.modeling_bert.___torch_mangle_6945.BertAttention = prim::GetAttr[name="attention"](%58)
  %1952 : __torch__.transformers.modeling_bert.___torch_mangle_6944.BertSelfOutput = prim::GetAttr[name="output"](%1951)
  %1953 : __torch__.transformers.modeling_bert.___torch_mangle_6940.BertSelfAttention = prim::GetAttr[name="self"](%1951)
  %1954 : __torch__.torch.nn.modules.linear.___torch_mangle_6938.Linear = prim::GetAttr[name="value"](%1953)
  %1955 : __torch__.torch.nn.modules.linear.___torch_mangle_6937.Linear = prim::GetAttr[name="key"](%1953)
  %1956 : __torch__.torch.nn.modules.linear.___torch_mangle_6936.Linear = prim::GetAttr[name="query"](%1953)
  %1957 : Tensor = prim::GetAttr[name="bias"](%1956)
  %1958 : Tensor = prim::GetAttr[name="weight"](%1956)
  %1959 : Float(1024:1, 1024:1024) = aten::t(%1958), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.query # torch/nn/functional.py:1676:0
  %output.127 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.214, %1959), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.query # torch/nn/functional.py:1676:0
  %x.127 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.127, %1957, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.query # torch/nn/functional.py:1678:0
  %1962 : Tensor = prim::GetAttr[name="bias"](%1955)
  %1963 : Tensor = prim::GetAttr[name="weight"](%1955)
  %1964 : Float(1024:1, 1024:1024) = aten::t(%1963), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.key # torch/nn/functional.py:1676:0
  %output.128 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.214, %1964), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.key # torch/nn/functional.py:1676:0
  %x.129 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.128, %1962, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.key # torch/nn/functional.py:1678:0
  %1967 : Tensor = prim::GetAttr[name="bias"](%1954)
  %1968 : Tensor = prim::GetAttr[name="weight"](%1954)
  %1969 : Float(1024:1, 1024:1024) = aten::t(%1968), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.value # torch/nn/functional.py:1676:0
  %output.129 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.214, %1969), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.value # torch/nn/functional.py:1676:0
  %x.131 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.129, %1967, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.value # torch/nn/functional.py:1678:0
  %1972 : int = aten::size(%x.127, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1973 : int = aten::size(%x.127, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1974 : int[] = prim::ListConstruct(%1972, %1973, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
  %x.128 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.127, %1974), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
  %1976 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
  %query_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.128, %1976), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
  %1978 : int = aten::size(%x.129, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1979 : int = aten::size(%x.129, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1980 : int[] = prim::ListConstruct(%1978, %1979, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
  %x.130 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.129, %1980), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
  %1982 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
  %key_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.130, %1982), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
  %1984 : int = aten::size(%x.131, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1985 : int = aten::size(%x.131, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:227:0
  %1986 : int[] = prim::ListConstruct(%1984, %1985, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
  %x.132 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.131, %1986), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:228:0
  %1988 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
  %value_layer.22 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.132, %1988), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:229:0
  %1990 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.22, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.43 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.22, %1990), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.44 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.43, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:259:0
  %input.215 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.44, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:262:0
  %input.216 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.215, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.216, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self/__module.bert.encoder.layer.21.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.43 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.22, %value_layer.22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:275:0
  %1997 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
  %1998 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.43, %1997), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.44 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%1998, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:277:0
  %2000 : int = aten::size(%context_layer.44, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:278:0
  %2001 : int = aten::size(%context_layer.44, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:278:0
  %2002 : int[] = prim::ListConstruct(%2000, %2001, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self
  %input.217 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.44, %2002), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.self # transformers/modeling_bert.py:279:0
  %2004 : __torch__.torch.nn.modules.normalization.___torch_mangle_6942.LayerNorm = prim::GetAttr[name="LayerNorm"](%1952)
  %2005 : __torch__.torch.nn.modules.linear.___torch_mangle_6941.Linear = prim::GetAttr[name="dense"](%1952)
  %2006 : Tensor = prim::GetAttr[name="bias"](%2005)
  %2007 : Tensor = prim::GetAttr[name="weight"](%2005)
  %2008 : Float(1024:1, 1024:1024) = aten::t(%2007), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1676:0
  %output.130 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.217, %2008), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1676:0
  %input.218 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.130, %2006, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.43 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.218, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.dropout # torch/nn/functional.py:973:0
  %input.219 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.43, %input.214, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output # transformers/modeling_bert.py:295:0
  %2013 : Tensor = prim::GetAttr[name="bias"](%2004)
  %2014 : Tensor = prim::GetAttr[name="weight"](%2004)
  %2015 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.LayerNorm
  %input_tensor.22 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.219, %2015, %2014, %2013, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.attention/__module.bert.encoder.layer.21.attention.output/__module.bert.encoder.layer.21.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2017 : __torch__.torch.nn.modules.linear.___torch_mangle_6946.Linear = prim::GetAttr[name="dense"](%1950)
  %2018 : Tensor = prim::GetAttr[name="bias"](%2017)
  %2019 : Tensor = prim::GetAttr[name="weight"](%2017)
  %2020 : Float(1024:1, 4096:1024) = aten::t(%2019), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate/__module.bert.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1676:0
  %output.131 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.22, %2020), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate/__module.bert.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1676:0
  %input.220 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.131, %2018, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate/__module.bert.encoder.layer.21.intermediate.dense # torch/nn/functional.py:1678:0
  %input.221 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.220), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.intermediate # torch/nn/functional.py:1369:0
  %2024 : __torch__.torch.nn.modules.normalization.___torch_mangle_6949.LayerNorm = prim::GetAttr[name="LayerNorm"](%1949)
  %2025 : __torch__.torch.nn.modules.linear.___torch_mangle_6948.Linear = prim::GetAttr[name="dense"](%1949)
  %2026 : Tensor = prim::GetAttr[name="bias"](%2025)
  %2027 : Tensor = prim::GetAttr[name="weight"](%2025)
  %2028 : Float(4096:1, 1024:4096) = aten::t(%2027), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dense # torch/nn/functional.py:1676:0
  %output.132 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.221, %2028), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dense # torch/nn/functional.py:1676:0
  %input.222 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.132, %2026, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.44 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.222, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.dropout # torch/nn/functional.py:973:0
  %input.223 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.44, %input_tensor.22, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output # transformers/modeling_bert.py:371:0
  %2033 : Tensor = prim::GetAttr[name="bias"](%2024)
  %2034 : Tensor = prim::GetAttr[name="weight"](%2024)
  %2035 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.LayerNorm
  %input.224 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.223, %2035, %2034, %2033, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.21/__module.bert.encoder.layer.21.output/__module.bert.encoder.layer.21.output.LayerNorm # torch/nn/functional.py:2048:0
  %2037 : __torch__.transformers.modeling_bert.___torch_mangle_6968.BertOutput = prim::GetAttr[name="output"](%56)
  %2038 : __torch__.transformers.modeling_bert.___torch_mangle_6964.BertIntermediate = prim::GetAttr[name="intermediate"](%56)
  %2039 : __torch__.transformers.modeling_bert.___torch_mangle_6962.BertAttention = prim::GetAttr[name="attention"](%56)
  %2040 : __torch__.transformers.modeling_bert.___torch_mangle_6961.BertSelfOutput = prim::GetAttr[name="output"](%2039)
  %2041 : __torch__.transformers.modeling_bert.___torch_mangle_6957.BertSelfAttention = prim::GetAttr[name="self"](%2039)
  %2042 : __torch__.torch.nn.modules.linear.___torch_mangle_6955.Linear = prim::GetAttr[name="value"](%2041)
  %2043 : __torch__.torch.nn.modules.linear.___torch_mangle_6954.Linear = prim::GetAttr[name="key"](%2041)
  %2044 : __torch__.torch.nn.modules.linear.___torch_mangle_6953.Linear = prim::GetAttr[name="query"](%2041)
  %2045 : Tensor = prim::GetAttr[name="bias"](%2044)
  %2046 : Tensor = prim::GetAttr[name="weight"](%2044)
  %2047 : Float(1024:1, 1024:1024) = aten::t(%2046), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.query # torch/nn/functional.py:1676:0
  %output.133 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.224, %2047), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.query # torch/nn/functional.py:1676:0
  %x.133 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.133, %2045, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.query # torch/nn/functional.py:1678:0
  %2050 : Tensor = prim::GetAttr[name="bias"](%2043)
  %2051 : Tensor = prim::GetAttr[name="weight"](%2043)
  %2052 : Float(1024:1, 1024:1024) = aten::t(%2051), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.key # torch/nn/functional.py:1676:0
  %output.134 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.224, %2052), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.key # torch/nn/functional.py:1676:0
  %x.135 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.134, %2050, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.key # torch/nn/functional.py:1678:0
  %2055 : Tensor = prim::GetAttr[name="bias"](%2042)
  %2056 : Tensor = prim::GetAttr[name="weight"](%2042)
  %2057 : Float(1024:1, 1024:1024) = aten::t(%2056), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.value # torch/nn/functional.py:1676:0
  %output.135 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.224, %2057), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.value # torch/nn/functional.py:1676:0
  %x.137 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.135, %2055, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.value # torch/nn/functional.py:1678:0
  %2060 : int = aten::size(%x.133, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2061 : int = aten::size(%x.133, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2062 : int[] = prim::ListConstruct(%2060, %2061, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
  %x.134 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.133, %2062), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
  %2064 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
  %query_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.134, %2064), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
  %2066 : int = aten::size(%x.135, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2067 : int = aten::size(%x.135, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2068 : int[] = prim::ListConstruct(%2066, %2067, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
  %x.136 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.135, %2068), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
  %2070 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
  %key_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.136, %2070), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
  %2072 : int = aten::size(%x.137, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2073 : int = aten::size(%x.137, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:227:0
  %2074 : int[] = prim::ListConstruct(%2072, %2073, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
  %x.138 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.137, %2074), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:228:0
  %2076 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
  %value_layer.23 : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.138, %2076), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:229:0
  %2078 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer.23, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer.23, %2078), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.45, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:259:0
  %input.225 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores.46, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:262:0
  %input.226 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.225, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.226, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self/__module.bert.encoder.layer.22.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.45 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs.23, %value_layer.23), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:275:0
  %2085 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
  %2086 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.45, %2085), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.46 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%2086, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:277:0
  %2088 : int = aten::size(%context_layer.46, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:278:0
  %2089 : int = aten::size(%context_layer.46, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:278:0
  %2090 : int[] = prim::ListConstruct(%2088, %2089, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self
  %input.227 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer.46, %2090), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.self # transformers/modeling_bert.py:279:0
  %2092 : __torch__.torch.nn.modules.normalization.___torch_mangle_6959.LayerNorm = prim::GetAttr[name="LayerNorm"](%2040)
  %2093 : __torch__.torch.nn.modules.linear.___torch_mangle_6958.Linear = prim::GetAttr[name="dense"](%2040)
  %2094 : Tensor = prim::GetAttr[name="bias"](%2093)
  %2095 : Tensor = prim::GetAttr[name="weight"](%2093)
  %2096 : Float(1024:1, 1024:1024) = aten::t(%2095), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1676:0
  %output.136 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.227, %2096), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1676:0
  %input.228 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.136, %2094, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.45 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.228, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.dropout # torch/nn/functional.py:973:0
  %input.229 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.45, %input.224, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output # transformers/modeling_bert.py:295:0
  %2101 : Tensor = prim::GetAttr[name="bias"](%2092)
  %2102 : Tensor = prim::GetAttr[name="weight"](%2092)
  %2103 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.LayerNorm
  %input_tensor.23 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.229, %2103, %2102, %2101, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.attention/__module.bert.encoder.layer.22.attention.output/__module.bert.encoder.layer.22.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2105 : __torch__.torch.nn.modules.linear.___torch_mangle_6963.Linear = prim::GetAttr[name="dense"](%2038)
  %2106 : Tensor = prim::GetAttr[name="bias"](%2105)
  %2107 : Tensor = prim::GetAttr[name="weight"](%2105)
  %2108 : Float(1024:1, 4096:1024) = aten::t(%2107), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate/__module.bert.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1676:0
  %output.137 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor.23, %2108), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate/__module.bert.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1676:0
  %input.230 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.137, %2106, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate/__module.bert.encoder.layer.22.intermediate.dense # torch/nn/functional.py:1678:0
  %input.231 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.230), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.intermediate # torch/nn/functional.py:1369:0
  %2112 : __torch__.torch.nn.modules.normalization.___torch_mangle_6966.LayerNorm = prim::GetAttr[name="LayerNorm"](%2037)
  %2113 : __torch__.torch.nn.modules.linear.___torch_mangle_6965.Linear = prim::GetAttr[name="dense"](%2037)
  %2114 : Tensor = prim::GetAttr[name="bias"](%2113)
  %2115 : Tensor = prim::GetAttr[name="weight"](%2113)
  %2116 : Float(4096:1, 1024:4096) = aten::t(%2115), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dense # torch/nn/functional.py:1676:0
  %output.138 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.231, %2116), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dense # torch/nn/functional.py:1676:0
  %input.232 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.138, %2114, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.46 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.232, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.dropout # torch/nn/functional.py:973:0
  %input.233 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.46, %input_tensor.23, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output # transformers/modeling_bert.py:371:0
  %2121 : Tensor = prim::GetAttr[name="bias"](%2112)
  %2122 : Tensor = prim::GetAttr[name="weight"](%2112)
  %2123 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.LayerNorm
  %input.234 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.233, %2123, %2122, %2121, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.22/__module.bert.encoder.layer.22.output/__module.bert.encoder.layer.22.output.LayerNorm # torch/nn/functional.py:2048:0
  %2125 : __torch__.transformers.modeling_bert.___torch_mangle_6985.BertOutput = prim::GetAttr[name="output"](%54)
  %2126 : __torch__.transformers.modeling_bert.___torch_mangle_6981.BertIntermediate = prim::GetAttr[name="intermediate"](%54)
  %2127 : __torch__.transformers.modeling_bert.___torch_mangle_6979.BertAttention = prim::GetAttr[name="attention"](%54)
  %2128 : __torch__.transformers.modeling_bert.___torch_mangle_6978.BertSelfOutput = prim::GetAttr[name="output"](%2127)
  %2129 : __torch__.transformers.modeling_bert.___torch_mangle_6974.BertSelfAttention = prim::GetAttr[name="self"](%2127)
  %2130 : __torch__.torch.nn.modules.linear.___torch_mangle_6972.Linear = prim::GetAttr[name="value"](%2129)
  %2131 : __torch__.torch.nn.modules.linear.___torch_mangle_6971.Linear = prim::GetAttr[name="key"](%2129)
  %2132 : __torch__.torch.nn.modules.linear.___torch_mangle_6970.Linear = prim::GetAttr[name="query"](%2129)
  %2133 : Tensor = prim::GetAttr[name="bias"](%2132)
  %2134 : Tensor = prim::GetAttr[name="weight"](%2132)
  %2135 : Float(1024:1, 1024:1024) = aten::t(%2134), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.query # torch/nn/functional.py:1676:0
  %output.139 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.234, %2135), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.query # torch/nn/functional.py:1676:0
  %x.139 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.139, %2133, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.query # torch/nn/functional.py:1678:0
  %2138 : Tensor = prim::GetAttr[name="bias"](%2131)
  %2139 : Tensor = prim::GetAttr[name="weight"](%2131)
  %2140 : Float(1024:1, 1024:1024) = aten::t(%2139), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.key # torch/nn/functional.py:1676:0
  %output.140 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.234, %2140), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.key # torch/nn/functional.py:1676:0
  %x.141 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.140, %2138, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.key # torch/nn/functional.py:1678:0
  %2143 : Tensor = prim::GetAttr[name="bias"](%2130)
  %2144 : Tensor = prim::GetAttr[name="weight"](%2130)
  %2145 : Float(1024:1, 1024:1024) = aten::t(%2144), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.value # torch/nn/functional.py:1676:0
  %output.141 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.234, %2145), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.value # torch/nn/functional.py:1676:0
  %x.143 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.141, %2143, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.value # torch/nn/functional.py:1678:0
  %2148 : int = aten::size(%x.139, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2149 : int = aten::size(%x.139, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2150 : int[] = prim::ListConstruct(%2148, %2149, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
  %x.140 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.139, %2150), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
  %2152 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
  %query_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.140, %2152), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
  %2154 : int = aten::size(%x.141, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2155 : int = aten::size(%x.141, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2156 : int[] = prim::ListConstruct(%2154, %2155, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
  %x.142 : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.141, %2156), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
  %2158 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
  %key_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x.142, %2158), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
  %2160 : int = aten::size(%x.143, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2161 : int = aten::size(%x.143, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:227:0
  %2162 : int[] = prim::ListConstruct(%2160, %2161, %11, %10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
  %x : Float(17:13312, 13:1024, 16:64, 64:1) = aten::view(%x.143, %2162), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:228:0
  %2164 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
  %value_layer : Float(17:13312, 16:64, 13:1024, 64:1) = aten::permute(%x, %2164), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:229:0
  %2166 : Float(17:13312, 16:64, 64:1, 13:1024) = aten::transpose(%key_layer, %12, %9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.47 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%query_layer, %2166), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::div(%attention_scores.47, %8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:259:0
  %input.235 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:262:0
  %input.236 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.235, %12, %19), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.236, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self/__module.bert.encoder.layer.23.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.47 : Float(17:13312, 16:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:275:0
  %2173 : int[] = prim::ListConstruct(%26, %23, %24, %22), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
  %2174 : Float(17:13312, 13:64, 16:832, 64:1) = aten::permute(%context_layer.47, %2173), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
  %context_layer : Float(17:13312, 13:1024, 16:64, 64:1) = aten::contiguous(%2174, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:277:0
  %2176 : int = aten::size(%context_layer, %26), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:278:0
  %2177 : int = aten::size(%context_layer, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:278:0
  %2178 : int[] = prim::ListConstruct(%2176, %2177, %15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self
  %input.237 : Float(17:13312, 13:1024, 1024:1) = aten::view(%context_layer, %2178), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.self # transformers/modeling_bert.py:279:0
  %2180 : __torch__.torch.nn.modules.normalization.___torch_mangle_6976.LayerNorm = prim::GetAttr[name="LayerNorm"](%2128)
  %2181 : __torch__.torch.nn.modules.linear.___torch_mangle_6975.Linear = prim::GetAttr[name="dense"](%2128)
  %2182 : Tensor = prim::GetAttr[name="bias"](%2181)
  %2183 : Tensor = prim::GetAttr[name="weight"](%2181)
  %2184 : Float(1024:1, 1024:1024) = aten::t(%2183), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1676:0
  %output.142 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.237, %2184), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1676:0
  %input.238 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.142, %2182, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.47 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.238, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.dropout # torch/nn/functional.py:973:0
  %input.239 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states.47, %input.234, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output # transformers/modeling_bert.py:295:0
  %2189 : Tensor = prim::GetAttr[name="bias"](%2180)
  %2190 : Tensor = prim::GetAttr[name="weight"](%2180)
  %2191 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.LayerNorm
  %input_tensor : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.239, %2191, %2190, %2189, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.attention/__module.bert.encoder.layer.23.attention.output/__module.bert.encoder.layer.23.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %2193 : __torch__.torch.nn.modules.linear.___torch_mangle_6980.Linear = prim::GetAttr[name="dense"](%2126)
  %2194 : Tensor = prim::GetAttr[name="bias"](%2193)
  %2195 : Tensor = prim::GetAttr[name="weight"](%2193)
  %2196 : Float(1024:1, 4096:1024) = aten::t(%2195), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate/__module.bert.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1676:0
  %output.143 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input_tensor, %2196), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate/__module.bert.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1676:0
  %input.240 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.143, %2194, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate/__module.bert.encoder.layer.23.intermediate.dense # torch/nn/functional.py:1678:0
  %input.241 : Float(17:53248, 13:4096, 4096:1) = aten::gelu(%input.240), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.intermediate # torch/nn/functional.py:1369:0
  %2200 : __torch__.torch.nn.modules.normalization.___torch_mangle_6983.LayerNorm = prim::GetAttr[name="LayerNorm"](%2125)
  %2201 : __torch__.torch.nn.modules.linear.___torch_mangle_6982.Linear = prim::GetAttr[name="dense"](%2125)
  %2202 : Tensor = prim::GetAttr[name="bias"](%2201)
  %2203 : Tensor = prim::GetAttr[name="weight"](%2201)
  %2204 : Float(4096:1, 1024:4096) = aten::t(%2203), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dense # torch/nn/functional.py:1676:0
  %output.144 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%input.241, %2204), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dense # torch/nn/functional.py:1676:0
  %input.242 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.144, %2202, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.242, %16, %20), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.dropout # torch/nn/functional.py:973:0
  %input.243 : Float(17:13312, 13:1024, 1024:1) = aten::add(%hidden_states, %input_tensor, %24), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output # transformers/modeling_bert.py:371:0
  %2209 : Tensor = prim::GetAttr[name="bias"](%2200)
  %2210 : Tensor = prim::GetAttr[name="weight"](%2200)
  %2211 : int[] = prim::ListConstruct(%15), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.LayerNorm
  %input : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.243, %2211, %2210, %2209, %14, %13), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.23/__module.bert.encoder.layer.23.output/__module.bert.encoder.layer.23.output.LayerNorm # torch/nn/functional.py:2048:0
  %2213 : int = prim::Constant[value=1](), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1678:0
  %2214 : Tensor = prim::GetAttr[name="bias"](%3)
  %2215 : __torch__.torch.nn.modules.linear.___torch_mangle_6989.Linear = prim::GetAttr[name="decoder"](%3)
  %2216 : Tensor = prim::GetAttr[name="weight"](%2215)
  %2217 : Float(1024:1, 50358:1024) = aten::t(%2216), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1676:0
  %output : Float(17:654654, 13:50358, 50358:1) = aten::matmul(%input, %2217), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1676:0
  %2219 : Float(17:654654, 13:50358, 50358:1) = aten::add_(%output, %2214, %2213), scope: __module.lm_head/__module.lm_head.decoder # torch/nn/functional.py:1678:0
  %7 : (Float(17:654654, 13:50358, 50358:1)) = prim::TupleConstruct(%2219)
  return (%7)
