XLNetForTokenClassification(
  (transformer): XLNetModel(
    (word_embedding): Embedding(32000, 1024)
    (layer): ModuleList(
      (0): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (12): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (13): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (14): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (15): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (16): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (17): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (18): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (19): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (20): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (21): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (22): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (23): XLNetLayer(
        (rel_attn): XLNetRelativeAttention(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (ff): XLNetFeedForward(
          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (classifier): Linear(in_features=1024, out_features=2, bias=True)
)

XLNetForTokenClassification._actual_script_module
XLNetForTokenClassification.forward
  graph(%self.1 : __torch__.transformers.modeling_xlnet.XLNetForTokenClassification,
        %input_ids.1 : Long(17:13, 13:1),
        %attention_mask : Long(17:13, 13:1)):
    %6277 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="classifier"](%self.1)
    %6274 : __torch__.transformers.modeling_xlnet.XLNetModel = prim::GetAttr[name="transformer"](%self.1)
    %6829 : (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) = prim::CallMethod[name="forward"](%6274, %input_ids.1, %attention_mask)
    %6804 : Float(17:13312, 13:1024, 1024:1), %6805 : Float(13:17408, 17:1024, 1024:1), %6806 : Float(13:17408, 17:1024, 1024:1), %6807 : Float(13:17408, 17:1024, 1024:1), %6808 : Float(13:17408, 17:1024, 1024:1), %6809 : Float(13:17408, 17:1024, 1024:1), %6810 : Float(13:17408, 17:1024, 1024:1), %6811 : Float(13:17408, 17:1024, 1024:1), %6812 : Float(13:17408, 17:1024, 1024:1), %6813 : Float(13:17408, 17:1024, 1024:1), %6814 : Float(13:17408, 17:1024, 1024:1), %6815 : Float(13:17408, 17:1024, 1024:1), %6816 : Float(13:17408, 17:1024, 1024:1), %6817 : Float(13:17408, 17:1024, 1024:1), %6818 : Float(13:17408, 17:1024, 1024:1), %6819 : Float(13:17408, 17:1024, 1024:1), %6820 : Float(13:17408, 17:1024, 1024:1), %6821 : Float(13:17408, 17:1024, 1024:1), %6822 : Float(13:17408, 17:1024, 1024:1), %6823 : Float(13:17408, 17:1024, 1024:1), %6824 : Float(13:17408, 17:1024, 1024:1), %6825 : Float(13:17408, 17:1024, 1024:1), %6826 : Float(13:17408, 17:1024, 1024:1), %6827 : Float(13:17408, 17:1024, 1024:1), %6828 : Float(13:17408, 17:1024, 1024:1) = prim::TupleUnpack(%6829)
    %6830 : Tensor = prim::CallMethod[name="forward"](%6277, %6804)
    %5399 : (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%6805, %6806, %6807, %6808, %6809, %6810, %6811, %6812, %6813, %6814, %6815, %6816, %6817, %6818, %6819, %6820, %6821, %6822, %6823, %6824, %6825, %6826, %6827, %6828)
    %5400 : (Float(17:26, 13:2, 2:1), (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1))) = prim::TupleConstruct(%6830, %5399)
    return (%5400)

XLNetForTokenClassification.classifier
Linear._actual_script_module
  graph(%self : __torch__.torch.nn.modules.linear.Linear,
        %5 : Float(17:13312, 13:1024, 1024:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self)
    %2 : Tensor = prim::GetAttr[name="weight"](%self)
    %3 : Float(1024:1, 2:1024) = aten::t(%2), scope: __module.classifier # torch/nn/functional.py:1676:0
    %output : Float(17:26, 13:2, 2:1) = aten::matmul(%5, %3), scope: __module.classifier # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
    %7 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1, %6), scope: __module.classifier # torch/nn/functional.py:1678:0
    return (%7)

XLNetForTokenClassification.transformer
XLNetModel._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_xlnet.XLNetModel,
        %input_ids.1 : Long(17:13, 13:1),
        %attention_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %2 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="23"](%1)
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %4 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="22"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %6 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="21"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %8 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="20"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %10 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="19"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %12 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="18"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %14 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="17"](%13)
    %15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %16 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="16"](%15)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %18 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="15"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %20 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="14"](%19)
    %21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %22 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="13"](%21)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %24 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="12"](%23)
    %25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %26 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="11"](%25)
    %27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %28 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="10"](%27)
    %29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %30 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="9"](%29)
    %31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %32 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="8"](%31)
    %33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %34 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="7"](%33)
    %35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %36 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="6"](%35)
    %37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %38 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="5"](%37)
    %39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %40 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="4"](%39)
    %41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %42 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="3"](%41)
    %43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %44 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="2"](%43)
    %45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %46 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="1"](%45)
    %47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.2)
    %48 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="0"](%47)
    %49 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.2)
    %50 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embedding"](%self.2)
    %51 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %52 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %53 : Long(13:1, 17:13) = aten::transpose(%input_ids.1, %51, %52), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %input_ids : Long(13:17, 17:1) = aten::contiguous(%53, %55), scope: __module.transformer # transformers/modeling_xlnet.py:1101:0
    %57 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
    %58 : int = aten::size(%input_ids, %57), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
    %qlen : Long() = prim::NumToTensor(%58), scope: __module.transformer
    %60 : int = aten::Int(%qlen), scope: __module.transformer
    %67 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
    %68 : int = aten::size(%input_ids, %67), scope: __module.transformer # transformers/modeling_xlnet.py:1102:0
    %bsz : Long() = prim::NumToTensor(%68), scope: __module.transformer
    %70 : int = aten::Int(%bsz), scope: __module.transformer
    %71 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
    %72 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
    %73 : Long(13:1, 17:13) = aten::transpose(%attention_mask, %71, %72), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
    %76 : Long(13:17, 17:1) = aten::contiguous(%73, %75), scope: __module.transformer # transformers/modeling_xlnet.py:1111:0
    %77 : Long() = prim::Constant[value={0}](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
    %klen.1 : Long() = aten::add(%qlen, %77, %78), scope: __module.transformer # transformers/modeling_xlnet.py:1116:0
    %80 : Scalar = aten::ScalarImplicit(%klen.1), scope: __module.transformer
    %81 : float = prim::Constant[value=1.](), scope: __module.transformer # torch/tensor.py:396:0
    %82 : int = prim::Constant[value=1](), scope: __module.transformer # torch/tensor.py:396:0
    %input_mask : Float(13:17, 17:1) = aten::rsub(%76, %81, %82), scope: __module.transformer # torch/tensor.py:396:0
    %84 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1139:0
    %data_mask : Float(1:221, 13:17, 17:1) = aten::unsqueeze(%input_mask, %84), scope: __module.transformer # transformers/modeling_xlnet.py:1139:0
    %86 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %87 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %88 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %89 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %90 : Float(1:221, 13:17, 17:1) = aten::slice(%data_mask, %86, %87, %88, %89), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %91 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %92 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %93 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %94 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %95 : Float(1:221, 13:17, 17:1) = aten::slice(%90, %91, %92, %93, %94), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %96 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %97 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %98 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %99 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %100 : Float(1:221, 13:17, 17:1) = aten::slice(%95, %96, %97, %98, %99), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %101 : int = prim::Constant[value=3](), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %102 : Float(1:221, 13:17, 17:1, 1:1) = aten::unsqueeze(%100, %101), scope: __module.transformer # transformers/modeling_xlnet.py:1151:0
    %103 : int = prim::Constant[value=0](), scope: __module.transformer # torch/tensor.py:22:0
    %104 : Bool(1:221, 13:17, 17:1, 1:1) = aten::gt(%102, %103), scope: __module.transformer # torch/tensor.py:22:0
    %105 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
    %106 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
    %107 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
    %108 : None = prim::Constant(), scope: __module.transformer
    %attn_mask : Float(1:221, 13:17, 17:1, 1:1) = aten::to(%104, %105, %106, %107, %108), scope: __module.transformer # transformers/modeling_xlnet.py:1156:0
    %110 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %111 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %112 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %113 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %114 : Float(13:13, 13:1) = aten::eye(%60, %110, %111, %112, %113), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %115 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %116 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %117 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %118 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %119 : None = prim::Constant(), scope: __module.transformer
    %120 : Float(13:13, 13:1) = aten::to(%114, %115, %116, %117, %118, %119), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %non_tgt_mask : Float(13:13, 13:1) = aten::neg(%120), scope: __module.transformer # transformers/modeling_xlnet.py:1159:0
    %122 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %123 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %124 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %125 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %126 : Float(13:13, 13:1) = aten::slice(%non_tgt_mask, %122, %123, %124, %125), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %127 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %128 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %129 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %130 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %131 : Float(13:13, 13:1) = aten::slice(%126, %127, %128, %129, %130), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %132 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %133 : Float(13:13, 13:1, 1:1) = aten::unsqueeze(%131, %132), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %134 : int = prim::Constant[value=3](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %135 : Float(13:13, 13:1, 1:1, 1:1) = aten::unsqueeze(%133, %134), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %136 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %137 : Float(13:221, 13:17, 17:1, 1:1) = aten::add(%attn_mask, %135, %136), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %138 : int = prim::Constant[value=0](), scope: __module.transformer # torch/tensor.py:22:0
    %139 : Bool(13:221, 13:17, 17:1, 1:1) = aten::gt(%137, %138), scope: __module.transformer # torch/tensor.py:22:0
    %140 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %141 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %142 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %143 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %144 : None = prim::Constant(), scope: __module.transformer
    %145 : Float(13:221, 13:17, 17:1, 1:1) = aten::to(%139, %140, %141, %142, %143, %144), scope: __module.transformer # transformers/modeling_xlnet.py:1162:0
    %428 : Tensor = prim::CallMethod[name="forward"](%50, %input_ids)
    %429 : Tensor = prim::CallMethod[name="forward"](%49, %428)
    %148 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %149 : int = prim::Constant[value=1024](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %150 : float = prim::Constant[value=2.](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %151 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %152 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %153 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %154 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %freq_seq : Float(512:1) = aten::arange(%148, %149, %150, %151, %152, %153, %154), scope: __module.transformer # transformers/modeling_xlnet.py:1028:0
    %156 : Long() = prim::Constant[value={1024}](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
    %157 : Float(512:1) = aten::div(%freq_seq, %156), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
    %158 : int = prim::Constant[value=10000](), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
    %159 : Float(512:1) = aten::pow(%158, %157), scope: __module.transformer # transformers/modeling_xlnet.py:1029:0
    %160 : Float(512:1) = aten::reciprocal(%159), scope: __module.transformer # torch/tensor.py:400:0
    %161 : Long() = prim::Constant[value={1}](), scope: __module.transformer # torch/tensor.py:400:0
    %162 : Float(512:1) = aten::mul(%160, %161), scope: __module.transformer # torch/tensor.py:400:0
    %end : Long() = aten::neg(%qlen), scope: __module.transformer # transformers/modeling_xlnet.py:1033:0
    %164 : Scalar = aten::ScalarImplicit(%end), scope: __module.transformer
    %165 : float = prim::Constant[value=-1.](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %166 : None = prim::Constant(), scope: __module.transformer
    %167 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %168 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %169 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %170 : Float(26:1) = aten::arange(%80, %164, %165, %166, %167, %168, %169), scope: __module.transformer # transformers/modeling_xlnet.py:1057:0
    %171 : str = prim::Constant[value="i,d->id"](), scope: __module.transformer # torch/functional.py:327:0
    %172 : Tensor[] = prim::ListConstruct(%170, %162), scope: __module.transformer
    %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%171, %172), scope: __module.transformer # torch/functional.py:327:0
    %174 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
    %175 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
    %176 : Tensor[] = prim::ListConstruct(%174, %175), scope: __module.transformer
    %177 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
    %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%176, %177), scope: __module.transformer # transformers/modeling_xlnet.py:1018:0
    %179 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %180 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %181 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %182 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %183 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %179, %180, %181, %182), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %184 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %185 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%183, %184), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %186 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %187 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %188 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %189 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%185, %186, %187, %188, %189), scope: __module.transformer # transformers/modeling_xlnet.py:1019:0
    %191 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
    %192 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
    %193 : int[] = prim::ListConstruct(%191, %70, %192), scope: __module.transformer
    %194 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
    %pos_emb : Float(26:1024, 17:0, 1024:1) = aten::expand(%pos_emb.2, %193, %194), scope: __module.transformer # transformers/modeling_xlnet.py:1022:0
    %196 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %197 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %198 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %199 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %200 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %201 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %202 : None = prim::Constant(), scope: __module.transformer
    %input.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%pos_emb, %196, %197, %198, %199, %200, %201, %202), scope: __module.transformer # transformers/modeling_xlnet.py:1062:0
    %430 : Tensor = prim::CallMethod[name="forward1"](%49, %input.2)
    %205 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %206 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %207 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %208 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.1 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%429, %205, %206, %207, %208), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %210 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.1), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %431 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%48, %429, %430, %145)
    %212 : Float(13:17408, 17:1024, 1024:1), %213 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%431)
    %214 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %215 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %216 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %217 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.2 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%212, %214, %215, %216, %217), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %219 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.2), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %432 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%46, %212, %213, %145)
    %221 : Float(13:17408, 17:1024, 1024:1), %222 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%432)
    %223 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %224 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %225 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %226 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.3 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%221, %223, %224, %225, %226), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %228 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.3), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %433 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%44, %221, %222, %145)
    %230 : Float(13:17408, 17:1024, 1024:1), %231 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%433)
    %232 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %233 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %234 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %235 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.4 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%230, %232, %233, %234, %235), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %237 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.4), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %434 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%42, %230, %231, %145)
    %239 : Float(13:17408, 17:1024, 1024:1), %240 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%434)
    %241 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %242 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %243 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %244 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.5 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%239, %241, %242, %243, %244), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %246 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.5), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %435 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%40, %239, %240, %145)
    %248 : Float(13:17408, 17:1024, 1024:1), %249 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%435)
    %250 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %251 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %252 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %253 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.6 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%248, %250, %251, %252, %253), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %255 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.6), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %436 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%38, %248, %249, %145)
    %257 : Float(13:17408, 17:1024, 1024:1), %258 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%436)
    %259 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %260 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %261 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %262 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.7 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%257, %259, %260, %261, %262), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %264 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.7), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %437 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%36, %257, %258, %145)
    %266 : Float(13:17408, 17:1024, 1024:1), %267 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%437)
    %268 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %269 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %270 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %271 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.8 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%266, %268, %269, %270, %271), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %273 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.8), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %438 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%34, %266, %267, %145)
    %275 : Float(13:17408, 17:1024, 1024:1), %276 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%438)
    %277 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %278 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %279 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %280 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.9 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%275, %277, %278, %279, %280), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %282 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.9), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %439 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%32, %275, %276, %145)
    %284 : Float(13:17408, 17:1024, 1024:1), %285 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%439)
    %286 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %287 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %288 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %289 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.10 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%284, %286, %287, %288, %289), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %291 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.10), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %440 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%30, %284, %285, %145)
    %293 : Float(13:17408, 17:1024, 1024:1), %294 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%440)
    %295 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %296 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %297 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %298 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.11 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%293, %295, %296, %297, %298), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %300 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.11), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %441 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%28, %293, %294, %145)
    %302 : Float(13:17408, 17:1024, 1024:1), %303 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%441)
    %304 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %305 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %306 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %307 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.12 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%302, %304, %305, %306, %307), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %309 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.12), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %442 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%26, %302, %303, %145)
    %311 : Float(13:17408, 17:1024, 1024:1), %312 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%442)
    %313 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %314 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %315 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %316 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.13 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%311, %313, %314, %315, %316), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %318 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.13), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %443 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%24, %311, %312, %145)
    %320 : Float(13:17408, 17:1024, 1024:1), %321 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%443)
    %322 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %323 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %324 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %325 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.14 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%320, %322, %323, %324, %325), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %327 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.14), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %444 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%22, %320, %321, %145)
    %329 : Float(13:17408, 17:1024, 1024:1), %330 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%444)
    %331 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %332 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %333 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %334 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.15 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%329, %331, %332, %333, %334), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %336 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.15), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %445 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%20, %329, %330, %145)
    %338 : Float(13:17408, 17:1024, 1024:1), %339 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%445)
    %340 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %341 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %342 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %343 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.16 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%338, %340, %341, %342, %343), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %345 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.16), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %446 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%18, %338, %339, %145)
    %347 : Float(13:17408, 17:1024, 1024:1), %348 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%446)
    %349 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %350 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %351 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %352 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.17 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%347, %349, %350, %351, %352), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %354 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.17), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %447 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%16, %347, %348, %145)
    %356 : Float(13:17408, 17:1024, 1024:1), %357 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%447)
    %358 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %359 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %360 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %361 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.18 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%356, %358, %359, %360, %361), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %363 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.18), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %448 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%14, %356, %357, %145)
    %365 : Float(13:17408, 17:1024, 1024:1), %366 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%448)
    %367 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %368 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %369 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %370 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.19 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%365, %367, %368, %369, %370), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %372 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.19), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %449 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%12, %365, %366, %145)
    %374 : Float(13:17408, 17:1024, 1024:1), %375 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%449)
    %376 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %377 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %378 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %379 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.20 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%374, %376, %377, %378, %379), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %381 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.20), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %450 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%10, %374, %375, %145)
    %383 : Float(13:17408, 17:1024, 1024:1), %384 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%450)
    %385 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %386 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %387 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %388 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.21 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%383, %385, %386, %387, %388), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %390 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.21), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %451 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%8, %383, %384, %145)
    %392 : Float(13:17408, 17:1024, 1024:1), %393 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%451)
    %394 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %395 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %396 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %397 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.22 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%392, %394, %395, %396, %397), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %399 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.22), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %452 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6, %392, %393, %145)
    %401 : Float(13:17408, 17:1024, 1024:1), %402 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%452)
    %403 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %404 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %405 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %406 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem.23 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%401, %403, %404, %405, %406), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %408 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.23), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %453 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%4, %401, %402, %145)
    %410 : Float(13:17408, 17:1024, 1024:1), %411 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%453)
    %412 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %413 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %414 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %415 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %new_mem : Float(13:17408, 17:1024, 1024:1) = aten::slice(%410, %412, %413, %414, %415), scope: __module.transformer # transformers/modeling_xlnet.py:1009:0
    %417 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem), scope: __module.transformer # transformers/modeling_xlnet.py:1013:0
    %454 : Tensor = prim::CallMethod[name="forward"](%2, %410, %411, %145)
    %455 : Tensor = prim::CallMethod[name="forward2"](%49, %454)
    %420 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %421 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %422 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %423 : int[] = prim::ListConstruct(%420, %421, %422), scope: __module.transformer
    %424 : Float(17:1024, 13:17408, 1024:1) = aten::permute(%455, %423), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %425 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %input : Float(17:13312, 13:1024, 1024:1) = aten::contiguous(%424, %425), scope: __module.transformer # transformers/modeling_xlnet.py:1253:0
    %427 : (Float(17:13312, 13:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%input, %210, %219, %228, %237, %246, %255, %264, %273, %282, %291, %300, %309, %318, %327, %336, %345, %354, %363, %372, %381, %390, %399, %408, %417)
    return (%427)

XLNetModel.dropout
Dropout._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
    %curr_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%1, %2, %3), scope: __module.transformer/__module.transformer.dropout # torch/nn/functional.py:973:0
    return (%curr_out.1)

XLNetModel.word_embedding
Embedding._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(13:17, 17:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.3)
    %3 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
    %input.1 : Float(13:17408, 17:1024, 1024:1) = aten::embedding(%2, %input_ids, %3, %4, %5), scope: __module.transformer/__module.transformer.word_embedding # torch/nn/functional.py:1814:0
    return (%input.1)

ModuleList.*
Dropout.*
  module had no methods with graph attrs.

XLNetLayer._actual_script_module
  graph(%self.6 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.6)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.6)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.11)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.11)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.11)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.11)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.8 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.8)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
    %input.11 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.11)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.7 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.7)
    %5 : Tensor = prim::GetAttr[name="o"](%self.7)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.7)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.7)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.7)
    %9 : Tensor = prim::GetAttr[name="r"](%self.7)
    %10 : Tensor = prim::GetAttr[name="v"](%self.7)
    %11 : Tensor = prim::GetAttr[name="k"](%self.7)
    %12 : Tensor = prim::GetAttr[name="q"](%self.7)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %q_head.1 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %r.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.2, %9), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %8, %31), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %ac.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %7, %36), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %x.1 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.1, %50), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.2 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.2), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.1, %54), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.1, %59), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.1, %64), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.1, %69), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %x.2 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %73), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.3 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %x.4 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %99), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %106, %105), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %108), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.1, %119, %120), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %input.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %122, %123), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.4)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn
    %input.5 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.5)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.6)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.2)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.8 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.4 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %2, %3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.10 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.6 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.10)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.10)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.6, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.rel_attn/__module.transformer.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.8 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
    %input.9 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.8, %2, %3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.dropout # torch/nn/functional.py:973:0
    return (%input.9)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.12)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.1 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.7 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.1, %2, %6), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.7)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.10 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %2, %6), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.10)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.11 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.11, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.0/__module.transformer.layer.0.ff/__module.transformer.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.2)

XLNetLayer._actual_script_module
  graph(%self.17 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.17)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.17)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.22 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.22)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.22)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.22)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.22)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.17 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.17)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
    %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.20)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.18 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.18)
    %5 : Tensor = prim::GetAttr[name="o"](%self.18)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.18)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.18)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.18)
    %9 : Tensor = prim::GetAttr[name="r"](%self.18)
    %10 : Tensor = prim::GetAttr[name="v"](%self.18)
    %11 : Tensor = prim::GetAttr[name="k"](%self.18)
    %12 : Tensor = prim::GetAttr[name="q"](%self.18)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %q_head.2 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %r.3 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.3, %9), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %8, %31), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %ac.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %7, %36), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %x.5 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.2, %50), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.3 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.5, %54), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.5, %59), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.5, %64), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.5, %69), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %x.6 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %73), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.7 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %x.8 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %99), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %106, %105), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %108), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.2, %119, %120), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %input.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %122, %123), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.13)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn
    %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.14)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.15 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.15)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.3)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.13 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %2, %3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.15 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.21)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.21)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.15, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.rel_attn/__module.transformer.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.17 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
    %input.18 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.17, %2, %3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.dropout # torch/nn/functional.py:973:0
    return (%input.18)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.23)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.23)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.4 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.16 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.4, %2, %6), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.16)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.25 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.25)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.25)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.5 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.5, %2, %6), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.19)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.20 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.27)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.27)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.20, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.1/__module.transformer.layer.1.ff/__module.transformer.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.3)

XLNetLayer._actual_script_module
  graph(%self.28 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.28)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.28)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.33 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.33)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.33)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.33)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.33)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.26 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.26)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
    %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.29)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.29 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.29)
    %5 : Tensor = prim::GetAttr[name="o"](%self.29)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.29)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.29)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.29)
    %9 : Tensor = prim::GetAttr[name="r"](%self.29)
    %10 : Tensor = prim::GetAttr[name="v"](%self.29)
    %11 : Tensor = prim::GetAttr[name="k"](%self.29)
    %12 : Tensor = prim::GetAttr[name="q"](%self.29)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %q_head.3 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %r.4 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.4, %9), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %8, %31), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %ac.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %7, %36), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %x.9 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.3, %50), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.4 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.9, %54), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.9, %59), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.9, %64), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.9, %69), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %x.10 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %73), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.11 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %x.12 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %99), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %106, %105), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %108), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.3, %119, %120), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %input.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %122, %123), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.22)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn
    %input.23 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.23)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.24)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.4)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.22 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %2, %3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.24 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.24, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.rel_attn/__module.transformer.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.26 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
    %input.27 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.26, %2, %3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.dropout # torch/nn/functional.py:973:0
    return (%input.27)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.7 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.25 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.7, %2, %6), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.25)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.36)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %2, %6), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.28)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.29 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.29, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.2/__module.transformer.layer.2.ff/__module.transformer.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.4)

XLNetLayer._actual_script_module
  graph(%self.39 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.39)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.39)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.44 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.44)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.44)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.44)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.44)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.35 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.35)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
    %input.38 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.38)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.40 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.40)
    %5 : Tensor = prim::GetAttr[name="o"](%self.40)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.40)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.40)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.40)
    %9 : Tensor = prim::GetAttr[name="r"](%self.40)
    %10 : Tensor = prim::GetAttr[name="v"](%self.40)
    %11 : Tensor = prim::GetAttr[name="k"](%self.40)
    %12 : Tensor = prim::GetAttr[name="q"](%self.40)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %q_head.4 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %r.5 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.5, %9), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %8, %31), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %ac.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %7, %36), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %x.13 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.4, %50), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.5 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.5), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.13, %54), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.13, %59), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.13, %64), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.13, %69), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %x.14 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %73), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.15 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %x.16 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %99), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %106, %105), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %108), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.4, %119, %120), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %input.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %122, %123), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.31)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn
    %input.32 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.32)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.33 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.33)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.5)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.41 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.31 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %2, %3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.33 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.43)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.33, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.rel_attn/__module.transformer.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.35 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
    %input.36 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.35, %2, %3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.dropout # torch/nn/functional.py:973:0
    return (%input.36)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.45)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.45)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.10 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.10, %2, %6), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.34)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.47 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.47)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.47)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.11 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.37 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.11, %2, %6), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.37)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.38 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.49)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.49)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.38, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.3/__module.transformer.layer.3.ff/__module.transformer.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.5)

XLNetLayer._actual_script_module
  graph(%self.50 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.50)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.50)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.55 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.55)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.55)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.55)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.55)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.44)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
    %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.47)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.51 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.51)
    %5 : Tensor = prim::GetAttr[name="o"](%self.51)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.51)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.51)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.51)
    %9 : Tensor = prim::GetAttr[name="r"](%self.51)
    %10 : Tensor = prim::GetAttr[name="v"](%self.51)
    %11 : Tensor = prim::GetAttr[name="k"](%self.51)
    %12 : Tensor = prim::GetAttr[name="q"](%self.51)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %q_head.5 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %r.6 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.6, %9), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %8, %31), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %ac.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %7, %36), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %x.17 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.5, %50), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.6 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.6), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.17, %54), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.17, %59), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.17, %64), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.17, %69), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %x.18 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %73), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.19 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %x.20 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %99), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %106, %105), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %108), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.5, %119, %120), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %input.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %122, %123), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.40)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn
    %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.41)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.42)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.6)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.40 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %2, %3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.42 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.42, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.rel_attn/__module.transformer.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.44 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
    %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %2, %3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.dropout # torch/nn/functional.py:973:0
    return (%input.45)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.56 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.56)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.56)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.13 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.13, %2, %6), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.43)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.58)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %2, %6), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.46)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.47 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.4/__module.transformer.layer.4.ff/__module.transformer.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.6)

XLNetLayer._actual_script_module
  graph(%self.61 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.61)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.61)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.66 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.66)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.66)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.66)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.66)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.53 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.53)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
    %input.56 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.56)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.62 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.62)
    %5 : Tensor = prim::GetAttr[name="o"](%self.62)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.62)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.62)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.62)
    %9 : Tensor = prim::GetAttr[name="r"](%self.62)
    %10 : Tensor = prim::GetAttr[name="v"](%self.62)
    %11 : Tensor = prim::GetAttr[name="k"](%self.62)
    %12 : Tensor = prim::GetAttr[name="q"](%self.62)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %q_head.6 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %r.7 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.7, %9), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %8, %31), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %ac.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %7, %36), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %x.21 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.6, %50), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.7 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.7), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.21, %54), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.21, %59), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.21, %64), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.21, %69), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %x.22 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %73), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.23 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %x.24 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %99), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %106, %105), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %108), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.48 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.6, %119, %120), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %input.49 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %122, %123), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.49)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn
    %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.50)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.51)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.7)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.49 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %2, %3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.51 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.51, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.rel_attn/__module.transformer.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.53 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
    %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.53, %2, %3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.dropout # torch/nn/functional.py:973:0
    return (%input.54)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.67)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.67)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.16 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.52 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.16, %2, %6), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.52)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.69)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.69)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.17 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.55 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.17, %2, %6), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.55)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.71 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.56 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.71)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.71)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.56, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.5/__module.transformer.layer.5.ff/__module.transformer.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.7)

XLNetLayer._actual_script_module
  graph(%self.72 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.72)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.72)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.77 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.77)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.77)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.77)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.77)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.62 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.62)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
    %input.65 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.65)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.73 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.73)
    %5 : Tensor = prim::GetAttr[name="o"](%self.73)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.73)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.73)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.73)
    %9 : Tensor = prim::GetAttr[name="r"](%self.73)
    %10 : Tensor = prim::GetAttr[name="v"](%self.73)
    %11 : Tensor = prim::GetAttr[name="k"](%self.73)
    %12 : Tensor = prim::GetAttr[name="q"](%self.73)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %q_head.7 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %r.8 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.8, %9), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %8, %31), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %ac.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %7, %36), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %x.25 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.7, %50), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.8 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.8), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.25, %54), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.25, %59), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.25, %64), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.25, %69), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %x.26 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %73), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.27 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %x.28 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %99), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %106, %105), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %108), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.7, %119, %120), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %input.58 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %122, %123), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.58)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn
    %input.59 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.59)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.60 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.60)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.8)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.58 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %2, %3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.60 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.60, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.rel_attn/__module.transformer.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.7)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.62 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
    %input.63 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.62, %2, %3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.dropout # torch/nn/functional.py:973:0
    return (%input.63)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.19 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.61 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.19, %2, %6), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.61)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %2, %6), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.64)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.65 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.65, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.6/__module.transformer.layer.6.ff/__module.transformer.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.8)

XLNetLayer._actual_script_module
  graph(%self.83 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.83)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.83)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.88 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.88)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.88)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.88)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.88)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.71 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.71)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
    %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.74)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.84 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.84)
    %5 : Tensor = prim::GetAttr[name="o"](%self.84)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.84)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.84)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.84)
    %9 : Tensor = prim::GetAttr[name="r"](%self.84)
    %10 : Tensor = prim::GetAttr[name="v"](%self.84)
    %11 : Tensor = prim::GetAttr[name="k"](%self.84)
    %12 : Tensor = prim::GetAttr[name="q"](%self.84)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %q_head.8 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %r.9 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.9, %9), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %8, %31), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %ac.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %7, %36), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %x.29 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.8, %50), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.9 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.9), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.29, %54), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.29, %59), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.29, %64), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.29, %69), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %x.30 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %73), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.31 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %x.32 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %99), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %106, %105), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %108), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.8, %119, %120), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %input.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %122, %123), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.67)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn
    %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.68)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.69)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.9)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.67 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %2, %3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.rel_attn/__module.transformer.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.8)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.71 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
    %input.72 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.71, %2, %3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.dropout # torch/nn/functional.py:973:0
    return (%input.72)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.22 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.70 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.22, %2, %6), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.70)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.91 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.91)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.91)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.23 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.23, %2, %6), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.73)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.74 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.7/__module.transformer.layer.7.ff/__module.transformer.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.9)

XLNetLayer._actual_script_module
  graph(%self.94 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.94)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.94)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.99 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.99)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.99)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.99)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.99)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.80 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.80)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
    %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.83)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.95 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.95)
    %5 : Tensor = prim::GetAttr[name="o"](%self.95)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.95)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.95)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.95)
    %9 : Tensor = prim::GetAttr[name="r"](%self.95)
    %10 : Tensor = prim::GetAttr[name="v"](%self.95)
    %11 : Tensor = prim::GetAttr[name="k"](%self.95)
    %12 : Tensor = prim::GetAttr[name="q"](%self.95)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %q_head.9 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %r.10 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.10, %9), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %8, %31), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %ac.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %7, %36), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %x.33 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.9, %50), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.10 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.10), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.33, %54), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.33, %59), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.33, %64), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.33, %69), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %x.34 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %73), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.35 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %x.36 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %99), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %106, %105), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %108), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.9, %119, %120), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %122, %123), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.76)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn
    %input.77 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.77)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.78 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.78)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.10)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.96 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.76 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %2, %3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.78 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.98)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.98)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.78, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.rel_attn/__module.transformer.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.101 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.80 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
    %input.81 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.80, %2, %3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.dropout # torch/nn/functional.py:973:0
    return (%input.81)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.100)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.25 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.79 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.25, %2, %6), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.79)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.102)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.102)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.82 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %2, %6), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.82)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.83 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.83, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.8/__module.transformer.layer.8.ff/__module.transformer.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.10)

XLNetLayer._actual_script_module
  graph(%self.105 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.105)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.105)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.110 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.110)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.110)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.110)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.110)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.89)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
    %input.92 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.92)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.106 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.106)
    %5 : Tensor = prim::GetAttr[name="o"](%self.106)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.106)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.106)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.106)
    %9 : Tensor = prim::GetAttr[name="r"](%self.106)
    %10 : Tensor = prim::GetAttr[name="v"](%self.106)
    %11 : Tensor = prim::GetAttr[name="k"](%self.106)
    %12 : Tensor = prim::GetAttr[name="q"](%self.106)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %q_head.10 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %r.11 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.11, %9), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %8, %31), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %ac.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %7, %36), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %x.37 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.10, %50), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.11 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.11), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.37, %54), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.37, %59), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.37, %64), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.37, %69), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %x.38 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %73), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.39 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %x.40 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %99), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %106, %105), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %108), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.84 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.10, %119, %120), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %122, %123), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.85)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn
    %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.86)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.87 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.87)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.11)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.107 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.85 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %2, %3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.87 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.109)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.87, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.rel_attn/__module.transformer.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.10)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.89 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
    %input.90 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.89, %2, %3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.dropout # torch/nn/functional.py:973:0
    return (%input.90)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.111)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.111)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.28 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.28, %2, %6), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.88)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.113)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.113)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.29 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.29, %2, %6), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.91)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.115 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.92 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.115)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.115)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.92, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.9/__module.transformer.layer.9.ff/__module.transformer.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.11)

XLNetLayer._actual_script_module
  graph(%self.116 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.116)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.116)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.121 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.121)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.121)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.121)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.121)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.98)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
    %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.101)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.117 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.117)
    %5 : Tensor = prim::GetAttr[name="o"](%self.117)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.117)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.117)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.117)
    %9 : Tensor = prim::GetAttr[name="r"](%self.117)
    %10 : Tensor = prim::GetAttr[name="v"](%self.117)
    %11 : Tensor = prim::GetAttr[name="k"](%self.117)
    %12 : Tensor = prim::GetAttr[name="q"](%self.117)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %q_head.11 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %r.12 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.12, %9), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %8, %31), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %ac.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %7, %36), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %x.41 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.11, %50), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.12 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.12), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.41, %54), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.41, %59), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.41, %64), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.41, %69), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %x.42 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %73), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.43 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %x.44 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %99), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %106, %105), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %108), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.93 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.11, %119, %120), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %input.94 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %122, %123), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.94)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn
    %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.95)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.96)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.12)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.118 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.94 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %2, %3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.96 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.120)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.rel_attn/__module.transformer.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.11)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.98 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
    %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.98, %2, %3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.dropout # torch/nn/functional.py:973:0
    return (%input.99)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.122)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.122)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.31 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.97 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.31, %2, %6), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.97)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.124 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.124)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.124)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.100 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %2, %6), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.100)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.126 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.101 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.126)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.126)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.101, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.10/__module.transformer.layer.10.ff/__module.transformer.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.12)

XLNetLayer._actual_script_module
  graph(%self.127 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.127)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.127)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.132 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.132)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.132)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.132)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.132)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.107 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.107)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
    %input.110 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.110)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.128 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.128)
    %5 : Tensor = prim::GetAttr[name="o"](%self.128)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.128)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.128)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.128)
    %9 : Tensor = prim::GetAttr[name="r"](%self.128)
    %10 : Tensor = prim::GetAttr[name="v"](%self.128)
    %11 : Tensor = prim::GetAttr[name="k"](%self.128)
    %12 : Tensor = prim::GetAttr[name="q"](%self.128)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %q_head.12 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %r.13 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.13, %9), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %8, %31), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %ac.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %7, %36), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %x.45 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.12, %50), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.13 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.13), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.45, %54), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.45, %59), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.45, %64), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.45, %69), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %x.46 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %73), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.47 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %x.48 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %99), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %106, %105), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %108), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.12, %119, %120), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %input.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %122, %123), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.103)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn
    %input.104 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.104)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.105)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.13)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.129 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.103 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %2, %3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.131 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.105 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.131)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.131)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.105, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.rel_attn/__module.transformer.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.12)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.107 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
    %input.108 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.107, %2, %3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.dropout # torch/nn/functional.py:973:0
    return (%input.108)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.133 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.133)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.133)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.34 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.106 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.34, %2, %6), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.106)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.135 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.135)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.135)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.35 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.109 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.35, %2, %6), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.109)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.137 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.110 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.137)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.137)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.110, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.11/__module.transformer.layer.11.ff/__module.transformer.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.13)

XLNetLayer._actual_script_module
  graph(%self.138 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.138)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.138)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.143 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.143)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.143)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.143)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.143)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.116 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.116)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
    %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.119)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.139 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.139)
    %5 : Tensor = prim::GetAttr[name="o"](%self.139)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.139)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.139)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.139)
    %9 : Tensor = prim::GetAttr[name="r"](%self.139)
    %10 : Tensor = prim::GetAttr[name="v"](%self.139)
    %11 : Tensor = prim::GetAttr[name="k"](%self.139)
    %12 : Tensor = prim::GetAttr[name="q"](%self.139)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %q_head.13 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %r.14 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.14, %9), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %8, %31), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %ac.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %7, %36), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %x.49 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.13, %50), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.14 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.14), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.49, %54), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.49, %59), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.49, %64), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.49, %69), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %x.50 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %73), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.51 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %x.52 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %99), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %106, %105), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %108), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.111 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.13, %119, %120), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %input.112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %122, %123), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.112)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn
    %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.113)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.114 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.114)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.14)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.112 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %2, %3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.142 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.114 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.142)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.142)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.114, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.rel_attn/__module.transformer.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.13)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.145 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.116 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
    %input.117 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.116, %2, %3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.dropout # torch/nn/functional.py:973:0
    return (%input.117)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.144 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.144)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.144)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.37 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.115 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.37, %2, %6), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.115)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.146 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.146)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.146)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %2, %6), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.118)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.148 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.119 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.148)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.148)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.119, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.12/__module.transformer.layer.12.ff/__module.transformer.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.14)

XLNetLayer._actual_script_module
  graph(%self.149 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.149)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.149)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.154 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.154)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.154)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.154)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.154)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.125 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.125)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
    %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.128)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.150 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.150)
    %5 : Tensor = prim::GetAttr[name="o"](%self.150)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.150)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.150)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.150)
    %9 : Tensor = prim::GetAttr[name="r"](%self.150)
    %10 : Tensor = prim::GetAttr[name="v"](%self.150)
    %11 : Tensor = prim::GetAttr[name="k"](%self.150)
    %12 : Tensor = prim::GetAttr[name="q"](%self.150)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %q_head.14 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %r.15 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.15, %9), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %8, %31), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %ac.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %7, %36), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %x.53 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.14, %50), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.15 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.15), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.53, %54), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.53, %59), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.53, %64), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.53, %69), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %x.54 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %73), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.55 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %x.56 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %99), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %106, %105), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %108), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.14, %119, %120), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %input.121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %122, %123), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.121)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn
    %input.122 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.122)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.123)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.15)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.151 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.121 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %2, %3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.153 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.123 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.153)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.153)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.123, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.rel_attn/__module.transformer.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.14)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.156 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.125 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
    %input.126 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.125, %2, %3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.dropout # torch/nn/functional.py:973:0
    return (%input.126)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.155)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.155)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.40 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.124 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.40, %2, %6), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.124)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.157 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.157)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.157)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.41 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.41, %2, %6), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.127)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.159 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.128 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.159)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.159)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.128, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.13/__module.transformer.layer.13.ff/__module.transformer.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.15)

XLNetLayer._actual_script_module
  graph(%self.160 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.160)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.160)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.165 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.165)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.165)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.165)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.165)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.134 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.134)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
    %input.137 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.137)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.161 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.161)
    %5 : Tensor = prim::GetAttr[name="o"](%self.161)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.161)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.161)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.161)
    %9 : Tensor = prim::GetAttr[name="r"](%self.161)
    %10 : Tensor = prim::GetAttr[name="v"](%self.161)
    %11 : Tensor = prim::GetAttr[name="k"](%self.161)
    %12 : Tensor = prim::GetAttr[name="q"](%self.161)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %q_head.15 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %r.16 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.16, %9), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %8, %31), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %ac.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %7, %36), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %x.57 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.15, %50), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.16 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.16), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.57, %54), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.57, %59), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.57, %64), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.57, %69), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %x.58 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %73), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.59 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %x.60 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %99), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %106, %105), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %108), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.15, %119, %120), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %input.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %122, %123), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.130)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn
    %input.131 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.131)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.132 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.132)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.16)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.162 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.130 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %2, %3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.164 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.132 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.164)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.164)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.132, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.rel_attn/__module.transformer.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.15)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.134 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
    %input.135 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.134, %2, %3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.dropout # torch/nn/functional.py:973:0
    return (%input.135)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.166 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.166)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.166)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.43 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.43, %2, %6), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.133)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.168 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.168)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.168)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.136 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %2, %6), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.136)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.170 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.137 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.170)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.170)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.137, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.14/__module.transformer.layer.14.ff/__module.transformer.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.16)

XLNetLayer._actual_script_module
  graph(%self.171 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.171)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.171)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.176 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.176)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.176)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.176)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.176)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.143 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.143)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
    %input.146 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.146)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.172 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.172)
    %5 : Tensor = prim::GetAttr[name="o"](%self.172)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.172)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.172)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.172)
    %9 : Tensor = prim::GetAttr[name="r"](%self.172)
    %10 : Tensor = prim::GetAttr[name="v"](%self.172)
    %11 : Tensor = prim::GetAttr[name="k"](%self.172)
    %12 : Tensor = prim::GetAttr[name="q"](%self.172)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %q_head.16 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %r.17 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.17, %9), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %8, %31), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %ac.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %7, %36), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %x.61 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.16, %50), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.17 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.17), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.61, %54), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.61, %59), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.61, %64), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.61, %69), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %x.62 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %73), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.63 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %x.64 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %99), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %106, %105), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %108), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.138 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.16, %119, %120), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %input.139 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %122, %123), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.139)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn
    %input.140 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.140)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.141)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.17)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.173 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.139 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %2, %3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.175 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.141 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.175)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.175)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.141, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.rel_attn/__module.transformer.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.16)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.143 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
    %input.144 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.143, %2, %3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.dropout # torch/nn/functional.py:973:0
    return (%input.144)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.177 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.177)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.177)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.46 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.142 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.46, %2, %6), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.142)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.179 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.179)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.179)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.47 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.145 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.47, %2, %6), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.145)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.181 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.146 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.181)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.181)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.146, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.15/__module.transformer.layer.15.ff/__module.transformer.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.17)

XLNetLayer._actual_script_module
  graph(%self.182 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.182)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.182)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.187 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.187)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.187)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.187)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.187)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.152)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
    %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.155)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.183 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.183)
    %5 : Tensor = prim::GetAttr[name="o"](%self.183)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.183)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.183)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.183)
    %9 : Tensor = prim::GetAttr[name="r"](%self.183)
    %10 : Tensor = prim::GetAttr[name="v"](%self.183)
    %11 : Tensor = prim::GetAttr[name="k"](%self.183)
    %12 : Tensor = prim::GetAttr[name="q"](%self.183)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %q_head.17 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %r.18 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.18, %9), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %8, %31), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %ac.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %7, %36), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %x.65 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.17, %50), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.18 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.18), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.65, %54), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.65, %59), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.65, %64), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.65, %69), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %x.66 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %73), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.67 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %x.68 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %99), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %106, %105), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %108), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.147 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.17, %119, %120), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %input.148 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %122, %123), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.148)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn
    %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.149)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.150)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.18)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.184 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.148 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %2, %3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.186 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.150 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.186)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.186)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.150, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.rel_attn/__module.transformer.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.17)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.152 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
    %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %2, %3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.dropout # torch/nn/functional.py:973:0
    return (%input.153)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.188 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.188)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.188)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.49 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.49, %2, %6), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.151)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.190 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.190)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.190)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %2, %6), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.154)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.192 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.155 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.192)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.192)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.16/__module.transformer.layer.16.ff/__module.transformer.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.18)

XLNetLayer._actual_script_module
  graph(%self.193 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.193)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.193)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.198 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.198)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.198)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.198)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.198)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.161 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.161)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
    %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.164)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.194 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.194)
    %5 : Tensor = prim::GetAttr[name="o"](%self.194)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.194)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.194)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.194)
    %9 : Tensor = prim::GetAttr[name="r"](%self.194)
    %10 : Tensor = prim::GetAttr[name="v"](%self.194)
    %11 : Tensor = prim::GetAttr[name="k"](%self.194)
    %12 : Tensor = prim::GetAttr[name="q"](%self.194)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %q_head.18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %r.19 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.19, %9), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %8, %31), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %ac.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %7, %36), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %x.69 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.18, %50), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.19 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.19), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.69, %54), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.69, %59), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.69, %64), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.69, %69), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %x.70 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %73), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.71 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %x.72 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %99), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %106, %105), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %108), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.18, %119, %120), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %input.157 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %122, %123), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.157)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn
    %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.158)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.159)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.19)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.195 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.157 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %2, %3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.197 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.159 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.197)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.197)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.159, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.rel_attn/__module.transformer.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.18)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.200 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.161 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
    %input.162 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.161, %2, %3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.dropout # torch/nn/functional.py:973:0
    return (%input.162)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.199 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.199)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.199)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.52 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.160 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.52, %2, %6), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.160)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.201 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.201)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.201)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.53 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.53, %2, %6), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.163)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.203 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.164 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.203)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.203)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.164, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.17/__module.transformer.layer.17.ff/__module.transformer.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.19)

XLNetLayer._actual_script_module
  graph(%self.204 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.204)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.204)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.209 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.209)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.209)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.209)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.209)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.170 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.170)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
    %input.173 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.173)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.205 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.205)
    %5 : Tensor = prim::GetAttr[name="o"](%self.205)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.205)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.205)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.205)
    %9 : Tensor = prim::GetAttr[name="r"](%self.205)
    %10 : Tensor = prim::GetAttr[name="v"](%self.205)
    %11 : Tensor = prim::GetAttr[name="k"](%self.205)
    %12 : Tensor = prim::GetAttr[name="q"](%self.205)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %q_head.19 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %r.20 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.20, %9), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %8, %31), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %ac.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %7, %36), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %x.73 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.19, %50), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.20 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.20), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.73, %54), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.73, %59), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.73, %64), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.73, %69), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %x.74 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %73), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.75 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %x.76 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %99), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %106, %105), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %108), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.19, %119, %120), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %122, %123), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.166)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn
    %input.167 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.167)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.168 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.168)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.20)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.206 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.166 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %2, %3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.208 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.168 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.208)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.208)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.168, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.rel_attn/__module.transformer.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.19)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.211 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.170 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
    %input.171 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.170, %2, %3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.dropout # torch/nn/functional.py:973:0
    return (%input.171)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.210 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.210)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.210)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.55 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.55, %2, %6), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.169)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.212 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.212)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.212)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.172 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %2, %6), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.172)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.214 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.173 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.214)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.214)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.173, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.18/__module.transformer.layer.18.ff/__module.transformer.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.20)

XLNetLayer._actual_script_module
  graph(%self.215 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.215)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.215)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.220 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.220)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.220)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.220)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.220)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.179 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.179)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
    %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.182)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.216 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.216)
    %5 : Tensor = prim::GetAttr[name="o"](%self.216)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.216)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.216)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.216)
    %9 : Tensor = prim::GetAttr[name="r"](%self.216)
    %10 : Tensor = prim::GetAttr[name="v"](%self.216)
    %11 : Tensor = prim::GetAttr[name="k"](%self.216)
    %12 : Tensor = prim::GetAttr[name="q"](%self.216)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %q_head.20 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %r.21 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.21, %9), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %8, %31), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %ac.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %7, %36), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %x.77 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.20, %50), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.21 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.21), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.77, %54), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.77, %59), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.77, %64), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.77, %69), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %x.78 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %73), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.79 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %x.80 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %99), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %106, %105), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %108), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.174 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.20, %119, %120), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %122, %123), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.175)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn
    %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.176)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.177 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.177)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.21)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.217 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.175 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %2, %3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.219 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.177 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.219)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.219)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.177, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.rel_attn/__module.transformer.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.20)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.222 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.179 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
    %input.180 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.179, %2, %3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.dropout # torch/nn/functional.py:973:0
    return (%input.180)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.221 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.221)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.221)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.58 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.178 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.58, %2, %6), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.178)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.223 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.223)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.223)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.59 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.59, %2, %6), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.181)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.225 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.182 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.225)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.225)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.182, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.19/__module.transformer.layer.19.ff/__module.transformer.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.21)

XLNetLayer._actual_script_module
  graph(%self.226 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.226)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.226)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.231 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.231)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.231)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.231)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.231)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.188 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.188)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
    %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.191)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.227 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.227)
    %5 : Tensor = prim::GetAttr[name="o"](%self.227)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.227)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.227)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.227)
    %9 : Tensor = prim::GetAttr[name="r"](%self.227)
    %10 : Tensor = prim::GetAttr[name="v"](%self.227)
    %11 : Tensor = prim::GetAttr[name="k"](%self.227)
    %12 : Tensor = prim::GetAttr[name="q"](%self.227)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %q_head.21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %r.22 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.22, %9), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %8, %31), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %ac.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %7, %36), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %x.81 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.21, %50), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.22 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.22), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.81, %54), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.81, %59), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.81, %64), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.81, %69), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %x.82 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %73), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.83 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %x.84 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %99), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %106, %105), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %108), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.21, %119, %120), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %input.184 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %122, %123), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.184)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn
    %input.185 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.185)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.186)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.22)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.228 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.184 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %2, %3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.230 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.186 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.230)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.230)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.186, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.rel_attn/__module.transformer.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.21)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.233 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.188 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
    %input.189 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.188, %2, %3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.dropout # torch/nn/functional.py:973:0
    return (%input.189)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.232 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.232)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.232)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.61 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.187 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.61, %2, %6), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.187)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.234 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.234)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.234)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %2, %6), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.190)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.236 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.191 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.236)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.236)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.191, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.20/__module.transformer.layer.20.ff/__module.transformer.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.22)

XLNetLayer._actual_script_module
  graph(%self.237 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.237)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.237)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.242 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.242)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.242)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.242)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.242)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.197 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.197)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
    %input.200 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.200)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.238 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.238)
    %5 : Tensor = prim::GetAttr[name="o"](%self.238)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.238)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.238)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.238)
    %9 : Tensor = prim::GetAttr[name="r"](%self.238)
    %10 : Tensor = prim::GetAttr[name="v"](%self.238)
    %11 : Tensor = prim::GetAttr[name="k"](%self.238)
    %12 : Tensor = prim::GetAttr[name="q"](%self.238)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %q_head.22 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %r.23 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.23, %9), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %8, %31), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %ac.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %7, %36), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %x.85 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.22, %50), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.23 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.23), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.85, %54), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.85, %59), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.85, %64), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.85, %69), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %x.86 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %73), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.87 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %x.88 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %99), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %106, %105), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %108), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.192 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.22, %119, %120), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %input.193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %122, %123), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.193)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn
    %input.194 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.194)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.195)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.23)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.239 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.193 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %2, %3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.241 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.195 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.241)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.241)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.195, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.rel_attn/__module.transformer.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.22)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.244 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.197 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
    %input.198 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.197, %2, %3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.dropout # torch/nn/functional.py:973:0
    return (%input.198)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.243 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.243)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.243)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.64 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.196 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.64, %2, %6), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.196)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.245 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.245)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.245)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.65 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.199 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.65, %2, %6), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.199)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.247 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.200 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.247)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.247)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.200, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.21/__module.transformer.layer.21.ff/__module.transformer.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.23)

XLNetLayer._actual_script_module
  graph(%self.248 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.248)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.248)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%4, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.253 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.253)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.253)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.253)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.253)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.206 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.206)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
    %input.209 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.209)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.249 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.249)
    %5 : Tensor = prim::GetAttr[name="o"](%self.249)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.249)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.249)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.249)
    %9 : Tensor = prim::GetAttr[name="r"](%self.249)
    %10 : Tensor = prim::GetAttr[name="v"](%self.249)
    %11 : Tensor = prim::GetAttr[name="k"](%self.249)
    %12 : Tensor = prim::GetAttr[name="q"](%self.249)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %q_head.23 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %r : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r, %9), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %8, %31), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %ac.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %7, %36), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %x.89 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.23, %50), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.24 : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.24), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.89, %54), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.89, %59), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.89, %64), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.89, %69), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %x.90 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %73), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.91 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %x.92 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %99), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %106, %105), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %108), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.201 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.23, %119, %120), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %input.202 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %122, %123), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.202)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn
    %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.203)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.204 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.204)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.250 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.202 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %2, %3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.252 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.204 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.252)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.252)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.204, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.rel_attn/__module.transformer.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.23)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.255 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.206 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
    %input.207 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.206, %2, %3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.dropout # torch/nn/functional.py:973:0
    return (%input.207)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.254 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.254)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.254)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.67 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.205 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.67, %2, %6), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.205)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.256 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.256)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.256)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %2, %6), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.208)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.258 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.209 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.258)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.258)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.209, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.22/__module.transformer.layer.22.ff/__module.transformer.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out)

XLNetLayer._actual_script_module
  graph(%self.259 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.259)
    %5 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.259)
    %29 : Tensor = prim::CallMethod[name="forward"](%5, %1, %2, %3)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %29)
    return (%30)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.264 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.264)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.264)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.264)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.264)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.215)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
    %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.218)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.260 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.260)
    %5 : Tensor = prim::GetAttr[name="o"](%self.260)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.260)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.260)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.260)
    %9 : Tensor = prim::GetAttr[name="r"](%self.260)
    %10 : Tensor = prim::GetAttr[name="v"](%self.260)
    %11 : Tensor = prim::GetAttr[name="k"](%self.260)
    %12 : Tensor = prim::GetAttr[name="q"](%self.260)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %q_head : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %27 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%27, %9), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %8, %31), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %ac : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %7, %36), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %x.93 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac, %50), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen : Long() = prim::NumToTensor(%51), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.93, %54), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %57 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %58 : int = aten::Int(%56), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.93, %59), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %62 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %63 : int = aten::Int(%61), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.93, %64), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %67 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %68 : int = aten::Int(%66), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.93, %69), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %72 : int = aten::Int(%71), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %x.94 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %73), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %75, %76, %77, %78), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.95 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %x : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %99), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %106, %105), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %108), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score, %119, %120), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %input.211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %122, %123), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/nn/functional.py:1498:0
    %136 : Tensor = prim::CallMethod[name="forward"](%6, %input.211)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%136, %21), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn
    %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # torch/functional.py:327:0
    %137 : Tensor = prim::CallMethod[name="forward1"](%6, %input.212)
    %133 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%137, %1, %133), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
    %138 : Tensor = prim::CallMethod[name="forward"](%4, %input.213)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.261 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.211 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %2, %3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.263 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.213 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.263)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.263)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.rel_attn/__module.transformer.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.266 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.215 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
    %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.215, %2, %3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.dropout # torch/nn/functional.py:973:0
    return (%input.216)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.265 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.265)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.265)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.70 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.214 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.70, %2, %6), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.214)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.267 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.267)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.267)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.71 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.217 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.71, %2, %6), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.217)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.269 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.218 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.269)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.269)
    %4 : int = prim::Constant[value=1024](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.218, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer.23/__module.transformer.layer.23.ff/__module.transformer.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%input.219)

