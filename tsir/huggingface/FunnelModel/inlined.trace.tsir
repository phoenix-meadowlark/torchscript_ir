graph(%self.1 : __torch__.transformers.modeling_funnel.___torch_mangle_4796.FunnelModel,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_funnel.___torch_mangle_4795.FunnelDecoder = prim::GetAttr[name="decoder"](%self.1)
  %4 : __torch__.transformers.modeling_funnel.___torch_mangle_4760.FunnelEncoder = prim::GetAttr[name="encoder"](%self.1)
  %5 : __torch__.transformers.modeling_funnel.___torch_mangle_4572.FunnelEmbeddings = prim::GetAttr[name="embeddings"](%self.1)
  %6 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:1009:0
  %7 : int = aten::size(%input_ids, %6) # transformers/modeling_funnel.py:1009:0
  %8 : Long() = prim::NumToTensor(%7)
  %9 : int = aten::Int(%8)
  %10 : int = prim::Constant[value=1]() # transformers/modeling_funnel.py:1009:0
  %11 : int = aten::size(%input_ids, %10) # transformers/modeling_funnel.py:1009:0
  %12 : Long() = prim::NumToTensor(%11)
  %13 : int = aten::Int(%12)
  %14 : int[] = prim::ListConstruct(%9, %13)
  %15 : int = prim::Constant[value=4]() # transformers/modeling_funnel.py:1020:0
  %16 : int = prim::Constant[value=0]() # transformers/modeling_funnel.py:1020:0
  %17 : Device = prim::Constant[value="cpu"]() # transformers/modeling_funnel.py:1020:0
  %18 : bool = prim::Constant[value=0]() # transformers/modeling_funnel.py:1020:0
  %token_type_ids : Long(17:13, 13:1) = aten::zeros(%14, %15, %16, %17, %18) # transformers/modeling_funnel.py:1020:0
  %29 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %30 : int = prim::Constant[value=768](), scope: __module.embeddings/__module.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %31 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.embeddings/__module.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %32 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %33 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %34 : bool = prim::Constant[value=0](), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %35 : __torch__.torch.nn.modules.normalization.___torch_mangle_4570.LayerNorm = prim::GetAttr[name="layer_norm"](%5)
  %36 : __torch__.torch.nn.modules.sparse.___torch_mangle_4569.Embedding = prim::GetAttr[name="word_embeddings"](%5)
  %37 : Tensor = prim::GetAttr[name="weight"](%36)
  %input.1 : Float(17:9984, 13:768, 768:1) = aten::embedding(%37, %input_ids, %33, %34, %34), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %39 : Tensor = prim::GetAttr[name="bias"](%35)
  %40 : Tensor = prim::GetAttr[name="weight"](%35)
  %41 : int[] = prim::ListConstruct(%30), scope: __module.embeddings/__module.embeddings.layer_norm
  %input.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.1, %41, %40, %39, %31, %32), scope: __module.embeddings/__module.embeddings.layer_norm # torch/nn/functional.py:2048:0
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.2, %29, %34), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %44 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %45 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %46 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %47 : Double() = prim::Constant[value={1e+06}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %48 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %49 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %50 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %51 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %52 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %53 : Double() = prim::Constant[value={0.125}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %54 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %55 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %56 : float = prim::Constant[value=0.](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %57 : Double() = prim::Constant[value={1}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %58 : Double() = prim::Constant[value={0.797885}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %59 : Double() = prim::Constant[value={0.044715}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %60 : float = prim::Constant[value=3.](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %61 : Double() = prim::Constant[value={0.5}](), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %62 : int = prim::Constant[value=-4](), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %63 : Long() = prim::Constant[value={16}](), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %64 : Float(1:1) = prim::Constant[value={-3}](), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %65 : int = prim::Constant[value=-2](), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %66 : Long() = prim::Constant[value={14}](), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %67 : int = prim::Constant[value=2](), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %68 : Float(1:1) = prim::Constant[value={-1}](), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %69 : int = prim::Constant[value=768](), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %70 : int = prim::Constant[value=4](), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %71 : Long() = prim::Constant[value={13}](), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %72 : int = prim::Constant[value=-1](), scope: __module.encoder # transformers/modeling_funnel.py:255:0
  %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %74 : int = prim::Constant[value=9223372036854775807](), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %75 : Long() = prim::Constant[value={2}](), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %76 : Long() = prim::Constant[value={1}](), scope: __module.encoder # torch/tensor.py:400:0
  %77 : None = prim::Constant(), scope: __module.encoder
  %78 : Float() = prim::Constant[value={10000}](), scope: __module.encoder # torch/tensor.py:420:0
  %79 : Long() = prim::Constant[value={384}](), scope: __module.encoder # transformers/modeling_funnel.py:248:0
  %80 : bool = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %81 : Device = prim::Constant[value="cpu"](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %82 : int = prim::Constant[value=6](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %83 : float = prim::Constant[value=1.](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %84 : int = prim::Constant[value=384](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %85 : int = prim::Constant[value=0](), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %86 : int = prim::Constant[value=1](), scope: __module.encoder # transformers/modeling_funnel.py:195:0
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_4758.ModuleList = prim::GetAttr[name="2"](%87)
  %89 : __torch__.transformers.modeling_funnel.___torch_mangle_4757.FunnelLayer = prim::GetAttr[name="3"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_4758.ModuleList = prim::GetAttr[name="2"](%90)
  %92 : __torch__.transformers.modeling_funnel.___torch_mangle_4742.FunnelLayer = prim::GetAttr[name="2"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_4758.ModuleList = prim::GetAttr[name="2"](%93)
  %95 : __torch__.transformers.modeling_funnel.___torch_mangle_4727.FunnelLayer = prim::GetAttr[name="1"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_4758.ModuleList = prim::GetAttr[name="2"](%96)
  %98 : __torch__.transformers.modeling_funnel.___torch_mangle_4712.FunnelLayer = prim::GetAttr[name="0"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_4697.ModuleList = prim::GetAttr[name="1"](%99)
  %101 : __torch__.transformers.modeling_funnel.___torch_mangle_4696.FunnelLayer = prim::GetAttr[name="3"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_4697.ModuleList = prim::GetAttr[name="1"](%102)
  %104 : __torch__.transformers.modeling_funnel.___torch_mangle_4681.FunnelLayer = prim::GetAttr[name="2"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_4697.ModuleList = prim::GetAttr[name="1"](%105)
  %107 : __torch__.transformers.modeling_funnel.___torch_mangle_4666.FunnelLayer = prim::GetAttr[name="1"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_4697.ModuleList = prim::GetAttr[name="1"](%108)
  %110 : __torch__.transformers.modeling_funnel.___torch_mangle_4651.FunnelLayer = prim::GetAttr[name="0"](%109)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_4636.ModuleList = prim::GetAttr[name="0"](%111)
  %113 : __torch__.transformers.modeling_funnel.___torch_mangle_4635.FunnelLayer = prim::GetAttr[name="3"](%112)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %115 : __torch__.torch.nn.modules.container.___torch_mangle_4636.ModuleList = prim::GetAttr[name="0"](%114)
  %116 : __torch__.transformers.modeling_funnel.___torch_mangle_4620.FunnelLayer = prim::GetAttr[name="2"](%115)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %118 : __torch__.torch.nn.modules.container.___torch_mangle_4636.ModuleList = prim::GetAttr[name="0"](%117)
  %119 : __torch__.transformers.modeling_funnel.___torch_mangle_4605.FunnelLayer = prim::GetAttr[name="1"](%118)
  %120 : __torch__.torch.nn.modules.container.___torch_mangle_4759.ModuleList = prim::GetAttr[name="blocks"](%4)
  %121 : __torch__.torch.nn.modules.container.___torch_mangle_4636.ModuleList = prim::GetAttr[name="0"](%120)
  %122 : __torch__.transformers.modeling_funnel.___torch_mangle_4590.FunnelLayer = prim::GetAttr[name="0"](%121)
  %attention_mask.2 : Float(17:13, 13:1) = aten::type_as(%attention_mask.1, %inputs_embeds), scope: __module.encoder # transformers/modeling_funnel.py:625:0
  %124 : int = aten::size(%inputs_embeds, %86), scope: __module.encoder # transformers/modeling_funnel.py:195:0
  %seq_len.1 : Long() = prim::NumToTensor(%124), scope: __module.encoder
  %freq_seq.1 : Float(384:1) = aten::arange(%85, %84, %83, %82, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:247:0
  %127 : Float(384:1) = aten::div(%freq_seq.1, %79), scope: __module.encoder # transformers/modeling_funnel.py:248:0
  %128 : Float() = aten::to(%78, %81, %82, %80, %80, %77), scope: __module.encoder # torch/tensor.py:420:0
  %129 : Float() = aten::detach(%128), scope: __module.encoder # torch/tensor.py:420:0
  %130 : Float(384:1) = aten::pow(%129, %127), scope: __module.encoder # torch/tensor.py:420:0
  %131 : Float(384:1) = aten::reciprocal(%130), scope: __module.encoder # torch/tensor.py:400:0
  %inv_freq.1 : Float(384:1) = aten::mul(%131, %76), scope: __module.encoder # torch/tensor.py:400:0
  %133 : Long() = aten::neg(%seq_len.1), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %134 : Long() = aten::mul(%133, %75), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %135 : Scalar = aten::ScalarImplicit(%134), scope: __module.encoder
  %136 : Long() = aten::mul(%seq_len.1, %75), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %137 : Scalar = aten::ScalarImplicit(%136), scope: __module.encoder
  %rel_pos_id.1 : Float(52:1) = aten::arange(%135, %137, %83, %82, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:250:0
  %zero_offset.1 : Long() = aten::mul(%seq_len.1, %75), scope: __module.encoder # transformers/modeling_funnel.py:251:0
  %140 : Float(52:1) = aten::slice(%rel_pos_id.1, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %141 : Float(52:1, 1:1) = aten::unsqueeze(%140, %86), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %142 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq.1, %85), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %sinusoid.1 : Float(52:384, 384:1) = aten::mul(%141, %142), scope: __module.encoder # transformers/modeling_funnel.py:252:0
  %input.3 : Float(52:384, 384:1) = aten::sin(%sinusoid.1), scope: __module.encoder # transformers/modeling_funnel.py:253:0
  %sin_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.3, %73, %80), scope: __module.encoder/__module.encoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.4 : Float(52:384, 384:1) = aten::cos(%sinusoid.1), scope: __module.encoder # transformers/modeling_funnel.py:254:0
  %cos_embed.1 : Float(52:384, 384:1) = aten::dropout(%input.4, %73, %80), scope: __module.encoder/__module.encoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %148 : Tensor[] = prim::ListConstruct(%sin_embed.1, %cos_embed.1), scope: __module.encoder
  %pos_embed.1 : Float(52:768, 768:1) = aten::cat(%148, %72), scope: __module.encoder # transformers/modeling_funnel.py:255:0
  %pos.1 : Float(13:1) = aten::arange(%85, %124, %86, %82, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:257:0
  %151 : Float() = aten::select(%pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %152 : Float() = aten::select(%pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.1 : Float() = aten::sub(%151, %152, %86), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.1 : Float() = aten::add(%ref_point.1, %71, %86), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %155 : Scalar = aten::ScalarImplicit(%max_dist.1), scope: __module.encoder
  %156 : Float() = aten::select(%pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %157 : Float() = aten::select(%pos.1, %85, %72), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.1 : Float() = aten::sub(%156, %157, %86), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %159 : Float() = aten::sub(%min_dist.1, %76, %86), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %160 : Scalar = aten::ScalarImplicit(%159), scope: __module.encoder
  %rel_pos.1 : Long(26:1) = aten::arange(%155, %160, %72, %70, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %162 : Long(26:1) = aten::slice(%rel_pos.1, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %163 : Long(26:1, 1:1) = aten::unsqueeze(%162, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.2 : Long(26:1, 1:1) = aten::add(%163, %zero_offset.1, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %165 : int = aten::size(%rel_pos.2, %85), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %166 : int[] = prim::ListConstruct(%165, %69), scope: __module.encoder
  %rel_pos.3 : Long(26:1, 768:0) = aten::expand(%rel_pos.2, %166, %80), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %168 : Float(26:768, 768:1) = aten::gather(%pos_embed.1, %85, %rel_pos.3, %80), scope: __module.encoder # transformers/modeling_funnel.py:286:0
  %169 : Float(1:1) = aten::to(%68, %81, %82, %80, %80, %77), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.1 : Float(1:1) = aten::detach(%169), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.1 : Float(11:1) = aten::slice(%pos.1, %85, %86, %72, %86), scope: __module.encoder # transformers/modeling_funnel.py:301:0
  %172 : Float(6:2) = aten::slice(%pooled_pos_id.1, %85, %85, %74, %67), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %173 : Tensor[] = prim::ListConstruct(%cls_pos.1, %172), scope: __module.encoder
  %pooled_pos.1 : Float(7:1) = aten::cat(%173, %85), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %175 : Float() = aten::select(%pooled_pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %176 : Float() = aten::select(%pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.2 : Float() = aten::sub(%175, %176, %86), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.2 : Float() = aten::add(%ref_point.2, %66, %86), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %179 : Scalar = aten::ScalarImplicit(%max_dist.2), scope: __module.encoder
  %180 : Float() = aten::select(%pooled_pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %181 : Float() = aten::select(%pos.1, %85, %72), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.2 : Float() = aten::sub(%180, %181, %86), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %183 : Float() = aten::sub(%min_dist.2, %76, %86), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %184 : Scalar = aten::ScalarImplicit(%183), scope: __module.encoder
  %rel_pos.4 : Long(27:1) = aten::arange(%179, %184, %72, %70, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %186 : Long(27:1) = aten::slice(%rel_pos.4, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %187 : Long(27:1, 1:1) = aten::unsqueeze(%186, %86), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.5 : Long(27:1, 1:1) = aten::add(%187, %zero_offset.1, %86), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %189 : int = aten::size(%rel_pos.5, %85), scope: __module.encoder # transformers/modeling_funnel.py:276:0
  %190 : int[] = prim::ListConstruct(%189, %69), scope: __module.encoder
  %rel_pos.6 : Long(27:1, 768:0) = aten::expand(%rel_pos.5, %190, %80), scope: __module.encoder # transformers/modeling_funnel.py:276:0
  %192 : Float(27:768, 768:1) = aten::gather(%pos_embed.1, %85, %rel_pos.6, %80), scope: __module.encoder # transformers/modeling_funnel.py:277:0
  %193 : Float() = aten::select(%pooled_pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %194 : Float() = aten::select(%pooled_pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.3 : Float() = aten::sub(%193, %194, %86), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.3 : Float() = aten::add(%ref_point.3, %66, %86), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %197 : Scalar = aten::ScalarImplicit(%max_dist.3), scope: __module.encoder
  %198 : Float() = aten::select(%pooled_pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %199 : Float() = aten::select(%pooled_pos.1, %85, %72), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.3 : Float() = aten::sub(%198, %199, %86), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %201 : Float() = aten::sub(%min_dist.3, %76, %86), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %202 : Scalar = aten::ScalarImplicit(%201), scope: __module.encoder
  %rel_pos.7 : Long(14:1) = aten::arange(%197, %202, %65, %70, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %204 : Long(14:1) = aten::slice(%rel_pos.7, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %205 : Long(14:1, 1:1) = aten::unsqueeze(%204, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.8 : Long(14:1, 1:1) = aten::add(%205, %zero_offset.1, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %207 : int = aten::size(%rel_pos.8, %85), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %208 : int[] = prim::ListConstruct(%207, %69), scope: __module.encoder
  %rel_pos.9 : Long(14:1, 768:0) = aten::expand(%rel_pos.8, %208, %80), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %210 : Float(14:768, 768:1) = aten::gather(%pos_embed.1, %85, %rel_pos.9, %80), scope: __module.encoder # transformers/modeling_funnel.py:286:0
  %211 : Float(1:1) = aten::to(%64, %81, %82, %80, %80, %77), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %cls_pos.2 : Float(1:1) = aten::detach(%211), scope: __module.encoder # transformers/modeling_funnel.py:300:0
  %pooled_pos_id.2 : Float(5:1) = aten::slice(%pooled_pos.1, %85, %86, %72, %86), scope: __module.encoder # transformers/modeling_funnel.py:301:0
  %214 : Float(3:2) = aten::slice(%pooled_pos_id.2, %85, %85, %74, %67), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %215 : Tensor[] = prim::ListConstruct(%cls_pos.2, %214), scope: __module.encoder
  %pooled_pos.2 : Float(4:1) = aten::cat(%215, %85), scope: __module.encoder # transformers/modeling_funnel.py:302:0
  %217 : Float() = aten::select(%pooled_pos.2, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %218 : Float() = aten::select(%pooled_pos.1, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.4 : Float() = aten::sub(%217, %218, %86), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.4 : Float() = aten::add(%ref_point.4, %63, %86), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %221 : Scalar = aten::ScalarImplicit(%max_dist.4), scope: __module.encoder
  %222 : Float() = aten::select(%pooled_pos.2, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %223 : Float() = aten::select(%pooled_pos.1, %85, %72), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.4 : Float() = aten::sub(%222, %223, %86), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %225 : Float() = aten::sub(%min_dist.4, %76, %86), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %226 : Scalar = aten::ScalarImplicit(%225), scope: __module.encoder
  %rel_pos.10 : Long(15:1) = aten::arange(%221, %226, %65, %70, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %228 : Long(15:1) = aten::slice(%rel_pos.10, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %229 : Long(15:1, 1:1) = aten::unsqueeze(%228, %86), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %rel_pos.11 : Long(15:1, 1:1) = aten::add(%229, %zero_offset.1, %86), scope: __module.encoder # transformers/modeling_funnel.py:275:0
  %231 : int = aten::size(%rel_pos.11, %85), scope: __module.encoder # transformers/modeling_funnel.py:276:0
  %232 : int[] = prim::ListConstruct(%231, %69), scope: __module.encoder
  %rel_pos.12 : Long(15:1, 768:0) = aten::expand(%rel_pos.11, %232, %80), scope: __module.encoder # transformers/modeling_funnel.py:276:0
  %234 : Float(15:768, 768:1) = aten::gather(%pos_embed.1, %85, %rel_pos.12, %80), scope: __module.encoder # transformers/modeling_funnel.py:277:0
  %235 : Float() = aten::select(%pooled_pos.2, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %236 : Float() = aten::select(%pooled_pos.2, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %ref_point.5 : Float() = aten::sub(%235, %236, %86), scope: __module.encoder # transformers/modeling_funnel.py:313:0
  %max_dist.5 : Float() = aten::add(%ref_point.5, %63, %86), scope: __module.encoder # transformers/modeling_funnel.py:315:0
  %239 : Scalar = aten::ScalarImplicit(%max_dist.5), scope: __module.encoder
  %240 : Float() = aten::select(%pooled_pos.2, %85, %85), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %241 : Float() = aten::select(%pooled_pos.2, %85, %72), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %min_dist.5 : Float() = aten::sub(%240, %241, %86), scope: __module.encoder # transformers/modeling_funnel.py:316:0
  %243 : Float() = aten::sub(%min_dist.5, %76, %86), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %244 : Scalar = aten::ScalarImplicit(%243), scope: __module.encoder
  %rel_pos.13 : Long(8:1) = aten::arange(%239, %244, %62, %70, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:318:0
  %246 : Long(8:1) = aten::slice(%rel_pos.13, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %247 : Long(8:1, 1:1) = aten::unsqueeze(%246, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %rel_pos.14 : Long(8:1, 1:1) = aten::add(%247, %zero_offset.1, %86), scope: __module.encoder # transformers/modeling_funnel.py:284:0
  %249 : int = aten::size(%rel_pos.14, %85), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %250 : int[] = prim::ListConstruct(%249, %69), scope: __module.encoder
  %rel_pos.15 : Long(8:1, 768:0) = aten::expand(%rel_pos.14, %250, %80), scope: __module.encoder # transformers/modeling_funnel.py:285:0
  %252 : Float(8:768, 768:1) = aten::gather(%pos_embed.1, %85, %rel_pos.15, %80), scope: __module.encoder # transformers/modeling_funnel.py:286:0
  %253 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %254 : Long(17:13, 13:1) = aten::slice(%253, %86, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %255 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%254, %67), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %256 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %257 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%256, %86), scope: __module.encoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.1 : Bool(17:169, 13:13, 13:1) = aten::eq(%255, %257), scope: __module.encoder # torch/tensor.py:22:0
  %cls_ids.1 : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %67), scope: __module.encoder # torch/tensor.py:22:0
  %260 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %261 : Bool(17:13, 13:1) = aten::slice(%260, %86, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %262 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%261, %67), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %263 : Bool(17:13, 13:1) = aten::slice(%cls_ids.1, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %264 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%263, %86), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %cls_mat.1 : Bool(17:169, 13:13, 13:1) = aten::__or__(%262, %264), scope: __module.encoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.2 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat.1, %token_type_mat.1), scope: __module.encoder # transformers/modeling_funnel.py:211:0
  %267 : Long() = aten::sub(%seq_len.1, %76, %86), scope: __module.encoder # transformers/modeling_funnel.py:199:0
  %268 : int = aten::Int(%267), scope: __module.encoder
  %269 : Long() = aten::sub(%seq_len.1, %76, %86), scope: __module.encoder # transformers/modeling_funnel.py:199:0
  %270 : int = aten::Int(%269), scope: __module.encoder
  %271 : int[] = prim::ListConstruct(%268, %270), scope: __module.encoder
  %input.5 : Float(12:12, 12:1) = aten::ones(%271, %82, %85, %81, %80), scope: __module.encoder # transformers/modeling_funnel.py:199:0
  %273 : int[] = prim::ListConstruct(%86, %85, %86, %85), scope: __module.encoder
  %cls_mask.1 : Float(13:13, 13:1) = aten::constant_pad_nd(%input.5, %273, %85), scope: __module.encoder # torch/nn/functional.py:3552:0
  %275 : __torch__.transformers.modeling_funnel.___torch_mangle_4589.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%122)
  %276 : __torch__.transformers.modeling_funnel.___torch_mangle_4583.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%122)
  %277 : __torch__.torch.nn.modules.normalization.___torch_mangle_4582.LayerNorm = prim::GetAttr[name="layer_norm"](%276)
  %278 : __torch__.torch.nn.modules.linear.___torch_mangle_4581.Linear = prim::GetAttr[name="post_proj"](%276)
  %279 : Tensor = prim::GetAttr[name="seg_embed"](%276)
  %280 : Tensor = prim::GetAttr[name="r_s_bias"](%276)
  %281 : Tensor = prim::GetAttr[name="r_kernel"](%276)
  %282 : Tensor = prim::GetAttr[name="r_r_bias"](%276)
  %283 : Tensor = prim::GetAttr[name="r_w_bias"](%276)
  %284 : __torch__.torch.nn.modules.linear.___torch_mangle_4580.Linear = prim::GetAttr[name="v_head"](%276)
  %285 : __torch__.torch.nn.modules.linear.___torch_mangle_4579.Linear = prim::GetAttr[name="k_head"](%276)
  %286 : __torch__.torch.nn.modules.linear.___torch_mangle_4578.Linear = prim::GetAttr[name="q_head"](%276)
  %287 : int = aten::size(%inputs_embeds, %85), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %288 : int = aten::size(%inputs_embeds, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:530:0
  %289 : int = aten::size(%inputs_embeds, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:531:0
  %290 : Tensor = prim::GetAttr[name="weight"](%286)
  %291 : Float(768:1, 768:768) = aten::t(%290), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %292 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %291), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.q_head # torch/nn/functional.py:1676:0
  %293 : int[] = prim::ListConstruct(%287, %288, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %q_head.1 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%292, %293), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:535:0
  %295 : Tensor = prim::GetAttr[name="bias"](%285)
  %296 : Tensor = prim::GetAttr[name="weight"](%285)
  %297 : Float(768:1, 768:768) = aten::t(%296), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %297), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1676:0
  %299 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %295, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.k_head # torch/nn/functional.py:1678:0
  %300 : int[] = prim::ListConstruct(%287, %289, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %301 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%299, %300), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:537:0
  %302 : Tensor = prim::GetAttr[name="bias"](%284)
  %303 : Tensor = prim::GetAttr[name="weight"](%284)
  %304 : Float(768:1, 768:768) = aten::t(%303), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%inputs_embeds, %304), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1676:0
  %306 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %302, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.v_head # torch/nn/functional.py:1678:0
  %307 : int[] = prim::ListConstruct(%287, %289, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %308 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%306, %307), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.1, %53), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.1 : Float(12:64, 64:1) = aten::mul(%283, %53), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:542:0
  %311 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_w_bias.1, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:544:0
  %312 : Tensor[] = prim::ListConstruct(%311, %301), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %content_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%52, %312), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %v.1 : Float(12:64, 64:1) = aten::mul(%282, %53), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:486:0
  %315 : Tensor[] = prim::ListConstruct(%168, %281), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %316 : Float(26:768, 12:64, 64:1) = aten::einsum(%51, %315), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %317 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %v.1, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:493:0
  %318 : Tensor[] = prim::ListConstruct(%317, %316), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %positional_attn.1 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%50, %318), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %320 : int = aten::size(%positional_attn.1, %85), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %321 : int = aten::size(%positional_attn.1, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %322 : int = aten::size(%positional_attn.1, %67), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %323 : int = aten::size(%positional_attn.1, %49), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.1 : Long() = prim::NumToTensor(%323), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %325 : int[] = prim::ListConstruct(%320, %321, %323, %322), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %positional_attn.2 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.1, %325), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:428:0
  %327 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %328 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%327, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %329 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%328, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.3 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%329, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:429:0
  %331 : Long() = aten::sub(%max_rel_len.1, %76, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %332 : int = aten::Int(%331), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %333 : int[] = prim::ListConstruct(%320, %321, %322, %332), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %positional_attn.4 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.3, %333), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.5 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.4, %49, %85, %289, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.6 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.5, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:498:0
  %337 : int = aten::size(%token_type_mat.2, %85), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %338 : int = aten::size(%token_type_mat.2, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %339 : int = aten::size(%token_type_mat.2, %67), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.1 : Float(12:64, 64:1) = aten::mul(%280, %53), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:508:0
  %341 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.2, %r_s_bias.1, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:511:0
  %342 : Tensor[] = prim::ListConstruct(%341, %279), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %343 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%48, %342), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %344 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %345 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%344, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %346 : int = aten::size(%q_head.2, %67), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %347 : int[] = prim::ListConstruct(%337, %346, %338, %339), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %token_type_mat.3 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%345, %347, %80), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:513:0
  %349 : Tensor[] = aten::split(%343, %86, %72), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/tensor.py:371:0
  %diff_token_type.1 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.1 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%349), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %352 : int = aten::size(%token_type_mat.3, %85), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %353 : int = aten::size(%token_type_mat.3, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %354 : int = aten::size(%token_type_mat.3, %67), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %355 : int = aten::size(%token_type_mat.3, %49), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %356 : int[] = prim::ListConstruct(%352, %353, %354, %355), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %357 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.1, %356, %80), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %358 : int = aten::size(%token_type_mat.3, %85), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %359 : int = aten::size(%token_type_mat.3, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %360 : int = aten::size(%token_type_mat.3, %67), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %361 : int = aten::size(%token_type_mat.3, %49), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %362 : int[] = prim::ListConstruct(%358, %359, %360, %361), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %363 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.1, %362, %80), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.3, %357, %363), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.1, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:522:0
  %366 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.1, %positional_attn.6, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%366, %token_type_attn.2, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.1, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:553:0
  %369 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %370 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%369, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %371 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%370, %67), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %372 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%371, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %373 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%372, %86, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/tensor.py:396:0
  %374 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%373, %47), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.2, %374, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:556:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.3, %72, %82), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:558:0
  %377 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.6, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %378 : Tensor[] = prim::ListConstruct(%377, %308), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %attn_vec.1 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%46, %378), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # torch/functional.py:327:0
  %380 : int[] = prim::ListConstruct(%287, %288, %69), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention
  %input.7 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.1, %380), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:565:0
  %382 : Tensor = prim::GetAttr[name="bias"](%278)
  %383 : Tensor = prim::GetAttr[name="weight"](%278)
  %384 : Float(768:1, 768:768) = aten::t(%383), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.7, %384), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %382, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.8, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %attn_out.1, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention # transformers/modeling_funnel.py:568:0
  %389 : Tensor = prim::GetAttr[name="bias"](%277)
  %390 : Tensor = prim::GetAttr[name="weight"](%277)
  %391 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.layer_norm
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %391, %390, %389, %44, %45), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.attention/__module.encoder.blocks.0.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %393 : __torch__.torch.nn.modules.normalization.___torch_mangle_4588.LayerNorm = prim::GetAttr[name="layer_norm"](%275)
  %394 : __torch__.torch.nn.modules.linear.___torch_mangle_4586.Linear = prim::GetAttr[name="linear_2"](%275)
  %395 : __torch__.torch.nn.modules.linear.___torch_mangle_4584.Linear = prim::GetAttr[name="linear_1"](%275)
  %396 : Tensor = prim::GetAttr[name="bias"](%395)
  %397 : Tensor = prim::GetAttr[name="weight"](%395)
  %398 : Float(768:1, 3072:768) = aten::t(%397), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.10, %398), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.1 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.4, %396, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %401 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.1, %61), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %402 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.1, %60), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %403 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%402, %59), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %404 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.1, %403, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %405 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%404, %58), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %406 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%405), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %407 : Float(17:39936, 13:3072, 3072:1) = aten::add(%406, %57, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%401, %407), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/activations.py:30:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.11, %56, %80), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %410 : Tensor = prim::GetAttr[name="bias"](%394)
  %411 : Tensor = prim::GetAttr[name="weight"](%394)
  %412 : Float(3072:1, 768:3072) = aten::t(%411), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %412), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.5, %410, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%input.10, %h.1, %86), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn # transformers/modeling_funnel.py:588:0
  %417 : Tensor = prim::GetAttr[name="bias"](%393)
  %418 : Tensor = prim::GetAttr[name="weight"](%393)
  %419 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.layer_norm
  %query.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %419, %418, %417, %44, %45), scope: __module.encoder/__module.encoder.blocks.0.0/__module.encoder.blocks.0.0.ffn/__module.encoder.blocks.0.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %421 : __torch__.transformers.modeling_funnel.___torch_mangle_4604.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%119)
  %422 : __torch__.transformers.modeling_funnel.___torch_mangle_4598.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%119)
  %423 : __torch__.torch.nn.modules.normalization.___torch_mangle_4597.LayerNorm = prim::GetAttr[name="layer_norm"](%422)
  %424 : __torch__.torch.nn.modules.linear.___torch_mangle_4596.Linear = prim::GetAttr[name="post_proj"](%422)
  %425 : Tensor = prim::GetAttr[name="seg_embed"](%422)
  %426 : Tensor = prim::GetAttr[name="r_s_bias"](%422)
  %427 : Tensor = prim::GetAttr[name="r_kernel"](%422)
  %428 : Tensor = prim::GetAttr[name="r_r_bias"](%422)
  %429 : Tensor = prim::GetAttr[name="r_w_bias"](%422)
  %430 : __torch__.torch.nn.modules.linear.___torch_mangle_4595.Linear = prim::GetAttr[name="v_head"](%422)
  %431 : __torch__.torch.nn.modules.linear.___torch_mangle_4594.Linear = prim::GetAttr[name="k_head"](%422)
  %432 : __torch__.torch.nn.modules.linear.___torch_mangle_4593.Linear = prim::GetAttr[name="q_head"](%422)
  %433 : int = aten::size(%query.1, %85), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %434 : int = aten::size(%query.1, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:530:0
  %435 : int = aten::size(%query.1, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:531:0
  %436 : Tensor = prim::GetAttr[name="weight"](%432)
  %437 : Float(768:1, 768:768) = aten::t(%436), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %438 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %437), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.q_head # torch/nn/functional.py:1676:0
  %439 : int[] = prim::ListConstruct(%433, %434, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %q_head.3 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%438, %439), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:535:0
  %441 : Tensor = prim::GetAttr[name="bias"](%431)
  %442 : Tensor = prim::GetAttr[name="weight"](%431)
  %443 : Float(768:1, 768:768) = aten::t(%442), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %443), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1676:0
  %445 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %441, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.k_head # torch/nn/functional.py:1678:0
  %446 : int[] = prim::ListConstruct(%433, %435, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %447 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%445, %446), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:537:0
  %448 : Tensor = prim::GetAttr[name="bias"](%430)
  %449 : Tensor = prim::GetAttr[name="weight"](%430)
  %450 : Float(768:1, 768:768) = aten::t(%449), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.1, %450), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1676:0
  %452 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %448, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.v_head # torch/nn/functional.py:1678:0
  %453 : int[] = prim::ListConstruct(%433, %435, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %454 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%452, %453), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.3, %53), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.2 : Float(12:64, 64:1) = aten::mul(%429, %53), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:542:0
  %457 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_w_bias.2, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:544:0
  %458 : Tensor[] = prim::ListConstruct(%457, %447), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %content_score.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%52, %458), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %v.2 : Float(12:64, 64:1) = aten::mul(%428, %53), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:486:0
  %461 : Tensor[] = prim::ListConstruct(%168, %427), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %462 : Float(26:768, 12:64, 64:1) = aten::einsum(%51, %461), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %463 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %v.2, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:493:0
  %464 : Tensor[] = prim::ListConstruct(%463, %462), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %positional_attn.7 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%50, %464), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %466 : int = aten::size(%positional_attn.7, %85), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %467 : int = aten::size(%positional_attn.7, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %468 : int = aten::size(%positional_attn.7, %67), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %469 : int = aten::size(%positional_attn.7, %49), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.2 : Long() = prim::NumToTensor(%469), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %471 : int[] = prim::ListConstruct(%466, %467, %469, %468), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %positional_attn.8 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.7, %471), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:428:0
  %473 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.8, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %474 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%473, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %475 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%474, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.9 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%475, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:429:0
  %477 : Long() = aten::sub(%max_rel_len.2, %76, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %478 : int = aten::Int(%477), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %479 : int[] = prim::ListConstruct(%466, %467, %468, %478), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %positional_attn.10 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.9, %479), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.11 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.10, %49, %85, %435, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.12 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.11, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:498:0
  %483 : int = aten::size(%token_type_mat.2, %85), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %484 : int = aten::size(%token_type_mat.2, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %485 : int = aten::size(%token_type_mat.2, %67), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.2 : Float(12:64, 64:1) = aten::mul(%426, %53), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:508:0
  %487 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.4, %r_s_bias.2, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:511:0
  %488 : Tensor[] = prim::ListConstruct(%487, %425), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %489 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%48, %488), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %490 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %491 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%490, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %492 : int = aten::size(%q_head.4, %67), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %493 : int[] = prim::ListConstruct(%483, %492, %484, %485), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %token_type_mat.4 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%491, %493, %80), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:513:0
  %495 : Tensor[] = aten::split(%489, %86, %72), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/tensor.py:371:0
  %diff_token_type.2 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.2 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%495), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %498 : int = aten::size(%token_type_mat.4, %85), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %499 : int = aten::size(%token_type_mat.4, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %500 : int = aten::size(%token_type_mat.4, %67), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %501 : int = aten::size(%token_type_mat.4, %49), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %502 : int[] = prim::ListConstruct(%498, %499, %500, %501), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %503 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.2, %502, %80), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %504 : int = aten::size(%token_type_mat.4, %85), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %505 : int = aten::size(%token_type_mat.4, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %506 : int = aten::size(%token_type_mat.4, %67), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %507 : int = aten::size(%token_type_mat.4, %49), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %508 : int[] = prim::ListConstruct(%504, %505, %506, %507), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %509 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.2, %508, %80), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.4, %503, %509), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.3, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:522:0
  %512 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.2, %positional_attn.12, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%512, %token_type_attn.4, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.4, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:553:0
  %515 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %516 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%515, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %517 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%516, %67), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %518 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%517, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %519 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%518, %86, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/tensor.py:396:0
  %520 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%519, %47), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.5, %520, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:556:0
  %input.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.6, %72, %82), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:558:0
  %523 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.15, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %524 : Tensor[] = prim::ListConstruct(%523, %454), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %attn_vec.2 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%46, %524), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # torch/functional.py:327:0
  %526 : int[] = prim::ListConstruct(%433, %434, %69), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention
  %input.16 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.2, %526), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:565:0
  %528 : Tensor = prim::GetAttr[name="bias"](%424)
  %529 : Tensor = prim::GetAttr[name="weight"](%424)
  %530 : Float(768:1, 768:768) = aten::t(%529), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.16, %530), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %528, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.17, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::add(%query.1, %attn_out.2, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention # transformers/modeling_funnel.py:568:0
  %535 : Tensor = prim::GetAttr[name="bias"](%423)
  %536 : Tensor = prim::GetAttr[name="weight"](%423)
  %537 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.layer_norm
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.18, %537, %536, %535, %44, %45), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.attention/__module.encoder.blocks.0.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %539 : __torch__.torch.nn.modules.normalization.___torch_mangle_4603.LayerNorm = prim::GetAttr[name="layer_norm"](%421)
  %540 : __torch__.torch.nn.modules.linear.___torch_mangle_4601.Linear = prim::GetAttr[name="linear_2"](%421)
  %541 : __torch__.torch.nn.modules.linear.___torch_mangle_4599.Linear = prim::GetAttr[name="linear_1"](%421)
  %542 : Tensor = prim::GetAttr[name="bias"](%541)
  %543 : Tensor = prim::GetAttr[name="weight"](%541)
  %544 : Float(768:1, 3072:768) = aten::t(%543), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.9 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.19, %544), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.2 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.9, %542, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %547 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.2, %61), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %548 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.2, %60), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %549 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%548, %59), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %550 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.2, %549, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %551 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%550, %58), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %552 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%551), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %553 : Float(17:39936, 13:3072, 3072:1) = aten::add(%552, %57, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.20 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%547, %553), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/activations.py:30:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.20, %56, %80), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %556 : Tensor = prim::GetAttr[name="bias"](%540)
  %557 : Tensor = prim::GetAttr[name="weight"](%540)
  %558 : Float(3072:1, 768:3072) = aten::t(%557), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.21, %558), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %556, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.22, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add(%input.19, %h.2, %86), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn # transformers/modeling_funnel.py:588:0
  %563 : Tensor = prim::GetAttr[name="bias"](%539)
  %564 : Tensor = prim::GetAttr[name="weight"](%539)
  %565 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.layer_norm
  %query.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %565, %564, %563, %44, %45), scope: __module.encoder/__module.encoder.blocks.0.1/__module.encoder.blocks.0.1.ffn/__module.encoder.blocks.0.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %567 : __torch__.transformers.modeling_funnel.___torch_mangle_4619.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%116)
  %568 : __torch__.transformers.modeling_funnel.___torch_mangle_4613.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%116)
  %569 : __torch__.torch.nn.modules.normalization.___torch_mangle_4612.LayerNorm = prim::GetAttr[name="layer_norm"](%568)
  %570 : __torch__.torch.nn.modules.linear.___torch_mangle_4611.Linear = prim::GetAttr[name="post_proj"](%568)
  %571 : Tensor = prim::GetAttr[name="seg_embed"](%568)
  %572 : Tensor = prim::GetAttr[name="r_s_bias"](%568)
  %573 : Tensor = prim::GetAttr[name="r_kernel"](%568)
  %574 : Tensor = prim::GetAttr[name="r_r_bias"](%568)
  %575 : Tensor = prim::GetAttr[name="r_w_bias"](%568)
  %576 : __torch__.torch.nn.modules.linear.___torch_mangle_4610.Linear = prim::GetAttr[name="v_head"](%568)
  %577 : __torch__.torch.nn.modules.linear.___torch_mangle_4609.Linear = prim::GetAttr[name="k_head"](%568)
  %578 : __torch__.torch.nn.modules.linear.___torch_mangle_4608.Linear = prim::GetAttr[name="q_head"](%568)
  %579 : int = aten::size(%query.2, %85), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %580 : int = aten::size(%query.2, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:530:0
  %581 : int = aten::size(%query.2, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:531:0
  %582 : Tensor = prim::GetAttr[name="weight"](%578)
  %583 : Float(768:1, 768:768) = aten::t(%582), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %584 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %583), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.q_head # torch/nn/functional.py:1676:0
  %585 : int[] = prim::ListConstruct(%579, %580, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %q_head.5 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%584, %585), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:535:0
  %587 : Tensor = prim::GetAttr[name="bias"](%577)
  %588 : Tensor = prim::GetAttr[name="weight"](%577)
  %589 : Float(768:1, 768:768) = aten::t(%588), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.11 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %589), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1676:0
  %591 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.11, %587, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.k_head # torch/nn/functional.py:1678:0
  %592 : int[] = prim::ListConstruct(%579, %581, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %593 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%591, %592), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:537:0
  %594 : Tensor = prim::GetAttr[name="bias"](%576)
  %595 : Tensor = prim::GetAttr[name="weight"](%576)
  %596 : Float(768:1, 768:768) = aten::t(%595), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.2, %596), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1676:0
  %598 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %594, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.v_head # torch/nn/functional.py:1678:0
  %599 : int[] = prim::ListConstruct(%579, %581, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %600 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%598, %599), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.5, %53), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.3 : Float(12:64, 64:1) = aten::mul(%575, %53), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:542:0
  %603 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_w_bias.3, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:544:0
  %604 : Tensor[] = prim::ListConstruct(%603, %593), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %content_score.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%52, %604), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %v.3 : Float(12:64, 64:1) = aten::mul(%574, %53), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:486:0
  %607 : Tensor[] = prim::ListConstruct(%168, %573), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %608 : Float(26:768, 12:64, 64:1) = aten::einsum(%51, %607), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %609 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %v.3, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:493:0
  %610 : Tensor[] = prim::ListConstruct(%609, %608), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %positional_attn.13 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%50, %610), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %612 : int = aten::size(%positional_attn.13, %85), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %613 : int = aten::size(%positional_attn.13, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %614 : int = aten::size(%positional_attn.13, %67), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %615 : int = aten::size(%positional_attn.13, %49), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.3 : Long() = prim::NumToTensor(%615), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %617 : int[] = prim::ListConstruct(%612, %613, %615, %614), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %positional_attn.14 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.13, %617), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:428:0
  %619 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.14, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %620 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%619, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %621 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%620, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.15 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%621, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:429:0
  %623 : Long() = aten::sub(%max_rel_len.3, %76, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %624 : int = aten::Int(%623), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %625 : int[] = prim::ListConstruct(%612, %613, %614, %624), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %positional_attn.16 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.15, %625), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.17 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.16, %49, %85, %581, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.18 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.17, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:498:0
  %629 : int = aten::size(%token_type_mat.2, %85), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %630 : int = aten::size(%token_type_mat.2, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %631 : int = aten::size(%token_type_mat.2, %67), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.3 : Float(12:64, 64:1) = aten::mul(%572, %53), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:508:0
  %633 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.6, %r_s_bias.3, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:511:0
  %634 : Tensor[] = prim::ListConstruct(%633, %571), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %635 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%48, %634), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %636 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %637 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%636, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %638 : int = aten::size(%q_head.6, %67), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %639 : int[] = prim::ListConstruct(%629, %638, %630, %631), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %token_type_mat.5 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%637, %639, %80), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:513:0
  %641 : Tensor[] = aten::split(%635, %86, %72), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/tensor.py:371:0
  %diff_token_type.3 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.3 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%641), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %644 : int = aten::size(%token_type_mat.5, %85), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %645 : int = aten::size(%token_type_mat.5, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %646 : int = aten::size(%token_type_mat.5, %67), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %647 : int = aten::size(%token_type_mat.5, %49), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %648 : int[] = prim::ListConstruct(%644, %645, %646, %647), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %649 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.3, %648, %80), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %650 : int = aten::size(%token_type_mat.5, %85), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %651 : int = aten::size(%token_type_mat.5, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %652 : int = aten::size(%token_type_mat.5, %67), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %653 : int = aten::size(%token_type_mat.5, %49), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %654 : int[] = prim::ListConstruct(%650, %651, %652, %653), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %655 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.3, %654, %80), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.5, %649, %655), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.5, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:522:0
  %658 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.3, %positional_attn.18, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%658, %token_type_attn.6, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.7, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:553:0
  %661 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %662 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%661, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %663 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%662, %67), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %664 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%663, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %665 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%664, %86, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/tensor.py:396:0
  %666 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%665, %47), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.8, %666, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:556:0
  %input.24 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.9, %72, %82), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:558:0
  %669 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.24, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %670 : Tensor[] = prim::ListConstruct(%669, %600), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %attn_vec.3 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%46, %670), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # torch/functional.py:327:0
  %672 : int[] = prim::ListConstruct(%579, %580, %69), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.3, %672), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:565:0
  %674 : Tensor = prim::GetAttr[name="bias"](%570)
  %675 : Tensor = prim::GetAttr[name="weight"](%570)
  %676 : Float(768:1, 768:768) = aten::t(%675), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %676), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.26 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %674, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.26, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.27 : Float(17:9984, 13:768, 768:1) = aten::add(%query.2, %attn_out.3, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention # transformers/modeling_funnel.py:568:0
  %681 : Tensor = prim::GetAttr[name="bias"](%569)
  %682 : Tensor = prim::GetAttr[name="weight"](%569)
  %683 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.layer_norm
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %683, %682, %681, %44, %45), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.attention/__module.encoder.blocks.0.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %685 : __torch__.torch.nn.modules.normalization.___torch_mangle_4618.LayerNorm = prim::GetAttr[name="layer_norm"](%567)
  %686 : __torch__.torch.nn.modules.linear.___torch_mangle_4616.Linear = prim::GetAttr[name="linear_2"](%567)
  %687 : __torch__.torch.nn.modules.linear.___torch_mangle_4614.Linear = prim::GetAttr[name="linear_1"](%567)
  %688 : Tensor = prim::GetAttr[name="bias"](%687)
  %689 : Tensor = prim::GetAttr[name="weight"](%687)
  %690 : Float(768:1, 3072:768) = aten::t(%689), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.14 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.28, %690), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.3 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.14, %688, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %693 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.3, %61), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %694 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.3, %60), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %695 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%694, %59), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %696 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.3, %695, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %697 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%696, %58), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %698 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%697), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %699 : Float(17:39936, 13:3072, 3072:1) = aten::add(%698, %57, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.29 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%693, %699), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/activations.py:30:0
  %input.30 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.29, %56, %80), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %702 : Tensor = prim::GetAttr[name="bias"](%686)
  %703 : Tensor = prim::GetAttr[name="weight"](%686)
  %704 : Float(3072:1, 768:3072) = aten::t(%703), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.30, %704), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %702, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.31, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.32 : Float(17:9984, 13:768, 768:1) = aten::add(%input.28, %h.3, %86), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn # transformers/modeling_funnel.py:588:0
  %709 : Tensor = prim::GetAttr[name="bias"](%685)
  %710 : Tensor = prim::GetAttr[name="weight"](%685)
  %711 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.layer_norm
  %query.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.32, %711, %710, %709, %44, %45), scope: __module.encoder/__module.encoder.blocks.0.2/__module.encoder.blocks.0.2.ffn/__module.encoder.blocks.0.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %713 : __torch__.transformers.modeling_funnel.___torch_mangle_4634.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%113)
  %714 : __torch__.transformers.modeling_funnel.___torch_mangle_4628.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%113)
  %715 : __torch__.torch.nn.modules.normalization.___torch_mangle_4627.LayerNorm = prim::GetAttr[name="layer_norm"](%714)
  %716 : __torch__.torch.nn.modules.linear.___torch_mangle_4626.Linear = prim::GetAttr[name="post_proj"](%714)
  %717 : Tensor = prim::GetAttr[name="seg_embed"](%714)
  %718 : Tensor = prim::GetAttr[name="r_s_bias"](%714)
  %719 : Tensor = prim::GetAttr[name="r_kernel"](%714)
  %720 : Tensor = prim::GetAttr[name="r_r_bias"](%714)
  %721 : Tensor = prim::GetAttr[name="r_w_bias"](%714)
  %722 : __torch__.torch.nn.modules.linear.___torch_mangle_4625.Linear = prim::GetAttr[name="v_head"](%714)
  %723 : __torch__.torch.nn.modules.linear.___torch_mangle_4624.Linear = prim::GetAttr[name="k_head"](%714)
  %724 : __torch__.torch.nn.modules.linear.___torch_mangle_4623.Linear = prim::GetAttr[name="q_head"](%714)
  %725 : int = aten::size(%query.3, %85), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %726 : int = aten::size(%query.3, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:530:0
  %727 : int = aten::size(%query.3, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:531:0
  %728 : Tensor = prim::GetAttr[name="weight"](%724)
  %729 : Float(768:1, 768:768) = aten::t(%728), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %730 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %729), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.q_head # torch/nn/functional.py:1676:0
  %731 : int[] = prim::ListConstruct(%725, %726, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %q_head.7 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%730, %731), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:535:0
  %733 : Tensor = prim::GetAttr[name="bias"](%723)
  %734 : Tensor = prim::GetAttr[name="weight"](%723)
  %735 : Float(768:1, 768:768) = aten::t(%734), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %735), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1676:0
  %737 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %733, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.k_head # torch/nn/functional.py:1678:0
  %738 : int[] = prim::ListConstruct(%725, %727, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %739 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%737, %738), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:537:0
  %740 : Tensor = prim::GetAttr[name="bias"](%722)
  %741 : Tensor = prim::GetAttr[name="weight"](%722)
  %742 : Float(768:1, 768:768) = aten::t(%741), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.17 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query.3, %742), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1676:0
  %744 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.17, %740, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.v_head # torch/nn/functional.py:1678:0
  %745 : int[] = prim::ListConstruct(%725, %727, %55, %54), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %746 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%744, %745), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.7, %53), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.4 : Float(12:64, 64:1) = aten::mul(%721, %53), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:542:0
  %749 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_w_bias.4, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:544:0
  %750 : Tensor[] = prim::ListConstruct(%749, %739), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %content_score.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%52, %750), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %v.4 : Float(12:64, 64:1) = aten::mul(%720, %53), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:486:0
  %753 : Tensor[] = prim::ListConstruct(%168, %719), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %754 : Float(26:768, 12:64, 64:1) = aten::einsum(%51, %753), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %755 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %v.4, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:493:0
  %756 : Tensor[] = prim::ListConstruct(%755, %754), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %positional_attn.19 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%50, %756), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %758 : int = aten::size(%positional_attn.19, %85), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %759 : int = aten::size(%positional_attn.19, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %760 : int = aten::size(%positional_attn.19, %67), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %761 : int = aten::size(%positional_attn.19, %49), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.4 : Long() = prim::NumToTensor(%761), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %763 : int[] = prim::ListConstruct(%758, %759, %761, %760), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %positional_attn.20 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.19, %763), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:428:0
  %765 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.20, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %766 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%765, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %767 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%766, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.21 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%767, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:429:0
  %769 : Long() = aten::sub(%max_rel_len.4, %76, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %770 : int = aten::Int(%769), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %771 : int[] = prim::ListConstruct(%758, %759, %760, %770), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %positional_attn.22 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.21, %771), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.23 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.22, %49, %85, %727, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.24 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.23, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:498:0
  %775 : int = aten::size(%token_type_mat.2, %85), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %776 : int = aten::size(%token_type_mat.2, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %777 : int = aten::size(%token_type_mat.2, %67), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.4 : Float(12:64, 64:1) = aten::mul(%718, %53), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:508:0
  %779 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.8, %r_s_bias.4, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:511:0
  %780 : Tensor[] = prim::ListConstruct(%779, %717), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %781 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%48, %780), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %782 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %783 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%782, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %784 : int = aten::size(%q_head.8, %67), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %785 : int[] = prim::ListConstruct(%775, %784, %776, %777), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %token_type_mat.6 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%783, %785, %80), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:513:0
  %787 : Tensor[] = aten::split(%781, %86, %72), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/tensor.py:371:0
  %diff_token_type.4 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.4 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%787), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %790 : int = aten::size(%token_type_mat.6, %85), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %791 : int = aten::size(%token_type_mat.6, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %792 : int = aten::size(%token_type_mat.6, %67), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %793 : int = aten::size(%token_type_mat.6, %49), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %794 : int[] = prim::ListConstruct(%790, %791, %792, %793), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %795 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.4, %794, %80), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %796 : int = aten::size(%token_type_mat.6, %85), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %797 : int = aten::size(%token_type_mat.6, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %798 : int = aten::size(%token_type_mat.6, %67), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %799 : int = aten::size(%token_type_mat.6, %49), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %800 : int[] = prim::ListConstruct(%796, %797, %798, %799), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %801 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.4, %800, %80), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.6, %795, %801), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.7, %cls_mask.1), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:522:0
  %804 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.4, %positional_attn.24, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%804, %token_type_attn.8, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.10, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:553:0
  %807 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %808 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%807, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %809 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%808, %67), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %810 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%809, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %811 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%810, %86, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/tensor.py:396:0
  %812 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%811, %47), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.11, %812, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:556:0
  %input.33 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.12, %72, %82), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:558:0
  %815 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.33, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %816 : Tensor[] = prim::ListConstruct(%815, %746), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %attn_vec.4 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%46, %816), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # torch/functional.py:327:0
  %818 : int[] = prim::ListConstruct(%725, %726, %69), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.4, %818), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:565:0
  %820 : Tensor = prim::GetAttr[name="bias"](%716)
  %821 : Tensor = prim::GetAttr[name="weight"](%716)
  %822 : Float(768:1, 768:768) = aten::t(%821), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.34, %822), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %820, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.35, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.36 : Float(17:9984, 13:768, 768:1) = aten::add(%query.3, %attn_out.4, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention # transformers/modeling_funnel.py:568:0
  %827 : Tensor = prim::GetAttr[name="bias"](%715)
  %828 : Tensor = prim::GetAttr[name="weight"](%715)
  %829 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.layer_norm
  %input.37 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.36, %829, %828, %827, %44, %45), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.attention/__module.encoder.blocks.0.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %831 : __torch__.torch.nn.modules.normalization.___torch_mangle_4633.LayerNorm = prim::GetAttr[name="layer_norm"](%713)
  %832 : __torch__.torch.nn.modules.linear.___torch_mangle_4631.Linear = prim::GetAttr[name="linear_2"](%713)
  %833 : __torch__.torch.nn.modules.linear.___torch_mangle_4629.Linear = prim::GetAttr[name="linear_1"](%713)
  %834 : Tensor = prim::GetAttr[name="bias"](%833)
  %835 : Tensor = prim::GetAttr[name="weight"](%833)
  %836 : Float(768:1, 3072:768) = aten::t(%835), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.37, %836), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.4 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.19, %834, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %839 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.4, %61), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %840 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.4, %60), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %841 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%840, %59), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %842 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.4, %841, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %843 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%842, %58), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %844 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%843), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %845 : Float(17:39936, 13:3072, 3072:1) = aten::add(%844, %57, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.38 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%839, %845), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/activations.py:30:0
  %input.39 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.38, %56, %80), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %848 : Tensor = prim::GetAttr[name="bias"](%832)
  %849 : Tensor = prim::GetAttr[name="weight"](%832)
  %850 : Float(3072:1, 768:3072) = aten::t(%849), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.39, %850), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %848, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.40, %73, %80), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.41 : Float(17:9984, 13:768, 768:1) = aten::add(%input.37, %h.4, %86), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn # transformers/modeling_funnel.py:588:0
  %855 : Tensor = prim::GetAttr[name="bias"](%831)
  %856 : Tensor = prim::GetAttr[name="weight"](%831)
  %857 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.layer_norm
  %hidden.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.41, %857, %856, %855, %44, %45), scope: __module.encoder/__module.encoder.blocks.0.3/__module.encoder.blocks.0.3.ffn/__module.encoder.blocks.0.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %859 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.2, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %860 : Bool(17:169, 1:13, 13:1) = aten::slice(%859, %86, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %861 : Tensor[] = prim::ListConstruct(%860, %token_type_mat.2), scope: __module.encoder
  %tensor.1 : Bool(17:182, 14:13, 13:1) = aten::cat(%861, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %863 : Bool(17:182, 14:13, 13:1) = aten::slice(%tensor.1, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.7 : Bool(17:182, 7:26, 13:1) = aten::slice(%863, %86, %85, %72, %67), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %865 : Float(1:13, 13:1) = aten::slice(%cls_mask.1, %85, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %866 : Tensor[] = prim::ListConstruct(%865, %cls_mask.1), scope: __module.encoder
  %tensor.2 : Float(14:13, 13:1) = aten::cat(%866, %85), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.2 : Float(7:26, 13:1) = aten::slice(%tensor.2, %85, %85, %72, %67), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %869 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %suffix.1 : Float(17:9984, 12:768, 768:1) = aten::slice(%869, %86, %85, %72, %86), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %871 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden.1, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %872 : Float(17:9984, 1:768, 768:1) = aten::slice(%871, %86, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %873 : Tensor[] = prim::ListConstruct(%872, %suffix.1), scope: __module.encoder
  %tensor.3 : Float(17:9984, 13:768, 768:1) = aten::cat(%873, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %875 : Float(17:9984, 13:768, 768:1) = aten::slice(%tensor.3, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %876 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::unsqueeze(%875, %86), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %877 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%876, %67, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %tensor.4 : Float(17:9984, 1:9984, 13:768, 768:1) = aten::slice(%877, %49, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %879 : int[] = prim::ListConstruct(%67, %86), scope: __module.encoder
  %880 : int[] = prim::ListConstruct(%67, %86), scope: __module.encoder
  %881 : int[] = prim::ListConstruct(%85, %85), scope: __module.encoder
  %tensor.5 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::avg_pool2d(%tensor.4, %879, %880, %881, %45, %45, %77), scope: __module.encoder # transformers/modeling_funnel.py:371:0
  %883 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%tensor.5, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:382:0
  %query.4 : Float(17:5376, 7:768, 768:1) = aten::select(%883, %86, %85), scope: __module.encoder # transformers/modeling_funnel.py:382:0
  %885 : __torch__.transformers.modeling_funnel.___torch_mangle_4650.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%110)
  %886 : __torch__.transformers.modeling_funnel.___torch_mangle_4644.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%110)
  %887 : __torch__.torch.nn.modules.normalization.___torch_mangle_4643.LayerNorm = prim::GetAttr[name="layer_norm"](%886)
  %888 : __torch__.torch.nn.modules.linear.___torch_mangle_4642.Linear = prim::GetAttr[name="post_proj"](%886)
  %889 : Tensor = prim::GetAttr[name="seg_embed"](%886)
  %890 : Tensor = prim::GetAttr[name="r_s_bias"](%886)
  %891 : Tensor = prim::GetAttr[name="r_kernel"](%886)
  %892 : Tensor = prim::GetAttr[name="r_r_bias"](%886)
  %893 : Tensor = prim::GetAttr[name="r_w_bias"](%886)
  %894 : __torch__.torch.nn.modules.linear.___torch_mangle_4641.Linear = prim::GetAttr[name="v_head"](%886)
  %895 : __torch__.torch.nn.modules.linear.___torch_mangle_4640.Linear = prim::GetAttr[name="k_head"](%886)
  %896 : __torch__.torch.nn.modules.linear.___torch_mangle_4639.Linear = prim::GetAttr[name="q_head"](%886)
  %897 : int = aten::size(%query.4, %85), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %898 : int = aten::size(%query.4, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:530:0
  %899 : int = aten::size(%hidden.1, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:531:0
  %900 : Tensor = prim::GetAttr[name="weight"](%896)
  %901 : Float(768:1, 768:768) = aten::t(%900), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %902 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.4, %901), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.q_head # torch/nn/functional.py:1676:0
  %903 : int[] = prim::ListConstruct(%897, %898, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %q_head.9 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%902, %903), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:535:0
  %905 : Tensor = prim::GetAttr[name="bias"](%895)
  %906 : Tensor = prim::GetAttr[name="weight"](%895)
  %907 : Float(768:1, 768:768) = aten::t(%906), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %907), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1676:0
  %909 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %905, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.k_head # torch/nn/functional.py:1678:0
  %910 : int[] = prim::ListConstruct(%897, %899, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %911 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%909, %910), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:537:0
  %912 : Tensor = prim::GetAttr[name="bias"](%894)
  %913 : Tensor = prim::GetAttr[name="weight"](%894)
  %914 : Float(768:1, 768:768) = aten::t(%913), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%hidden.1, %914), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1676:0
  %916 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %912, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.v_head # torch/nn/functional.py:1678:0
  %917 : int[] = prim::ListConstruct(%897, %899, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %918 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%916, %917), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.10 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.9, %53), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.5 : Float(12:64, 64:1) = aten::mul(%893, %53), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:542:0
  %921 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_w_bias.5, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:544:0
  %922 : Tensor[] = prim::ListConstruct(%921, %911), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %content_score.5 : Float(17:1092, 12:91, 7:13, 13:1) = aten::einsum(%52, %922), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %v.5 : Float(12:64, 64:1) = aten::mul(%892, %53), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:486:0
  %925 : Tensor[] = prim::ListConstruct(%192, %891), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %926 : Float(27:768, 12:64, 64:1) = aten::einsum(%51, %925), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %927 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %v.5, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:493:0
  %928 : Tensor[] = prim::ListConstruct(%927, %926), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %positional_attn.25 : Float(17:189, 12:3213, 7:27, 27:1) = aten::einsum(%50, %928), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %930 : int = aten::size(%positional_attn.25, %85), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %931 : int = aten::size(%positional_attn.25, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %932 : int = aten::size(%positional_attn.25, %67), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %933 : int = aten::size(%positional_attn.25, %49), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.5 : Long() = prim::NumToTensor(%933), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %935 : int[] = prim::ListConstruct(%930, %931, %933, %932), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %positional_attn.26 : Float(17:189, 12:3213, 27:7, 7:1) = aten::reshape(%positional_attn.25, %935), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:428:0
  %937 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%positional_attn.26, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %938 : Float(17:189, 12:3213, 27:7, 7:1) = aten::slice(%937, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %939 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%938, %67, %67, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.27 : Float(17:189, 12:3213, 25:7, 7:1) = aten::slice(%939, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:429:0
  %941 : Long() = aten::sub(%max_rel_len.5, %75, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %942 : int = aten::Int(%941), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %943 : int[] = prim::ListConstruct(%930, %931, %932, %942), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %positional_attn.28 : Float(17:189, 12:3213, 7:25, 25:1) = aten::reshape(%positional_attn.27, %943), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.29 : Float(17:189, 12:3213, 7:25, 13:1) = aten::slice(%positional_attn.28, %49, %85, %899, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.30 : Float(17:189, 12:3213, 7:25, 13:1) = aten::mul_(%positional_attn.29, %cls_mask.2), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:498:0
  %947 : int = aten::size(%token_type_mat.7, %85), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %948 : int = aten::size(%token_type_mat.7, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %949 : int = aten::size(%token_type_mat.7, %67), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.5 : Float(12:64, 64:1) = aten::mul(%890, %53), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:508:0
  %951 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.10, %r_s_bias.5, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:511:0
  %952 : Tensor[] = prim::ListConstruct(%951, %889), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %953 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%48, %952), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %954 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %955 : Bool(17:182, 1:182, 7:26, 13:1) = aten::unsqueeze(%954, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %956 : int = aten::size(%q_head.10, %67), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %957 : int[] = prim::ListConstruct(%947, %956, %948, %949), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %token_type_mat.8 : Bool(17:182, 12:0, 7:26, 13:1) = aten::expand(%955, %957, %80), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:513:0
  %959 : Tensor[] = aten::split(%953, %86, %72), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/tensor.py:371:0
  %diff_token_type.5 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.5 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%959), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %962 : int = aten::size(%token_type_mat.8, %85), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %963 : int = aten::size(%token_type_mat.8, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %964 : int = aten::size(%token_type_mat.8, %67), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %965 : int = aten::size(%token_type_mat.8, %49), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %966 : int[] = prim::ListConstruct(%962, %963, %964, %965), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %967 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%same_token_type.5, %966, %80), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %968 : int = aten::size(%token_type_mat.8, %85), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %969 : int = aten::size(%token_type_mat.8, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %970 : int = aten::size(%token_type_mat.8, %67), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %971 : int = aten::size(%token_type_mat.8, %49), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %972 : int[] = prim::ListConstruct(%968, %969, %970, %971), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %973 : Float(17:14, 12:238, 7:2, 13:0) = aten::expand(%diff_token_type.5, %972, %80), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.9 : Float(17:1092, 12:91, 7:13, 13:1) = aten::where(%token_type_mat.8, %967, %973), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.10 : Float(17:1092, 12:91, 7:13, 13:1) = aten::mul_(%token_type_attn.9, %cls_mask.2), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:522:0
  %976 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%content_score.5, %positional_attn.30, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.13 : Float(17:1092, 12:91, 7:13, 13:1) = aten::add(%976, %token_type_attn.10, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.14 : Float(17:1092, 12:91, 7:13, 13:1) = aten::to(%attn_score.13, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:553:0
  %979 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %980 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%979, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %981 : Float(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%980, %67), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %982 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%981, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %983 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%982, %86, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/tensor.py:396:0
  %984 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%983, %47), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.15 : Float(17:1092, 12:91, 7:13, 13:1) = aten::sub(%attn_score.14, %984, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:556:0
  %input.42 : Float(17:1092, 12:91, 7:13, 13:1) = aten::softmax(%attn_score.15, %72, %82), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:558:0
  %987 : Float(17:1092, 12:91, 7:13, 13:1) = aten::dropout(%input.42, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %988 : Tensor[] = prim::ListConstruct(%987, %918), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %attn_vec.5 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%46, %988), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # torch/functional.py:327:0
  %990 : int[] = prim::ListConstruct(%897, %898, %69), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention
  %input.43 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.5, %990), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:565:0
  %992 : Tensor = prim::GetAttr[name="bias"](%888)
  %993 : Tensor = prim::GetAttr[name="weight"](%888)
  %994 : Float(768:1, 768:768) = aten::t(%993), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.23 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.43, %994), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.44 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.23, %992, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.44, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:5376, 7:768, 768:1) = aten::add(%query.4, %attn_out.5, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention # transformers/modeling_funnel.py:568:0
  %999 : Tensor = prim::GetAttr[name="bias"](%887)
  %1000 : Tensor = prim::GetAttr[name="weight"](%887)
  %1001 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.layer_norm
  %input.46 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.45, %1001, %1000, %999, %44, %45), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.attention/__module.encoder.blocks.1.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1003 : __torch__.torch.nn.modules.normalization.___torch_mangle_4649.LayerNorm = prim::GetAttr[name="layer_norm"](%885)
  %1004 : __torch__.torch.nn.modules.linear.___torch_mangle_4647.Linear = prim::GetAttr[name="linear_2"](%885)
  %1005 : __torch__.torch.nn.modules.linear.___torch_mangle_4645.Linear = prim::GetAttr[name="linear_1"](%885)
  %1006 : Tensor = prim::GetAttr[name="bias"](%1005)
  %1007 : Tensor = prim::GetAttr[name="weight"](%1005)
  %1008 : Float(768:1, 3072:768) = aten::t(%1007), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.46, %1008), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.5 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.24, %1006, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1011 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.5, %61), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1012 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.5, %60), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1013 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1012, %59), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1014 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.5, %1013, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1015 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1014, %58), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1016 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1015), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %1017 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1016, %57, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.47 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1011, %1017), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/activations.py:30:0
  %input.48 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.47, %56, %80), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1020 : Tensor = prim::GetAttr[name="bias"](%1004)
  %1021 : Tensor = prim::GetAttr[name="weight"](%1004)
  %1022 : Float(3072:1, 768:3072) = aten::t(%1021), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.25 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.48, %1022), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.49 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.25, %1020, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.5 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.49, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:5376, 7:768, 768:1) = aten::add(%input.46, %h.5, %86), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn # transformers/modeling_funnel.py:588:0
  %1027 : Tensor = prim::GetAttr[name="bias"](%1003)
  %1028 : Tensor = prim::GetAttr[name="weight"](%1003)
  %1029 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.layer_norm
  %query.5 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.50, %1029, %1028, %1027, %44, %45), scope: __module.encoder/__module.encoder.blocks.1.0/__module.encoder.blocks.1.0.ffn/__module.encoder.blocks.1.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1031 : Bool(17:182, 7:26, 13:1) = aten::slice(%token_type_mat.7, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1032 : Bool(17:182, 7:26, 13:1) = aten::slice(%1031, %86, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1033 : Bool(17:182, 7:26, 1:1) = aten::slice(%1032, %67, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1034 : Tensor[] = prim::ListConstruct(%1033, %token_type_mat.7), scope: __module.encoder
  %tensor.6 : Bool(17:98, 7:14, 14:1) = aten::cat(%1034, %67), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1036 : Bool(17:98, 7:14, 14:1) = aten::slice(%tensor.6, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1037 : Bool(17:98, 7:14, 14:1) = aten::slice(%1036, %86, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.9 : Bool(17:98, 7:14, 7:2) = aten::slice(%1037, %67, %85, %72, %67), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1039 : Float(7:26, 13:1) = aten::slice(%cls_mask.2, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1040 : Float(7:26, 1:1) = aten::slice(%1039, %86, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1041 : Tensor[] = prim::ListConstruct(%1040, %cls_mask.2), scope: __module.encoder
  %tensor.7 : Float(7:14, 14:1) = aten::cat(%1041, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1043 : Float(7:14, 14:1) = aten::slice(%tensor.7, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.3 : Float(7:14, 7:2) = aten::slice(%1043, %86, %85, %72, %67), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1045 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %suffix.2 : Float(17:13, 12:1) = aten::slice(%1045, %86, %85, %72, %86), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %1047 : Float(17:13, 13:1) = aten::slice(%attention_mask.2, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1048 : Float(17:13, 1:1) = aten::slice(%1047, %86, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1049 : Tensor[] = prim::ListConstruct(%1048, %suffix.2), scope: __module.encoder
  %tensor.8 : Float(17:13, 13:1) = aten::cat(%1049, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1051 : Float(17:13, 13:1) = aten::slice(%tensor.8, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %1052 : Float(17:13, 1:13, 13:1) = aten::unsqueeze(%1051, %86), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %1053 : Float(17:13, 1:13, 13:1) = aten::slice(%1052, %67, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %tensor.9 : Float(17:13, 1:13, 13:1, 1:1) = aten::unsqueeze(%1053, %49), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %input.51 : Float(17:13, 1:13, 13:1, 1:1) = aten::neg(%tensor.9), scope: __module.encoder # transformers/modeling_funnel.py:375:0
  %1056 : int[] = prim::ListConstruct(%67, %86), scope: __module.encoder
  %1057 : int[] = prim::ListConstruct(%67, %86), scope: __module.encoder
  %1058 : int[] = prim::ListConstruct(%85, %85), scope: __module.encoder
  %1059 : int[] = prim::ListConstruct(%86, %86), scope: __module.encoder
  %1060 : Float(17:7, 1:7, 7:1, 1:1) = aten::max_pool2d(%input.51, %1056, %1057, %1058, %1059, %45), scope: __module.encoder # torch/nn/functional.py:575:0
  %tensor.10 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%1060), scope: __module.encoder # transformers/modeling_funnel.py:375:0
  %1062 : Float(17:7, 1:7, 7:1, 1:1) = aten::slice(%tensor.10, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1063 : Float(17:7, 7:1, 1:1) = aten::select(%1062, %86, %85), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1064 : Float(17:7, 7:1, 1:1) = aten::slice(%1063, %86, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask.3 : Float(17:7, 7:1) = aten::select(%1064, %67, %85), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1066 : __torch__.transformers.modeling_funnel.___torch_mangle_4665.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%107)
  %1067 : __torch__.transformers.modeling_funnel.___torch_mangle_4659.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%107)
  %1068 : __torch__.torch.nn.modules.normalization.___torch_mangle_4658.LayerNorm = prim::GetAttr[name="layer_norm"](%1067)
  %1069 : __torch__.torch.nn.modules.linear.___torch_mangle_4657.Linear = prim::GetAttr[name="post_proj"](%1067)
  %1070 : Tensor = prim::GetAttr[name="seg_embed"](%1067)
  %1071 : Tensor = prim::GetAttr[name="r_s_bias"](%1067)
  %1072 : Tensor = prim::GetAttr[name="r_kernel"](%1067)
  %1073 : Tensor = prim::GetAttr[name="r_r_bias"](%1067)
  %1074 : Tensor = prim::GetAttr[name="r_w_bias"](%1067)
  %1075 : __torch__.torch.nn.modules.linear.___torch_mangle_4656.Linear = prim::GetAttr[name="v_head"](%1067)
  %1076 : __torch__.torch.nn.modules.linear.___torch_mangle_4655.Linear = prim::GetAttr[name="k_head"](%1067)
  %1077 : __torch__.torch.nn.modules.linear.___torch_mangle_4654.Linear = prim::GetAttr[name="q_head"](%1067)
  %1078 : int = aten::size(%query.5, %85), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1079 : int = aten::size(%query.5, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:530:0
  %1080 : int = aten::size(%query.5, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:531:0
  %1081 : Tensor = prim::GetAttr[name="weight"](%1077)
  %1082 : Float(768:1, 768:768) = aten::t(%1081), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1083 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1082), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.q_head # torch/nn/functional.py:1676:0
  %1084 : int[] = prim::ListConstruct(%1078, %1079, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %q_head.11 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1083, %1084), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:535:0
  %1086 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1087 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1088 : Float(768:1, 768:768) = aten::t(%1087), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.26 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1088), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1676:0
  %1090 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.26, %1086, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.k_head # torch/nn/functional.py:1678:0
  %1091 : int[] = prim::ListConstruct(%1078, %1080, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1092 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1090, %1091), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:537:0
  %1093 : Tensor = prim::GetAttr[name="bias"](%1075)
  %1094 : Tensor = prim::GetAttr[name="weight"](%1075)
  %1095 : Float(768:1, 768:768) = aten::t(%1094), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.27 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.5, %1095), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1676:0
  %1097 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.27, %1093, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.v_head # torch/nn/functional.py:1678:0
  %1098 : int[] = prim::ListConstruct(%1078, %1080, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1099 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1097, %1098), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.12 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.11, %53), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.6 : Float(12:64, 64:1) = aten::mul(%1074, %53), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:542:0
  %1102 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_w_bias.6, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:544:0
  %1103 : Tensor[] = prim::ListConstruct(%1102, %1092), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %content_score.6 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%52, %1103), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %v.6 : Float(12:64, 64:1) = aten::mul(%1073, %53), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:486:0
  %1106 : Tensor[] = prim::ListConstruct(%210, %1072), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1107 : Float(14:768, 12:64, 64:1) = aten::einsum(%51, %1106), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1108 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %v.6, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:493:0
  %1109 : Tensor[] = prim::ListConstruct(%1108, %1107), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %positional_attn.31 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%50, %1109), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1111 : int = aten::size(%positional_attn.31, %85), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1112 : int = aten::size(%positional_attn.31, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1113 : int = aten::size(%positional_attn.31, %67), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %1114 : int = aten::size(%positional_attn.31, %49), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.6 : Long() = prim::NumToTensor(%1114), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1116 : int[] = prim::ListConstruct(%1111, %1112, %1114, %1113), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %positional_attn.32 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.31, %1116), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:428:0
  %1118 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.32, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1119 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1118, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1120 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1119, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.33 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1120, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:429:0
  %1122 : Long() = aten::sub(%max_rel_len.6, %76, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %1123 : int = aten::Int(%1122), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1124 : int[] = prim::ListConstruct(%1111, %1112, %1113, %1123), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %positional_attn.34 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.33, %1124), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.35 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.34, %49, %85, %1080, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.36 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.35, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:498:0
  %1128 : int = aten::size(%token_type_mat.9, %85), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1129 : int = aten::size(%token_type_mat.9, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %1130 : int = aten::size(%token_type_mat.9, %67), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.6 : Float(12:64, 64:1) = aten::mul(%1071, %53), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:508:0
  %1132 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.12, %r_s_bias.6, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:511:0
  %1133 : Tensor[] = prim::ListConstruct(%1132, %1070), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1134 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%48, %1133), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1135 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1136 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1135, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1137 : int = aten::size(%q_head.12, %67), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1138 : int[] = prim::ListConstruct(%1128, %1137, %1129, %1130), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %token_type_mat.10 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1136, %1138, %80), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:513:0
  %1140 : Tensor[] = aten::split(%1134, %86, %72), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/tensor.py:371:0
  %diff_token_type.6 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.6 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1140), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1143 : int = aten::size(%token_type_mat.10, %85), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1144 : int = aten::size(%token_type_mat.10, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1145 : int = aten::size(%token_type_mat.10, %67), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1146 : int = aten::size(%token_type_mat.10, %49), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1147 : int[] = prim::ListConstruct(%1143, %1144, %1145, %1146), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1148 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.6, %1147, %80), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1149 : int = aten::size(%token_type_mat.10, %85), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1150 : int = aten::size(%token_type_mat.10, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1151 : int = aten::size(%token_type_mat.10, %67), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1152 : int = aten::size(%token_type_mat.10, %49), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %1153 : int[] = prim::ListConstruct(%1149, %1150, %1151, %1152), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %1154 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.6, %1153, %80), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.11 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.10, %1148, %1154), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.12 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.11, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:522:0
  %1157 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.6, %positional_attn.36, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1157, %token_type_attn.12, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.17 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.16, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:553:0
  %1160 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1161 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1160, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1162 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1161, %67), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1163 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1162, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %1164 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1163, %86, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/tensor.py:396:0
  %1165 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1164, %47), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.18 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.17, %1165, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:556:0
  %input.52 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.18, %72, %82), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:558:0
  %1168 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.52, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1169 : Tensor[] = prim::ListConstruct(%1168, %1099), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %attn_vec.6 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%46, %1169), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # torch/functional.py:327:0
  %1171 : int[] = prim::ListConstruct(%1078, %1079, %69), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention
  %input.53 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.6, %1171), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:565:0
  %1173 : Tensor = prim::GetAttr[name="bias"](%1069)
  %1174 : Tensor = prim::GetAttr[name="weight"](%1069)
  %1175 : Float(768:1, 768:768) = aten::t(%1174), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.53, %1175), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.54 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.28, %1173, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.54, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:5376, 7:768, 768:1) = aten::add(%query.5, %attn_out.6, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention # transformers/modeling_funnel.py:568:0
  %1180 : Tensor = prim::GetAttr[name="bias"](%1068)
  %1181 : Tensor = prim::GetAttr[name="weight"](%1068)
  %1182 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.layer_norm
  %input.56 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.55, %1182, %1181, %1180, %44, %45), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.attention/__module.encoder.blocks.1.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1184 : __torch__.torch.nn.modules.normalization.___torch_mangle_4664.LayerNorm = prim::GetAttr[name="layer_norm"](%1066)
  %1185 : __torch__.torch.nn.modules.linear.___torch_mangle_4662.Linear = prim::GetAttr[name="linear_2"](%1066)
  %1186 : __torch__.torch.nn.modules.linear.___torch_mangle_4660.Linear = prim::GetAttr[name="linear_1"](%1066)
  %1187 : Tensor = prim::GetAttr[name="bias"](%1186)
  %1188 : Tensor = prim::GetAttr[name="weight"](%1186)
  %1189 : Float(768:1, 3072:768) = aten::t(%1188), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.56, %1189), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.6 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.29, %1187, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1192 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.6, %61), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1193 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.6, %60), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1194 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1193, %59), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1195 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.6, %1194, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1196 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1195, %58), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1197 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1196), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %1198 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1197, %57, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.57 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1192, %1198), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/activations.py:30:0
  %input.58 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.57, %56, %80), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1201 : Tensor = prim::GetAttr[name="bias"](%1185)
  %1202 : Tensor = prim::GetAttr[name="weight"](%1185)
  %1203 : Float(3072:1, 768:3072) = aten::t(%1202), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.58, %1203), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.59 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.30, %1201, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.6 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.59, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:5376, 7:768, 768:1) = aten::add(%input.56, %h.6, %86), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn # transformers/modeling_funnel.py:588:0
  %1208 : Tensor = prim::GetAttr[name="bias"](%1184)
  %1209 : Tensor = prim::GetAttr[name="weight"](%1184)
  %1210 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.layer_norm
  %query.6 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.60, %1210, %1209, %1208, %44, %45), scope: __module.encoder/__module.encoder.blocks.1.1/__module.encoder.blocks.1.1.ffn/__module.encoder.blocks.1.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1212 : __torch__.transformers.modeling_funnel.___torch_mangle_4680.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%104)
  %1213 : __torch__.transformers.modeling_funnel.___torch_mangle_4674.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%104)
  %1214 : __torch__.torch.nn.modules.normalization.___torch_mangle_4673.LayerNorm = prim::GetAttr[name="layer_norm"](%1213)
  %1215 : __torch__.torch.nn.modules.linear.___torch_mangle_4672.Linear = prim::GetAttr[name="post_proj"](%1213)
  %1216 : Tensor = prim::GetAttr[name="seg_embed"](%1213)
  %1217 : Tensor = prim::GetAttr[name="r_s_bias"](%1213)
  %1218 : Tensor = prim::GetAttr[name="r_kernel"](%1213)
  %1219 : Tensor = prim::GetAttr[name="r_r_bias"](%1213)
  %1220 : Tensor = prim::GetAttr[name="r_w_bias"](%1213)
  %1221 : __torch__.torch.nn.modules.linear.___torch_mangle_4671.Linear = prim::GetAttr[name="v_head"](%1213)
  %1222 : __torch__.torch.nn.modules.linear.___torch_mangle_4670.Linear = prim::GetAttr[name="k_head"](%1213)
  %1223 : __torch__.torch.nn.modules.linear.___torch_mangle_4669.Linear = prim::GetAttr[name="q_head"](%1213)
  %1224 : int = aten::size(%query.6, %85), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1225 : int = aten::size(%query.6, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:530:0
  %1226 : int = aten::size(%query.6, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:531:0
  %1227 : Tensor = prim::GetAttr[name="weight"](%1223)
  %1228 : Float(768:1, 768:768) = aten::t(%1227), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1229 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1228), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.q_head # torch/nn/functional.py:1676:0
  %1230 : int[] = prim::ListConstruct(%1224, %1225, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %q_head.13 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1229, %1230), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:535:0
  %1232 : Tensor = prim::GetAttr[name="bias"](%1222)
  %1233 : Tensor = prim::GetAttr[name="weight"](%1222)
  %1234 : Float(768:1, 768:768) = aten::t(%1233), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.31 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1234), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1676:0
  %1236 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.31, %1232, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.k_head # torch/nn/functional.py:1678:0
  %1237 : int[] = prim::ListConstruct(%1224, %1226, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1238 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1236, %1237), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:537:0
  %1239 : Tensor = prim::GetAttr[name="bias"](%1221)
  %1240 : Tensor = prim::GetAttr[name="weight"](%1221)
  %1241 : Float(768:1, 768:768) = aten::t(%1240), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.32 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.6, %1241), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1676:0
  %1243 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.32, %1239, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.v_head # torch/nn/functional.py:1678:0
  %1244 : int[] = prim::ListConstruct(%1224, %1226, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1245 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1243, %1244), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.14 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.13, %53), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.7 : Float(12:64, 64:1) = aten::mul(%1220, %53), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:542:0
  %1248 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_w_bias.7, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:544:0
  %1249 : Tensor[] = prim::ListConstruct(%1248, %1238), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %content_score.7 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%52, %1249), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %v.7 : Float(12:64, 64:1) = aten::mul(%1219, %53), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:486:0
  %1252 : Tensor[] = prim::ListConstruct(%210, %1218), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1253 : Float(14:768, 12:64, 64:1) = aten::einsum(%51, %1252), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1254 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %v.7, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:493:0
  %1255 : Tensor[] = prim::ListConstruct(%1254, %1253), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %positional_attn.37 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%50, %1255), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1257 : int = aten::size(%positional_attn.37, %85), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1258 : int = aten::size(%positional_attn.37, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1259 : int = aten::size(%positional_attn.37, %67), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %1260 : int = aten::size(%positional_attn.37, %49), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.7 : Long() = prim::NumToTensor(%1260), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1262 : int[] = prim::ListConstruct(%1257, %1258, %1260, %1259), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %positional_attn.38 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.37, %1262), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:428:0
  %1264 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.38, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1265 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1264, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1266 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1265, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.39 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1266, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:429:0
  %1268 : Long() = aten::sub(%max_rel_len.7, %76, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %1269 : int = aten::Int(%1268), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1270 : int[] = prim::ListConstruct(%1257, %1258, %1259, %1269), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %positional_attn.40 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.39, %1270), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.41 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.40, %49, %85, %1226, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.42 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.41, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:498:0
  %1274 : int = aten::size(%token_type_mat.9, %85), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1275 : int = aten::size(%token_type_mat.9, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %1276 : int = aten::size(%token_type_mat.9, %67), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.7 : Float(12:64, 64:1) = aten::mul(%1217, %53), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:508:0
  %1278 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.14, %r_s_bias.7, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:511:0
  %1279 : Tensor[] = prim::ListConstruct(%1278, %1216), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1280 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%48, %1279), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1281 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1282 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1281, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1283 : int = aten::size(%q_head.14, %67), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1284 : int[] = prim::ListConstruct(%1274, %1283, %1275, %1276), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %token_type_mat.11 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1282, %1284, %80), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:513:0
  %1286 : Tensor[] = aten::split(%1280, %86, %72), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/tensor.py:371:0
  %diff_token_type.7 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.7 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1286), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1289 : int = aten::size(%token_type_mat.11, %85), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1290 : int = aten::size(%token_type_mat.11, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1291 : int = aten::size(%token_type_mat.11, %67), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1292 : int = aten::size(%token_type_mat.11, %49), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1293 : int[] = prim::ListConstruct(%1289, %1290, %1291, %1292), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1294 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.7, %1293, %80), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1295 : int = aten::size(%token_type_mat.11, %85), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1296 : int = aten::size(%token_type_mat.11, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1297 : int = aten::size(%token_type_mat.11, %67), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1298 : int = aten::size(%token_type_mat.11, %49), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %1299 : int[] = prim::ListConstruct(%1295, %1296, %1297, %1298), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %1300 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.7, %1299, %80), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.13 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.11, %1294, %1300), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.14 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.13, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:522:0
  %1303 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.7, %positional_attn.42, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.19 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1303, %token_type_attn.14, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.20 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.19, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:553:0
  %1306 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1307 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1306, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1308 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1307, %67), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1309 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1308, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %1310 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1309, %86, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/tensor.py:396:0
  %1311 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1310, %47), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.21 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.20, %1311, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:556:0
  %input.61 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.21, %72, %82), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:558:0
  %1314 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.61, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1315 : Tensor[] = prim::ListConstruct(%1314, %1245), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %attn_vec.7 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%46, %1315), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # torch/functional.py:327:0
  %1317 : int[] = prim::ListConstruct(%1224, %1225, %69), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention
  %input.62 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.7, %1317), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:565:0
  %1319 : Tensor = prim::GetAttr[name="bias"](%1215)
  %1320 : Tensor = prim::GetAttr[name="weight"](%1215)
  %1321 : Float(768:1, 768:768) = aten::t(%1320), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.62, %1321), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.63 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.33, %1319, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.63, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:5376, 7:768, 768:1) = aten::add(%query.6, %attn_out.7, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention # transformers/modeling_funnel.py:568:0
  %1326 : Tensor = prim::GetAttr[name="bias"](%1214)
  %1327 : Tensor = prim::GetAttr[name="weight"](%1214)
  %1328 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.layer_norm
  %input.65 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.64, %1328, %1327, %1326, %44, %45), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.attention/__module.encoder.blocks.1.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1330 : __torch__.torch.nn.modules.normalization.___torch_mangle_4679.LayerNorm = prim::GetAttr[name="layer_norm"](%1212)
  %1331 : __torch__.torch.nn.modules.linear.___torch_mangle_4677.Linear = prim::GetAttr[name="linear_2"](%1212)
  %1332 : __torch__.torch.nn.modules.linear.___torch_mangle_4675.Linear = prim::GetAttr[name="linear_1"](%1212)
  %1333 : Tensor = prim::GetAttr[name="bias"](%1332)
  %1334 : Tensor = prim::GetAttr[name="weight"](%1332)
  %1335 : Float(768:1, 3072:768) = aten::t(%1334), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.65, %1335), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.7 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.34, %1333, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1338 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.7, %61), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1339 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.7, %60), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1340 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1339, %59), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1341 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.7, %1340, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1342 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1341, %58), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1343 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1342), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %1344 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1343, %57, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.66 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1338, %1344), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/activations.py:30:0
  %input.67 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.66, %56, %80), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1347 : Tensor = prim::GetAttr[name="bias"](%1331)
  %1348 : Tensor = prim::GetAttr[name="weight"](%1331)
  %1349 : Float(3072:1, 768:3072) = aten::t(%1348), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.67, %1349), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.68 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.35, %1347, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.7 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.68, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:5376, 7:768, 768:1) = aten::add(%input.65, %h.7, %86), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn # transformers/modeling_funnel.py:588:0
  %1354 : Tensor = prim::GetAttr[name="bias"](%1330)
  %1355 : Tensor = prim::GetAttr[name="weight"](%1330)
  %1356 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.layer_norm
  %query.7 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.69, %1356, %1355, %1354, %44, %45), scope: __module.encoder/__module.encoder.blocks.1.2/__module.encoder.blocks.1.2.ffn/__module.encoder.blocks.1.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1358 : __torch__.transformers.modeling_funnel.___torch_mangle_4695.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%101)
  %1359 : __torch__.transformers.modeling_funnel.___torch_mangle_4689.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%101)
  %1360 : __torch__.torch.nn.modules.normalization.___torch_mangle_4688.LayerNorm = prim::GetAttr[name="layer_norm"](%1359)
  %1361 : __torch__.torch.nn.modules.linear.___torch_mangle_4687.Linear = prim::GetAttr[name="post_proj"](%1359)
  %1362 : Tensor = prim::GetAttr[name="seg_embed"](%1359)
  %1363 : Tensor = prim::GetAttr[name="r_s_bias"](%1359)
  %1364 : Tensor = prim::GetAttr[name="r_kernel"](%1359)
  %1365 : Tensor = prim::GetAttr[name="r_r_bias"](%1359)
  %1366 : Tensor = prim::GetAttr[name="r_w_bias"](%1359)
  %1367 : __torch__.torch.nn.modules.linear.___torch_mangle_4686.Linear = prim::GetAttr[name="v_head"](%1359)
  %1368 : __torch__.torch.nn.modules.linear.___torch_mangle_4685.Linear = prim::GetAttr[name="k_head"](%1359)
  %1369 : __torch__.torch.nn.modules.linear.___torch_mangle_4684.Linear = prim::GetAttr[name="q_head"](%1359)
  %1370 : int = aten::size(%query.7, %85), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1371 : int = aten::size(%query.7, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:530:0
  %1372 : int = aten::size(%query.7, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:531:0
  %1373 : Tensor = prim::GetAttr[name="weight"](%1369)
  %1374 : Float(768:1, 768:768) = aten::t(%1373), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1375 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1374), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.q_head # torch/nn/functional.py:1676:0
  %1376 : int[] = prim::ListConstruct(%1370, %1371, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %q_head.15 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1375, %1376), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:535:0
  %1378 : Tensor = prim::GetAttr[name="bias"](%1368)
  %1379 : Tensor = prim::GetAttr[name="weight"](%1368)
  %1380 : Float(768:1, 768:768) = aten::t(%1379), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.36 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1380), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1676:0
  %1382 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.36, %1378, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.k_head # torch/nn/functional.py:1678:0
  %1383 : int[] = prim::ListConstruct(%1370, %1372, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1384 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1382, %1383), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:537:0
  %1385 : Tensor = prim::GetAttr[name="bias"](%1367)
  %1386 : Tensor = prim::GetAttr[name="weight"](%1367)
  %1387 : Float(768:1, 768:768) = aten::t(%1386), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.37 : Float(17:5376, 7:768, 768:1) = aten::matmul(%query.7, %1387), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1676:0
  %1389 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.37, %1385, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.v_head # torch/nn/functional.py:1678:0
  %1390 : int[] = prim::ListConstruct(%1370, %1372, %55, %54), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1391 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1389, %1390), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.16 : Float(17:5376, 7:768, 12:64, 64:1) = aten::mul(%q_head.15, %53), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.8 : Float(12:64, 64:1) = aten::mul(%1366, %53), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:542:0
  %1394 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_w_bias.8, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:544:0
  %1395 : Tensor[] = prim::ListConstruct(%1394, %1384), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %content_score.8 : Float(17:588, 12:49, 7:7, 7:1) = aten::einsum(%52, %1395), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %v.8 : Float(12:64, 64:1) = aten::mul(%1365, %53), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:486:0
  %1398 : Tensor[] = prim::ListConstruct(%210, %1364), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1399 : Float(14:768, 12:64, 64:1) = aten::einsum(%51, %1398), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1400 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %v.8, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:493:0
  %1401 : Tensor[] = prim::ListConstruct(%1400, %1399), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %positional_attn.43 : Float(17:98, 12:1666, 7:14, 14:1) = aten::einsum(%50, %1401), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1403 : int = aten::size(%positional_attn.43, %85), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1404 : int = aten::size(%positional_attn.43, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1405 : int = aten::size(%positional_attn.43, %67), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %1406 : int = aten::size(%positional_attn.43, %49), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.8 : Long() = prim::NumToTensor(%1406), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1408 : int[] = prim::ListConstruct(%1403, %1404, %1406, %1405), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %positional_attn.44 : Float(17:98, 12:1666, 14:7, 7:1) = aten::reshape(%positional_attn.43, %1408), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:428:0
  %1410 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%positional_attn.44, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1411 : Float(17:98, 12:1666, 14:7, 7:1) = aten::slice(%1410, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1412 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1411, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.45 : Float(17:98, 12:1666, 13:7, 7:1) = aten::slice(%1412, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:429:0
  %1414 : Long() = aten::sub(%max_rel_len.8, %76, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %1415 : int = aten::Int(%1414), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1416 : int[] = prim::ListConstruct(%1403, %1404, %1405, %1415), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %positional_attn.46 : Float(17:98, 12:1666, 7:13, 13:1) = aten::reshape(%positional_attn.45, %1416), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.47 : Float(17:98, 12:1666, 7:13, 7:1) = aten::slice(%positional_attn.46, %49, %85, %1372, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.48 : Float(17:98, 12:1666, 7:13, 7:1) = aten::mul_(%positional_attn.47, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:498:0
  %1420 : int = aten::size(%token_type_mat.9, %85), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1421 : int = aten::size(%token_type_mat.9, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %1422 : int = aten::size(%token_type_mat.9, %67), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.8 : Float(12:64, 64:1) = aten::mul(%1363, %53), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:508:0
  %1424 : Float(17:5376, 7:768, 12:64, 64:1) = aten::add(%q_head.16, %r_s_bias.8, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:511:0
  %1425 : Tensor[] = prim::ListConstruct(%1424, %1362), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1426 : Float(17:14, 12:238, 7:2, 2:1) = aten::einsum(%48, %1425), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1427 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1428 : Bool(17:98, 1:98, 7:14, 7:2) = aten::unsqueeze(%1427, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1429 : int = aten::size(%q_head.16, %67), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1430 : int[] = prim::ListConstruct(%1420, %1429, %1421, %1422), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %token_type_mat.12 : Bool(17:98, 12:0, 7:14, 7:2) = aten::expand(%1428, %1430, %80), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:513:0
  %1432 : Tensor[] = aten::split(%1426, %86, %72), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/tensor.py:371:0
  %diff_token_type.8 : Float(17:14, 12:238, 7:2, 1:1), %same_token_type.8 : Float(17:14, 12:238, 7:2, 1:1) = prim::ListUnpack(%1432), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1435 : int = aten::size(%token_type_mat.12, %85), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1436 : int = aten::size(%token_type_mat.12, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1437 : int = aten::size(%token_type_mat.12, %67), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1438 : int = aten::size(%token_type_mat.12, %49), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1439 : int[] = prim::ListConstruct(%1435, %1436, %1437, %1438), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1440 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%same_token_type.8, %1439, %80), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1441 : int = aten::size(%token_type_mat.12, %85), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1442 : int = aten::size(%token_type_mat.12, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1443 : int = aten::size(%token_type_mat.12, %67), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1444 : int = aten::size(%token_type_mat.12, %49), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %1445 : int[] = prim::ListConstruct(%1441, %1442, %1443, %1444), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %1446 : Float(17:14, 12:238, 7:2, 7:0) = aten::expand(%diff_token_type.8, %1445, %80), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.15 : Float(17:588, 12:49, 7:7, 7:1) = aten::where(%token_type_mat.12, %1440, %1446), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.16 : Float(17:588, 12:49, 7:7, 7:1) = aten::mul_(%token_type_attn.15, %cls_mask.3), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:522:0
  %1449 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%content_score.8, %positional_attn.48, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.22 : Float(17:588, 12:49, 7:7, 7:1) = aten::add(%1449, %token_type_attn.16, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.23 : Float(17:588, 12:49, 7:7, 7:1) = aten::to(%attn_score.22, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:553:0
  %1452 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1453 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1452, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1454 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1453, %67), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1455 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1454, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %1456 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1455, %86, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/tensor.py:396:0
  %1457 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1456, %47), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.24 : Float(17:588, 12:49, 7:7, 7:1) = aten::sub(%attn_score.23, %1457, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:556:0
  %input.70 : Float(17:588, 12:49, 7:7, 7:1) = aten::softmax(%attn_score.24, %72, %82), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:558:0
  %1460 : Float(17:588, 12:49, 7:7, 7:1) = aten::dropout(%input.70, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %1461 : Tensor[] = prim::ListConstruct(%1460, %1391), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %attn_vec.8 : Float(17:5376, 7:64, 12:448, 64:1) = aten::einsum(%46, %1461), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # torch/functional.py:327:0
  %1463 : int[] = prim::ListConstruct(%1370, %1371, %69), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention
  %input.71 : Float(17:5376, 7:768, 768:1) = aten::reshape(%attn_vec.8, %1463), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:565:0
  %1465 : Tensor = prim::GetAttr[name="bias"](%1361)
  %1466 : Tensor = prim::GetAttr[name="weight"](%1361)
  %1467 : Float(768:1, 768:768) = aten::t(%1466), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.71, %1467), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.72 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.38, %1465, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.72, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:5376, 7:768, 768:1) = aten::add(%query.7, %attn_out.8, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention # transformers/modeling_funnel.py:568:0
  %1472 : Tensor = prim::GetAttr[name="bias"](%1360)
  %1473 : Tensor = prim::GetAttr[name="weight"](%1360)
  %1474 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.layer_norm
  %input.74 : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.73, %1474, %1473, %1472, %44, %45), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.attention/__module.encoder.blocks.1.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %1476 : __torch__.torch.nn.modules.normalization.___torch_mangle_4694.LayerNorm = prim::GetAttr[name="layer_norm"](%1358)
  %1477 : __torch__.torch.nn.modules.linear.___torch_mangle_4692.Linear = prim::GetAttr[name="linear_2"](%1358)
  %1478 : __torch__.torch.nn.modules.linear.___torch_mangle_4690.Linear = prim::GetAttr[name="linear_1"](%1358)
  %1479 : Tensor = prim::GetAttr[name="bias"](%1478)
  %1480 : Tensor = prim::GetAttr[name="weight"](%1478)
  %1481 : Float(768:1, 3072:768) = aten::t(%1480), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.39 : Float(17:21504, 7:3072, 3072:1) = aten::matmul(%input.74, %1481), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.8 : Float(17:21504, 7:3072, 3072:1) = aten::add_(%output.39, %1479, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1484 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%x.8, %61), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1485 : Float(17:21504, 7:3072, 3072:1) = aten::pow(%x.8, %60), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1486 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1485, %59), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1487 : Float(17:21504, 7:3072, 3072:1) = aten::add(%x.8, %1486, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1488 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1487, %58), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1489 : Float(17:21504, 7:3072, 3072:1) = aten::tanh(%1488), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %1490 : Float(17:21504, 7:3072, 3072:1) = aten::add(%1489, %57, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.75 : Float(17:21504, 7:3072, 3072:1) = aten::mul(%1484, %1490), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/activations.py:30:0
  %input.76 : Float(17:21504, 7:3072, 3072:1) = aten::dropout(%input.75, %56, %80), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1493 : Tensor = prim::GetAttr[name="bias"](%1477)
  %1494 : Tensor = prim::GetAttr[name="weight"](%1477)
  %1495 : Float(3072:1, 768:3072) = aten::t(%1494), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.40 : Float(17:5376, 7:768, 768:1) = aten::matmul(%input.76, %1495), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.77 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.40, %1493, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.8 : Float(17:5376, 7:768, 768:1) = aten::dropout(%input.77, %73, %80), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(17:5376, 7:768, 768:1) = aten::add(%input.74, %h.8, %86), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn # transformers/modeling_funnel.py:588:0
  %1500 : Tensor = prim::GetAttr[name="bias"](%1476)
  %1501 : Tensor = prim::GetAttr[name="weight"](%1476)
  %1502 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.layer_norm
  %hidden : Float(17:5376, 7:768, 768:1) = aten::layer_norm(%input.78, %1502, %1501, %1500, %44, %45), scope: __module.encoder/__module.encoder.blocks.1.3/__module.encoder.blocks.1.3.ffn/__module.encoder.blocks.1.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1504 : Bool(17:98, 7:14, 7:2) = aten::slice(%token_type_mat.9, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1505 : Bool(17:98, 1:14, 7:2) = aten::slice(%1504, %86, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1506 : Tensor[] = prim::ListConstruct(%1505, %token_type_mat.9), scope: __module.encoder
  %tensor.11 : Bool(17:56, 8:7, 7:1) = aten::cat(%1506, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1508 : Bool(17:56, 8:7, 7:1) = aten::slice(%tensor.11, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.13 : Bool(17:56, 4:14, 7:1) = aten::slice(%1508, %86, %85, %72, %67), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1510 : Float(1:14, 7:2) = aten::slice(%cls_mask.3, %85, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1511 : Tensor[] = prim::ListConstruct(%1510, %cls_mask.3), scope: __module.encoder
  %tensor.12 : Float(8:7, 7:1) = aten::cat(%1511, %85), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %cls_mask.4 : Float(4:14, 7:1) = aten::slice(%tensor.12, %85, %85, %72, %67), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1514 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %suffix.3 : Float(17:5376, 6:768, 768:1) = aten::slice(%1514, %86, %85, %72, %86), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %1516 : Float(17:5376, 7:768, 768:1) = aten::slice(%hidden, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1517 : Float(17:5376, 1:768, 768:1) = aten::slice(%1516, %86, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1518 : Tensor[] = prim::ListConstruct(%1517, %suffix.3), scope: __module.encoder
  %tensor.13 : Float(17:5376, 7:768, 768:1) = aten::cat(%1518, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1520 : Float(17:5376, 7:768, 768:1) = aten::slice(%tensor.13, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %1521 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::unsqueeze(%1520, %86), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %1522 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1521, %67, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %tensor.14 : Float(17:5376, 1:5376, 7:768, 768:1) = aten::slice(%1522, %49, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:366:0
  %1524 : int[] = prim::ListConstruct(%67, %86), scope: __module.encoder
  %1525 : int[] = prim::ListConstruct(%67, %86), scope: __module.encoder
  %1526 : int[] = prim::ListConstruct(%85, %85), scope: __module.encoder
  %tensor.15 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::avg_pool2d(%tensor.14, %1524, %1525, %1526, %45, %45, %77), scope: __module.encoder # transformers/modeling_funnel.py:371:0
  %1528 : Float(17:3072, 1:3072, 4:768, 768:1) = aten::slice(%tensor.15, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:382:0
  %query.8 : Float(17:3072, 4:768, 768:1) = aten::select(%1528, %86, %85), scope: __module.encoder # transformers/modeling_funnel.py:382:0
  %1530 : __torch__.transformers.modeling_funnel.___torch_mangle_4711.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%98)
  %1531 : __torch__.transformers.modeling_funnel.___torch_mangle_4705.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%98)
  %1532 : __torch__.torch.nn.modules.normalization.___torch_mangle_4704.LayerNorm = prim::GetAttr[name="layer_norm"](%1531)
  %1533 : __torch__.torch.nn.modules.linear.___torch_mangle_4703.Linear = prim::GetAttr[name="post_proj"](%1531)
  %1534 : Tensor = prim::GetAttr[name="seg_embed"](%1531)
  %1535 : Tensor = prim::GetAttr[name="r_s_bias"](%1531)
  %1536 : Tensor = prim::GetAttr[name="r_kernel"](%1531)
  %1537 : Tensor = prim::GetAttr[name="r_r_bias"](%1531)
  %1538 : Tensor = prim::GetAttr[name="r_w_bias"](%1531)
  %1539 : __torch__.torch.nn.modules.linear.___torch_mangle_4702.Linear = prim::GetAttr[name="v_head"](%1531)
  %1540 : __torch__.torch.nn.modules.linear.___torch_mangle_4701.Linear = prim::GetAttr[name="k_head"](%1531)
  %1541 : __torch__.torch.nn.modules.linear.___torch_mangle_4700.Linear = prim::GetAttr[name="q_head"](%1531)
  %1542 : int = aten::size(%query.8, %85), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1543 : int = aten::size(%query.8, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:530:0
  %1544 : int = aten::size(%hidden, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:531:0
  %1545 : Tensor = prim::GetAttr[name="weight"](%1541)
  %1546 : Float(768:1, 768:768) = aten::t(%1545), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1547 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.8, %1546), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.q_head # torch/nn/functional.py:1676:0
  %1548 : int[] = prim::ListConstruct(%1542, %1543, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %q_head.17 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1547, %1548), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:535:0
  %1550 : Tensor = prim::GetAttr[name="bias"](%1540)
  %1551 : Tensor = prim::GetAttr[name="weight"](%1540)
  %1552 : Float(768:1, 768:768) = aten::t(%1551), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.41 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1552), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1676:0
  %1554 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.41, %1550, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.k_head # torch/nn/functional.py:1678:0
  %1555 : int[] = prim::ListConstruct(%1542, %1544, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1556 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1554, %1555), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:537:0
  %1557 : Tensor = prim::GetAttr[name="bias"](%1539)
  %1558 : Tensor = prim::GetAttr[name="weight"](%1539)
  %1559 : Float(768:1, 768:768) = aten::t(%1558), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.42 : Float(17:5376, 7:768, 768:1) = aten::matmul(%hidden, %1559), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1676:0
  %1561 : Float(17:5376, 7:768, 768:1) = aten::add_(%output.42, %1557, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.v_head # torch/nn/functional.py:1678:0
  %1562 : int[] = prim::ListConstruct(%1542, %1544, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1563 : Float(17:5376, 7:768, 12:64, 64:1) = aten::view(%1561, %1562), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.18 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.17, %53), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.9 : Float(12:64, 64:1) = aten::mul(%1538, %53), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:542:0
  %1566 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_w_bias.9, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:544:0
  %1567 : Tensor[] = prim::ListConstruct(%1566, %1556), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %content_score.9 : Float(17:336, 12:28, 4:7, 7:1) = aten::einsum(%52, %1567), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %v.9 : Float(12:64, 64:1) = aten::mul(%1537, %53), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:486:0
  %1570 : Tensor[] = prim::ListConstruct(%234, %1536), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1571 : Float(15:768, 12:64, 64:1) = aten::einsum(%51, %1570), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1572 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %v.9, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:493:0
  %1573 : Tensor[] = prim::ListConstruct(%1572, %1571), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %positional_attn.49 : Float(17:60, 12:1020, 4:15, 15:1) = aten::einsum(%50, %1573), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1575 : int = aten::size(%positional_attn.49, %85), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1576 : int = aten::size(%positional_attn.49, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1577 : int = aten::size(%positional_attn.49, %67), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %1578 : int = aten::size(%positional_attn.49, %49), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.9 : Long() = prim::NumToTensor(%1578), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1580 : int[] = prim::ListConstruct(%1575, %1576, %1578, %1577), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %positional_attn.50 : Float(17:60, 12:1020, 15:4, 4:1) = aten::reshape(%positional_attn.49, %1580), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:428:0
  %1582 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%positional_attn.50, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1583 : Float(17:60, 12:1020, 15:4, 4:1) = aten::slice(%1582, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1584 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1583, %67, %67, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.51 : Float(17:60, 12:1020, 13:4, 4:1) = aten::slice(%1584, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:429:0
  %1586 : Long() = aten::sub(%max_rel_len.9, %75, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %1587 : int = aten::Int(%1586), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1588 : int[] = prim::ListConstruct(%1575, %1576, %1577, %1587), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %positional_attn.52 : Float(17:60, 12:1020, 4:13, 13:1) = aten::reshape(%positional_attn.51, %1588), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.53 : Float(17:60, 12:1020, 4:13, 7:1) = aten::slice(%positional_attn.52, %49, %85, %1544, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.54 : Float(17:60, 12:1020, 4:13, 7:1) = aten::mul_(%positional_attn.53, %cls_mask.4), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:498:0
  %1592 : int = aten::size(%token_type_mat.13, %85), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1593 : int = aten::size(%token_type_mat.13, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %1594 : int = aten::size(%token_type_mat.13, %67), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.9 : Float(12:64, 64:1) = aten::mul(%1535, %53), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:508:0
  %1596 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.18, %r_s_bias.9, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:511:0
  %1597 : Tensor[] = prim::ListConstruct(%1596, %1534), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1598 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%48, %1597), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1599 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1600 : Bool(17:56, 1:56, 4:14, 7:1) = aten::unsqueeze(%1599, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1601 : int = aten::size(%q_head.18, %67), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1602 : int[] = prim::ListConstruct(%1592, %1601, %1593, %1594), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %token_type_mat.14 : Bool(17:56, 12:0, 4:14, 7:1) = aten::expand(%1600, %1602, %80), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:513:0
  %1604 : Tensor[] = aten::split(%1598, %86, %72), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/tensor.py:371:0
  %diff_token_type.9 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.9 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1604), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1607 : int = aten::size(%token_type_mat.14, %85), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1608 : int = aten::size(%token_type_mat.14, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1609 : int = aten::size(%token_type_mat.14, %67), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1610 : int = aten::size(%token_type_mat.14, %49), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1611 : int[] = prim::ListConstruct(%1607, %1608, %1609, %1610), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1612 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%same_token_type.9, %1611, %80), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1613 : int = aten::size(%token_type_mat.14, %85), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1614 : int = aten::size(%token_type_mat.14, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1615 : int = aten::size(%token_type_mat.14, %67), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1616 : int = aten::size(%token_type_mat.14, %49), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %1617 : int[] = prim::ListConstruct(%1613, %1614, %1615, %1616), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %1618 : Float(17:8, 12:136, 4:2, 7:0) = aten::expand(%diff_token_type.9, %1617, %80), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.17 : Float(17:336, 12:28, 4:7, 7:1) = aten::where(%token_type_mat.14, %1612, %1618), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.18 : Float(17:336, 12:28, 4:7, 7:1) = aten::mul_(%token_type_attn.17, %cls_mask.4), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:522:0
  %1621 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%content_score.9, %positional_attn.54, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.25 : Float(17:336, 12:28, 4:7, 7:1) = aten::add(%1621, %token_type_attn.18, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.26 : Float(17:336, 12:28, 4:7, 7:1) = aten::to(%attn_score.25, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:553:0
  %1624 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1625 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1624, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1626 : Float(17:7, 1:7, 1:7, 7:1) = aten::unsqueeze(%1625, %67), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1627 : Float(17:7, 1:7, 1:7, 7:1) = aten::to(%1626, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %1628 : Float(17:7, 1:7, 1:7, 7:1) = aten::rsub(%1627, %86, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/tensor.py:396:0
  %1629 : Float(17:7, 1:7, 1:7, 7:1) = aten::mul(%1628, %47), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.27 : Float(17:336, 12:28, 4:7, 7:1) = aten::sub(%attn_score.26, %1629, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:556:0
  %input.79 : Float(17:336, 12:28, 4:7, 7:1) = aten::softmax(%attn_score.27, %72, %82), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:558:0
  %1632 : Float(17:336, 12:28, 4:7, 7:1) = aten::dropout(%input.79, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %1633 : Tensor[] = prim::ListConstruct(%1632, %1563), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %attn_vec.9 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%46, %1633), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # torch/functional.py:327:0
  %1635 : int[] = prim::ListConstruct(%1542, %1543, %69), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention
  %input.80 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.9, %1635), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:565:0
  %1637 : Tensor = prim::GetAttr[name="bias"](%1533)
  %1638 : Tensor = prim::GetAttr[name="weight"](%1533)
  %1639 : Float(768:1, 768:768) = aten::t(%1638), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.80, %1639), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.81 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.43, %1637, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.81, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.82 : Float(17:3072, 4:768, 768:1) = aten::add(%query.8, %attn_out.9, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention # transformers/modeling_funnel.py:568:0
  %1644 : Tensor = prim::GetAttr[name="bias"](%1532)
  %1645 : Tensor = prim::GetAttr[name="weight"](%1532)
  %1646 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.layer_norm
  %input.83 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.82, %1646, %1645, %1644, %44, %45), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.attention/__module.encoder.blocks.2.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %1648 : __torch__.torch.nn.modules.normalization.___torch_mangle_4710.LayerNorm = prim::GetAttr[name="layer_norm"](%1530)
  %1649 : __torch__.torch.nn.modules.linear.___torch_mangle_4708.Linear = prim::GetAttr[name="linear_2"](%1530)
  %1650 : __torch__.torch.nn.modules.linear.___torch_mangle_4706.Linear = prim::GetAttr[name="linear_1"](%1530)
  %1651 : Tensor = prim::GetAttr[name="bias"](%1650)
  %1652 : Tensor = prim::GetAttr[name="weight"](%1650)
  %1653 : Float(768:1, 3072:768) = aten::t(%1652), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.44 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.83, %1653), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.9 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.44, %1651, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1656 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.9, %61), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1657 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.9, %60), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1658 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1657, %59), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1659 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.9, %1658, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1660 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1659, %58), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1661 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1660), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %1662 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1661, %57, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.84 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1656, %1662), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/activations.py:30:0
  %input.85 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.84, %56, %80), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1665 : Tensor = prim::GetAttr[name="bias"](%1649)
  %1666 : Tensor = prim::GetAttr[name="weight"](%1649)
  %1667 : Float(3072:1, 768:3072) = aten::t(%1666), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.45 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.85, %1667), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.86 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.45, %1665, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.9 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.86, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:3072, 4:768, 768:1) = aten::add(%input.83, %h.9, %86), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn # transformers/modeling_funnel.py:588:0
  %1672 : Tensor = prim::GetAttr[name="bias"](%1648)
  %1673 : Tensor = prim::GetAttr[name="weight"](%1648)
  %1674 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.layer_norm
  %query.9 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.87, %1674, %1673, %1672, %44, %45), scope: __module.encoder/__module.encoder.blocks.2.0/__module.encoder.blocks.2.0.ffn/__module.encoder.blocks.2.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1676 : Bool(17:56, 4:14, 7:1) = aten::slice(%token_type_mat.13, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1677 : Bool(17:56, 4:14, 7:1) = aten::slice(%1676, %86, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1678 : Bool(17:56, 4:14, 1:1) = aten::slice(%1677, %67, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1679 : Tensor[] = prim::ListConstruct(%1678, %token_type_mat.13), scope: __module.encoder
  %tensor.16 : Bool(17:32, 4:8, 8:1) = aten::cat(%1679, %67), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1681 : Bool(17:32, 4:8, 8:1) = aten::slice(%tensor.16, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1682 : Bool(17:32, 4:8, 8:1) = aten::slice(%1681, %86, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %token_type_mat.15 : Bool(17:32, 4:8, 4:2) = aten::slice(%1682, %67, %85, %72, %67), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1684 : Float(4:14, 7:1) = aten::slice(%cls_mask.4, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1685 : Float(4:14, 1:1) = aten::slice(%1684, %86, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1686 : Tensor[] = prim::ListConstruct(%1685, %cls_mask.4), scope: __module.encoder
  %tensor.17 : Float(4:8, 8:1) = aten::cat(%1686, %86), scope: __module.encoder # transformers/modeling_funnel.py:346:0
  %1688 : Float(4:8, 8:1) = aten::slice(%tensor.17, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %cls_mask.5 : Float(4:8, 4:2) = aten::slice(%1688, %86, %85, %72, %67), scope: __module.encoder # transformers/modeling_funnel.py:347:0
  %1690 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %suffix : Float(17:7, 6:1) = aten::slice(%1690, %86, %85, %72, %86), scope: __module.encoder # transformers/modeling_funnel.py:359:0
  %1692 : Float(17:7, 7:1) = aten::slice(%attention_mask.3, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1693 : Float(17:7, 1:1) = aten::slice(%1692, %86, %85, %86, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1694 : Tensor[] = prim::ListConstruct(%1693, %suffix), scope: __module.encoder
  %tensor.18 : Float(17:7, 7:1) = aten::cat(%1694, %86), scope: __module.encoder # transformers/modeling_funnel.py:360:0
  %1696 : Float(17:7, 7:1) = aten::slice(%tensor.18, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %1697 : Float(17:7, 1:7, 7:1) = aten::unsqueeze(%1696, %86), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %1698 : Float(17:7, 1:7, 7:1) = aten::slice(%1697, %67, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %tensor.19 : Float(17:7, 1:7, 7:1, 1:1) = aten::unsqueeze(%1698, %49), scope: __module.encoder # transformers/modeling_funnel.py:364:0
  %input.88 : Float(17:7, 1:7, 7:1, 1:1) = aten::neg(%tensor.19), scope: __module.encoder # transformers/modeling_funnel.py:375:0
  %1701 : int[] = prim::ListConstruct(%67, %86), scope: __module.encoder
  %1702 : int[] = prim::ListConstruct(%67, %86), scope: __module.encoder
  %1703 : int[] = prim::ListConstruct(%85, %85), scope: __module.encoder
  %1704 : int[] = prim::ListConstruct(%86, %86), scope: __module.encoder
  %1705 : Float(17:4, 1:4, 4:1, 1:1) = aten::max_pool2d(%input.88, %1701, %1702, %1703, %1704, %45), scope: __module.encoder # torch/nn/functional.py:575:0
  %tensor : Float(17:4, 1:4, 4:1, 1:1) = aten::neg(%1705), scope: __module.encoder # transformers/modeling_funnel.py:375:0
  %1707 : Float(17:4, 1:4, 4:1, 1:1) = aten::slice(%tensor, %85, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1708 : Float(17:4, 4:1, 1:1) = aten::select(%1707, %86, %85), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1709 : Float(17:4, 4:1, 1:1) = aten::slice(%1708, %86, %85, %74, %86), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %attention_mask : Float(17:4, 4:1) = aten::select(%1709, %67, %85), scope: __module.encoder # transformers/modeling_funnel.py:380:0
  %1711 : __torch__.transformers.modeling_funnel.___torch_mangle_4726.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%95)
  %1712 : __torch__.transformers.modeling_funnel.___torch_mangle_4720.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%95)
  %1713 : __torch__.torch.nn.modules.normalization.___torch_mangle_4719.LayerNorm = prim::GetAttr[name="layer_norm"](%1712)
  %1714 : __torch__.torch.nn.modules.linear.___torch_mangle_4718.Linear = prim::GetAttr[name="post_proj"](%1712)
  %1715 : Tensor = prim::GetAttr[name="seg_embed"](%1712)
  %1716 : Tensor = prim::GetAttr[name="r_s_bias"](%1712)
  %1717 : Tensor = prim::GetAttr[name="r_kernel"](%1712)
  %1718 : Tensor = prim::GetAttr[name="r_r_bias"](%1712)
  %1719 : Tensor = prim::GetAttr[name="r_w_bias"](%1712)
  %1720 : __torch__.torch.nn.modules.linear.___torch_mangle_4717.Linear = prim::GetAttr[name="v_head"](%1712)
  %1721 : __torch__.torch.nn.modules.linear.___torch_mangle_4716.Linear = prim::GetAttr[name="k_head"](%1712)
  %1722 : __torch__.torch.nn.modules.linear.___torch_mangle_4715.Linear = prim::GetAttr[name="q_head"](%1712)
  %1723 : int = aten::size(%query.9, %85), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1724 : int = aten::size(%query.9, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:530:0
  %1725 : int = aten::size(%query.9, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:531:0
  %1726 : Tensor = prim::GetAttr[name="weight"](%1722)
  %1727 : Float(768:1, 768:768) = aten::t(%1726), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1728 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1727), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.q_head # torch/nn/functional.py:1676:0
  %1729 : int[] = prim::ListConstruct(%1723, %1724, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %q_head.19 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1728, %1729), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:535:0
  %1731 : Tensor = prim::GetAttr[name="bias"](%1721)
  %1732 : Tensor = prim::GetAttr[name="weight"](%1721)
  %1733 : Float(768:1, 768:768) = aten::t(%1732), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.46 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1733), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1676:0
  %1735 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.46, %1731, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.k_head # torch/nn/functional.py:1678:0
  %1736 : int[] = prim::ListConstruct(%1723, %1725, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1737 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1735, %1736), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:537:0
  %1738 : Tensor = prim::GetAttr[name="bias"](%1720)
  %1739 : Tensor = prim::GetAttr[name="weight"](%1720)
  %1740 : Float(768:1, 768:768) = aten::t(%1739), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.47 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.9, %1740), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1676:0
  %1742 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.47, %1738, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.v_head # torch/nn/functional.py:1678:0
  %1743 : int[] = prim::ListConstruct(%1723, %1725, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1744 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1742, %1743), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:538:0
  %q_head.20 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.19, %53), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.10 : Float(12:64, 64:1) = aten::mul(%1719, %53), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:542:0
  %1747 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_w_bias.10, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:544:0
  %1748 : Tensor[] = prim::ListConstruct(%1747, %1737), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %content_score.10 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%52, %1748), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %v.10 : Float(12:64, 64:1) = aten::mul(%1718, %53), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:486:0
  %1751 : Tensor[] = prim::ListConstruct(%252, %1717), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1752 : Float(8:768, 12:64, 64:1) = aten::einsum(%51, %1751), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1753 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %v.10, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:493:0
  %1754 : Tensor[] = prim::ListConstruct(%1753, %1752), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %positional_attn.55 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%50, %1754), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1756 : int = aten::size(%positional_attn.55, %85), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1757 : int = aten::size(%positional_attn.55, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1758 : int = aten::size(%positional_attn.55, %67), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %1759 : int = aten::size(%positional_attn.55, %49), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.10 : Long() = prim::NumToTensor(%1759), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1761 : int[] = prim::ListConstruct(%1756, %1757, %1759, %1758), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %positional_attn.56 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.55, %1761), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:428:0
  %1763 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.56, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1764 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1763, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1765 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1764, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.57 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1765, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:429:0
  %1767 : Long() = aten::sub(%max_rel_len.10, %76, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %1768 : int = aten::Int(%1767), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1769 : int[] = prim::ListConstruct(%1756, %1757, %1758, %1768), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %positional_attn.58 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.57, %1769), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.59 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.58, %49, %85, %1725, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.60 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.59, %cls_mask.5), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:498:0
  %1773 : int = aten::size(%token_type_mat.15, %85), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1774 : int = aten::size(%token_type_mat.15, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %1775 : int = aten::size(%token_type_mat.15, %67), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.10 : Float(12:64, 64:1) = aten::mul(%1716, %53), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:508:0
  %1777 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.20, %r_s_bias.10, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:511:0
  %1778 : Tensor[] = prim::ListConstruct(%1777, %1715), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1779 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%48, %1778), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1780 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1781 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1780, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1782 : int = aten::size(%q_head.20, %67), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1783 : int[] = prim::ListConstruct(%1773, %1782, %1774, %1775), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %token_type_mat.16 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1781, %1783, %80), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:513:0
  %1785 : Tensor[] = aten::split(%1779, %86, %72), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/tensor.py:371:0
  %diff_token_type.10 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.10 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1785), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1788 : int = aten::size(%token_type_mat.16, %85), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1789 : int = aten::size(%token_type_mat.16, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1790 : int = aten::size(%token_type_mat.16, %67), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1791 : int = aten::size(%token_type_mat.16, %49), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1792 : int[] = prim::ListConstruct(%1788, %1789, %1790, %1791), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1793 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.10, %1792, %80), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1794 : int = aten::size(%token_type_mat.16, %85), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1795 : int = aten::size(%token_type_mat.16, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1796 : int = aten::size(%token_type_mat.16, %67), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1797 : int = aten::size(%token_type_mat.16, %49), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %1798 : int[] = prim::ListConstruct(%1794, %1795, %1796, %1797), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %1799 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.10, %1798, %80), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.19 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.16, %1793, %1799), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.20 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.19, %cls_mask.5), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:522:0
  %1802 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.10, %positional_attn.60, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.28 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1802, %token_type_attn.20, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.29 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.28, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:553:0
  %1805 : Float(17:4, 4:1) = aten::slice(%attention_mask, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1806 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1805, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1807 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1806, %67), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1808 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1807, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %1809 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1808, %86, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/tensor.py:396:0
  %1810 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1809, %47), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score.30 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.29, %1810, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:556:0
  %input.89 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.30, %72, %82), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:558:0
  %1813 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.89, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %1814 : Tensor[] = prim::ListConstruct(%1813, %1744), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %attn_vec.10 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%46, %1814), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # torch/functional.py:327:0
  %1816 : int[] = prim::ListConstruct(%1723, %1724, %69), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention
  %input.90 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.10, %1816), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:565:0
  %1818 : Tensor = prim::GetAttr[name="bias"](%1714)
  %1819 : Tensor = prim::GetAttr[name="weight"](%1714)
  %1820 : Float(768:1, 768:768) = aten::t(%1819), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.48 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.90, %1820), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.91 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.48, %1818, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.91, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.92 : Float(17:3072, 4:768, 768:1) = aten::add(%query.9, %attn_out.10, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention # transformers/modeling_funnel.py:568:0
  %1825 : Tensor = prim::GetAttr[name="bias"](%1713)
  %1826 : Tensor = prim::GetAttr[name="weight"](%1713)
  %1827 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.layer_norm
  %input.93 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.92, %1827, %1826, %1825, %44, %45), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.attention/__module.encoder.blocks.2.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %1829 : __torch__.torch.nn.modules.normalization.___torch_mangle_4725.LayerNorm = prim::GetAttr[name="layer_norm"](%1711)
  %1830 : __torch__.torch.nn.modules.linear.___torch_mangle_4723.Linear = prim::GetAttr[name="linear_2"](%1711)
  %1831 : __torch__.torch.nn.modules.linear.___torch_mangle_4721.Linear = prim::GetAttr[name="linear_1"](%1711)
  %1832 : Tensor = prim::GetAttr[name="bias"](%1831)
  %1833 : Tensor = prim::GetAttr[name="weight"](%1831)
  %1834 : Float(768:1, 3072:768) = aten::t(%1833), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.93, %1834), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.10 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.49, %1832, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1837 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.10, %61), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1838 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.10, %60), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1839 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1838, %59), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1840 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.10, %1839, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1841 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1840, %58), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1842 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1841), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %1843 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1842, %57, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.94 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1837, %1843), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/activations.py:30:0
  %input.95 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.94, %56, %80), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1846 : Tensor = prim::GetAttr[name="bias"](%1830)
  %1847 : Tensor = prim::GetAttr[name="weight"](%1830)
  %1848 : Float(3072:1, 768:3072) = aten::t(%1847), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.95, %1848), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.96 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.50, %1846, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.10 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.96, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.dropout # torch/nn/functional.py:973:0
  %input.97 : Float(17:3072, 4:768, 768:1) = aten::add(%input.93, %h.10, %86), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn # transformers/modeling_funnel.py:588:0
  %1853 : Tensor = prim::GetAttr[name="bias"](%1829)
  %1854 : Tensor = prim::GetAttr[name="weight"](%1829)
  %1855 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.layer_norm
  %query.10 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.97, %1855, %1854, %1853, %44, %45), scope: __module.encoder/__module.encoder.blocks.2.1/__module.encoder.blocks.2.1.ffn/__module.encoder.blocks.2.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %1857 : __torch__.transformers.modeling_funnel.___torch_mangle_4741.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%92)
  %1858 : __torch__.transformers.modeling_funnel.___torch_mangle_4735.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%92)
  %1859 : __torch__.torch.nn.modules.normalization.___torch_mangle_4734.LayerNorm = prim::GetAttr[name="layer_norm"](%1858)
  %1860 : __torch__.torch.nn.modules.linear.___torch_mangle_4733.Linear = prim::GetAttr[name="post_proj"](%1858)
  %1861 : Tensor = prim::GetAttr[name="seg_embed"](%1858)
  %1862 : Tensor = prim::GetAttr[name="r_s_bias"](%1858)
  %1863 : Tensor = prim::GetAttr[name="r_kernel"](%1858)
  %1864 : Tensor = prim::GetAttr[name="r_r_bias"](%1858)
  %1865 : Tensor = prim::GetAttr[name="r_w_bias"](%1858)
  %1866 : __torch__.torch.nn.modules.linear.___torch_mangle_4732.Linear = prim::GetAttr[name="v_head"](%1858)
  %1867 : __torch__.torch.nn.modules.linear.___torch_mangle_4731.Linear = prim::GetAttr[name="k_head"](%1858)
  %1868 : __torch__.torch.nn.modules.linear.___torch_mangle_4730.Linear = prim::GetAttr[name="q_head"](%1858)
  %1869 : int = aten::size(%query.10, %85), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1870 : int = aten::size(%query.10, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:530:0
  %1871 : int = aten::size(%query.10, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:531:0
  %1872 : Tensor = prim::GetAttr[name="weight"](%1868)
  %1873 : Float(768:1, 768:768) = aten::t(%1872), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1874 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1873), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.q_head # torch/nn/functional.py:1676:0
  %1875 : int[] = prim::ListConstruct(%1869, %1870, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %q_head.21 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1874, %1875), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:535:0
  %1877 : Tensor = prim::GetAttr[name="bias"](%1867)
  %1878 : Tensor = prim::GetAttr[name="weight"](%1867)
  %1879 : Float(768:1, 768:768) = aten::t(%1878), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %output.51 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1879), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1676:0
  %1881 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.51, %1877, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.k_head # torch/nn/functional.py:1678:0
  %1882 : int[] = prim::ListConstruct(%1869, %1871, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1883 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1881, %1882), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:537:0
  %1884 : Tensor = prim::GetAttr[name="bias"](%1866)
  %1885 : Tensor = prim::GetAttr[name="weight"](%1866)
  %1886 : Float(768:1, 768:768) = aten::t(%1885), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %output.52 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.10, %1886), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1676:0
  %1888 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.52, %1884, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.v_head # torch/nn/functional.py:1678:0
  %1889 : int[] = prim::ListConstruct(%1869, %1871, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1890 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%1888, %1889), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:538:0
  %q_head.22 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.21, %53), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.11 : Float(12:64, 64:1) = aten::mul(%1865, %53), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:542:0
  %1893 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_w_bias.11, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:544:0
  %1894 : Tensor[] = prim::ListConstruct(%1893, %1883), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %content_score.11 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%52, %1894), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %v.11 : Float(12:64, 64:1) = aten::mul(%1864, %53), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:486:0
  %1897 : Tensor[] = prim::ListConstruct(%252, %1863), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1898 : Float(8:768, 12:64, 64:1) = aten::einsum(%51, %1897), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1899 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %v.11, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:493:0
  %1900 : Tensor[] = prim::ListConstruct(%1899, %1898), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %positional_attn.61 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%50, %1900), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1902 : int = aten::size(%positional_attn.61, %85), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1903 : int = aten::size(%positional_attn.61, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1904 : int = aten::size(%positional_attn.61, %67), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %1905 : int = aten::size(%positional_attn.61, %49), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.11 : Long() = prim::NumToTensor(%1905), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1907 : int[] = prim::ListConstruct(%1902, %1903, %1905, %1904), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %positional_attn.62 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.61, %1907), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:428:0
  %1909 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.62, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1910 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%1909, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1911 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1910, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.63 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%1911, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:429:0
  %1913 : Long() = aten::sub(%max_rel_len.11, %76, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %1914 : int = aten::Int(%1913), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1915 : int[] = prim::ListConstruct(%1902, %1903, %1904, %1914), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %positional_attn.64 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.63, %1915), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.65 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.64, %49, %85, %1871, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.66 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.65, %cls_mask.5), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:498:0
  %1919 : int = aten::size(%token_type_mat.15, %85), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1920 : int = aten::size(%token_type_mat.15, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %1921 : int = aten::size(%token_type_mat.15, %67), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.11 : Float(12:64, 64:1) = aten::mul(%1862, %53), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:508:0
  %1923 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.22, %r_s_bias.11, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:511:0
  %1924 : Tensor[] = prim::ListConstruct(%1923, %1861), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1925 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%48, %1924), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1926 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1927 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%1926, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1928 : int = aten::size(%q_head.22, %67), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1929 : int[] = prim::ListConstruct(%1919, %1928, %1920, %1921), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %token_type_mat.17 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%1927, %1929, %80), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:513:0
  %1931 : Tensor[] = aten::split(%1925, %86, %72), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/tensor.py:371:0
  %diff_token_type.11 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.11 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%1931), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1934 : int = aten::size(%token_type_mat.17, %85), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1935 : int = aten::size(%token_type_mat.17, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1936 : int = aten::size(%token_type_mat.17, %67), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1937 : int = aten::size(%token_type_mat.17, %49), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1938 : int[] = prim::ListConstruct(%1934, %1935, %1936, %1937), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1939 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.11, %1938, %80), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1940 : int = aten::size(%token_type_mat.17, %85), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1941 : int = aten::size(%token_type_mat.17, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1942 : int = aten::size(%token_type_mat.17, %67), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1943 : int = aten::size(%token_type_mat.17, %49), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %1944 : int[] = prim::ListConstruct(%1940, %1941, %1942, %1943), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %1945 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.11, %1944, %80), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.21 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.17, %1939, %1945), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.22 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.21, %cls_mask.5), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:522:0
  %1948 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.11, %positional_attn.66, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.31 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%1948, %token_type_attn.22, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:549:0
  %attn_score.32 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.31, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:553:0
  %1951 : Float(17:4, 4:1) = aten::slice(%attention_mask, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1952 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%1951, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1953 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%1952, %67), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1954 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%1953, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %1955 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%1954, %86, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/tensor.py:396:0
  %1956 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%1955, %47), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %attn_score.33 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.32, %1956, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:556:0
  %input.98 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.33, %72, %82), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:558:0
  %1959 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.98, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.attention_dropout # torch/nn/functional.py:973:0
  %1960 : Tensor[] = prim::ListConstruct(%1959, %1890), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %attn_vec.11 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%46, %1960), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # torch/functional.py:327:0
  %1962 : int[] = prim::ListConstruct(%1869, %1870, %69), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention
  %input.99 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.11, %1962), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:565:0
  %1964 : Tensor = prim::GetAttr[name="bias"](%1860)
  %1965 : Tensor = prim::GetAttr[name="weight"](%1860)
  %1966 : Float(768:1, 768:768) = aten::t(%1965), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %output.53 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.99, %1966), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1676:0
  %input.100 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.53, %1964, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.100, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.101 : Float(17:3072, 4:768, 768:1) = aten::add(%query.10, %attn_out.11, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention # transformers/modeling_funnel.py:568:0
  %1971 : Tensor = prim::GetAttr[name="bias"](%1859)
  %1972 : Tensor = prim::GetAttr[name="weight"](%1859)
  %1973 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.layer_norm
  %input.102 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.101, %1973, %1972, %1971, %44, %45), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.attention/__module.encoder.blocks.2.2.attention.layer_norm # torch/nn/functional.py:2048:0
  %1975 : __torch__.torch.nn.modules.normalization.___torch_mangle_4740.LayerNorm = prim::GetAttr[name="layer_norm"](%1857)
  %1976 : __torch__.torch.nn.modules.linear.___torch_mangle_4738.Linear = prim::GetAttr[name="linear_2"](%1857)
  %1977 : __torch__.torch.nn.modules.linear.___torch_mangle_4736.Linear = prim::GetAttr[name="linear_1"](%1857)
  %1978 : Tensor = prim::GetAttr[name="bias"](%1977)
  %1979 : Tensor = prim::GetAttr[name="weight"](%1977)
  %1980 : Float(768:1, 3072:768) = aten::t(%1979), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.102, %1980), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.11 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.54, %1978, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_1 # torch/nn/functional.py:1678:0
  %1983 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.11, %61), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1984 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.11, %60), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1985 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1984, %59), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1986 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.11, %1985, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1987 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1986, %58), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1988 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%1987), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %1989 : Float(17:12288, 4:3072, 3072:1) = aten::add(%1988, %57, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.103 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%1983, %1989), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/activations.py:30:0
  %input.104 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.103, %56, %80), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.activation_dropout # torch/nn/functional.py:973:0
  %1992 : Tensor = prim::GetAttr[name="bias"](%1976)
  %1993 : Tensor = prim::GetAttr[name="weight"](%1976)
  %1994 : Float(3072:1, 768:3072) = aten::t(%1993), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.55 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.104, %1994), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.105 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.55, %1992, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.11 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.105, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.dropout # torch/nn/functional.py:973:0
  %input.106 : Float(17:3072, 4:768, 768:1) = aten::add(%input.102, %h.11, %86), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn # transformers/modeling_funnel.py:588:0
  %1999 : Tensor = prim::GetAttr[name="bias"](%1975)
  %2000 : Tensor = prim::GetAttr[name="weight"](%1975)
  %2001 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.layer_norm
  %query.11 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.106, %2001, %2000, %1999, %44, %45), scope: __module.encoder/__module.encoder.blocks.2.2/__module.encoder.blocks.2.2.ffn/__module.encoder.blocks.2.2.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2003 : __torch__.transformers.modeling_funnel.___torch_mangle_4756.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%89)
  %2004 : __torch__.transformers.modeling_funnel.___torch_mangle_4750.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%89)
  %2005 : __torch__.torch.nn.modules.normalization.___torch_mangle_4749.LayerNorm = prim::GetAttr[name="layer_norm"](%2004)
  %2006 : __torch__.torch.nn.modules.linear.___torch_mangle_4748.Linear = prim::GetAttr[name="post_proj"](%2004)
  %2007 : Tensor = prim::GetAttr[name="seg_embed"](%2004)
  %2008 : Tensor = prim::GetAttr[name="r_s_bias"](%2004)
  %2009 : Tensor = prim::GetAttr[name="r_kernel"](%2004)
  %2010 : Tensor = prim::GetAttr[name="r_r_bias"](%2004)
  %2011 : Tensor = prim::GetAttr[name="r_w_bias"](%2004)
  %2012 : __torch__.torch.nn.modules.linear.___torch_mangle_4747.Linear = prim::GetAttr[name="v_head"](%2004)
  %2013 : __torch__.torch.nn.modules.linear.___torch_mangle_4746.Linear = prim::GetAttr[name="k_head"](%2004)
  %2014 : __torch__.torch.nn.modules.linear.___torch_mangle_4745.Linear = prim::GetAttr[name="q_head"](%2004)
  %2015 : int = aten::size(%query.11, %85), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2016 : int = aten::size(%query.11, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:530:0
  %2017 : int = aten::size(%query.11, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:531:0
  %2018 : Tensor = prim::GetAttr[name="weight"](%2014)
  %2019 : Float(768:1, 768:768) = aten::t(%2018), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2020 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2019), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.q_head # torch/nn/functional.py:1676:0
  %2021 : int[] = prim::ListConstruct(%2015, %2016, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %q_head.23 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2020, %2021), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:535:0
  %2023 : Tensor = prim::GetAttr[name="bias"](%2013)
  %2024 : Tensor = prim::GetAttr[name="weight"](%2013)
  %2025 : Float(768:1, 768:768) = aten::t(%2024), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %output.56 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2025), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1676:0
  %2027 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.56, %2023, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.k_head # torch/nn/functional.py:1678:0
  %2028 : int[] = prim::ListConstruct(%2015, %2017, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2029 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2027, %2028), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:537:0
  %2030 : Tensor = prim::GetAttr[name="bias"](%2012)
  %2031 : Tensor = prim::GetAttr[name="weight"](%2012)
  %2032 : Float(768:1, 768:768) = aten::t(%2031), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %output.57 : Float(17:3072, 4:768, 768:1) = aten::matmul(%query.11, %2032), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1676:0
  %2034 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.57, %2030, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.v_head # torch/nn/functional.py:1678:0
  %2035 : int[] = prim::ListConstruct(%2015, %2017, %55, %54), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2036 : Float(17:3072, 4:768, 12:64, 64:1) = aten::view(%2034, %2035), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:538:0
  %q_head.24 : Float(17:3072, 4:768, 12:64, 64:1) = aten::mul(%q_head.23, %53), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.12 : Float(12:64, 64:1) = aten::mul(%2011, %53), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:542:0
  %2039 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_w_bias.12, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:544:0
  %2040 : Tensor[] = prim::ListConstruct(%2039, %2029), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %content_score.12 : Float(17:192, 12:16, 4:4, 4:1) = aten::einsum(%52, %2040), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %v.12 : Float(12:64, 64:1) = aten::mul(%2010, %53), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:486:0
  %2043 : Tensor[] = prim::ListConstruct(%252, %2009), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2044 : Float(8:768, 12:64, 64:1) = aten::einsum(%51, %2043), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2045 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %v.12, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:493:0
  %2046 : Tensor[] = prim::ListConstruct(%2045, %2044), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %positional_attn.67 : Float(17:32, 12:544, 4:8, 8:1) = aten::einsum(%50, %2046), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2048 : int = aten::size(%positional_attn.67, %85), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2049 : int = aten::size(%positional_attn.67, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2050 : int = aten::size(%positional_attn.67, %67), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %2051 : int = aten::size(%positional_attn.67, %49), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.12 : Long() = prim::NumToTensor(%2051), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2053 : int[] = prim::ListConstruct(%2048, %2049, %2051, %2050), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %positional_attn.68 : Float(17:32, 12:544, 8:4, 4:1) = aten::reshape(%positional_attn.67, %2053), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:428:0
  %2055 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%positional_attn.68, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2056 : Float(17:32, 12:544, 8:4, 4:1) = aten::slice(%2055, %86, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2057 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2056, %67, %86, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.69 : Float(17:32, 12:544, 7:4, 4:1) = aten::slice(%2057, %49, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:429:0
  %2059 : Long() = aten::sub(%max_rel_len.12, %76, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %2060 : int = aten::Int(%2059), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2061 : int[] = prim::ListConstruct(%2048, %2049, %2050, %2060), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %positional_attn.70 : Float(17:32, 12:544, 4:7, 7:1) = aten::reshape(%positional_attn.69, %2061), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.71 : Float(17:32, 12:544, 4:7, 4:1) = aten::slice(%positional_attn.70, %49, %85, %2017, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.72 : Float(17:32, 12:544, 4:7, 4:1) = aten::mul_(%positional_attn.71, %cls_mask.5), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:498:0
  %2065 : int = aten::size(%token_type_mat.15, %85), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2066 : int = aten::size(%token_type_mat.15, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %2067 : int = aten::size(%token_type_mat.15, %67), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.12 : Float(12:64, 64:1) = aten::mul(%2008, %53), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:508:0
  %2069 : Float(17:3072, 4:768, 12:64, 64:1) = aten::add(%q_head.24, %r_s_bias.12, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:511:0
  %2070 : Tensor[] = prim::ListConstruct(%2069, %2007), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2071 : Float(17:8, 12:136, 4:2, 2:1) = aten::einsum(%48, %2070), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2072 : Bool(17:32, 4:8, 4:2) = aten::slice(%token_type_mat.15, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2073 : Bool(17:32, 1:32, 4:8, 4:2) = aten::unsqueeze(%2072, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2074 : int = aten::size(%q_head.24, %67), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2075 : int[] = prim::ListConstruct(%2065, %2074, %2066, %2067), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %token_type_mat.18 : Bool(17:32, 12:0, 4:8, 4:2) = aten::expand(%2073, %2075, %80), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:513:0
  %2077 : Tensor[] = aten::split(%2071, %86, %72), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/tensor.py:371:0
  %diff_token_type.12 : Float(17:8, 12:136, 4:2, 1:1), %same_token_type.12 : Float(17:8, 12:136, 4:2, 1:1) = prim::ListUnpack(%2077), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2080 : int = aten::size(%token_type_mat.18, %85), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2081 : int = aten::size(%token_type_mat.18, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2082 : int = aten::size(%token_type_mat.18, %67), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2083 : int = aten::size(%token_type_mat.18, %49), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2084 : int[] = prim::ListConstruct(%2080, %2081, %2082, %2083), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2085 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%same_token_type.12, %2084, %80), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2086 : int = aten::size(%token_type_mat.18, %85), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2087 : int = aten::size(%token_type_mat.18, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2088 : int = aten::size(%token_type_mat.18, %67), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2089 : int = aten::size(%token_type_mat.18, %49), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %2090 : int[] = prim::ListConstruct(%2086, %2087, %2088, %2089), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %2091 : Float(17:8, 12:136, 4:2, 4:0) = aten::expand(%diff_token_type.12, %2090, %80), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.23 : Float(17:192, 12:16, 4:4, 4:1) = aten::where(%token_type_mat.18, %2085, %2091), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.24 : Float(17:192, 12:16, 4:4, 4:1) = aten::mul_(%token_type_attn.23, %cls_mask.5), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:522:0
  %2094 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%content_score.12, %positional_attn.72, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.34 : Float(17:192, 12:16, 4:4, 4:1) = aten::add(%2094, %token_type_attn.24, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:549:0
  %attn_score.35 : Float(17:192, 12:16, 4:4, 4:1) = aten::to(%attn_score.34, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:553:0
  %2097 : Float(17:4, 4:1) = aten::slice(%attention_mask, %85, %85, %74, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2098 : Float(17:4, 1:4, 4:1) = aten::unsqueeze(%2097, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2099 : Float(17:4, 1:4, 1:4, 4:1) = aten::unsqueeze(%2098, %67), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2100 : Float(17:4, 1:4, 1:4, 4:1) = aten::to(%2099, %82, %80, %80, %77), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %2101 : Float(17:4, 1:4, 1:4, 4:1) = aten::rsub(%2100, %86, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/tensor.py:396:0
  %2102 : Float(17:4, 1:4, 1:4, 4:1) = aten::mul(%2101, %47), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %attn_score.36 : Float(17:192, 12:16, 4:4, 4:1) = aten::sub(%attn_score.35, %2102, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:556:0
  %input.107 : Float(17:192, 12:16, 4:4, 4:1) = aten::softmax(%attn_score.36, %72, %82), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:558:0
  %2105 : Float(17:192, 12:16, 4:4, 4:1) = aten::dropout(%input.107, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.attention_dropout # torch/nn/functional.py:973:0
  %2106 : Tensor[] = prim::ListConstruct(%2105, %2036), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %attn_vec.12 : Float(17:3072, 4:64, 12:256, 64:1) = aten::einsum(%46, %2106), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # torch/functional.py:327:0
  %2108 : int[] = prim::ListConstruct(%2015, %2016, %69), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention
  %input.108 : Float(17:3072, 4:768, 768:1) = aten::reshape(%attn_vec.12, %2108), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:565:0
  %2110 : Tensor = prim::GetAttr[name="bias"](%2006)
  %2111 : Tensor = prim::GetAttr[name="weight"](%2006)
  %2112 : Float(768:1, 768:768) = aten::t(%2111), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.108, %2112), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1676:0
  %input.109 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.58, %2110, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.109, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:3072, 4:768, 768:1) = aten::add(%query.11, %attn_out.12, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention # transformers/modeling_funnel.py:568:0
  %2117 : Tensor = prim::GetAttr[name="bias"](%2005)
  %2118 : Tensor = prim::GetAttr[name="weight"](%2005)
  %2119 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.layer_norm
  %input.111 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.110, %2119, %2118, %2117, %44, %45), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.attention/__module.encoder.blocks.2.3.attention.layer_norm # torch/nn/functional.py:2048:0
  %2121 : __torch__.torch.nn.modules.normalization.___torch_mangle_4755.LayerNorm = prim::GetAttr[name="layer_norm"](%2003)
  %2122 : __torch__.torch.nn.modules.linear.___torch_mangle_4753.Linear = prim::GetAttr[name="linear_2"](%2003)
  %2123 : __torch__.torch.nn.modules.linear.___torch_mangle_4751.Linear = prim::GetAttr[name="linear_1"](%2003)
  %2124 : Tensor = prim::GetAttr[name="bias"](%2123)
  %2125 : Tensor = prim::GetAttr[name="weight"](%2123)
  %2126 : Float(768:1, 3072:768) = aten::t(%2125), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:12288, 4:3072, 3072:1) = aten::matmul(%input.111, %2126), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.12 : Float(17:12288, 4:3072, 3072:1) = aten::add_(%output.59, %2124, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2129 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%x.12, %61), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2130 : Float(17:12288, 4:3072, 3072:1) = aten::pow(%x.12, %60), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2131 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2130, %59), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2132 : Float(17:12288, 4:3072, 3072:1) = aten::add(%x.12, %2131, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2133 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2132, %58), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2134 : Float(17:12288, 4:3072, 3072:1) = aten::tanh(%2133), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %2135 : Float(17:12288, 4:3072, 3072:1) = aten::add(%2134, %57, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.112 : Float(17:12288, 4:3072, 3072:1) = aten::mul(%2129, %2135), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/activations.py:30:0
  %input.113 : Float(17:12288, 4:3072, 3072:1) = aten::dropout(%input.112, %56, %80), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2138 : Tensor = prim::GetAttr[name="bias"](%2122)
  %2139 : Tensor = prim::GetAttr[name="weight"](%2122)
  %2140 : Float(3072:1, 768:3072) = aten::t(%2139), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:3072, 4:768, 768:1) = aten::matmul(%input.113, %2140), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.114 : Float(17:3072, 4:768, 768:1) = aten::add_(%output.60, %2138, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.12 : Float(17:3072, 4:768, 768:1) = aten::dropout(%input.114, %73, %80), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.dropout # torch/nn/functional.py:973:0
  %input.115 : Float(17:3072, 4:768, 768:1) = aten::add(%input.111, %h.12, %86), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn # transformers/modeling_funnel.py:588:0
  %2145 : Tensor = prim::GetAttr[name="bias"](%2121)
  %2146 : Tensor = prim::GetAttr[name="weight"](%2121)
  %2147 : int[] = prim::ListConstruct(%69), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.layer_norm
  %x.13 : Float(17:3072, 4:768, 768:1) = aten::layer_norm(%input.115, %2147, %2146, %2145, %44, %45), scope: __module.encoder/__module.encoder.blocks.2.3/__module.encoder.blocks.2.3.ffn/__module.encoder.blocks.2.3.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2149 : (Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:9984, 13:768, 768:1), Float(17:3072, 4:768, 768:1), Float(17:9984, 13:768, 768:1)) = prim::TupleConstruct(%hidden.1, %hidden.1, %hidden.1, %x.13, %hidden.1)
  %22 : Float(17:9984, 13:768, 768:1), %23 : Float(17:9984, 13:768, 768:1), %24 : Float(17:9984, 13:768, 768:1), %25 : Float(17:3072, 4:768, 768:1), %26 : Float(17:9984, 13:768, 768:1) = prim::TupleUnpack(%2149)
  %2150 : float = prim::Constant[value=1.0000000000000001e-09](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %2151 : bool = prim::Constant[value=1](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %2152 : str = prim::Constant[value="bnij,bjnd->bind"](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2153 : Double() = prim::Constant[value={1e+06}](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2154 : str = prim::Constant[value="bind,snd->bnis"](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2155 : str = prim::Constant[value="binh,tnh->bnit"](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2156 : str = prim::Constant[value="td,dnh->tnh"](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2157 : str = prim::Constant[value="bind,bjnd->bnij"](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2158 : Double() = prim::Constant[value={0.125}](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:540:0
  %2159 : int = prim::Constant[value=64](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
  %2160 : int = prim::Constant[value=12](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
  %2161 : float = prim::Constant[value=0.](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2162 : Double() = prim::Constant[value={1}](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2163 : Double() = prim::Constant[value={0.797885}](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2164 : Double() = prim::Constant[value={0.044715}](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2165 : float = prim::Constant[value=3.](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2166 : Double() = prim::Constant[value={0.5}](), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2167 : int = prim::Constant[value=2](), scope: __module.decoder # transformers/modeling_funnel.py:207:0
  %2168 : int = prim::Constant[value=768](), scope: __module.decoder # transformers/modeling_funnel.py:285:0
  %2169 : Long() = prim::Constant[value={13}](), scope: __module.decoder # transformers/modeling_funnel.py:315:0
  %2170 : int = prim::Constant[value=-1](), scope: __module.decoder # transformers/modeling_funnel.py:255:0
  %2171 : float = prim::Constant[value=0.10000000000000001](), scope: __module.decoder/__module.decoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %2172 : Long() = prim::Constant[value={2}](), scope: __module.decoder # transformers/modeling_funnel.py:250:0
  %2173 : None = prim::Constant(), scope: __module.decoder
  %2174 : Float() = prim::Constant[value={10000}](), scope: __module.decoder # torch/tensor.py:420:0
  %2175 : Long() = prim::Constant[value={384}](), scope: __module.decoder # transformers/modeling_funnel.py:248:0
  %2176 : bool = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_funnel.py:247:0
  %2177 : Device = prim::Constant[value="cpu"](), scope: __module.decoder # transformers/modeling_funnel.py:247:0
  %2178 : int = prim::Constant[value=6](), scope: __module.decoder # transformers/modeling_funnel.py:247:0
  %2179 : float = prim::Constant[value=1.](), scope: __module.decoder # transformers/modeling_funnel.py:247:0
  %2180 : int = prim::Constant[value=384](), scope: __module.decoder # transformers/modeling_funnel.py:247:0
  %2181 : Long() = prim::Constant[value={1}](), scope: __module.decoder # transformers/modeling_funnel.py:678:0
  %2182 : int = prim::Constant[value=3](), scope: __module.decoder # torch/nn/functional.py:3552:0
  %2183 : int = prim::Constant[value=4](), scope: __module.decoder # transformers/modeling_funnel.py:674:0
  %2184 : int = prim::Constant[value=9223372036854775807](), scope: __module.decoder # transformers/modeling_funnel.py:672:0
  %2185 : int = prim::Constant[value=0](), scope: __module.decoder # transformers/modeling_funnel.py:672:0
  %2186 : int = prim::Constant[value=1](), scope: __module.decoder # transformers/modeling_funnel.py:705:0
  %2187 : __torch__.torch.nn.modules.container.___torch_mangle_4794.ModuleList = prim::GetAttr[name="layers"](%3)
  %2188 : __torch__.transformers.modeling_funnel.___torch_mangle_4793.FunnelLayer = prim::GetAttr[name="1"](%2187)
  %2189 : __torch__.torch.nn.modules.container.___torch_mangle_4794.ModuleList = prim::GetAttr[name="layers"](%3)
  %2190 : __torch__.transformers.modeling_funnel.___torch_mangle_4778.FunnelLayer = prim::GetAttr[name="0"](%2189)
  %2191 : int = aten::size(%23, %2186), scope: __module.decoder # transformers/modeling_funnel.py:705:0
  %target_len : Long() = prim::NumToTensor(%2191), scope: __module.decoder
  %2193 : Float(17:3072, 4:768, 768:1) = aten::slice(%25, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:672:0
  %cls : Float(17:3072, 1:768, 768:1) = aten::slice(%2193, %2186, %2185, %2186, %2186), scope: __module.decoder # transformers/modeling_funnel.py:672:0
  %2195 : Float(17:3072, 4:768, 768:1) = aten::slice(%25, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:673:0
  %x.14 : Float(17:3072, 3:768, 768:1) = aten::slice(%2195, %2186, %2186, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:673:0
  %input.116 : Float(17:9216, 12:768, 768:1) = aten::repeat_interleave(%x.14, %2183, %2186), scope: __module.decoder # transformers/modeling_funnel.py:674:0
  %2198 : int[] = prim::ListConstruct(%2185, %2185, %2185, %2182, %2185, %2185), scope: __module.decoder
  %output.61 : Float(17:11520, 15:768, 768:1) = aten::constant_pad_nd(%input.116, %2198, %2185), scope: __module.decoder # torch/nn/functional.py:3552:0
  %2200 : Long() = aten::sub(%target_len, %2181, %2186), scope: __module.decoder # transformers/modeling_funnel.py:678:0
  %2201 : int = aten::Int(%2200), scope: __module.decoder
  %2202 : Float(17:11520, 15:768, 768:1) = aten::slice(%output.61, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:678:0
  %output.62 : Float(17:11520, 12:768, 768:1) = aten::slice(%2202, %2186, %2185, %2201, %2186), scope: __module.decoder # transformers/modeling_funnel.py:678:0
  %2204 : Tensor[] = prim::ListConstruct(%cls, %output.62), scope: __module.decoder
  %upsampled_hidden : Float(17:9984, 13:768, 768:1) = aten::cat(%2204, %2186), scope: __module.decoder # transformers/modeling_funnel.py:679:0
  %input_embeds : Float(17:9984, 13:768, 768:1) = aten::add(%upsampled_hidden, %26, %2186), scope: __module.decoder # transformers/modeling_funnel.py:710:0
  %2207 : int = aten::size(%input_embeds, %2186), scope: __module.decoder # transformers/modeling_funnel.py:195:0
  %seq_len.38 : Long() = prim::NumToTensor(%2207), scope: __module.decoder
  %freq_seq : Float(384:1) = aten::arange(%2185, %2180, %2179, %2178, %2185, %2177, %2176), scope: __module.decoder # transformers/modeling_funnel.py:247:0
  %2210 : Float(384:1) = aten::div(%freq_seq, %2175), scope: __module.decoder # transformers/modeling_funnel.py:248:0
  %2211 : Float() = aten::to(%2174, %2177, %2178, %2176, %2176, %2173), scope: __module.decoder # torch/tensor.py:420:0
  %2212 : Float() = aten::detach(%2211), scope: __module.decoder # torch/tensor.py:420:0
  %2213 : Float(384:1) = aten::pow(%2212, %2210), scope: __module.decoder # torch/tensor.py:420:0
  %2214 : Float(384:1) = aten::reciprocal(%2213), scope: __module.decoder # torch/tensor.py:400:0
  %inv_freq : Float(384:1) = aten::mul(%2214, %2181), scope: __module.decoder # torch/tensor.py:400:0
  %2216 : Long() = aten::neg(%seq_len.38), scope: __module.decoder # transformers/modeling_funnel.py:250:0
  %2217 : Long() = aten::mul(%2216, %2172), scope: __module.decoder # transformers/modeling_funnel.py:250:0
  %2218 : Scalar = aten::ScalarImplicit(%2217), scope: __module.decoder
  %2219 : Long() = aten::mul(%seq_len.38, %2172), scope: __module.decoder # transformers/modeling_funnel.py:250:0
  %2220 : Scalar = aten::ScalarImplicit(%2219), scope: __module.decoder
  %rel_pos_id : Float(52:1) = aten::arange(%2218, %2220, %2179, %2178, %2185, %2177, %2176), scope: __module.decoder # transformers/modeling_funnel.py:250:0
  %zero_offset : Long() = aten::mul(%seq_len.38, %2172), scope: __module.decoder # transformers/modeling_funnel.py:251:0
  %2223 : Float(52:1) = aten::slice(%rel_pos_id, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:252:0
  %2224 : Float(52:1, 1:1) = aten::unsqueeze(%2223, %2186), scope: __module.decoder # transformers/modeling_funnel.py:252:0
  %2225 : Float(1:384, 384:1) = aten::unsqueeze(%inv_freq, %2185), scope: __module.decoder # transformers/modeling_funnel.py:252:0
  %sinusoid : Float(52:384, 384:1) = aten::mul(%2224, %2225), scope: __module.decoder # transformers/modeling_funnel.py:252:0
  %input.117 : Float(52:384, 384:1) = aten::sin(%sinusoid), scope: __module.decoder # transformers/modeling_funnel.py:253:0
  %sin_embed : Float(52:384, 384:1) = aten::dropout(%input.117, %2171, %2176), scope: __module.decoder/__module.decoder.attention_structure.sin_dropout # torch/nn/functional.py:973:0
  %input.118 : Float(52:384, 384:1) = aten::cos(%sinusoid), scope: __module.decoder # transformers/modeling_funnel.py:254:0
  %cos_embed : Float(52:384, 384:1) = aten::dropout(%input.118, %2171, %2176), scope: __module.decoder/__module.decoder.attention_structure.cos_dropout # torch/nn/functional.py:973:0
  %2231 : Tensor[] = prim::ListConstruct(%sin_embed, %cos_embed), scope: __module.decoder
  %pos_embed : Float(52:768, 768:1) = aten::cat(%2231, %2170), scope: __module.decoder # transformers/modeling_funnel.py:255:0
  %pos : Float(13:1) = aten::arange(%2185, %2207, %2186, %2178, %2185, %2177, %2176), scope: __module.decoder # transformers/modeling_funnel.py:257:0
  %2234 : Float() = aten::select(%pos, %2185, %2185), scope: __module.decoder # transformers/modeling_funnel.py:313:0
  %2235 : Float() = aten::select(%pos, %2185, %2185), scope: __module.decoder # transformers/modeling_funnel.py:313:0
  %ref_point.6 : Float() = aten::sub(%2234, %2235, %2186), scope: __module.decoder # transformers/modeling_funnel.py:313:0
  %max_dist.6 : Float() = aten::add(%ref_point.6, %2169, %2186), scope: __module.decoder # transformers/modeling_funnel.py:315:0
  %2238 : Scalar = aten::ScalarImplicit(%max_dist.6), scope: __module.decoder
  %2239 : Float() = aten::select(%pos, %2185, %2185), scope: __module.decoder # transformers/modeling_funnel.py:316:0
  %2240 : Float() = aten::select(%pos, %2185, %2170), scope: __module.decoder # transformers/modeling_funnel.py:316:0
  %min_dist.6 : Float() = aten::sub(%2239, %2240, %2186), scope: __module.decoder # transformers/modeling_funnel.py:316:0
  %2242 : Float() = aten::sub(%min_dist.6, %2181, %2186), scope: __module.decoder # transformers/modeling_funnel.py:318:0
  %2243 : Scalar = aten::ScalarImplicit(%2242), scope: __module.decoder
  %rel_pos.16 : Long(26:1) = aten::arange(%2238, %2243, %2170, %2183, %2185, %2177, %2176), scope: __module.decoder # transformers/modeling_funnel.py:318:0
  %2245 : Long(26:1) = aten::slice(%rel_pos.16, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:284:0
  %2246 : Long(26:1, 1:1) = aten::unsqueeze(%2245, %2186), scope: __module.decoder # transformers/modeling_funnel.py:284:0
  %rel_pos.17 : Long(26:1, 1:1) = aten::add(%2246, %zero_offset, %2186), scope: __module.decoder # transformers/modeling_funnel.py:284:0
  %2248 : int = aten::size(%rel_pos.17, %2185), scope: __module.decoder # transformers/modeling_funnel.py:285:0
  %2249 : int[] = prim::ListConstruct(%2248, %2168), scope: __module.decoder
  %rel_pos.18 : Long(26:1, 768:0) = aten::expand(%rel_pos.17, %2249, %2176), scope: __module.decoder # transformers/modeling_funnel.py:285:0
  %2251 : Float(26:768, 768:1) = aten::gather(%pos_embed, %2185, %rel_pos.18, %2176), scope: __module.decoder # transformers/modeling_funnel.py:286:0
  %2252 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:207:0
  %2253 : Long(17:13, 13:1) = aten::slice(%2252, %2186, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:207:0
  %2254 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%2253, %2167), scope: __module.decoder # transformers/modeling_funnel.py:207:0
  %2255 : Long(17:13, 13:1) = aten::slice(%token_type_ids, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:207:0
  %2256 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2255, %2186), scope: __module.decoder # transformers/modeling_funnel.py:207:0
  %token_type_mat.19 : Bool(17:169, 13:13, 13:1) = aten::eq(%2254, %2256), scope: __module.decoder # torch/tensor.py:22:0
  %cls_ids : Bool(17:13, 13:1) = aten::eq(%token_type_ids, %2167), scope: __module.decoder # torch/tensor.py:22:0
  %2259 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:210:0
  %2260 : Bool(17:13, 13:1) = aten::slice(%2259, %2186, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:210:0
  %2261 : Bool(17:13, 13:1, 1:1) = aten::unsqueeze(%2260, %2167), scope: __module.decoder # transformers/modeling_funnel.py:210:0
  %2262 : Bool(17:13, 13:1) = aten::slice(%cls_ids, %2185, %2185, %2184, %2186), scope: __module.decoder # transformers/modeling_funnel.py:210:0
  %2263 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%2262, %2186), scope: __module.decoder # transformers/modeling_funnel.py:210:0
  %cls_mat : Bool(17:169, 13:13, 13:1) = aten::__or__(%2261, %2263), scope: __module.decoder # transformers/modeling_funnel.py:210:0
  %token_type_mat.20 : Bool(17:169, 13:13, 13:1) = aten::__or__(%cls_mat, %token_type_mat.19), scope: __module.decoder # transformers/modeling_funnel.py:211:0
  %2266 : Long() = aten::sub(%seq_len.38, %2181, %2186), scope: __module.decoder # transformers/modeling_funnel.py:199:0
  %2267 : int = aten::Int(%2266), scope: __module.decoder
  %2268 : Long() = aten::sub(%seq_len.38, %2181, %2186), scope: __module.decoder # transformers/modeling_funnel.py:199:0
  %2269 : int = aten::Int(%2268), scope: __module.decoder
  %2270 : int[] = prim::ListConstruct(%2267, %2269), scope: __module.decoder
  %input.119 : Float(12:12, 12:1) = aten::ones(%2270, %2178, %2185, %2177, %2176), scope: __module.decoder # transformers/modeling_funnel.py:199:0
  %2272 : int[] = prim::ListConstruct(%2186, %2185, %2186, %2185), scope: __module.decoder
  %cls_mask : Float(13:13, 13:1) = aten::constant_pad_nd(%input.119, %2272, %2185), scope: __module.decoder # torch/nn/functional.py:3552:0
  %2274 : __torch__.transformers.modeling_funnel.___torch_mangle_4777.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%2190)
  %2275 : __torch__.transformers.modeling_funnel.___torch_mangle_4771.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%2190)
  %2276 : __torch__.torch.nn.modules.normalization.___torch_mangle_4770.LayerNorm = prim::GetAttr[name="layer_norm"](%2275)
  %2277 : __torch__.torch.nn.modules.linear.___torch_mangle_4769.Linear = prim::GetAttr[name="post_proj"](%2275)
  %2278 : Tensor = prim::GetAttr[name="seg_embed"](%2275)
  %2279 : Tensor = prim::GetAttr[name="r_s_bias"](%2275)
  %2280 : Tensor = prim::GetAttr[name="r_kernel"](%2275)
  %2281 : Tensor = prim::GetAttr[name="r_r_bias"](%2275)
  %2282 : Tensor = prim::GetAttr[name="r_w_bias"](%2275)
  %2283 : __torch__.torch.nn.modules.linear.___torch_mangle_4768.Linear = prim::GetAttr[name="v_head"](%2275)
  %2284 : __torch__.torch.nn.modules.linear.___torch_mangle_4767.Linear = prim::GetAttr[name="k_head"](%2275)
  %2285 : __torch__.torch.nn.modules.linear.___torch_mangle_4766.Linear = prim::GetAttr[name="q_head"](%2275)
  %2286 : int = aten::size(%input_embeds, %2185), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
  %2287 : int = aten::size(%input_embeds, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:530:0
  %2288 : int = aten::size(%input_embeds, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:531:0
  %2289 : Tensor = prim::GetAttr[name="weight"](%2285)
  %2290 : Float(768:1, 768:768) = aten::t(%2289), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
  %2291 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2290), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.q_head # torch/nn/functional.py:1676:0
  %2292 : int[] = prim::ListConstruct(%2286, %2287, %2160, %2159), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %q_head.25 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2291, %2292), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:535:0
  %2294 : Tensor = prim::GetAttr[name="bias"](%2284)
  %2295 : Tensor = prim::GetAttr[name="weight"](%2284)
  %2296 : Float(768:1, 768:768) = aten::t(%2295), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2296), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.k_head # torch/nn/functional.py:1676:0
  %2298 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %2294, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.k_head # torch/nn/functional.py:1678:0
  %2299 : int[] = prim::ListConstruct(%2286, %2288, %2160, %2159), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2300 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2298, %2299), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:537:0
  %2301 : Tensor = prim::GetAttr[name="bias"](%2283)
  %2302 : Tensor = prim::GetAttr[name="weight"](%2283)
  %2303 : Float(768:1, 768:768) = aten::t(%2302), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input_embeds, %2303), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.v_head # torch/nn/functional.py:1676:0
  %2305 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %2301, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.v_head # torch/nn/functional.py:1678:0
  %2306 : int[] = prim::ListConstruct(%2286, %2288, %2160, %2159), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2307 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2305, %2306), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:538:0
  %q_head.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.25, %2158), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias.13 : Float(12:64, 64:1) = aten::mul(%2282, %2158), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:542:0
  %2310 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_w_bias.13, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:544:0
  %2311 : Tensor[] = prim::ListConstruct(%2310, %2300), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %content_score.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%2157, %2311), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %v.13 : Float(12:64, 64:1) = aten::mul(%2281, %2158), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:486:0
  %2314 : Tensor[] = prim::ListConstruct(%2251, %2280), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2315 : Float(26:768, 12:64, 64:1) = aten::einsum(%2156, %2314), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2316 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %v.13, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:493:0
  %2317 : Tensor[] = prim::ListConstruct(%2316, %2315), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %positional_attn.73 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%2155, %2317), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2319 : int = aten::size(%positional_attn.73, %2185), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2320 : int = aten::size(%positional_attn.73, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2321 : int = aten::size(%positional_attn.73, %2167), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %2322 : int = aten::size(%positional_attn.73, %2182), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len.13 : Long() = prim::NumToTensor(%2322), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2324 : int[] = prim::ListConstruct(%2319, %2320, %2322, %2321), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %positional_attn.74 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.73, %2324), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:428:0
  %2326 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.74, %2185, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2327 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%2326, %2186, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2328 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2327, %2167, %2186, %2184, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.75 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2328, %2182, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:429:0
  %2330 : Long() = aten::sub(%max_rel_len.13, %2181, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
  %2331 : int = aten::Int(%2330), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2332 : int[] = prim::ListConstruct(%2319, %2320, %2321, %2331), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %positional_attn.76 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.75, %2332), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.77 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.76, %2182, %2185, %2288, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:431:0
  %positional_attn.78 : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.77, %cls_mask), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:498:0
  %2336 : int = aten::size(%token_type_mat.20, %2185), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %2337 : int = aten::size(%token_type_mat.20, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %2338 : int = aten::size(%token_type_mat.20, %2167), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias.13 : Float(12:64, 64:1) = aten::mul(%2279, %2158), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:508:0
  %2340 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head.26, %r_s_bias.13, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:511:0
  %2341 : Tensor[] = prim::ListConstruct(%2340, %2278), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2342 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%2154, %2341), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2343 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %2185, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2344 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%2343, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2345 : int = aten::size(%q_head.26, %2167), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2346 : int[] = prim::ListConstruct(%2336, %2345, %2337, %2338), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %token_type_mat.21 : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%2344, %2346, %2176), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:513:0
  %2348 : Tensor[] = aten::split(%2342, %2186, %2170), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/tensor.py:371:0
  %diff_token_type.13 : Float(17:26, 12:442, 13:2, 1:1), %same_token_type.13 : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%2348), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2351 : int = aten::size(%token_type_mat.21, %2185), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2352 : int = aten::size(%token_type_mat.21, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2353 : int = aten::size(%token_type_mat.21, %2167), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2354 : int = aten::size(%token_type_mat.21, %2182), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2355 : int[] = prim::ListConstruct(%2351, %2352, %2353, %2354), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2356 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type.13, %2355, %2176), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2357 : int = aten::size(%token_type_mat.21, %2185), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2358 : int = aten::size(%token_type_mat.21, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2359 : int = aten::size(%token_type_mat.21, %2167), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2360 : int = aten::size(%token_type_mat.21, %2182), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %2361 : int[] = prim::ListConstruct(%2357, %2358, %2359, %2360), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %2362 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type.13, %2361, %2176), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.25 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat.21, %2356, %2362), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.25, %cls_mask), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:522:0
  %2365 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score.13, %positional_attn.78, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%2365, %token_type_attn.26, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:549:0
  %attn_score.38 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.37, %2178, %2176, %2176, %2173), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:553:0
  %2368 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %2185, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2369 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2368, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2370 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2369, %2167), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2371 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%2370, %2178, %2176, %2176, %2173), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %2372 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%2371, %2186, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/tensor.py:396:0
  %2373 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%2372, %2153), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %attn_score.39 : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.38, %2373, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:556:0
  %input.120 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score.39, %2170, %2178), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:558:0
  %2376 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.120, %2171, %2176), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %2377 : Tensor[] = prim::ListConstruct(%2376, %2307), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %attn_vec.13 : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%2152, %2377), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # torch/functional.py:327:0
  %2379 : int[] = prim::ListConstruct(%2286, %2287, %2168), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention
  %input.121 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec.13, %2379), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:565:0
  %2381 : Tensor = prim::GetAttr[name="bias"](%2277)
  %2382 : Tensor = prim::GetAttr[name="weight"](%2277)
  %2383 : Float(768:1, 768:768) = aten::t(%2382), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
  %output.65 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.121, %2383), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1676:0
  %input.122 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.65, %2381, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.122, %2171, %2176), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add(%input_embeds, %attn_out.13, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention # transformers/modeling_funnel.py:568:0
  %2388 : Tensor = prim::GetAttr[name="bias"](%2276)
  %2389 : Tensor = prim::GetAttr[name="weight"](%2276)
  %2390 : int[] = prim::ListConstruct(%2168), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.layer_norm
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.123, %2390, %2389, %2388, %2150, %2151), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.attention/__module.decoder.layers.0.attention.layer_norm # torch/nn/functional.py:2048:0
  %2392 : __torch__.torch.nn.modules.normalization.___torch_mangle_4776.LayerNorm = prim::GetAttr[name="layer_norm"](%2274)
  %2393 : __torch__.torch.nn.modules.linear.___torch_mangle_4774.Linear = prim::GetAttr[name="linear_2"](%2274)
  %2394 : __torch__.torch.nn.modules.linear.___torch_mangle_4772.Linear = prim::GetAttr[name="linear_1"](%2274)
  %2395 : Tensor = prim::GetAttr[name="bias"](%2394)
  %2396 : Tensor = prim::GetAttr[name="weight"](%2394)
  %2397 : Float(768:1, 3072:768) = aten::t(%2396), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.124, %2397), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x.15 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.66, %2395, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2400 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.15, %2166), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2401 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.15, %2165), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2402 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2401, %2164), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2403 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.15, %2402, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2404 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2403, %2163), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2405 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%2404), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %2406 : Float(17:39936, 13:3072, 3072:1) = aten::add(%2405, %2162, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %input.125 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2400, %2406), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/activations.py:30:0
  %input.126 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.125, %2161, %2176), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2409 : Tensor = prim::GetAttr[name="bias"](%2393)
  %2410 : Tensor = prim::GetAttr[name="weight"](%2393)
  %2411 : Float(3072:1, 768:3072) = aten::t(%2410), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.126, %2411), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %2409, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.127, %2171, %2176), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(17:9984, 13:768, 768:1) = aten::add(%input.124, %h.13, %2186), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn # transformers/modeling_funnel.py:588:0
  %2416 : Tensor = prim::GetAttr[name="bias"](%2392)
  %2417 : Tensor = prim::GetAttr[name="weight"](%2392)
  %2418 : int[] = prim::ListConstruct(%2168), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.layer_norm
  %query : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.128, %2418, %2417, %2416, %2150, %2151), scope: __module.decoder/__module.decoder.layers.0/__module.decoder.layers.0.ffn/__module.decoder.layers.0.ffn.layer_norm # torch/nn/functional.py:2048:0
  %2420 : __torch__.transformers.modeling_funnel.___torch_mangle_4792.FunnelPositionwiseFFN = prim::GetAttr[name="ffn"](%2188)
  %2421 : __torch__.transformers.modeling_funnel.___torch_mangle_4786.FunnelRelMultiheadAttention = prim::GetAttr[name="attention"](%2188)
  %2422 : __torch__.torch.nn.modules.normalization.___torch_mangle_4785.LayerNorm = prim::GetAttr[name="layer_norm"](%2421)
  %2423 : __torch__.torch.nn.modules.linear.___torch_mangle_4784.Linear = prim::GetAttr[name="post_proj"](%2421)
  %2424 : Tensor = prim::GetAttr[name="seg_embed"](%2421)
  %2425 : Tensor = prim::GetAttr[name="r_s_bias"](%2421)
  %2426 : Tensor = prim::GetAttr[name="r_kernel"](%2421)
  %2427 : Tensor = prim::GetAttr[name="r_r_bias"](%2421)
  %2428 : Tensor = prim::GetAttr[name="r_w_bias"](%2421)
  %2429 : __torch__.torch.nn.modules.linear.___torch_mangle_4783.Linear = prim::GetAttr[name="v_head"](%2421)
  %2430 : __torch__.torch.nn.modules.linear.___torch_mangle_4782.Linear = prim::GetAttr[name="k_head"](%2421)
  %2431 : __torch__.torch.nn.modules.linear.___torch_mangle_4781.Linear = prim::GetAttr[name="q_head"](%2421)
  %2432 : int = aten::size(%query, %2185), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
  %2433 : int = aten::size(%query, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:530:0
  %2434 : int = aten::size(%query, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:531:0
  %2435 : Tensor = prim::GetAttr[name="weight"](%2431)
  %2436 : Float(768:1, 768:768) = aten::t(%2435), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
  %2437 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2436), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.q_head # torch/nn/functional.py:1676:0
  %2438 : int[] = prim::ListConstruct(%2432, %2433, %2160, %2159), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %q_head.27 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2437, %2438), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:535:0
  %2440 : Tensor = prim::GetAttr[name="bias"](%2430)
  %2441 : Tensor = prim::GetAttr[name="weight"](%2430)
  %2442 : Float(768:1, 768:768) = aten::t(%2441), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2442), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.k_head # torch/nn/functional.py:1676:0
  %2444 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %2440, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.k_head # torch/nn/functional.py:1678:0
  %2445 : int[] = prim::ListConstruct(%2432, %2434, %2160, %2159), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2446 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2444, %2445), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:537:0
  %2447 : Tensor = prim::GetAttr[name="bias"](%2429)
  %2448 : Tensor = prim::GetAttr[name="weight"](%2429)
  %2449 : Float(768:1, 768:768) = aten::t(%2448), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%query, %2449), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.v_head # torch/nn/functional.py:1676:0
  %2451 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %2447, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.v_head # torch/nn/functional.py:1678:0
  %2452 : int[] = prim::ListConstruct(%2432, %2434, %2160, %2159), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2453 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%2451, %2452), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:538:0
  %q_head : Float(17:9984, 13:768, 12:64, 64:1) = aten::mul(%q_head.27, %2158), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:540:0
  %r_w_bias : Float(12:64, 64:1) = aten::mul(%2428, %2158), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:542:0
  %2456 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_w_bias, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:544:0
  %2457 : Tensor[] = prim::ListConstruct(%2456, %2446), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %content_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::einsum(%2157, %2457), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # torch/functional.py:327:0
  %v : Float(12:64, 64:1) = aten::mul(%2427, %2158), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:486:0
  %2460 : Tensor[] = prim::ListConstruct(%2251, %2426), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2461 : Float(26:768, 12:64, 64:1) = aten::einsum(%2156, %2460), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # torch/functional.py:327:0
  %2462 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %v, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:493:0
  %2463 : Tensor[] = prim::ListConstruct(%2462, %2461), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %positional_attn.79 : Float(17:338, 12:5746, 13:26, 26:1) = aten::einsum(%2155, %2463), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # torch/functional.py:327:0
  %2465 : int = aten::size(%positional_attn.79, %2185), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2466 : int = aten::size(%positional_attn.79, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2467 : int = aten::size(%positional_attn.79, %2167), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %2468 : int = aten::size(%positional_attn.79, %2182), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:420:0
  %max_rel_len : Long() = prim::NumToTensor(%2468), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2470 : int[] = prim::ListConstruct(%2465, %2466, %2468, %2467), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %positional_attn.80 : Float(17:338, 12:5746, 26:13, 13:1) = aten::reshape(%positional_attn.79, %2470), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:428:0
  %2472 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%positional_attn.80, %2185, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2473 : Float(17:338, 12:5746, 26:13, 13:1) = aten::slice(%2472, %2186, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2474 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2473, %2167, %2186, %2184, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %positional_attn.81 : Float(17:338, 12:5746, 25:13, 13:1) = aten::slice(%2474, %2182, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:429:0
  %2476 : Long() = aten::sub(%max_rel_len, %2181, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
  %2477 : int = aten::Int(%2476), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2478 : int[] = prim::ListConstruct(%2465, %2466, %2467, %2477), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %positional_attn.82 : Float(17:338, 12:5746, 13:25, 25:1) = aten::reshape(%positional_attn.81, %2478), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:430:0
  %positional_attn.83 : Float(17:338, 12:5746, 13:25, 13:1) = aten::slice(%positional_attn.82, %2182, %2185, %2434, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:431:0
  %positional_attn : Float(17:338, 12:5746, 13:25, 13:1) = aten::mul_(%positional_attn.83, %cls_mask), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:498:0
  %2482 : int = aten::size(%token_type_mat.20, %2185), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %2483 : int = aten::size(%token_type_mat.20, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %2484 : int = aten::size(%token_type_mat.20, %2167), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:505:0
  %r_s_bias : Float(12:64, 64:1) = aten::mul(%2425, %2158), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:508:0
  %2486 : Float(17:9984, 13:768, 12:64, 64:1) = aten::add(%q_head, %r_s_bias, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:511:0
  %2487 : Tensor[] = prim::ListConstruct(%2486, %2424), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2488 : Float(17:26, 12:442, 13:2, 2:1) = aten::einsum(%2154, %2487), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # torch/functional.py:327:0
  %2489 : Bool(17:169, 13:13, 13:1) = aten::slice(%token_type_mat.20, %2185, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2490 : Bool(17:169, 1:169, 13:13, 13:1) = aten::unsqueeze(%2489, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2491 : int = aten::size(%q_head, %2167), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2492 : int[] = prim::ListConstruct(%2482, %2491, %2483, %2484), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %token_type_mat : Bool(17:169, 12:0, 13:13, 13:1) = aten::expand(%2490, %2492, %2176), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:513:0
  %2494 : Tensor[] = aten::split(%2488, %2186, %2170), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # torch/tensor.py:371:0
  %diff_token_type : Float(17:26, 12:442, 13:2, 1:1), %same_token_type : Float(17:26, 12:442, 13:2, 1:1) = prim::ListUnpack(%2494), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2497 : int = aten::size(%token_type_mat, %2185), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2498 : int = aten::size(%token_type_mat, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2499 : int = aten::size(%token_type_mat, %2167), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2500 : int = aten::size(%token_type_mat, %2182), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2501 : int[] = prim::ListConstruct(%2497, %2498, %2499, %2500), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2502 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%same_token_type, %2501, %2176), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2503 : int = aten::size(%token_type_mat, %2185), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2504 : int = aten::size(%token_type_mat, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2505 : int = aten::size(%token_type_mat, %2167), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2506 : int = aten::size(%token_type_mat, %2182), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %2507 : int[] = prim::ListConstruct(%2503, %2504, %2505, %2506), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %2508 : Float(17:26, 12:442, 13:2, 13:0) = aten::expand(%diff_token_type, %2507, %2176), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:518:0
  %token_type_attn.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::where(%token_type_mat, %2502, %2508), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:517:0
  %token_type_attn : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul_(%token_type_attn.27, %cls_mask), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:522:0
  %2511 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%content_score, %positional_attn, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.40 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%2511, %token_type_attn, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:549:0
  %attn_score.41 : Float(17:2028, 12:169, 13:13, 13:1) = aten::to(%attn_score.40, %2178, %2176, %2176, %2173), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:553:0
  %2514 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %2185, %2185, %2184, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2515 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%2514, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2516 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2515, %2167), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2517 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%2516, %2178, %2176, %2176, %2173), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %2518 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%2517, %2186, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # torch/tensor.py:396:0
  %2519 : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%2518, %2153), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %attn_score : Float(17:2028, 12:169, 13:13, 13:1) = aten::sub(%attn_score.41, %2519, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:556:0
  %input.129 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%attn_score, %2170, %2178), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:558:0
  %2522 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.129, %2171, %2176), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.attention_dropout # torch/nn/functional.py:973:0
  %2523 : Tensor[] = prim::ListConstruct(%2522, %2453), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %attn_vec : Float(17:9984, 13:64, 12:832, 64:1) = aten::einsum(%2152, %2523), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # torch/functional.py:327:0
  %2525 : int[] = prim::ListConstruct(%2432, %2433, %2168), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention
  %input.130 : Float(17:9984, 13:768, 768:1) = aten::reshape(%attn_vec, %2525), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:565:0
  %2527 : Tensor = prim::GetAttr[name="bias"](%2423)
  %2528 : Tensor = prim::GetAttr[name="weight"](%2423)
  %2529 : Float(768:1, 768:768) = aten::t(%2528), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.130, %2529), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1676:0
  %input.131 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %2527, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.post_proj # torch/nn/functional.py:1678:0
  %attn_out : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.131, %2171, %2176), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.hidden_dropout # torch/nn/functional.py:973:0
  %input.132 : Float(17:9984, 13:768, 768:1) = aten::add(%query, %attn_out, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention # transformers/modeling_funnel.py:568:0
  %2534 : Tensor = prim::GetAttr[name="bias"](%2422)
  %2535 : Tensor = prim::GetAttr[name="weight"](%2422)
  %2536 : int[] = prim::ListConstruct(%2168), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.layer_norm
  %input.133 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.132, %2536, %2535, %2534, %2150, %2151), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.attention/__module.decoder.layers.1.attention.layer_norm # torch/nn/functional.py:2048:0
  %2538 : __torch__.torch.nn.modules.normalization.___torch_mangle_4791.LayerNorm = prim::GetAttr[name="layer_norm"](%2420)
  %2539 : __torch__.torch.nn.modules.linear.___torch_mangle_4789.Linear = prim::GetAttr[name="linear_2"](%2420)
  %2540 : __torch__.torch.nn.modules.linear.___torch_mangle_4787.Linear = prim::GetAttr[name="linear_1"](%2420)
  %2541 : Tensor = prim::GetAttr[name="bias"](%2540)
  %2542 : Tensor = prim::GetAttr[name="weight"](%2540)
  %2543 : Float(768:1, 3072:768) = aten::t(%2542), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input.133, %2543), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1676:0
  %x : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %2541, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.linear_1 # torch/nn/functional.py:1678:0
  %2546 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x, %2166), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2547 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x, %2165), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2548 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2547, %2164), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2549 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x, %2548, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2550 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2549, %2163), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2551 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%2550), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/activations.py:30:0
  %2552 : Float(17:39936, 13:3072, 3072:1) = aten::add(%2551, %2162, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/activations.py:30:0
  %input.134 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%2546, %2552), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/activations.py:30:0
  %input.135 : Float(17:39936, 13:3072, 3072:1) = aten::dropout(%input.134, %2161, %2176), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.activation_dropout # torch/nn/functional.py:973:0
  %2555 : Tensor = prim::GetAttr[name="bias"](%2539)
  %2556 : Tensor = prim::GetAttr[name="weight"](%2539)
  %2557 : Float(3072:1, 768:3072) = aten::t(%2556), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %output : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.135, %2557), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(17:9984, 13:768, 768:1) = aten::add_(%output, %2555, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.linear_2 # torch/nn/functional.py:1678:0
  %h : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.136, %2171, %2176), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.dropout # torch/nn/functional.py:973:0
  %input : Float(17:9984, 13:768, 768:1) = aten::add(%input.133, %h, %2186), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn # transformers/modeling_funnel.py:588:0
  %2562 : Tensor = prim::GetAttr[name="bias"](%2538)
  %2563 : Tensor = prim::GetAttr[name="weight"](%2538)
  %2564 : int[] = prim::ListConstruct(%2168), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.layer_norm
  %2565 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input, %2564, %2563, %2562, %2150, %2151), scope: __module.decoder/__module.decoder.layers.1/__module.decoder.layers.1.ffn/__module.decoder.layers.1.ffn.layer_norm # torch/nn/functional.py:2048:0
  %28 : (Float(17:9984, 13:768, 768:1)) = prim::TupleConstruct(%2565)
  return (%28)
