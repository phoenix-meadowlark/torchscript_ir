graph(%self.1 : __torch__.transformers.modeling_flaubert.FlaubertForSequenceClassification,
      %input_ids : Long(17:13, 13:1),
      %padding_mask : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_utils.___torch_mangle_650.SequenceSummary = prim::GetAttr[name="sequence_summary"](%self.1)
  %4 : __torch__.transformers.modeling_flaubert.___torch_mangle_645.FlaubertModel = prim::GetAttr[name="transformer"](%self.1)
  %8 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %9 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %10 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %11 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %12 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %13 : None = prim::Constant(), scope: __module.transformer
  %14 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %15 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
  %16 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %17 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %18 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %19 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %20 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %21 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %22 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %23 : int = prim::Constant[value=4](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %24 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %25 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %26 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %27 : __torch__.torch.nn.modules.normalization.___torch_mangle_643.LayerNorm = prim::GetAttr[name="11"](%26)
  %28 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %29 : __torch__.transformers.modeling_xlm.___torch_mangle_630.TransformerFFN = prim::GetAttr[name="11"](%28)
  %30 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %31 : __torch__.torch.nn.modules.normalization.___torch_mangle_593.LayerNorm = prim::GetAttr[name="11"](%30)
  %32 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %33 : __torch__.transformers.modeling_xlm.___torch_mangle_580.MultiHeadAttention = prim::GetAttr[name="11"](%32)
  %34 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %35 : __torch__.torch.nn.modules.normalization.___torch_mangle_642.LayerNorm = prim::GetAttr[name="10"](%34)
  %36 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %37 : __torch__.transformers.modeling_xlm.___torch_mangle_627.TransformerFFN = prim::GetAttr[name="10"](%36)
  %38 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %39 : __torch__.torch.nn.modules.normalization.___torch_mangle_592.LayerNorm = prim::GetAttr[name="10"](%38)
  %40 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %41 : __torch__.transformers.modeling_xlm.___torch_mangle_575.MultiHeadAttention = prim::GetAttr[name="10"](%40)
  %42 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %43 : __torch__.torch.nn.modules.normalization.___torch_mangle_641.LayerNorm = prim::GetAttr[name="9"](%42)
  %44 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %45 : __torch__.transformers.modeling_xlm.___torch_mangle_624.TransformerFFN = prim::GetAttr[name="9"](%44)
  %46 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %47 : __torch__.torch.nn.modules.normalization.___torch_mangle_591.LayerNorm = prim::GetAttr[name="9"](%46)
  %48 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %49 : __torch__.transformers.modeling_xlm.___torch_mangle_570.MultiHeadAttention = prim::GetAttr[name="9"](%48)
  %50 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %51 : __torch__.torch.nn.modules.normalization.___torch_mangle_640.LayerNorm = prim::GetAttr[name="8"](%50)
  %52 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %53 : __torch__.transformers.modeling_xlm.___torch_mangle_621.TransformerFFN = prim::GetAttr[name="8"](%52)
  %54 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %55 : __torch__.torch.nn.modules.normalization.___torch_mangle_590.LayerNorm = prim::GetAttr[name="8"](%54)
  %56 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %57 : __torch__.transformers.modeling_xlm.___torch_mangle_565.MultiHeadAttention = prim::GetAttr[name="8"](%56)
  %58 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %59 : __torch__.torch.nn.modules.normalization.___torch_mangle_639.LayerNorm = prim::GetAttr[name="7"](%58)
  %60 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %61 : __torch__.transformers.modeling_xlm.___torch_mangle_618.TransformerFFN = prim::GetAttr[name="7"](%60)
  %62 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %63 : __torch__.torch.nn.modules.normalization.___torch_mangle_589.LayerNorm = prim::GetAttr[name="7"](%62)
  %64 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %65 : __torch__.transformers.modeling_xlm.___torch_mangle_560.MultiHeadAttention = prim::GetAttr[name="7"](%64)
  %66 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %67 : __torch__.torch.nn.modules.normalization.___torch_mangle_638.LayerNorm = prim::GetAttr[name="6"](%66)
  %68 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %69 : __torch__.transformers.modeling_xlm.___torch_mangle_615.TransformerFFN = prim::GetAttr[name="6"](%68)
  %70 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %71 : __torch__.torch.nn.modules.normalization.___torch_mangle_588.LayerNorm = prim::GetAttr[name="6"](%70)
  %72 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %73 : __torch__.transformers.modeling_xlm.___torch_mangle_555.MultiHeadAttention = prim::GetAttr[name="6"](%72)
  %74 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %75 : __torch__.torch.nn.modules.normalization.___torch_mangle_637.LayerNorm = prim::GetAttr[name="5"](%74)
  %76 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %77 : __torch__.transformers.modeling_xlm.___torch_mangle_612.TransformerFFN = prim::GetAttr[name="5"](%76)
  %78 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %79 : __torch__.torch.nn.modules.normalization.___torch_mangle_587.LayerNorm = prim::GetAttr[name="5"](%78)
  %80 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %81 : __torch__.transformers.modeling_xlm.___torch_mangle_550.MultiHeadAttention = prim::GetAttr[name="5"](%80)
  %82 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %83 : __torch__.torch.nn.modules.normalization.___torch_mangle_636.LayerNorm = prim::GetAttr[name="4"](%82)
  %84 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %85 : __torch__.transformers.modeling_xlm.___torch_mangle_609.TransformerFFN = prim::GetAttr[name="4"](%84)
  %86 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %87 : __torch__.torch.nn.modules.normalization.___torch_mangle_586.LayerNorm = prim::GetAttr[name="4"](%86)
  %88 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %89 : __torch__.transformers.modeling_xlm.___torch_mangle_545.MultiHeadAttention = prim::GetAttr[name="4"](%88)
  %90 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %91 : __torch__.torch.nn.modules.normalization.___torch_mangle_635.LayerNorm = prim::GetAttr[name="3"](%90)
  %92 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %93 : __torch__.transformers.modeling_xlm.___torch_mangle_606.TransformerFFN = prim::GetAttr[name="3"](%92)
  %94 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %95 : __torch__.torch.nn.modules.normalization.___torch_mangle_585.LayerNorm = prim::GetAttr[name="3"](%94)
  %96 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %97 : __torch__.transformers.modeling_xlm.___torch_mangle_540.MultiHeadAttention = prim::GetAttr[name="3"](%96)
  %98 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %99 : __torch__.torch.nn.modules.normalization.___torch_mangle_634.LayerNorm = prim::GetAttr[name="2"](%98)
  %100 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %101 : __torch__.transformers.modeling_xlm.___torch_mangle_603.TransformerFFN = prim::GetAttr[name="2"](%100)
  %102 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %103 : __torch__.torch.nn.modules.normalization.___torch_mangle_584.LayerNorm = prim::GetAttr[name="2"](%102)
  %104 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %105 : __torch__.transformers.modeling_xlm.___torch_mangle_535.MultiHeadAttention = prim::GetAttr[name="2"](%104)
  %106 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %107 : __torch__.torch.nn.modules.normalization.___torch_mangle_633.LayerNorm = prim::GetAttr[name="1"](%106)
  %108 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %109 : __torch__.transformers.modeling_xlm.___torch_mangle_600.TransformerFFN = prim::GetAttr[name="1"](%108)
  %110 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %111 : __torch__.torch.nn.modules.normalization.___torch_mangle_583.LayerNorm = prim::GetAttr[name="1"](%110)
  %112 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %113 : __torch__.transformers.modeling_xlm.___torch_mangle_530.MultiHeadAttention = prim::GetAttr[name="1"](%112)
  %114 : __torch__.torch.nn.modules.container.___torch_mangle_644.ModuleList = prim::GetAttr[name="layer_norm2"](%4)
  %115 : __torch__.torch.nn.modules.normalization.___torch_mangle_632.LayerNorm = prim::GetAttr[name="0"](%114)
  %116 : __torch__.torch.nn.modules.container.___torch_mangle_631.ModuleList = prim::GetAttr[name="ffns"](%4)
  %117 : __torch__.transformers.modeling_xlm.___torch_mangle_597.TransformerFFN = prim::GetAttr[name="0"](%116)
  %118 : __torch__.torch.nn.modules.container.___torch_mangle_594.ModuleList = prim::GetAttr[name="layer_norm1"](%4)
  %119 : __torch__.torch.nn.modules.normalization.___torch_mangle_582.LayerNorm = prim::GetAttr[name="0"](%118)
  %120 : __torch__.torch.nn.modules.container.___torch_mangle_581.ModuleList = prim::GetAttr[name="attentions"](%4)
  %121 : __torch__.transformers.modeling_xlm.___torch_mangle_525.MultiHeadAttention = prim::GetAttr[name="0"](%120)
  %122 : __torch__.torch.nn.modules.normalization.___torch_mangle_520.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%4)
  %123 : __torch__.torch.nn.modules.sparse.___torch_mangle_518.Embedding = prim::GetAttr[name="position_embeddings"](%4)
  %124 : __torch__.torch.nn.modules.sparse.___torch_mangle_519.Embedding = prim::GetAttr[name="embeddings"](%4)
  %125 : int = aten::size(%input_ids, %25), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %126 : int = aten::size(%input_ids, %24), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
  %position_ids : Long(13:1) = aten::arange(%126, %23, %25, %22, %21), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
  %128 : Long(1:13, 13:1) = aten::unsqueeze(%position_ids, %25), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
  %129 : int[] = prim::ListConstruct(%125, %126), scope: __module.transformer
  %input.1 : Long(17:0, 13:1) = aten::expand(%128, %129, %21), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
  %131 : Tensor = prim::GetAttr[name="weight"](%124)
  %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%131, %input_ids, %20, %21, %21), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
  %133 : Tensor = prim::GetAttr[name="weight"](%123)
  %134 : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%133, %input.1, %19, %21, %21), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
  %135 : Float(17:26624, 13:2048, 2048:1) = aten::expand_as(%134, %inputs_embeds), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
  %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%inputs_embeds, %135, %24), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
  %137 : Tensor = prim::GetAttr[name="bias"](%122)
  %138 : Tensor = prim::GetAttr[name="weight"](%122)
  %139 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm_emb
  %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %139, %138, %137, %17, %18), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
  %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.3, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %142 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %143 : Float(17:13, 13:1, 1:1) = aten::to(%142, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %143), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
  %145 : __torch__.torch.nn.modules.linear.___torch_mangle_524.Linear = prim::GetAttr[name="out_lin"](%121)
  %146 : __torch__.torch.nn.modules.linear.___torch_mangle_523.Linear = prim::GetAttr[name="v_lin"](%121)
  %147 : __torch__.torch.nn.modules.linear.___torch_mangle_522.Linear = prim::GetAttr[name="k_lin"](%121)
  %148 : __torch__.torch.nn.modules.linear.___torch_mangle_521.Linear = prim::GetAttr[name="q_lin"](%121)
  %149 : int = aten::size(%input.4, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %150 : int = aten::size(%input.4, %24), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
  %151 : Tensor = prim::GetAttr[name="bias"](%148)
  %152 : Tensor = prim::GetAttr[name="weight"](%148)
  %153 : Float(2048:1, 2048:2048) = aten::t(%152), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %153), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
  %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %151, %24), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1678:0
  %156 : int[] = prim::ListConstruct(%149, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.0
  %157 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.1, %156), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%157, %24, %20), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %159 : Tensor = prim::GetAttr[name="bias"](%147)
  %160 : Tensor = prim::GetAttr[name="weight"](%147)
  %161 : Float(2048:1, 2048:2048) = aten::t(%160), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %161), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
  %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %159, %24), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1678:0
  %164 : int[] = prim::ListConstruct(%149, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.0
  %165 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.2, %164), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%165, %24, %20), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %167 : Tensor = prim::GetAttr[name="bias"](%146)
  %168 : Tensor = prim::GetAttr[name="weight"](%146)
  %169 : Float(2048:1, 2048:2048) = aten::t(%168), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %169), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
  %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %167, %24), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1678:0
  %172 : int[] = prim::ListConstruct(%149, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.0
  %173 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.3, %172), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%173, %24, %20), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
  %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %10), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
  %176 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %20, %11), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %176), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
  %178 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.0 # torch/tensor.py:22:0
  %179 : int[] = prim::ListConstruct(%149, %24, %24, %150), scope: __module.transformer/__module.transformer.attentions.0
  %180 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%178, %179), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%180, %scores.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
  %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %12), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
  %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
  %184 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %19, %13), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:1498:0
  %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%184, %15, %21), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
  %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:200:0
  %187 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %24, %20), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %188 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%187, %25), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %189 : int[] = prim::ListConstruct(%149, %19, %16), scope: __module.transformer/__module.transformer.attentions.0
  %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%188, %189), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
  %191 : Tensor = prim::GetAttr[name="bias"](%145)
  %192 : Tensor = prim::GetAttr[name="weight"](%145)
  %193 : Float(2048:1, 2048:2048) = aten::t(%192), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %193), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
  %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %191, %24), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1678:0
  %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.8, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %198 : Tensor = prim::GetAttr[name="bias"](%119)
  %199 : Tensor = prim::GetAttr[name="weight"](%119)
  %200 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.0
  %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %200, %199, %198, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
  %202 : __torch__.torch.nn.modules.linear.___torch_mangle_596.Linear = prim::GetAttr[name="lin2"](%117)
  %203 : __torch__.torch.nn.modules.linear.___torch_mangle_595.Linear = prim::GetAttr[name="lin1"](%117)
  %204 : Tensor = prim::GetAttr[name="bias"](%203)
  %205 : Tensor = prim::GetAttr[name="weight"](%203)
  %206 : Float(2048:1, 8192:2048) = aten::t(%205), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.1, %206), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
  %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %204, %24), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1678:0
  %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.10), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:1369:0
  %210 : Tensor = prim::GetAttr[name="bias"](%202)
  %211 : Tensor = prim::GetAttr[name="weight"](%202)
  %212 : Float(8192:1, 2048:8192) = aten::t(%211), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %212), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
  %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %210, %24), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1678:0
  %215 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.12, %15, %21), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
  %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.1, %215, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %217 : Tensor = prim::GetAttr[name="bias"](%115)
  %218 : Tensor = prim::GetAttr[name="weight"](%115)
  %219 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.0
  %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %219, %218, %217, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
  %221 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %222 : Float(17:13, 13:1, 1:1) = aten::to(%221, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.2, %222), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %224 : __torch__.torch.nn.modules.linear.___torch_mangle_529.Linear = prim::GetAttr[name="out_lin"](%113)
  %225 : __torch__.torch.nn.modules.linear.___torch_mangle_528.Linear = prim::GetAttr[name="v_lin"](%113)
  %226 : __torch__.torch.nn.modules.linear.___torch_mangle_527.Linear = prim::GetAttr[name="k_lin"](%113)
  %227 : __torch__.torch.nn.modules.linear.___torch_mangle_526.Linear = prim::GetAttr[name="q_lin"](%113)
  %228 : int = aten::size(%input.14, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %229 : int = aten::size(%input.14, %24), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
  %230 : Tensor = prim::GetAttr[name="bias"](%227)
  %231 : Tensor = prim::GetAttr[name="weight"](%227)
  %232 : Float(2048:1, 2048:2048) = aten::t(%231), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %232), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
  %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %230, %24), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1678:0
  %235 : int[] = prim::ListConstruct(%228, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.1
  %236 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.5, %235), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%236, %24, %20), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %238 : Tensor = prim::GetAttr[name="bias"](%226)
  %239 : Tensor = prim::GetAttr[name="weight"](%226)
  %240 : Float(2048:1, 2048:2048) = aten::t(%239), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %240), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
  %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %238, %24), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1678:0
  %243 : int[] = prim::ListConstruct(%228, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.1
  %244 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.6, %243), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%244, %24, %20), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %246 : Tensor = prim::GetAttr[name="bias"](%225)
  %247 : Tensor = prim::GetAttr[name="weight"](%225)
  %248 : Float(2048:1, 2048:2048) = aten::t(%247), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %248), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
  %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %246, %24), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1678:0
  %251 : int[] = prim::ListConstruct(%228, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.1
  %252 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.7, %251), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%252, %24, %20), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
  %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %10), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:188:0
  %255 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %20, %11), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %255), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
  %257 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.1 # torch/tensor.py:22:0
  %258 : int[] = prim::ListConstruct(%228, %24, %24, %229), scope: __module.transformer/__module.transformer.attentions.1
  %259 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%257, %258), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%259, %scores.3), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
  %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %12), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:191:0
  %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
  %263 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %19, %13), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:1498:0
  %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%263, %15, %21), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
  %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:200:0
  %266 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %24, %20), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %267 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%266, %25), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %268 : int[] = prim::ListConstruct(%228, %19, %16), scope: __module.transformer/__module.transformer.attentions.1
  %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%267, %268), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
  %270 : Tensor = prim::GetAttr[name="bias"](%224)
  %271 : Tensor = prim::GetAttr[name="weight"](%224)
  %272 : Float(2048:1, 2048:2048) = aten::t(%271), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %272), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
  %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %270, %24), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1678:0
  %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.18, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %277 : Tensor = prim::GetAttr[name="bias"](%111)
  %278 : Tensor = prim::GetAttr[name="weight"](%111)
  %279 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.1
  %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %279, %278, %277, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
  %281 : __torch__.torch.nn.modules.linear.___torch_mangle_599.Linear = prim::GetAttr[name="lin2"](%109)
  %282 : __torch__.torch.nn.modules.linear.___torch_mangle_598.Linear = prim::GetAttr[name="lin1"](%109)
  %283 : Tensor = prim::GetAttr[name="bias"](%282)
  %284 : Tensor = prim::GetAttr[name="weight"](%282)
  %285 : Float(2048:1, 8192:2048) = aten::t(%284), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.2, %285), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
  %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %283, %24), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1678:0
  %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.20), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:1369:0
  %289 : Tensor = prim::GetAttr[name="bias"](%281)
  %290 : Tensor = prim::GetAttr[name="weight"](%281)
  %291 : Float(8192:1, 2048:8192) = aten::t(%290), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %291), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
  %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %289, %24), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1678:0
  %294 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.22, %15, %21), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
  %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.2, %294, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %296 : Tensor = prim::GetAttr[name="bias"](%107)
  %297 : Tensor = prim::GetAttr[name="weight"](%107)
  %298 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.1
  %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %298, %297, %296, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
  %300 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %301 : Float(17:13, 13:1, 1:1) = aten::to(%300, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.3, %301), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %303 : __torch__.torch.nn.modules.linear.___torch_mangle_534.Linear = prim::GetAttr[name="out_lin"](%105)
  %304 : __torch__.torch.nn.modules.linear.___torch_mangle_533.Linear = prim::GetAttr[name="v_lin"](%105)
  %305 : __torch__.torch.nn.modules.linear.___torch_mangle_532.Linear = prim::GetAttr[name="k_lin"](%105)
  %306 : __torch__.torch.nn.modules.linear.___torch_mangle_531.Linear = prim::GetAttr[name="q_lin"](%105)
  %307 : int = aten::size(%input.24, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %308 : int = aten::size(%input.24, %24), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
  %309 : Tensor = prim::GetAttr[name="bias"](%306)
  %310 : Tensor = prim::GetAttr[name="weight"](%306)
  %311 : Float(2048:1, 2048:2048) = aten::t(%310), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %311), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
  %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %309, %24), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1678:0
  %314 : int[] = prim::ListConstruct(%307, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.2
  %315 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.9, %314), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%315, %24, %20), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %317 : Tensor = prim::GetAttr[name="bias"](%305)
  %318 : Tensor = prim::GetAttr[name="weight"](%305)
  %319 : Float(2048:1, 2048:2048) = aten::t(%318), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %319), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
  %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %317, %24), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1678:0
  %322 : int[] = prim::ListConstruct(%307, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.2
  %323 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.10, %322), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%323, %24, %20), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %325 : Tensor = prim::GetAttr[name="bias"](%304)
  %326 : Tensor = prim::GetAttr[name="weight"](%304)
  %327 : Float(2048:1, 2048:2048) = aten::t(%326), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %327), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
  %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %325, %24), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1678:0
  %330 : int[] = prim::ListConstruct(%307, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.2
  %331 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.11, %330), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%331, %24, %20), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
  %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %10), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:188:0
  %334 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %20, %11), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %334), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
  %336 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.2 # torch/tensor.py:22:0
  %337 : int[] = prim::ListConstruct(%307, %24, %24, %308), scope: __module.transformer/__module.transformer.attentions.2
  %338 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%336, %337), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%338, %scores.5), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
  %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %12), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:191:0
  %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
  %342 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %19, %13), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:1498:0
  %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%342, %15, %21), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
  %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:200:0
  %345 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %24, %20), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %346 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%345, %25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %347 : int[] = prim::ListConstruct(%307, %19, %16), scope: __module.transformer/__module.transformer.attentions.2
  %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%346, %347), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
  %349 : Tensor = prim::GetAttr[name="bias"](%303)
  %350 : Tensor = prim::GetAttr[name="weight"](%303)
  %351 : Float(2048:1, 2048:2048) = aten::t(%350), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %351), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
  %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %349, %24), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1678:0
  %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.28, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %356 : Tensor = prim::GetAttr[name="bias"](%103)
  %357 : Tensor = prim::GetAttr[name="weight"](%103)
  %358 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.2
  %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %358, %357, %356, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
  %360 : __torch__.torch.nn.modules.linear.___torch_mangle_602.Linear = prim::GetAttr[name="lin2"](%101)
  %361 : __torch__.torch.nn.modules.linear.___torch_mangle_601.Linear = prim::GetAttr[name="lin1"](%101)
  %362 : Tensor = prim::GetAttr[name="bias"](%361)
  %363 : Tensor = prim::GetAttr[name="weight"](%361)
  %364 : Float(2048:1, 8192:2048) = aten::t(%363), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.3, %364), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
  %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %362, %24), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1678:0
  %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.30), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:1369:0
  %368 : Tensor = prim::GetAttr[name="bias"](%360)
  %369 : Tensor = prim::GetAttr[name="weight"](%360)
  %370 : Float(8192:1, 2048:8192) = aten::t(%369), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %370), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
  %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %368, %24), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1678:0
  %373 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.32, %15, %21), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
  %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.3, %373, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %375 : Tensor = prim::GetAttr[name="bias"](%99)
  %376 : Tensor = prim::GetAttr[name="weight"](%99)
  %377 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.2
  %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %377, %376, %375, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
  %379 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %380 : Float(17:13, 13:1, 1:1) = aten::to(%379, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.4, %380), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %382 : __torch__.torch.nn.modules.linear.___torch_mangle_539.Linear = prim::GetAttr[name="out_lin"](%97)
  %383 : __torch__.torch.nn.modules.linear.___torch_mangle_538.Linear = prim::GetAttr[name="v_lin"](%97)
  %384 : __torch__.torch.nn.modules.linear.___torch_mangle_537.Linear = prim::GetAttr[name="k_lin"](%97)
  %385 : __torch__.torch.nn.modules.linear.___torch_mangle_536.Linear = prim::GetAttr[name="q_lin"](%97)
  %386 : int = aten::size(%input.34, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %387 : int = aten::size(%input.34, %24), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
  %388 : Tensor = prim::GetAttr[name="bias"](%385)
  %389 : Tensor = prim::GetAttr[name="weight"](%385)
  %390 : Float(2048:1, 2048:2048) = aten::t(%389), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %390), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
  %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %388, %24), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1678:0
  %393 : int[] = prim::ListConstruct(%386, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.3
  %394 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.13, %393), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%394, %24, %20), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %396 : Tensor = prim::GetAttr[name="bias"](%384)
  %397 : Tensor = prim::GetAttr[name="weight"](%384)
  %398 : Float(2048:1, 2048:2048) = aten::t(%397), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %398), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
  %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %396, %24), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1678:0
  %401 : int[] = prim::ListConstruct(%386, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.3
  %402 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.14, %401), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%402, %24, %20), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %404 : Tensor = prim::GetAttr[name="bias"](%383)
  %405 : Tensor = prim::GetAttr[name="weight"](%383)
  %406 : Float(2048:1, 2048:2048) = aten::t(%405), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %406), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
  %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %404, %24), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1678:0
  %409 : int[] = prim::ListConstruct(%386, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.3
  %410 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.15, %409), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%410, %24, %20), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
  %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %10), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:188:0
  %413 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %20, %11), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %413), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
  %415 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.3 # torch/tensor.py:22:0
  %416 : int[] = prim::ListConstruct(%386, %24, %24, %387), scope: __module.transformer/__module.transformer.attentions.3
  %417 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%415, %416), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%417, %scores.7), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
  %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %12), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:191:0
  %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
  %421 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %19, %13), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:1498:0
  %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%421, %15, %21), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
  %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:200:0
  %424 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %24, %20), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %425 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%424, %25), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %426 : int[] = prim::ListConstruct(%386, %19, %16), scope: __module.transformer/__module.transformer.attentions.3
  %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%425, %426), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
  %428 : Tensor = prim::GetAttr[name="bias"](%382)
  %429 : Tensor = prim::GetAttr[name="weight"](%382)
  %430 : Float(2048:1, 2048:2048) = aten::t(%429), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %430), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
  %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %428, %24), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1678:0
  %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.38, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %435 : Tensor = prim::GetAttr[name="bias"](%95)
  %436 : Tensor = prim::GetAttr[name="weight"](%95)
  %437 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.3
  %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %437, %436, %435, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
  %439 : __torch__.torch.nn.modules.linear.___torch_mangle_605.Linear = prim::GetAttr[name="lin2"](%93)
  %440 : __torch__.torch.nn.modules.linear.___torch_mangle_604.Linear = prim::GetAttr[name="lin1"](%93)
  %441 : Tensor = prim::GetAttr[name="bias"](%440)
  %442 : Tensor = prim::GetAttr[name="weight"](%440)
  %443 : Float(2048:1, 8192:2048) = aten::t(%442), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.4, %443), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
  %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %441, %24), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1678:0
  %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.40), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:1369:0
  %447 : Tensor = prim::GetAttr[name="bias"](%439)
  %448 : Tensor = prim::GetAttr[name="weight"](%439)
  %449 : Float(8192:1, 2048:8192) = aten::t(%448), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %449), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
  %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %447, %24), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1678:0
  %452 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.42, %15, %21), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
  %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.4, %452, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %454 : Tensor = prim::GetAttr[name="bias"](%91)
  %455 : Tensor = prim::GetAttr[name="weight"](%91)
  %456 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.3
  %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %456, %455, %454, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
  %458 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %459 : Float(17:13, 13:1, 1:1) = aten::to(%458, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.5, %459), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %461 : __torch__.torch.nn.modules.linear.___torch_mangle_544.Linear = prim::GetAttr[name="out_lin"](%89)
  %462 : __torch__.torch.nn.modules.linear.___torch_mangle_543.Linear = prim::GetAttr[name="v_lin"](%89)
  %463 : __torch__.torch.nn.modules.linear.___torch_mangle_542.Linear = prim::GetAttr[name="k_lin"](%89)
  %464 : __torch__.torch.nn.modules.linear.___torch_mangle_541.Linear = prim::GetAttr[name="q_lin"](%89)
  %465 : int = aten::size(%input.44, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %466 : int = aten::size(%input.44, %24), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
  %467 : Tensor = prim::GetAttr[name="bias"](%464)
  %468 : Tensor = prim::GetAttr[name="weight"](%464)
  %469 : Float(2048:1, 2048:2048) = aten::t(%468), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %469), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
  %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %467, %24), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1678:0
  %472 : int[] = prim::ListConstruct(%465, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.4
  %473 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.17, %472), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%473, %24, %20), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %475 : Tensor = prim::GetAttr[name="bias"](%463)
  %476 : Tensor = prim::GetAttr[name="weight"](%463)
  %477 : Float(2048:1, 2048:2048) = aten::t(%476), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %477), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
  %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %475, %24), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1678:0
  %480 : int[] = prim::ListConstruct(%465, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.4
  %481 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.18, %480), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%481, %24, %20), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %483 : Tensor = prim::GetAttr[name="bias"](%462)
  %484 : Tensor = prim::GetAttr[name="weight"](%462)
  %485 : Float(2048:1, 2048:2048) = aten::t(%484), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %485), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
  %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %483, %24), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1678:0
  %488 : int[] = prim::ListConstruct(%465, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.4
  %489 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.19, %488), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%489, %24, %20), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
  %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %10), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:188:0
  %492 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %20, %11), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %492), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
  %494 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.4 # torch/tensor.py:22:0
  %495 : int[] = prim::ListConstruct(%465, %24, %24, %466), scope: __module.transformer/__module.transformer.attentions.4
  %496 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%494, %495), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%496, %scores.9), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
  %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %12), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:191:0
  %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
  %500 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %19, %13), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:1498:0
  %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%500, %15, %21), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
  %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:200:0
  %503 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %24, %20), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %504 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%503, %25), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %505 : int[] = prim::ListConstruct(%465, %19, %16), scope: __module.transformer/__module.transformer.attentions.4
  %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%504, %505), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
  %507 : Tensor = prim::GetAttr[name="bias"](%461)
  %508 : Tensor = prim::GetAttr[name="weight"](%461)
  %509 : Float(2048:1, 2048:2048) = aten::t(%508), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %509), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
  %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %507, %24), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1678:0
  %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.48, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %514 : Tensor = prim::GetAttr[name="bias"](%87)
  %515 : Tensor = prim::GetAttr[name="weight"](%87)
  %516 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.4
  %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %516, %515, %514, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
  %518 : __torch__.torch.nn.modules.linear.___torch_mangle_608.Linear = prim::GetAttr[name="lin2"](%85)
  %519 : __torch__.torch.nn.modules.linear.___torch_mangle_607.Linear = prim::GetAttr[name="lin1"](%85)
  %520 : Tensor = prim::GetAttr[name="bias"](%519)
  %521 : Tensor = prim::GetAttr[name="weight"](%519)
  %522 : Float(2048:1, 8192:2048) = aten::t(%521), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.5, %522), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
  %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %520, %24), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1678:0
  %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.50), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:1369:0
  %526 : Tensor = prim::GetAttr[name="bias"](%518)
  %527 : Tensor = prim::GetAttr[name="weight"](%518)
  %528 : Float(8192:1, 2048:8192) = aten::t(%527), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %528), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
  %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %526, %24), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1678:0
  %531 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.52, %15, %21), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
  %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.5, %531, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %533 : Tensor = prim::GetAttr[name="bias"](%83)
  %534 : Tensor = prim::GetAttr[name="weight"](%83)
  %535 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.4
  %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %535, %534, %533, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
  %537 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %538 : Float(17:13, 13:1, 1:1) = aten::to(%537, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.6, %538), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %540 : __torch__.torch.nn.modules.linear.___torch_mangle_549.Linear = prim::GetAttr[name="out_lin"](%81)
  %541 : __torch__.torch.nn.modules.linear.___torch_mangle_548.Linear = prim::GetAttr[name="v_lin"](%81)
  %542 : __torch__.torch.nn.modules.linear.___torch_mangle_547.Linear = prim::GetAttr[name="k_lin"](%81)
  %543 : __torch__.torch.nn.modules.linear.___torch_mangle_546.Linear = prim::GetAttr[name="q_lin"](%81)
  %544 : int = aten::size(%input.54, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %545 : int = aten::size(%input.54, %24), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
  %546 : Tensor = prim::GetAttr[name="bias"](%543)
  %547 : Tensor = prim::GetAttr[name="weight"](%543)
  %548 : Float(2048:1, 2048:2048) = aten::t(%547), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %548), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
  %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %546, %24), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1678:0
  %551 : int[] = prim::ListConstruct(%544, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.5
  %552 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.21, %551), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%552, %24, %20), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %554 : Tensor = prim::GetAttr[name="bias"](%542)
  %555 : Tensor = prim::GetAttr[name="weight"](%542)
  %556 : Float(2048:1, 2048:2048) = aten::t(%555), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %556), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
  %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %554, %24), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1678:0
  %559 : int[] = prim::ListConstruct(%544, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.5
  %560 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.22, %559), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%560, %24, %20), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %562 : Tensor = prim::GetAttr[name="bias"](%541)
  %563 : Tensor = prim::GetAttr[name="weight"](%541)
  %564 : Float(2048:1, 2048:2048) = aten::t(%563), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %564), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
  %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %562, %24), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1678:0
  %567 : int[] = prim::ListConstruct(%544, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.5
  %568 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.23, %567), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%568, %24, %20), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
  %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %10), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:188:0
  %571 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %20, %11), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %571), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
  %573 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.5 # torch/tensor.py:22:0
  %574 : int[] = prim::ListConstruct(%544, %24, %24, %545), scope: __module.transformer/__module.transformer.attentions.5
  %575 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%573, %574), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%575, %scores.11), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
  %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %12), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:191:0
  %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
  %579 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %19, %13), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:1498:0
  %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%579, %15, %21), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
  %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:200:0
  %582 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %24, %20), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %583 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%582, %25), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %584 : int[] = prim::ListConstruct(%544, %19, %16), scope: __module.transformer/__module.transformer.attentions.5
  %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%583, %584), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
  %586 : Tensor = prim::GetAttr[name="bias"](%540)
  %587 : Tensor = prim::GetAttr[name="weight"](%540)
  %588 : Float(2048:1, 2048:2048) = aten::t(%587), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %588), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
  %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %586, %24), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1678:0
  %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.58, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %593 : Tensor = prim::GetAttr[name="bias"](%79)
  %594 : Tensor = prim::GetAttr[name="weight"](%79)
  %595 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.5
  %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %595, %594, %593, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
  %597 : __torch__.torch.nn.modules.linear.___torch_mangle_611.Linear = prim::GetAttr[name="lin2"](%77)
  %598 : __torch__.torch.nn.modules.linear.___torch_mangle_610.Linear = prim::GetAttr[name="lin1"](%77)
  %599 : Tensor = prim::GetAttr[name="bias"](%598)
  %600 : Tensor = prim::GetAttr[name="weight"](%598)
  %601 : Float(2048:1, 8192:2048) = aten::t(%600), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.6, %601), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
  %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %599, %24), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1678:0
  %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.60), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:1369:0
  %605 : Tensor = prim::GetAttr[name="bias"](%597)
  %606 : Tensor = prim::GetAttr[name="weight"](%597)
  %607 : Float(8192:1, 2048:8192) = aten::t(%606), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %607), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
  %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %605, %24), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1678:0
  %610 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.62, %15, %21), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
  %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.6, %610, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %612 : Tensor = prim::GetAttr[name="bias"](%75)
  %613 : Tensor = prim::GetAttr[name="weight"](%75)
  %614 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.5
  %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %614, %613, %612, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
  %616 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %617 : Float(17:13, 13:1, 1:1) = aten::to(%616, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.7, %617), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %619 : __torch__.torch.nn.modules.linear.___torch_mangle_554.Linear = prim::GetAttr[name="out_lin"](%73)
  %620 : __torch__.torch.nn.modules.linear.___torch_mangle_553.Linear = prim::GetAttr[name="v_lin"](%73)
  %621 : __torch__.torch.nn.modules.linear.___torch_mangle_552.Linear = prim::GetAttr[name="k_lin"](%73)
  %622 : __torch__.torch.nn.modules.linear.___torch_mangle_551.Linear = prim::GetAttr[name="q_lin"](%73)
  %623 : int = aten::size(%input.64, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %624 : int = aten::size(%input.64, %24), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
  %625 : Tensor = prim::GetAttr[name="bias"](%622)
  %626 : Tensor = prim::GetAttr[name="weight"](%622)
  %627 : Float(2048:1, 2048:2048) = aten::t(%626), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %627), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
  %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %625, %24), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1678:0
  %630 : int[] = prim::ListConstruct(%623, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.6
  %631 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.25, %630), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%631, %24, %20), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %633 : Tensor = prim::GetAttr[name="bias"](%621)
  %634 : Tensor = prim::GetAttr[name="weight"](%621)
  %635 : Float(2048:1, 2048:2048) = aten::t(%634), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %635), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
  %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %633, %24), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1678:0
  %638 : int[] = prim::ListConstruct(%623, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.6
  %639 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.26, %638), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%639, %24, %20), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %641 : Tensor = prim::GetAttr[name="bias"](%620)
  %642 : Tensor = prim::GetAttr[name="weight"](%620)
  %643 : Float(2048:1, 2048:2048) = aten::t(%642), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %643), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
  %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %641, %24), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1678:0
  %646 : int[] = prim::ListConstruct(%623, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.6
  %647 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.27, %646), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%647, %24, %20), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
  %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %10), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:188:0
  %650 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %20, %11), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %650), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
  %652 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.6 # torch/tensor.py:22:0
  %653 : int[] = prim::ListConstruct(%623, %24, %24, %624), scope: __module.transformer/__module.transformer.attentions.6
  %654 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%652, %653), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%654, %scores.13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
  %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %12), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:191:0
  %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
  %658 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %19, %13), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:1498:0
  %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%658, %15, %21), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
  %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:200:0
  %661 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %24, %20), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %662 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%661, %25), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %663 : int[] = prim::ListConstruct(%623, %19, %16), scope: __module.transformer/__module.transformer.attentions.6
  %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%662, %663), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
  %665 : Tensor = prim::GetAttr[name="bias"](%619)
  %666 : Tensor = prim::GetAttr[name="weight"](%619)
  %667 : Float(2048:1, 2048:2048) = aten::t(%666), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %667), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
  %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %665, %24), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1678:0
  %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.68, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %672 : Tensor = prim::GetAttr[name="bias"](%71)
  %673 : Tensor = prim::GetAttr[name="weight"](%71)
  %674 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.6
  %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %674, %673, %672, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
  %676 : __torch__.torch.nn.modules.linear.___torch_mangle_614.Linear = prim::GetAttr[name="lin2"](%69)
  %677 : __torch__.torch.nn.modules.linear.___torch_mangle_613.Linear = prim::GetAttr[name="lin1"](%69)
  %678 : Tensor = prim::GetAttr[name="bias"](%677)
  %679 : Tensor = prim::GetAttr[name="weight"](%677)
  %680 : Float(2048:1, 8192:2048) = aten::t(%679), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.7, %680), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
  %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %678, %24), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1678:0
  %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.70), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:1369:0
  %684 : Tensor = prim::GetAttr[name="bias"](%676)
  %685 : Tensor = prim::GetAttr[name="weight"](%676)
  %686 : Float(8192:1, 2048:8192) = aten::t(%685), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %686), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
  %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %684, %24), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1678:0
  %689 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.72, %15, %21), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
  %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.7, %689, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %691 : Tensor = prim::GetAttr[name="bias"](%67)
  %692 : Tensor = prim::GetAttr[name="weight"](%67)
  %693 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.6
  %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %693, %692, %691, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
  %695 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %696 : Float(17:13, 13:1, 1:1) = aten::to(%695, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.8, %696), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %698 : __torch__.torch.nn.modules.linear.___torch_mangle_559.Linear = prim::GetAttr[name="out_lin"](%65)
  %699 : __torch__.torch.nn.modules.linear.___torch_mangle_558.Linear = prim::GetAttr[name="v_lin"](%65)
  %700 : __torch__.torch.nn.modules.linear.___torch_mangle_557.Linear = prim::GetAttr[name="k_lin"](%65)
  %701 : __torch__.torch.nn.modules.linear.___torch_mangle_556.Linear = prim::GetAttr[name="q_lin"](%65)
  %702 : int = aten::size(%input.74, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %703 : int = aten::size(%input.74, %24), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
  %704 : Tensor = prim::GetAttr[name="bias"](%701)
  %705 : Tensor = prim::GetAttr[name="weight"](%701)
  %706 : Float(2048:1, 2048:2048) = aten::t(%705), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %706), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
  %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %704, %24), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1678:0
  %709 : int[] = prim::ListConstruct(%702, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.7
  %710 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.29, %709), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%710, %24, %20), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %712 : Tensor = prim::GetAttr[name="bias"](%700)
  %713 : Tensor = prim::GetAttr[name="weight"](%700)
  %714 : Float(2048:1, 2048:2048) = aten::t(%713), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %714), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
  %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %712, %24), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1678:0
  %717 : int[] = prim::ListConstruct(%702, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.7
  %718 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.30, %717), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%718, %24, %20), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %720 : Tensor = prim::GetAttr[name="bias"](%699)
  %721 : Tensor = prim::GetAttr[name="weight"](%699)
  %722 : Float(2048:1, 2048:2048) = aten::t(%721), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %722), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
  %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %720, %24), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1678:0
  %725 : int[] = prim::ListConstruct(%702, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.7
  %726 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.31, %725), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%726, %24, %20), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
  %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %10), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:188:0
  %729 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %20, %11), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %729), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
  %731 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.7 # torch/tensor.py:22:0
  %732 : int[] = prim::ListConstruct(%702, %24, %24, %703), scope: __module.transformer/__module.transformer.attentions.7
  %733 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%731, %732), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%733, %scores.15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
  %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %12), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:191:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
  %737 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %19, %13), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:1498:0
  %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%737, %15, %21), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
  %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:200:0
  %740 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %24, %20), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %741 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%740, %25), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %742 : int[] = prim::ListConstruct(%702, %19, %16), scope: __module.transformer/__module.transformer.attentions.7
  %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%741, %742), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
  %744 : Tensor = prim::GetAttr[name="bias"](%698)
  %745 : Tensor = prim::GetAttr[name="weight"](%698)
  %746 : Float(2048:1, 2048:2048) = aten::t(%745), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %746), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
  %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %744, %24), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1678:0
  %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.78, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %751 : Tensor = prim::GetAttr[name="bias"](%63)
  %752 : Tensor = prim::GetAttr[name="weight"](%63)
  %753 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.7
  %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %753, %752, %751, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
  %755 : __torch__.torch.nn.modules.linear.___torch_mangle_617.Linear = prim::GetAttr[name="lin2"](%61)
  %756 : __torch__.torch.nn.modules.linear.___torch_mangle_616.Linear = prim::GetAttr[name="lin1"](%61)
  %757 : Tensor = prim::GetAttr[name="bias"](%756)
  %758 : Tensor = prim::GetAttr[name="weight"](%756)
  %759 : Float(2048:1, 8192:2048) = aten::t(%758), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.8, %759), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
  %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %757, %24), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1678:0
  %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.80), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:1369:0
  %763 : Tensor = prim::GetAttr[name="bias"](%755)
  %764 : Tensor = prim::GetAttr[name="weight"](%755)
  %765 : Float(8192:1, 2048:8192) = aten::t(%764), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %765), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
  %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %763, %24), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1678:0
  %768 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.82, %15, %21), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
  %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.8, %768, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %770 : Tensor = prim::GetAttr[name="bias"](%59)
  %771 : Tensor = prim::GetAttr[name="weight"](%59)
  %772 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.7
  %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %772, %771, %770, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
  %774 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %775 : Float(17:13, 13:1, 1:1) = aten::to(%774, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.9, %775), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %777 : __torch__.torch.nn.modules.linear.___torch_mangle_564.Linear = prim::GetAttr[name="out_lin"](%57)
  %778 : __torch__.torch.nn.modules.linear.___torch_mangle_563.Linear = prim::GetAttr[name="v_lin"](%57)
  %779 : __torch__.torch.nn.modules.linear.___torch_mangle_562.Linear = prim::GetAttr[name="k_lin"](%57)
  %780 : __torch__.torch.nn.modules.linear.___torch_mangle_561.Linear = prim::GetAttr[name="q_lin"](%57)
  %781 : int = aten::size(%input.84, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %782 : int = aten::size(%input.84, %24), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
  %783 : Tensor = prim::GetAttr[name="bias"](%780)
  %784 : Tensor = prim::GetAttr[name="weight"](%780)
  %785 : Float(2048:1, 2048:2048) = aten::t(%784), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %785), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
  %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %783, %24), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1678:0
  %788 : int[] = prim::ListConstruct(%781, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.8
  %789 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.33, %788), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%789, %24, %20), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %791 : Tensor = prim::GetAttr[name="bias"](%779)
  %792 : Tensor = prim::GetAttr[name="weight"](%779)
  %793 : Float(2048:1, 2048:2048) = aten::t(%792), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %793), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
  %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %791, %24), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1678:0
  %796 : int[] = prim::ListConstruct(%781, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.8
  %797 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.34, %796), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%797, %24, %20), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %799 : Tensor = prim::GetAttr[name="bias"](%778)
  %800 : Tensor = prim::GetAttr[name="weight"](%778)
  %801 : Float(2048:1, 2048:2048) = aten::t(%800), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %801), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
  %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %799, %24), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1678:0
  %804 : int[] = prim::ListConstruct(%781, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.8
  %805 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.35, %804), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%805, %24, %20), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
  %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %10), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:188:0
  %808 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %20, %11), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %808), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
  %810 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.8 # torch/tensor.py:22:0
  %811 : int[] = prim::ListConstruct(%781, %24, %24, %782), scope: __module.transformer/__module.transformer.attentions.8
  %812 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%810, %811), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%812, %scores.17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
  %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %12), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:191:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
  %816 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %19, %13), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:1498:0
  %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%816, %15, %21), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
  %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:200:0
  %819 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %24, %20), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %820 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%819, %25), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %821 : int[] = prim::ListConstruct(%781, %19, %16), scope: __module.transformer/__module.transformer.attentions.8
  %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%820, %821), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
  %823 : Tensor = prim::GetAttr[name="bias"](%777)
  %824 : Tensor = prim::GetAttr[name="weight"](%777)
  %825 : Float(2048:1, 2048:2048) = aten::t(%824), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %825), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
  %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %823, %24), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1678:0
  %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.88, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %830 : Tensor = prim::GetAttr[name="bias"](%55)
  %831 : Tensor = prim::GetAttr[name="weight"](%55)
  %832 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.8
  %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %832, %831, %830, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
  %834 : __torch__.torch.nn.modules.linear.___torch_mangle_620.Linear = prim::GetAttr[name="lin2"](%53)
  %835 : __torch__.torch.nn.modules.linear.___torch_mangle_619.Linear = prim::GetAttr[name="lin1"](%53)
  %836 : Tensor = prim::GetAttr[name="bias"](%835)
  %837 : Tensor = prim::GetAttr[name="weight"](%835)
  %838 : Float(2048:1, 8192:2048) = aten::t(%837), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.9, %838), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
  %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %836, %24), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1678:0
  %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.90), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:1369:0
  %842 : Tensor = prim::GetAttr[name="bias"](%834)
  %843 : Tensor = prim::GetAttr[name="weight"](%834)
  %844 : Float(8192:1, 2048:8192) = aten::t(%843), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %844), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
  %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %842, %24), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1678:0
  %847 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.92, %15, %21), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
  %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.9, %847, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %849 : Tensor = prim::GetAttr[name="bias"](%51)
  %850 : Tensor = prim::GetAttr[name="weight"](%51)
  %851 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.8
  %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %851, %850, %849, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
  %853 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %854 : Float(17:13, 13:1, 1:1) = aten::to(%853, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.10, %854), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %856 : __torch__.torch.nn.modules.linear.___torch_mangle_569.Linear = prim::GetAttr[name="out_lin"](%49)
  %857 : __torch__.torch.nn.modules.linear.___torch_mangle_568.Linear = prim::GetAttr[name="v_lin"](%49)
  %858 : __torch__.torch.nn.modules.linear.___torch_mangle_567.Linear = prim::GetAttr[name="k_lin"](%49)
  %859 : __torch__.torch.nn.modules.linear.___torch_mangle_566.Linear = prim::GetAttr[name="q_lin"](%49)
  %860 : int = aten::size(%input.94, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %861 : int = aten::size(%input.94, %24), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
  %862 : Tensor = prim::GetAttr[name="bias"](%859)
  %863 : Tensor = prim::GetAttr[name="weight"](%859)
  %864 : Float(2048:1, 2048:2048) = aten::t(%863), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %864), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
  %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %862, %24), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1678:0
  %867 : int[] = prim::ListConstruct(%860, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.9
  %868 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.37, %867), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%868, %24, %20), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %870 : Tensor = prim::GetAttr[name="bias"](%858)
  %871 : Tensor = prim::GetAttr[name="weight"](%858)
  %872 : Float(2048:1, 2048:2048) = aten::t(%871), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %872), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
  %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %870, %24), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1678:0
  %875 : int[] = prim::ListConstruct(%860, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.9
  %876 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.38, %875), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%876, %24, %20), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %878 : Tensor = prim::GetAttr[name="bias"](%857)
  %879 : Tensor = prim::GetAttr[name="weight"](%857)
  %880 : Float(2048:1, 2048:2048) = aten::t(%879), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %880), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
  %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %878, %24), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1678:0
  %883 : int[] = prim::ListConstruct(%860, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.9
  %884 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.39, %883), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%884, %24, %20), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
  %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %10), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:188:0
  %887 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %20, %11), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %887), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
  %889 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.9 # torch/tensor.py:22:0
  %890 : int[] = prim::ListConstruct(%860, %24, %24, %861), scope: __module.transformer/__module.transformer.attentions.9
  %891 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%889, %890), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%891, %scores.19), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
  %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %12), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:191:0
  %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
  %895 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %19, %13), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:1498:0
  %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%895, %15, %21), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
  %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:200:0
  %898 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %24, %20), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %899 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%898, %25), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %900 : int[] = prim::ListConstruct(%860, %19, %16), scope: __module.transformer/__module.transformer.attentions.9
  %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%899, %900), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
  %902 : Tensor = prim::GetAttr[name="bias"](%856)
  %903 : Tensor = prim::GetAttr[name="weight"](%856)
  %904 : Float(2048:1, 2048:2048) = aten::t(%903), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %904), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
  %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %902, %24), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1678:0
  %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.98, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %909 : Tensor = prim::GetAttr[name="bias"](%47)
  %910 : Tensor = prim::GetAttr[name="weight"](%47)
  %911 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.9
  %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %911, %910, %909, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
  %913 : __torch__.torch.nn.modules.linear.___torch_mangle_623.Linear = prim::GetAttr[name="lin2"](%45)
  %914 : __torch__.torch.nn.modules.linear.___torch_mangle_622.Linear = prim::GetAttr[name="lin1"](%45)
  %915 : Tensor = prim::GetAttr[name="bias"](%914)
  %916 : Tensor = prim::GetAttr[name="weight"](%914)
  %917 : Float(2048:1, 8192:2048) = aten::t(%916), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.10, %917), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
  %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %915, %24), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1678:0
  %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.100), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:1369:0
  %921 : Tensor = prim::GetAttr[name="bias"](%913)
  %922 : Tensor = prim::GetAttr[name="weight"](%913)
  %923 : Float(8192:1, 2048:8192) = aten::t(%922), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %923), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
  %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %921, %24), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1678:0
  %926 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.102, %15, %21), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
  %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.10, %926, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %928 : Tensor = prim::GetAttr[name="bias"](%43)
  %929 : Tensor = prim::GetAttr[name="weight"](%43)
  %930 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.9
  %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %930, %929, %928, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
  %932 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %933 : Float(17:13, 13:1, 1:1) = aten::to(%932, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.11, %933), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %935 : __torch__.torch.nn.modules.linear.___torch_mangle_574.Linear = prim::GetAttr[name="out_lin"](%41)
  %936 : __torch__.torch.nn.modules.linear.___torch_mangle_573.Linear = prim::GetAttr[name="v_lin"](%41)
  %937 : __torch__.torch.nn.modules.linear.___torch_mangle_572.Linear = prim::GetAttr[name="k_lin"](%41)
  %938 : __torch__.torch.nn.modules.linear.___torch_mangle_571.Linear = prim::GetAttr[name="q_lin"](%41)
  %939 : int = aten::size(%input.104, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %940 : int = aten::size(%input.104, %24), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
  %941 : Tensor = prim::GetAttr[name="bias"](%938)
  %942 : Tensor = prim::GetAttr[name="weight"](%938)
  %943 : Float(2048:1, 2048:2048) = aten::t(%942), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %943), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
  %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %941, %24), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1678:0
  %946 : int[] = prim::ListConstruct(%939, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.10
  %947 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.41, %946), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%947, %24, %20), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %949 : Tensor = prim::GetAttr[name="bias"](%937)
  %950 : Tensor = prim::GetAttr[name="weight"](%937)
  %951 : Float(2048:1, 2048:2048) = aten::t(%950), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %951), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
  %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %949, %24), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1678:0
  %954 : int[] = prim::ListConstruct(%939, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.10
  %955 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.42, %954), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%955, %24, %20), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %957 : Tensor = prim::GetAttr[name="bias"](%936)
  %958 : Tensor = prim::GetAttr[name="weight"](%936)
  %959 : Float(2048:1, 2048:2048) = aten::t(%958), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %959), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
  %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %957, %24), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1678:0
  %962 : int[] = prim::ListConstruct(%939, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.10
  %963 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.43, %962), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%963, %24, %20), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
  %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %10), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:188:0
  %966 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %20, %11), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %966), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
  %968 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.10 # torch/tensor.py:22:0
  %969 : int[] = prim::ListConstruct(%939, %24, %24, %940), scope: __module.transformer/__module.transformer.attentions.10
  %970 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%968, %969), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%970, %scores.21), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
  %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %12), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:191:0
  %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
  %974 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %19, %13), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:1498:0
  %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%974, %15, %21), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
  %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:200:0
  %977 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %24, %20), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %978 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%977, %25), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %979 : int[] = prim::ListConstruct(%939, %19, %16), scope: __module.transformer/__module.transformer.attentions.10
  %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%978, %979), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
  %981 : Tensor = prim::GetAttr[name="bias"](%935)
  %982 : Tensor = prim::GetAttr[name="weight"](%935)
  %983 : Float(2048:1, 2048:2048) = aten::t(%982), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %983), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
  %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %981, %24), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1678:0
  %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.108, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %988 : Tensor = prim::GetAttr[name="bias"](%39)
  %989 : Tensor = prim::GetAttr[name="weight"](%39)
  %990 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.10
  %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %990, %989, %988, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
  %992 : __torch__.torch.nn.modules.linear.___torch_mangle_626.Linear = prim::GetAttr[name="lin2"](%37)
  %993 : __torch__.torch.nn.modules.linear.___torch_mangle_625.Linear = prim::GetAttr[name="lin1"](%37)
  %994 : Tensor = prim::GetAttr[name="bias"](%993)
  %995 : Tensor = prim::GetAttr[name="weight"](%993)
  %996 : Float(2048:1, 8192:2048) = aten::t(%995), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor.11, %996), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
  %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %994, %24), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1678:0
  %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.110), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:1369:0
  %1000 : Tensor = prim::GetAttr[name="bias"](%992)
  %1001 : Tensor = prim::GetAttr[name="weight"](%992)
  %1002 : Float(8192:1, 2048:8192) = aten::t(%1001), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %1002), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
  %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %1000, %24), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1678:0
  %1005 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.112, %15, %21), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
  %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor.11, %1005, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %1007 : Tensor = prim::GetAttr[name="bias"](%35)
  %1008 : Tensor = prim::GetAttr[name="weight"](%35)
  %1009 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.10
  %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %1009, %1008, %1007, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
  %1011 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1012 : Float(17:13, 13:1, 1:1) = aten::to(%1011, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.12, %1012), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1014 : __torch__.torch.nn.modules.linear.___torch_mangle_579.Linear = prim::GetAttr[name="out_lin"](%33)
  %1015 : __torch__.torch.nn.modules.linear.___torch_mangle_578.Linear = prim::GetAttr[name="v_lin"](%33)
  %1016 : __torch__.torch.nn.modules.linear.___torch_mangle_577.Linear = prim::GetAttr[name="k_lin"](%33)
  %1017 : __torch__.torch.nn.modules.linear.___torch_mangle_576.Linear = prim::GetAttr[name="q_lin"](%33)
  %1018 : int = aten::size(%input.114, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1019 : int = aten::size(%input.114, %24), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
  %1020 : Tensor = prim::GetAttr[name="bias"](%1017)
  %1021 : Tensor = prim::GetAttr[name="weight"](%1017)
  %1022 : Float(2048:1, 2048:2048) = aten::t(%1021), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1022), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
  %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %1020, %24), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1678:0
  %1025 : int[] = prim::ListConstruct(%1018, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.11
  %1026 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.45, %1025), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1026, %24, %20), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1028 : Tensor = prim::GetAttr[name="bias"](%1016)
  %1029 : Tensor = prim::GetAttr[name="weight"](%1016)
  %1030 : Float(2048:1, 2048:2048) = aten::t(%1029), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1030), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
  %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %1028, %24), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1678:0
  %1033 : int[] = prim::ListConstruct(%1018, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.11
  %1034 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.46, %1033), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1034, %24, %20), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %1036 : Tensor = prim::GetAttr[name="bias"](%1015)
  %1037 : Tensor = prim::GetAttr[name="weight"](%1015)
  %1038 : Float(2048:1, 2048:2048) = aten::t(%1037), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %1038), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
  %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %1036, %24), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1678:0
  %1041 : int[] = prim::ListConstruct(%1018, %19, %8, %9), scope: __module.transformer/__module.transformer.attentions.11
  %1042 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%x.47, %1041), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%1042, %24, %20), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
  %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %10), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:188:0
  %1045 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %20, %11), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %1045), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
  %1047 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %25), scope: __module.transformer/__module.transformer.attentions.11 # torch/tensor.py:22:0
  %1048 : int[] = prim::ListConstruct(%1018, %24, %24, %1019), scope: __module.transformer/__module.transformer.attentions.11
  %1049 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%1047, %1048), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%1049, %scores.23), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
  %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %12), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:191:0
  %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %14, %21, %21, %13), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
  %1053 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %19, %13), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:1498:0
  %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%1053, %15, %21), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
  %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:200:0
  %1056 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %24, %20), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1057 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%1056, %25), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1058 : int[] = prim::ListConstruct(%1018, %19, %16), scope: __module.transformer/__module.transformer.attentions.11
  %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%1057, %1058), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
  %1060 : Tensor = prim::GetAttr[name="bias"](%1014)
  %1061 : Tensor = prim::GetAttr[name="weight"](%1014)
  %1062 : Float(2048:1, 2048:2048) = aten::t(%1061), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %1062), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
  %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %1060, %24), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1678:0
  %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.118, %15, %21), scope: __module.transformer # torch/nn/functional.py:973:0
  %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %24), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
  %1067 : Tensor = prim::GetAttr[name="bias"](%31)
  %1068 : Tensor = prim::GetAttr[name="weight"](%31)
  %1069 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm1.11
  %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %1069, %1068, %1067, %17, %18), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
  %1071 : __torch__.torch.nn.modules.linear.___torch_mangle_629.Linear = prim::GetAttr[name="lin2"](%29)
  %1072 : __torch__.torch.nn.modules.linear.___torch_mangle_628.Linear = prim::GetAttr[name="lin1"](%29)
  %1073 : Tensor = prim::GetAttr[name="bias"](%1072)
  %1074 : Tensor = prim::GetAttr[name="weight"](%1072)
  %1075 : Float(2048:1, 8192:2048) = aten::t(%1074), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%input_tensor, %1075), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
  %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %1073, %24), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1678:0
  %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%input.120), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:1369:0
  %1079 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1080 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1081 : Float(8192:1, 2048:8192) = aten::t(%1080), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %output : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %1081), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
  %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output, %1079, %24), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1678:0
  %1084 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%input.122, %15, %21), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
  %input.123 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input_tensor, %1084, %24), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
  %1086 : Tensor = prim::GetAttr[name="bias"](%27)
  %1087 : Tensor = prim::GetAttr[name="weight"](%27)
  %1088 : int[] = prim::ListConstruct(%16), scope: __module.transformer/__module.transformer.layer_norm2.11
  %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.123, %1088, %1087, %1086, %17, %18), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
  %1090 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %19), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1091 : Float(17:13, 13:1, 1:1) = aten::to(%1090, %14, %21, %21, %13), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %hidden_states : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor, %1091), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
  %1093 : float = prim::Constant[value=0.10000000000000001](), scope: __module.sequence_summary/__module.sequence_summary.first_dropout # torch/nn/functional.py:973:0
  %1094 : bool = prim::Constant[value=0](), scope: __module.sequence_summary/__module.sequence_summary.first_dropout # torch/nn/functional.py:973:0
  %1095 : int = prim::Constant[value=1](), scope: __module.sequence_summary # transformers/modeling_utils.py:1489:0
  %1096 : int = prim::Constant[value=9223372036854775807](), scope: __module.sequence_summary # transformers/modeling_utils.py:1489:0
  %1097 : int = prim::Constant[value=0](), scope: __module.sequence_summary # transformers/modeling_utils.py:1489:0
  %1098 : __torch__.torch.nn.modules.linear.___torch_mangle_646.Linear = prim::GetAttr[name="summary"](%3)
  %1099 : Float(17:26624, 13:2048, 2048:1) = aten::slice(%hidden_states, %1097, %1097, %1096, %1095), scope: __module.sequence_summary # transformers/modeling_utils.py:1489:0
  %input.124 : Float(17:26624, 2048:1) = aten::select(%1099, %1095, %1097), scope: __module.sequence_summary # transformers/modeling_utils.py:1489:0
  %input : Float(17:26624, 2048:1) = aten::dropout(%input.124, %1093, %1094), scope: __module.sequence_summary/__module.sequence_summary.first_dropout # torch/nn/functional.py:973:0
  %1102 : Tensor = prim::GetAttr[name="bias"](%1098)
  %1103 : Tensor = prim::GetAttr[name="weight"](%1098)
  %1104 : Float(2048:1, 2:2048) = aten::t(%1103), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
  %1105 : Float(17:2, 2:1) = aten::addmm(%1102, %input, %1104, %1095, %1095), scope: __module.sequence_summary/__module.sequence_summary.summary # torch/nn/functional.py:1674:0
  %7 : (Float(17:2, 2:1)) = prim::TupleConstruct(%1105)
  return (%7)
