DistilBertForMultipleChoice(
  (distilbert): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=1, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)

DistilBertForMultipleChoice._actual_script_module
DistilBertForMultipleChoice.forward
  graph(%self.1 : __torch__.transformers.modeling_distilbert.DistilBertForMultipleChoice,
        %input_ids.1 : Long(17:91, 7:13, 13:1),
        %attention_mask : Long(17:91, 7:13, 13:1)):
    %1471 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="classifier"](%self.1)
    %1468 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.1)
    %1467 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="pre_classifier"](%self.1)
    %1464 : __torch__.transformers.modeling_distilbert.DistilBertModel = prim::GetAttr[name="distilbert"](%self.1)
    %289 : int = prim::Constant[value=1]() # transformers/modeling_distilbert.py:912:0
    %290 : int = aten::size(%input_ids.1, %289) # transformers/modeling_distilbert.py:912:0
    %num_choices : Long() = prim::NumToTensor(%290)
    %1181 : int = aten::Int(%num_choices)
    %295 : int = prim::Constant[value=-1]() # transformers/modeling_distilbert.py:914:0
    %296 : int = aten::size(%input_ids.1, %295) # transformers/modeling_distilbert.py:914:0
    %297 : Long() = prim::NumToTensor(%296)
    %298 : int = aten::Int(%297)
    %299 : int = prim::Constant[value=-1]() # transformers/modeling_distilbert.py:914:0
    %300 : int[] = prim::ListConstruct(%299, %298)
    %input_ids : Long(119:13, 13:1) = aten::view(%input_ids.1, %300) # transformers/modeling_distilbert.py:914:0
    %302 : int = prim::Constant[value=-1]() # transformers/modeling_distilbert.py:915:0
    %303 : int = aten::size(%attention_mask, %302) # transformers/modeling_distilbert.py:915:0
    %304 : Long() = prim::NumToTensor(%303)
    %305 : int = aten::Int(%304)
    %306 : int = prim::Constant[value=-1]() # transformers/modeling_distilbert.py:915:0
    %307 : int[] = prim::ListConstruct(%306, %305)
    %308 : Long(119:13, 13:1) = aten::view(%attention_mask, %307) # transformers/modeling_distilbert.py:915:0
    %1560 : Tensor = prim::CallMethod[name="forward"](%1464, %input_ids, %308)
    %1161 : int = prim::Constant[value=0]() # transformers/modeling_distilbert.py:933:0
    %1162 : int = prim::Constant[value=0]() # transformers/modeling_distilbert.py:933:0
    %1163 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_distilbert.py:933:0
    %1164 : int = prim::Constant[value=1]() # transformers/modeling_distilbert.py:933:0
    %1165 : Float(119:9984, 13:768, 768:1) = aten::slice(%1560, %1161, %1162, %1163, %1164) # transformers/modeling_distilbert.py:933:0
    %1166 : int = prim::Constant[value=1]() # transformers/modeling_distilbert.py:933:0
    %1167 : int = prim::Constant[value=0]() # transformers/modeling_distilbert.py:933:0
    %input.52 : Float(119:9984, 768:1) = aten::select(%1165, %1166, %1167) # transformers/modeling_distilbert.py:933:0
    %1561 : Tensor = prim::CallMethod[name="forward"](%1467, %input.52)
    %input.54 : Float(119:768, 768:1) = aten::relu(%1561) # torch/nn/functional.py:1119:0
    %1562 : Tensor = prim::CallMethod[name="forward"](%1468, %input.54)
    %1563 : Tensor = prim::CallMethod[name="forward"](%1471, %1562)
    %1182 : int = prim::Constant[value=-1]() # transformers/modeling_distilbert.py:939:0
    %1183 : int[] = prim::ListConstruct(%1182, %1181)
    %1184 : Float(17:7, 7:1) = aten::view(%1563, %1183) # transformers/modeling_distilbert.py:939:0
    %1185 : (Float(17:7, 7:1)) = prim::TupleConstruct(%1184)
    return (%1185)

DistilBertForMultipleChoice.classifier
Linear._actual_script_module
  graph(%self : __torch__.torch.nn.modules.linear.Linear,
        %7 : Float(119:768, 768:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self)
    %2 : Tensor = prim::GetAttr[name="weight"](%self)
    %3 : Float(768:1, 1:768) = aten::t(%2), scope: __module.classifier # torch/nn/functional.py:1674:0
    %4 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1674:0
    %5 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1674:0
    %logits : Float(119:1, 1:1) = aten::addmm(%1, %7, %3, %4, %5), scope: __module.classifier # torch/nn/functional.py:1674:0
    return (%logits)

DistilBertForMultipleChoice.distilbert
DistilBertModel._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_distilbert.DistilBertModel,
        %input_ids : Long(119:13, 13:1),
        %12 : Long(119:13, 13:1)):
    %1 : __torch__.transformers.modeling_distilbert.Transformer = prim::GetAttr[name="transformer"](%self.2)
    %2 : __torch__.transformers.modeling_distilbert.Embeddings = prim::GetAttr[name="embeddings"](%self.2)
    %13 : Tensor = prim::CallMethod[name="forward"](%2, %input_ids)
    %14 : Tensor = prim::CallMethod[name="forward"](%1, %13, %12)
    return (%14)

DistilBertForMultipleChoice.dropout
Dropout._actual_script_module
  graph(%self.88 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.54 : Float(119:768, 768:1)):
    %1 : float = prim::Constant[value=0.20000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
    %2 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
    %input : Float(119:768, 768:1) = aten::dropout(%input.54, %1, %2), scope: __module.dropout # torch/nn/functional.py:973:0
    return (%input)

DistilBertForMultipleChoice.pre_classifier
Linear._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.linear.Linear,
        %input.52 : Float(119:9984, 768:1)):
    %1 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %2 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %3 : Float(768:1, 768:768) = aten::t(%2), scope: __module.pre_classifier # torch/nn/functional.py:1674:0
    %4 : int = prim::Constant[value=1](), scope: __module.pre_classifier # torch/nn/functional.py:1674:0
    %5 : int = prim::Constant[value=1](), scope: __module.pre_classifier # torch/nn/functional.py:1674:0
    %input.53 : Float(119:768, 768:1) = aten::addmm(%1, %input.52, %3, %4, %5), scope: __module.pre_classifier # torch/nn/functional.py:1674:0
    return (%input.53)

DistilBertModel.embeddings
Embeddings._actual_script_module
  graph(%self.3 : __torch__.transformers.modeling_distilbert.Embeddings,
        %input_ids : Long(119:13, 13:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.3)
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.3)
    %4 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="position_embeddings"](%self.3)
    %5 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embeddings"](%self.3)
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:108:0
    %7 : int = aten::size(%input_ids, %6), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:108:0
    %seq_length : Long() = prim::NumToTensor(%7), scope: __module.distilbert/__module.distilbert.embeddings
    %9 : Scalar = aten::ScalarImplicit(%seq_length), scope: __module.distilbert/__module.distilbert.embeddings
    %10 : int = prim::Constant[value=4](), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:109:0
    %11 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:109:0
    %12 : Device = prim::Constant[value="cpu"](), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:109:0
    %13 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:109:0
    %position_ids : Long(13:1) = aten::arange(%9, %10, %11, %12, %13), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:109:0
    %15 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:110:0
    %16 : Long(1:13, 13:1) = aten::unsqueeze(%position_ids, %15), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:110:0
    %input.1 : Long(119:0, 13:1) = aten::expand_as(%16, %input_ids), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:110:0
    %24 : Tensor = prim::CallMethod[name="forward"](%5, %input_ids)
    %25 : Tensor = prim::CallMethod[name="forward"](%4, %input.1)
    %20 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:115:0
    %input.2 : Float(119:9984, 13:768, 768:1) = aten::add(%24, %25, %20), scope: __module.distilbert/__module.distilbert.embeddings # transformers/modeling_distilbert.py:115:0
    %26 : Tensor = prim::CallMethod[name="forward"](%3, %input.2)
    %27 : Tensor = prim::CallMethod[name="forward"](%2, %26)
    return (%27)

DistilBertModel.transformer
Transformer._actual_script_module
  graph(%self.8 : __torch__.transformers.modeling_distilbert.Transformer,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %4 : __torch__.transformers.modeling_distilbert.TransformerBlock = prim::GetAttr[name="5"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %6 : __torch__.transformers.modeling_distilbert.TransformerBlock = prim::GetAttr[name="4"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %8 : __torch__.transformers.modeling_distilbert.TransformerBlock = prim::GetAttr[name="3"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %10 : __torch__.transformers.modeling_distilbert.TransformerBlock = prim::GetAttr[name="2"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %12 : __torch__.transformers.modeling_distilbert.TransformerBlock = prim::GetAttr[name="1"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.8)
    %14 : __torch__.transformers.modeling_distilbert.TransformerBlock = prim::GetAttr[name="0"](%13)
    %21 : Tensor = prim::CallMethod[name="forward"](%14, %1, %2)
    %22 : Tensor = prim::CallMethod[name="forward"](%12, %21, %2)
    %23 : Tensor = prim::CallMethod[name="forward"](%10, %22, %2)
    %24 : Tensor = prim::CallMethod[name="forward"](%8, %23, %2)
    %25 : Tensor = prim::CallMethod[name="forward"](%6, %24, %2)
    %26 : Tensor = prim::CallMethod[name="forward"](%4, %25, %2)
    return (%26)

Embeddings.LayerNorm
LayerNorm._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.2 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.6)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.6)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %input.3 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.2, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.3)

Embeddings.dropout
Dropout._actual_script_module
  graph(%self.7 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.dropout # torch/nn/functional.py:973:0
    %query.1 : Float(119:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.dropout # torch/nn/functional.py:973:0
    return (%query.1)

Embeddings.position_embeddings
Embedding._actual_script_module
  graph(%self.5 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.1 : Long(119:0, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.5)
    %3 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %position_embeddings : Float(119:9984, 13:768, 768:1) = aten::embedding(%2, %input.1, %3, %4, %5), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    return (%position_embeddings)

Embeddings.word_embeddings
Embedding._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(119:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %3 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %word_embeddings : Float(119:9984, 13:768, 768:1) = aten::embedding(%2, %input_ids, %3, %4, %5), scope: __module.distilbert/__module.distilbert.embeddings/__module.distilbert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    return (%word_embeddings)

ModuleList.*
  module had no methods with graph attrs.

TransformerBlock._actual_script_module
  graph(%self.9 : __torch__.transformers.modeling_distilbert.TransformerBlock,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="output_layer_norm"](%self.9)
    %4 : __torch__.transformers.modeling_distilbert.FFN = prim::GetAttr[name="ffn"](%self.9)
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="sa_layer_norm"](%self.9)
    %6 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention = prim::GetAttr[name="attention"](%self.9)
    %15 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %8 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0 # transformers/modeling_distilbert.py:276:0
    %input.7 : Float(119:9984, 13:768, 768:1) = aten::add(%15, %1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0 # transformers/modeling_distilbert.py:276:0
    %16 : Tensor = prim::CallMethod[name="forward"](%5, %input.7)
    %17 : Tensor = prim::CallMethod[name="forward"](%4, %16)
    %12 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0 # transformers/modeling_distilbert.py:280:0
    %input.11 : Float(119:9984, 13:768, 768:1) = aten::add(%17, %16, %12), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0 # transformers/modeling_distilbert.py:280:0
    %18 : Tensor = prim::CallMethod[name="forward"](%3, %input.11)
    return (%18)

TransformerBlock.attention
MultiHeadSelfAttention._actual_script_module
  graph(%self.10 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.10)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.10)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.10)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.10)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.10)
    %8 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:169:0
    %9 : int = aten::size(%1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:169:0
    %bs.1 : Long() = prim::NumToTensor(%9), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %11 : int = aten::Int(%bs.1), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %12 : int = aten::Int(%bs.1), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %13 : int = aten::Int(%bs.1), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %14 : int = aten::Int(%bs.1), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %15 : int = aten::Int(%bs.1), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %22 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:170:0
    %23 : int = aten::size(%1, %22), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:170:0
    %k_length.1 : Long() = prim::NumToTensor(%23), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %25 : int = aten::Int(%k_length.1), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %83 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %27 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %28 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %29 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %30 : int[] = prim::ListConstruct(%15, %27, %28, %29), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %31 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%83, %30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %32 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %33 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %q.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%31, %32, %33), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %84 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %36 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %37 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %38 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %39 : int[] = prim::ListConstruct(%14, %36, %37, %38), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %40 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%84, %39), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %41 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %42 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %k.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%40, %41, %42), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %85 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %45 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %46 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %47 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %48 : int[] = prim::ListConstruct(%13, %45, %46, %47), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %49 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%85, %48), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %50 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %51 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %v.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%49, %50, %51), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:180:0
    %53 : Double() = prim::Constant[value={8}](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:190:0
    %q.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::div(%q.1, %53), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:190:0
    %55 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:191:0
    %56 : int = prim::Constant[value=3](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:191:0
    %57 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%k.1, %55, %56), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:191:0
    %scores.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.2, %57), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:191:0
    %59 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # torch/tensor.py:22:0
    %60 : Bool(119:13, 13:1) = aten::eq(%2, %59), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # torch/tensor.py:22:0
    %61 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:192:0
    %62 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:192:0
    %63 : int[] = prim::ListConstruct(%12, %61, %62, %25), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %64 : Bool(119:13, 1:13, 1:13, 13:1) = aten::view(%60, %63), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:192:0
    %mask.1 : Bool(119:13, 12:0, 13:0, 13:1) = aten::expand_as(%64, %scores.1), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:192:0
    %66 : float = prim::Constant[value=-inf](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:193:0
    %input.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %66), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:193:0
    %68 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # torch/nn/functional.py:1498:0
    %69 : None = prim::Constant(), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %input.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.4, %68, %69), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # torch/nn/functional.py:1498:0
    %86 : Tensor = prim::CallMethod[name="forward"](%4, %input.5)
    %x.4 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%86, %v.1), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:202:0
    %73 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:184:0
    %74 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:184:0
    %75 : Float(119:9984, 13:64, 12:832, 64:1) = aten::transpose(%x.4, %73, %74), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:184:0
    %76 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:184:0
    %77 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%75, %76), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:184:0
    %78 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:184:0
    %79 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:184:0
    %80 : int[] = prim::ListConstruct(%11, %78, %79), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention
    %input.6 : Float(119:9984, 13:768, 768:1) = aten::view(%77, %80), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention # transformers/modeling_distilbert.py:184:0
    %87 : Tensor = prim::CallMethod[name="forward"](%3, %input.6)
    return (%87)

TransformerBlock.ffn
FFN._actual_script_module
  graph(%self.17 : __torch__.transformers.modeling_distilbert.FFN,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.17)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.17)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.17)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.9 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn # torch/nn/functional.py:1369:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.9)
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %31)
    return (%32)

TransformerBlock.output_layer_norm
LayerNorm._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.11 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.21)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.21)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.output_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.output_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.output_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.output_layer_norm # torch/nn/functional.py:2048:0
    %query.2 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.11, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.output_layer_norm # torch/nn/functional.py:2048:0
    return (%query.2)

TransformerBlock.sa_layer_norm
LayerNorm._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.7 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.sa_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.sa_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.sa_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.sa_layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.7, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.sa_layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

MultiHeadSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.5 : Float(119:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.dropout # torch/nn/functional.py:973:0
    %weights.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.5, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.dropout # torch/nn/functional.py:973:0
    return (%weights.1)

MultiHeadSelfAttention.k_lin
Linear._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.12)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.12)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.k_lin # torch/nn/functional.py:1676:0
    %output.2 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.k_lin # torch/nn/functional.py:1678:0
    %x.2 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.2, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.k_lin # torch/nn/functional.py:1678:0
    return (%x.2)

MultiHeadSelfAttention.out_lin
Linear._actual_script_module
  graph(%self.15 : __torch__.torch.nn.modules.linear.Linear,
        %input.6 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.15)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.15)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.out_lin # torch/nn/functional.py:1676:0
    %output.4 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.6, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.out_lin # torch/nn/functional.py:1678:0
    %sa_output.1 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.4, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.out_lin # torch/nn/functional.py:1678:0
    return (%sa_output.1)

MultiHeadSelfAttention.q_lin
Linear._actual_script_module
  graph(%self.11 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.11)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.11)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.q_lin # torch/nn/functional.py:1676:0
    %output.1 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.q_lin # torch/nn/functional.py:1678:0
    %x.1 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.1, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.q_lin # torch/nn/functional.py:1678:0
    return (%x.1)

MultiHeadSelfAttention.v_lin
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.v_lin # torch/nn/functional.py:1676:0
    %output.3 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.v_lin # torch/nn/functional.py:1678:0
    %x.3 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.3, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.attention/__module.distilbert.transformer.layer.0.attention.v_lin # torch/nn/functional.py:1678:0
    return (%x.3)

FFN.dropout
Dropout._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.dropout # torch/nn/functional.py:973:0
    %ffn_output.1 : Float(119:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.dropout # torch/nn/functional.py:973:0
    return (%ffn_output.1)

FFN.lin1
Linear._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.18)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.18)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.lin1 # torch/nn/functional.py:1676:0
    %output.5 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.lin1 # torch/nn/functional.py:1678:0
    %input.8 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.5, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.lin1 # torch/nn/functional.py:1678:0
    return (%input.8)

FFN.lin2
Linear._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.linear.Linear,
        %input.9 : Float(119:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.19)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.19)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.lin2 # torch/nn/functional.py:1676:0
    %output.6 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.9, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.lin2 # torch/nn/functional.py:1678:0
    %input.10 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.6, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.0/__module.distilbert.transformer.layer.0.ffn/__module.distilbert.transformer.layer.0.ffn.lin2 # torch/nn/functional.py:1678:0
    return (%input.10)

TransformerBlock._actual_script_module
  graph(%self.22 : __torch__.transformers.modeling_distilbert.TransformerBlock,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="output_layer_norm"](%self.22)
    %4 : __torch__.transformers.modeling_distilbert.FFN = prim::GetAttr[name="ffn"](%self.22)
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="sa_layer_norm"](%self.22)
    %6 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention = prim::GetAttr[name="attention"](%self.22)
    %15 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %8 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1 # transformers/modeling_distilbert.py:276:0
    %input.15 : Float(119:9984, 13:768, 768:1) = aten::add(%15, %1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1 # transformers/modeling_distilbert.py:276:0
    %16 : Tensor = prim::CallMethod[name="forward"](%5, %input.15)
    %17 : Tensor = prim::CallMethod[name="forward"](%4, %16)
    %12 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1 # transformers/modeling_distilbert.py:280:0
    %input.19 : Float(119:9984, 13:768, 768:1) = aten::add(%17, %16, %12), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1 # transformers/modeling_distilbert.py:280:0
    %18 : Tensor = prim::CallMethod[name="forward"](%3, %input.19)
    return (%18)

TransformerBlock.attention
MultiHeadSelfAttention._actual_script_module
  graph(%self.23 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.23)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.23)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.23)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.23)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.23)
    %8 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:169:0
    %9 : int = aten::size(%1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:169:0
    %bs.2 : Long() = prim::NumToTensor(%9), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %11 : int = aten::Int(%bs.2), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %12 : int = aten::Int(%bs.2), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %13 : int = aten::Int(%bs.2), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %14 : int = aten::Int(%bs.2), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %15 : int = aten::Int(%bs.2), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %22 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:170:0
    %23 : int = aten::size(%1, %22), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:170:0
    %k_length.2 : Long() = prim::NumToTensor(%23), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %25 : int = aten::Int(%k_length.2), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %83 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %27 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %28 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %29 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %30 : int[] = prim::ListConstruct(%15, %27, %28, %29), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %31 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%83, %30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %32 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %33 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %q.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%31, %32, %33), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %84 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %36 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %37 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %38 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %39 : int[] = prim::ListConstruct(%14, %36, %37, %38), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %40 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%84, %39), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %41 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %42 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %k.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%40, %41, %42), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %85 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %45 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %46 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %47 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %48 : int[] = prim::ListConstruct(%13, %45, %46, %47), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %49 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%85, %48), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %50 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %51 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %v.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%49, %50, %51), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:180:0
    %53 : Double() = prim::Constant[value={8}](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:190:0
    %q.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::div(%q.3, %53), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:190:0
    %55 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:191:0
    %56 : int = prim::Constant[value=3](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:191:0
    %57 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%k.2, %55, %56), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:191:0
    %scores.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.4, %57), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:191:0
    %59 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # torch/tensor.py:22:0
    %60 : Bool(119:13, 13:1) = aten::eq(%2, %59), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # torch/tensor.py:22:0
    %61 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:192:0
    %62 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:192:0
    %63 : int[] = prim::ListConstruct(%12, %61, %62, %25), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %64 : Bool(119:13, 1:13, 1:13, 13:1) = aten::view(%60, %63), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:192:0
    %mask.2 : Bool(119:13, 12:0, 13:0, 13:1) = aten::expand_as(%64, %scores.2), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:192:0
    %66 : float = prim::Constant[value=-inf](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:193:0
    %input.12 : Float(119:2028, 12:169, 13:13, 13:1) = aten::masked_fill_(%scores.2, %mask.2, %66), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:193:0
    %68 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # torch/nn/functional.py:1498:0
    %69 : None = prim::Constant(), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %input.13 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.12, %68, %69), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # torch/nn/functional.py:1498:0
    %86 : Tensor = prim::CallMethod[name="forward"](%4, %input.13)
    %x.8 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%86, %v.2), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:202:0
    %73 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:184:0
    %74 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:184:0
    %75 : Float(119:9984, 13:64, 12:832, 64:1) = aten::transpose(%x.8, %73, %74), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:184:0
    %76 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:184:0
    %77 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%75, %76), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:184:0
    %78 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:184:0
    %79 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:184:0
    %80 : int[] = prim::ListConstruct(%11, %78, %79), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention
    %input.14 : Float(119:9984, 13:768, 768:1) = aten::view(%77, %80), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention # transformers/modeling_distilbert.py:184:0
    %87 : Tensor = prim::CallMethod[name="forward"](%3, %input.14)
    return (%87)

TransformerBlock.ffn
FFN._actual_script_module
  graph(%self.30 : __torch__.transformers.modeling_distilbert.FFN,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.30)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.30)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.30)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.17 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn # torch/nn/functional.py:1369:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.17)
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %31)
    return (%32)

TransformerBlock.output_layer_norm
LayerNorm._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.19 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.output_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.output_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.output_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.output_layer_norm # torch/nn/functional.py:2048:0
    %query.3 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.19, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.output_layer_norm # torch/nn/functional.py:2048:0
    return (%query.3)

TransformerBlock.sa_layer_norm
LayerNorm._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.15 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.sa_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.sa_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.sa_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.sa_layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.15, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.sa_layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

MultiHeadSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.13 : Float(119:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.dropout # torch/nn/functional.py:973:0
    %weights.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.13, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.dropout # torch/nn/functional.py:973:0
    return (%weights.2)

MultiHeadSelfAttention.k_lin
Linear._actual_script_module
  graph(%self.25 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.25)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.25)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.k_lin # torch/nn/functional.py:1676:0
    %output.8 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.k_lin # torch/nn/functional.py:1678:0
    %x.6 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.8, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.k_lin # torch/nn/functional.py:1678:0
    return (%x.6)

MultiHeadSelfAttention.out_lin
Linear._actual_script_module
  graph(%self.28 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.28)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.28)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.out_lin # torch/nn/functional.py:1676:0
    %output.10 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.14, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.out_lin # torch/nn/functional.py:1678:0
    %sa_output.2 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.10, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.out_lin # torch/nn/functional.py:1678:0
    return (%sa_output.2)

MultiHeadSelfAttention.q_lin
Linear._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.24)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.q_lin # torch/nn/functional.py:1676:0
    %output.7 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.q_lin # torch/nn/functional.py:1678:0
    %x.5 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.7, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.q_lin # torch/nn/functional.py:1678:0
    return (%x.5)

MultiHeadSelfAttention.v_lin
Linear._actual_script_module
  graph(%self.26 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.26)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.26)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.v_lin # torch/nn/functional.py:1676:0
    %output.9 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.v_lin # torch/nn/functional.py:1678:0
    %x.7 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.9, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.attention/__module.distilbert.transformer.layer.1.attention.v_lin # torch/nn/functional.py:1678:0
    return (%x.7)

FFN.dropout
Dropout._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.dropout # torch/nn/functional.py:973:0
    %ffn_output.2 : Float(119:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.dropout # torch/nn/functional.py:973:0
    return (%ffn_output.2)

FFN.lin1
Linear._actual_script_module
  graph(%self.31 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.31)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.31)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.lin1 # torch/nn/functional.py:1676:0
    %output.11 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.lin1 # torch/nn/functional.py:1678:0
    %input.16 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.11, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.lin1 # torch/nn/functional.py:1678:0
    return (%input.16)

FFN.lin2
Linear._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.linear.Linear,
        %input.17 : Float(119:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.lin2 # torch/nn/functional.py:1676:0
    %output.12 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.17, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.lin2 # torch/nn/functional.py:1678:0
    %input.18 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.12, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.1/__module.distilbert.transformer.layer.1.ffn/__module.distilbert.transformer.layer.1.ffn.lin2 # torch/nn/functional.py:1678:0
    return (%input.18)

TransformerBlock._actual_script_module
  graph(%self.35 : __torch__.transformers.modeling_distilbert.TransformerBlock,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="output_layer_norm"](%self.35)
    %4 : __torch__.transformers.modeling_distilbert.FFN = prim::GetAttr[name="ffn"](%self.35)
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="sa_layer_norm"](%self.35)
    %6 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention = prim::GetAttr[name="attention"](%self.35)
    %15 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %8 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2 # transformers/modeling_distilbert.py:276:0
    %input.23 : Float(119:9984, 13:768, 768:1) = aten::add(%15, %1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2 # transformers/modeling_distilbert.py:276:0
    %16 : Tensor = prim::CallMethod[name="forward"](%5, %input.23)
    %17 : Tensor = prim::CallMethod[name="forward"](%4, %16)
    %12 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2 # transformers/modeling_distilbert.py:280:0
    %input.27 : Float(119:9984, 13:768, 768:1) = aten::add(%17, %16, %12), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2 # transformers/modeling_distilbert.py:280:0
    %18 : Tensor = prim::CallMethod[name="forward"](%3, %input.27)
    return (%18)

TransformerBlock.attention
MultiHeadSelfAttention._actual_script_module
  graph(%self.36 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.36)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.36)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.36)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.36)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.36)
    %8 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:169:0
    %9 : int = aten::size(%1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:169:0
    %bs.3 : Long() = prim::NumToTensor(%9), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %11 : int = aten::Int(%bs.3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %12 : int = aten::Int(%bs.3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %13 : int = aten::Int(%bs.3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %14 : int = aten::Int(%bs.3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %15 : int = aten::Int(%bs.3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %22 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:170:0
    %23 : int = aten::size(%1, %22), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:170:0
    %k_length.3 : Long() = prim::NumToTensor(%23), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %25 : int = aten::Int(%k_length.3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %83 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %27 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %28 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %29 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %30 : int[] = prim::ListConstruct(%15, %27, %28, %29), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %31 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%83, %30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %32 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %33 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %q.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%31, %32, %33), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %84 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %36 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %37 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %38 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %39 : int[] = prim::ListConstruct(%14, %36, %37, %38), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %40 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%84, %39), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %41 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %42 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %k.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%40, %41, %42), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %85 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %45 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %46 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %47 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %48 : int[] = prim::ListConstruct(%13, %45, %46, %47), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %49 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%85, %48), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %50 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %51 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %v.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%49, %50, %51), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:180:0
    %53 : Double() = prim::Constant[value={8}](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:190:0
    %q.6 : Float(119:9984, 12:64, 13:768, 64:1) = aten::div(%q.5, %53), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:190:0
    %55 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:191:0
    %56 : int = prim::Constant[value=3](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:191:0
    %57 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%k.3, %55, %56), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:191:0
    %scores.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.6, %57), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:191:0
    %59 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # torch/tensor.py:22:0
    %60 : Bool(119:13, 13:1) = aten::eq(%2, %59), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # torch/tensor.py:22:0
    %61 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:192:0
    %62 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:192:0
    %63 : int[] = prim::ListConstruct(%12, %61, %62, %25), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %64 : Bool(119:13, 1:13, 1:13, 13:1) = aten::view(%60, %63), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:192:0
    %mask.3 : Bool(119:13, 12:0, 13:0, 13:1) = aten::expand_as(%64, %scores.3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:192:0
    %66 : float = prim::Constant[value=-inf](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:193:0
    %input.20 : Float(119:2028, 12:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.3, %66), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:193:0
    %68 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # torch/nn/functional.py:1498:0
    %69 : None = prim::Constant(), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %input.21 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.20, %68, %69), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # torch/nn/functional.py:1498:0
    %86 : Tensor = prim::CallMethod[name="forward"](%4, %input.21)
    %x.12 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%86, %v.3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:202:0
    %73 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:184:0
    %74 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:184:0
    %75 : Float(119:9984, 13:64, 12:832, 64:1) = aten::transpose(%x.12, %73, %74), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:184:0
    %76 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:184:0
    %77 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%75, %76), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:184:0
    %78 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:184:0
    %79 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:184:0
    %80 : int[] = prim::ListConstruct(%11, %78, %79), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention
    %input.22 : Float(119:9984, 13:768, 768:1) = aten::view(%77, %80), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention # transformers/modeling_distilbert.py:184:0
    %87 : Tensor = prim::CallMethod[name="forward"](%3, %input.22)
    return (%87)

TransformerBlock.ffn
FFN._actual_script_module
  graph(%self.43 : __torch__.transformers.modeling_distilbert.FFN,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.43)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.43)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.43)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.25 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn # torch/nn/functional.py:1369:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.25)
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %31)
    return (%32)

TransformerBlock.output_layer_norm
LayerNorm._actual_script_module
  graph(%self.47 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.27 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.47)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.47)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.output_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.output_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.output_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.output_layer_norm # torch/nn/functional.py:2048:0
    %query.4 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.output_layer_norm # torch/nn/functional.py:2048:0
    return (%query.4)

TransformerBlock.sa_layer_norm
LayerNorm._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.23 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.42)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.sa_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.sa_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.sa_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.sa_layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.23, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.sa_layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

MultiHeadSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.21 : Float(119:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.dropout # torch/nn/functional.py:973:0
    %weights.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.21, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.dropout # torch/nn/functional.py:973:0
    return (%weights.3)

MultiHeadSelfAttention.k_lin
Linear._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.k_lin # torch/nn/functional.py:1676:0
    %output.14 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.k_lin # torch/nn/functional.py:1678:0
    %x.10 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.14, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.k_lin # torch/nn/functional.py:1678:0
    return (%x.10)

MultiHeadSelfAttention.out_lin
Linear._actual_script_module
  graph(%self.41 : __torch__.torch.nn.modules.linear.Linear,
        %input.22 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.41)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.41)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.out_lin # torch/nn/functional.py:1676:0
    %output.16 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.22, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.out_lin # torch/nn/functional.py:1678:0
    %sa_output.3 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.16, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.out_lin # torch/nn/functional.py:1678:0
    return (%sa_output.3)

MultiHeadSelfAttention.q_lin
Linear._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.37)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.37)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.q_lin # torch/nn/functional.py:1676:0
    %output.13 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.q_lin # torch/nn/functional.py:1678:0
    %x.9 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.13, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.q_lin # torch/nn/functional.py:1678:0
    return (%x.9)

MultiHeadSelfAttention.v_lin
Linear._actual_script_module
  graph(%self.39 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.39)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.39)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.v_lin # torch/nn/functional.py:1676:0
    %output.15 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.v_lin # torch/nn/functional.py:1678:0
    %x.11 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.15, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.attention/__module.distilbert.transformer.layer.2.attention.v_lin # torch/nn/functional.py:1678:0
    return (%x.11)

FFN.dropout
Dropout._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.dropout # torch/nn/functional.py:973:0
    %ffn_output.3 : Float(119:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.dropout # torch/nn/functional.py:973:0
    return (%ffn_output.3)

FFN.lin1
Linear._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.lin1 # torch/nn/functional.py:1676:0
    %output.17 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.lin1 # torch/nn/functional.py:1678:0
    %input.24 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.17, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.lin1 # torch/nn/functional.py:1678:0
    return (%input.24)

FFN.lin2
Linear._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.linear.Linear,
        %input.25 : Float(119:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.45)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.45)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.lin2 # torch/nn/functional.py:1676:0
    %output.18 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.25, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.lin2 # torch/nn/functional.py:1678:0
    %input.26 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.18, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.2/__module.distilbert.transformer.layer.2.ffn/__module.distilbert.transformer.layer.2.ffn.lin2 # torch/nn/functional.py:1678:0
    return (%input.26)

TransformerBlock._actual_script_module
  graph(%self.48 : __torch__.transformers.modeling_distilbert.TransformerBlock,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="output_layer_norm"](%self.48)
    %4 : __torch__.transformers.modeling_distilbert.FFN = prim::GetAttr[name="ffn"](%self.48)
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="sa_layer_norm"](%self.48)
    %6 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention = prim::GetAttr[name="attention"](%self.48)
    %15 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %8 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3 # transformers/modeling_distilbert.py:276:0
    %input.31 : Float(119:9984, 13:768, 768:1) = aten::add(%15, %1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3 # transformers/modeling_distilbert.py:276:0
    %16 : Tensor = prim::CallMethod[name="forward"](%5, %input.31)
    %17 : Tensor = prim::CallMethod[name="forward"](%4, %16)
    %12 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3 # transformers/modeling_distilbert.py:280:0
    %input.35 : Float(119:9984, 13:768, 768:1) = aten::add(%17, %16, %12), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3 # transformers/modeling_distilbert.py:280:0
    %18 : Tensor = prim::CallMethod[name="forward"](%3, %input.35)
    return (%18)

TransformerBlock.attention
MultiHeadSelfAttention._actual_script_module
  graph(%self.49 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.49)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.49)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.49)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.49)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.49)
    %8 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:169:0
    %9 : int = aten::size(%1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:169:0
    %bs.4 : Long() = prim::NumToTensor(%9), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %11 : int = aten::Int(%bs.4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %12 : int = aten::Int(%bs.4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %13 : int = aten::Int(%bs.4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %14 : int = aten::Int(%bs.4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %15 : int = aten::Int(%bs.4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %22 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:170:0
    %23 : int = aten::size(%1, %22), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:170:0
    %k_length.4 : Long() = prim::NumToTensor(%23), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %25 : int = aten::Int(%k_length.4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %83 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %27 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %28 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %29 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %30 : int[] = prim::ListConstruct(%15, %27, %28, %29), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %31 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%83, %30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %32 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %33 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %q.7 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%31, %32, %33), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %84 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %36 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %37 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %38 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %39 : int[] = prim::ListConstruct(%14, %36, %37, %38), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %40 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%84, %39), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %41 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %42 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %k.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%40, %41, %42), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %85 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %45 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %46 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %47 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %48 : int[] = prim::ListConstruct(%13, %45, %46, %47), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %49 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%85, %48), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %50 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %51 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %v.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%49, %50, %51), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:180:0
    %53 : Double() = prim::Constant[value={8}](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:190:0
    %q.8 : Float(119:9984, 12:64, 13:768, 64:1) = aten::div(%q.7, %53), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:190:0
    %55 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:191:0
    %56 : int = prim::Constant[value=3](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:191:0
    %57 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%k.4, %55, %56), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:191:0
    %scores.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.8, %57), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:191:0
    %59 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # torch/tensor.py:22:0
    %60 : Bool(119:13, 13:1) = aten::eq(%2, %59), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # torch/tensor.py:22:0
    %61 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:192:0
    %62 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:192:0
    %63 : int[] = prim::ListConstruct(%12, %61, %62, %25), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %64 : Bool(119:13, 1:13, 1:13, 13:1) = aten::view(%60, %63), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:192:0
    %mask.4 : Bool(119:13, 12:0, 13:0, 13:1) = aten::expand_as(%64, %scores.4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:192:0
    %66 : float = prim::Constant[value=-inf](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:193:0
    %input.28 : Float(119:2028, 12:169, 13:13, 13:1) = aten::masked_fill_(%scores.4, %mask.4, %66), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:193:0
    %68 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # torch/nn/functional.py:1498:0
    %69 : None = prim::Constant(), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %input.29 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.28, %68, %69), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # torch/nn/functional.py:1498:0
    %86 : Tensor = prim::CallMethod[name="forward"](%4, %input.29)
    %x.16 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%86, %v.4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:202:0
    %73 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:184:0
    %74 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:184:0
    %75 : Float(119:9984, 13:64, 12:832, 64:1) = aten::transpose(%x.16, %73, %74), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:184:0
    %76 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:184:0
    %77 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%75, %76), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:184:0
    %78 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:184:0
    %79 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:184:0
    %80 : int[] = prim::ListConstruct(%11, %78, %79), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention
    %input.30 : Float(119:9984, 13:768, 768:1) = aten::view(%77, %80), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention # transformers/modeling_distilbert.py:184:0
    %87 : Tensor = prim::CallMethod[name="forward"](%3, %input.30)
    return (%87)

TransformerBlock.ffn
FFN._actual_script_module
  graph(%self.56 : __torch__.transformers.modeling_distilbert.FFN,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.56)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.56)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.56)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.33 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn # torch/nn/functional.py:1369:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.33)
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %31)
    return (%32)

TransformerBlock.output_layer_norm
LayerNorm._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.35 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.output_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.output_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.output_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.output_layer_norm # torch/nn/functional.py:2048:0
    %query.5 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.35, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.output_layer_norm # torch/nn/functional.py:2048:0
    return (%query.5)

TransformerBlock.sa_layer_norm
LayerNorm._actual_script_module
  graph(%self.55 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.31 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.55)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.55)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.sa_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.sa_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.sa_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.sa_layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.31, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.sa_layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

MultiHeadSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.29 : Float(119:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.dropout # torch/nn/functional.py:973:0
    %weights.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.29, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.dropout # torch/nn/functional.py:973:0
    return (%weights.4)

MultiHeadSelfAttention.k_lin
Linear._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.51)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.51)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.k_lin # torch/nn/functional.py:1676:0
    %output.20 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.k_lin # torch/nn/functional.py:1678:0
    %x.14 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.20, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.k_lin # torch/nn/functional.py:1678:0
    return (%x.14)

MultiHeadSelfAttention.out_lin
Linear._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.linear.Linear,
        %input.30 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.out_lin # torch/nn/functional.py:1676:0
    %output.22 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.30, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.out_lin # torch/nn/functional.py:1678:0
    %sa_output.4 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.22, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.out_lin # torch/nn/functional.py:1678:0
    return (%sa_output.4)

MultiHeadSelfAttention.q_lin
Linear._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.50)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.q_lin # torch/nn/functional.py:1676:0
    %output.19 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.q_lin # torch/nn/functional.py:1678:0
    %x.13 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.19, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.q_lin # torch/nn/functional.py:1678:0
    return (%x.13)

MultiHeadSelfAttention.v_lin
Linear._actual_script_module
  graph(%self.52 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.52)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.52)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.v_lin # torch/nn/functional.py:1676:0
    %output.21 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.v_lin # torch/nn/functional.py:1678:0
    %x.15 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.21, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.attention/__module.distilbert.transformer.layer.3.attention.v_lin # torch/nn/functional.py:1678:0
    return (%x.15)

FFN.dropout
Dropout._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.dropout # torch/nn/functional.py:973:0
    %ffn_output.4 : Float(119:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.dropout # torch/nn/functional.py:973:0
    return (%ffn_output.4)

FFN.lin1
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.57)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.lin1 # torch/nn/functional.py:1676:0
    %output.23 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.lin1 # torch/nn/functional.py:1678:0
    %input.32 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.23, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.lin1 # torch/nn/functional.py:1678:0
    return (%input.32)

FFN.lin2
Linear._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.linear.Linear,
        %input.33 : Float(119:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.58)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.lin2 # torch/nn/functional.py:1676:0
    %output.24 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.33, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.lin2 # torch/nn/functional.py:1678:0
    %input.34 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.24, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.3/__module.distilbert.transformer.layer.3.ffn/__module.distilbert.transformer.layer.3.ffn.lin2 # torch/nn/functional.py:1678:0
    return (%input.34)

TransformerBlock._actual_script_module
  graph(%self.61 : __torch__.transformers.modeling_distilbert.TransformerBlock,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="output_layer_norm"](%self.61)
    %4 : __torch__.transformers.modeling_distilbert.FFN = prim::GetAttr[name="ffn"](%self.61)
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="sa_layer_norm"](%self.61)
    %6 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention = prim::GetAttr[name="attention"](%self.61)
    %15 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %8 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4 # transformers/modeling_distilbert.py:276:0
    %input.39 : Float(119:9984, 13:768, 768:1) = aten::add(%15, %1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4 # transformers/modeling_distilbert.py:276:0
    %16 : Tensor = prim::CallMethod[name="forward"](%5, %input.39)
    %17 : Tensor = prim::CallMethod[name="forward"](%4, %16)
    %12 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4 # transformers/modeling_distilbert.py:280:0
    %input.43 : Float(119:9984, 13:768, 768:1) = aten::add(%17, %16, %12), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4 # transformers/modeling_distilbert.py:280:0
    %18 : Tensor = prim::CallMethod[name="forward"](%3, %input.43)
    return (%18)

TransformerBlock.attention
MultiHeadSelfAttention._actual_script_module
  graph(%self.62 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.62)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.62)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.62)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.62)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.62)
    %8 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:169:0
    %9 : int = aten::size(%1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:169:0
    %bs.5 : Long() = prim::NumToTensor(%9), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %11 : int = aten::Int(%bs.5), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %12 : int = aten::Int(%bs.5), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %13 : int = aten::Int(%bs.5), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %14 : int = aten::Int(%bs.5), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %15 : int = aten::Int(%bs.5), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %22 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:170:0
    %23 : int = aten::size(%1, %22), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:170:0
    %k_length.5 : Long() = prim::NumToTensor(%23), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %25 : int = aten::Int(%k_length.5), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %83 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %27 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %28 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %29 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %30 : int[] = prim::ListConstruct(%15, %27, %28, %29), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %31 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%83, %30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %32 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %33 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %q.9 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%31, %32, %33), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %84 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %36 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %37 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %38 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %39 : int[] = prim::ListConstruct(%14, %36, %37, %38), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %40 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%84, %39), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %41 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %42 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %k.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%40, %41, %42), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %85 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %45 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %46 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %47 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %48 : int[] = prim::ListConstruct(%13, %45, %46, %47), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %49 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%85, %48), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %50 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %51 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %v.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%49, %50, %51), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:180:0
    %53 : Double() = prim::Constant[value={8}](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:190:0
    %q.10 : Float(119:9984, 12:64, 13:768, 64:1) = aten::div(%q.9, %53), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:190:0
    %55 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:191:0
    %56 : int = prim::Constant[value=3](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:191:0
    %57 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%k.5, %55, %56), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:191:0
    %scores.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.10, %57), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:191:0
    %59 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # torch/tensor.py:22:0
    %60 : Bool(119:13, 13:1) = aten::eq(%2, %59), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # torch/tensor.py:22:0
    %61 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:192:0
    %62 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:192:0
    %63 : int[] = prim::ListConstruct(%12, %61, %62, %25), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %64 : Bool(119:13, 1:13, 1:13, 13:1) = aten::view(%60, %63), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:192:0
    %mask.5 : Bool(119:13, 12:0, 13:0, 13:1) = aten::expand_as(%64, %scores.5), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:192:0
    %66 : float = prim::Constant[value=-inf](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:193:0
    %input.36 : Float(119:2028, 12:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.5, %66), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:193:0
    %68 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # torch/nn/functional.py:1498:0
    %69 : None = prim::Constant(), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %input.37 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %68, %69), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # torch/nn/functional.py:1498:0
    %86 : Tensor = prim::CallMethod[name="forward"](%4, %input.37)
    %x.20 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%86, %v.5), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:202:0
    %73 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:184:0
    %74 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:184:0
    %75 : Float(119:9984, 13:64, 12:832, 64:1) = aten::transpose(%x.20, %73, %74), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:184:0
    %76 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:184:0
    %77 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%75, %76), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:184:0
    %78 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:184:0
    %79 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:184:0
    %80 : int[] = prim::ListConstruct(%11, %78, %79), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention
    %input.38 : Float(119:9984, 13:768, 768:1) = aten::view(%77, %80), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention # transformers/modeling_distilbert.py:184:0
    %87 : Tensor = prim::CallMethod[name="forward"](%3, %input.38)
    return (%87)

TransformerBlock.ffn
FFN._actual_script_module
  graph(%self.69 : __torch__.transformers.modeling_distilbert.FFN,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.69)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.69)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.69)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.41 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn # torch/nn/functional.py:1369:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.41)
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %31)
    return (%32)

TransformerBlock.output_layer_norm
LayerNorm._actual_script_module
  graph(%self.73 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.43 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.73)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.73)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.output_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.output_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.output_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.output_layer_norm # torch/nn/functional.py:2048:0
    %query : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.43, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.output_layer_norm # torch/nn/functional.py:2048:0
    return (%query)

TransformerBlock.sa_layer_norm
LayerNorm._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.39 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.68)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.68)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.sa_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.sa_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.sa_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.sa_layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.39, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.sa_layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

MultiHeadSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.37 : Float(119:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.dropout # torch/nn/functional.py:973:0
    %weights.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.dropout # torch/nn/functional.py:973:0
    return (%weights.5)

MultiHeadSelfAttention.k_lin
Linear._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.k_lin # torch/nn/functional.py:1676:0
    %output.26 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.k_lin # torch/nn/functional.py:1678:0
    %x.18 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.26, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.k_lin # torch/nn/functional.py:1678:0
    return (%x.18)

MultiHeadSelfAttention.out_lin
Linear._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.linear.Linear,
        %input.38 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.67)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.67)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.out_lin # torch/nn/functional.py:1676:0
    %output.28 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.38, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.out_lin # torch/nn/functional.py:1678:0
    %sa_output.5 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.28, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.out_lin # torch/nn/functional.py:1678:0
    return (%sa_output.5)

MultiHeadSelfAttention.q_lin
Linear._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.63)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.63)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.q_lin # torch/nn/functional.py:1676:0
    %output.25 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.q_lin # torch/nn/functional.py:1678:0
    %x.17 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.25, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.q_lin # torch/nn/functional.py:1678:0
    return (%x.17)

MultiHeadSelfAttention.v_lin
Linear._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.v_lin # torch/nn/functional.py:1676:0
    %output.27 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.v_lin # torch/nn/functional.py:1678:0
    %x.19 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.27, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.attention/__module.distilbert.transformer.layer.4.attention.v_lin # torch/nn/functional.py:1678:0
    return (%x.19)

FFN.dropout
Dropout._actual_script_module
  graph(%self.72 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.dropout # torch/nn/functional.py:973:0
    %ffn_output.5 : Float(119:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.dropout # torch/nn/functional.py:973:0
    return (%ffn_output.5)

FFN.lin1
Linear._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.lin1 # torch/nn/functional.py:1676:0
    %output.29 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.lin1 # torch/nn/functional.py:1678:0
    %input.40 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.29, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.lin1 # torch/nn/functional.py:1678:0
    return (%input.40)

FFN.lin2
Linear._actual_script_module
  graph(%self.71 : __torch__.torch.nn.modules.linear.Linear,
        %input.41 : Float(119:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.71)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.71)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.lin2 # torch/nn/functional.py:1676:0
    %output.30 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.41, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.lin2 # torch/nn/functional.py:1678:0
    %input.42 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.30, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.4/__module.distilbert.transformer.layer.4.ffn/__module.distilbert.transformer.layer.4.ffn.lin2 # torch/nn/functional.py:1678:0
    return (%input.42)

TransformerBlock._actual_script_module
  graph(%self.74 : __torch__.transformers.modeling_distilbert.TransformerBlock,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="output_layer_norm"](%self.74)
    %4 : __torch__.transformers.modeling_distilbert.FFN = prim::GetAttr[name="ffn"](%self.74)
    %5 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="sa_layer_norm"](%self.74)
    %6 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention = prim::GetAttr[name="attention"](%self.74)
    %15 : Tensor = prim::CallMethod[name="forward"](%6, %1, %2)
    %8 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5 # transformers/modeling_distilbert.py:276:0
    %input.47 : Float(119:9984, 13:768, 768:1) = aten::add(%15, %1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5 # transformers/modeling_distilbert.py:276:0
    %16 : Tensor = prim::CallMethod[name="forward"](%5, %input.47)
    %17 : Tensor = prim::CallMethod[name="forward"](%4, %16)
    %12 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5 # transformers/modeling_distilbert.py:280:0
    %input.51 : Float(119:9984, 13:768, 768:1) = aten::add(%17, %16, %12), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5 # transformers/modeling_distilbert.py:280:0
    %18 : Tensor = prim::CallMethod[name="forward"](%3, %input.51)
    return (%18)

TransformerBlock.attention
MultiHeadSelfAttention._actual_script_module
  graph(%self.75 : __torch__.transformers.modeling_distilbert.MultiHeadSelfAttention,
        %1 : Float(119:9984, 13:768, 768:1),
        %2 : Long(119:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.75)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.75)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.75)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.75)
    %7 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.75)
    %8 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:169:0
    %9 : int = aten::size(%1, %8), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:169:0
    %bs : Long() = prim::NumToTensor(%9), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %11 : int = aten::Int(%bs), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %12 : int = aten::Int(%bs), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %13 : int = aten::Int(%bs), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %14 : int = aten::Int(%bs), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %15 : int = aten::Int(%bs), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %22 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:170:0
    %23 : int = aten::size(%1, %22), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:170:0
    %k_length : Long() = prim::NumToTensor(%23), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %25 : int = aten::Int(%k_length), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %83 : Tensor = prim::CallMethod[name="forward"](%7, %1)
    %27 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %28 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %29 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %30 : int[] = prim::ListConstruct(%15, %27, %28, %29), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %31 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%83, %30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %32 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %33 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %q.11 : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%31, %32, %33), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %84 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %36 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %37 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %38 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %39 : int[] = prim::ListConstruct(%14, %36, %37, %38), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %40 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%84, %39), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %41 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %42 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %k : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%40, %41, %42), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %85 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %45 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %46 : int = prim::Constant[value=12](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %47 : int = prim::Constant[value=64](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %48 : int[] = prim::ListConstruct(%13, %45, %46, %47), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %49 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%85, %48), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %50 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %51 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %v : Float(119:9984, 12:64, 13:768, 64:1) = aten::transpose(%49, %50, %51), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:180:0
    %53 : Double() = prim::Constant[value={8}](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:190:0
    %q : Float(119:9984, 12:64, 13:768, 64:1) = aten::div(%q.11, %53), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:190:0
    %55 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:191:0
    %56 : int = prim::Constant[value=3](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:191:0
    %57 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%k, %55, %56), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:191:0
    %scores : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%q, %57), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:191:0
    %59 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # torch/tensor.py:22:0
    %60 : Bool(119:13, 13:1) = aten::eq(%2, %59), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # torch/tensor.py:22:0
    %61 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:192:0
    %62 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:192:0
    %63 : int[] = prim::ListConstruct(%12, %61, %62, %25), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %64 : Bool(119:13, 1:13, 1:13, 13:1) = aten::view(%60, %63), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:192:0
    %mask : Bool(119:13, 12:0, 13:0, 13:1) = aten::expand_as(%64, %scores), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:192:0
    %66 : float = prim::Constant[value=-inf](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:193:0
    %input.44 : Float(119:2028, 12:169, 13:13, 13:1) = aten::masked_fill_(%scores, %mask, %66), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:193:0
    %68 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # torch/nn/functional.py:1498:0
    %69 : None = prim::Constant(), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %input.45 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.44, %68, %69), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # torch/nn/functional.py:1498:0
    %86 : Tensor = prim::CallMethod[name="forward"](%4, %input.45)
    %x : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%86, %v), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:202:0
    %73 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:184:0
    %74 : int = prim::Constant[value=2](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:184:0
    %75 : Float(119:9984, 13:64, 12:832, 64:1) = aten::transpose(%x, %73, %74), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:184:0
    %76 : int = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:184:0
    %77 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%75, %76), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:184:0
    %78 : int = prim::Constant[value=-1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:184:0
    %79 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:184:0
    %80 : int[] = prim::ListConstruct(%11, %78, %79), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention
    %input.46 : Float(119:9984, 13:768, 768:1) = aten::view(%77, %80), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention # transformers/modeling_distilbert.py:184:0
    %87 : Tensor = prim::CallMethod[name="forward"](%3, %input.46)
    return (%87)

TransformerBlock.ffn
FFN._actual_script_module
  graph(%self.82 : __torch__.transformers.modeling_distilbert.FFN,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.82)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.82)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.82)
    %30 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %input.49 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%30), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn # torch/nn/functional.py:1369:0
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %input.49)
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %31)
    return (%32)

TransformerBlock.output_layer_norm
LayerNorm._actual_script_module
  graph(%self.86 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.51 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.86)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.86)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.output_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.output_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.output_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.output_layer_norm # torch/nn/functional.py:2048:0
    %hidden_state : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.51, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.output_layer_norm # torch/nn/functional.py:2048:0
    return (%hidden_state)

TransformerBlock.sa_layer_norm
LayerNorm._actual_script_module
  graph(%self.81 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.47 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.81)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.81)
    %4 : int = prim::Constant[value=768](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.sa_layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.sa_layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.sa_layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.sa_layer_norm # torch/nn/functional.py:2048:0
    %input_tensor : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.47, %5, %3, %2, %6, %7), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.sa_layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor)

MultiHeadSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.45 : Float(119:2028, 12:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.dropout # torch/nn/functional.py:973:0
    %weights : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.45, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.dropout # torch/nn/functional.py:973:0
    return (%weights)

MultiHeadSelfAttention.k_lin
Linear._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.77)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.77)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.k_lin # torch/nn/functional.py:1676:0
    %output.32 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.k_lin # torch/nn/functional.py:1678:0
    %x.22 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.32, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.k_lin # torch/nn/functional.py:1678:0
    return (%x.22)

MultiHeadSelfAttention.out_lin
Linear._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.linear.Linear,
        %input.46 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.out_lin # torch/nn/functional.py:1676:0
    %output.34 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.46, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.out_lin # torch/nn/functional.py:1678:0
    %sa_output : Float(119:9984, 13:768, 768:1) = aten::add_(%output.34, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.out_lin # torch/nn/functional.py:1678:0
    return (%sa_output)

MultiHeadSelfAttention.q_lin
Linear._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.q_lin # torch/nn/functional.py:1676:0
    %output.31 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.q_lin # torch/nn/functional.py:1678:0
    %x.21 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.31, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.q_lin # torch/nn/functional.py:1678:0
    return (%x.21)

MultiHeadSelfAttention.v_lin
Linear._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : Float(768:1, 768:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.v_lin # torch/nn/functional.py:1676:0
    %output.33 : Float(119:9984, 13:768, 768:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.v_lin # torch/nn/functional.py:1678:0
    %x.23 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.33, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.attention/__module.distilbert.transformer.layer.5.attention.v_lin # torch/nn/functional.py:1678:0
    return (%x.23)

FFN.dropout
Dropout._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.dropout # torch/nn/functional.py:973:0
    %ffn_output : Float(119:9984, 13:768, 768:1) = aten::dropout(%1, %2, %3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.dropout # torch/nn/functional.py:973:0
    return (%ffn_output)

FFN.lin1
Linear._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(119:9984, 13:768, 768:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.83)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.83)
    %4 : Float(768:1, 3072:768) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.lin1 # torch/nn/functional.py:1676:0
    %output.35 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%1, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.lin1 # torch/nn/functional.py:1678:0
    %input.48 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.35, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.lin1 # torch/nn/functional.py:1678:0
    return (%input.48)

FFN.lin2
Linear._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.linear.Linear,
        %input.49 : Float(119:39936, 13:3072, 3072:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.84)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %4 : Float(3072:1, 768:3072) = aten::t(%3), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.lin2 # torch/nn/functional.py:1676:0
    %output : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.49, %4), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.lin2 # torch/nn/functional.py:1678:0
    %input.50 : Float(119:9984, 13:768, 768:1) = aten::add_(%output, %2, %6), scope: __module.distilbert/__module.distilbert.transformer/__module.distilbert.transformer.layer.5/__module.distilbert.transformer.layer.5.ffn/__module.distilbert.transformer.layer.5.ffn.lin2 # torch/nn/functional.py:1678:0
    return (%input.50)

