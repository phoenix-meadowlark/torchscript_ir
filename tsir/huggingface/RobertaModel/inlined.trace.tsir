graph(%self.1 : __torch__.transformers.modeling_roberta.___torch_mangle_33747.RobertaModel,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.transformers.modeling_roberta.___torch_mangle_33746.RobertaPooler = prim::GetAttr[name="pooler"](%self.1)
  %4 : __torch__.transformers.modeling_roberta.___torch_mangle_33743.RobertaEncoder = prim::GetAttr[name="encoder"](%self.1)
  %5 : __torch__.transformers.modeling_roberta.___torch_mangle_33537.RobertaEmbeddings = prim::GetAttr[name="embeddings"](%self.1)
  %6 : int = prim::Constant[value=0]() # transformers/modeling_roberta.py:639:0
  %7 : int = aten::size(%input_ids, %6) # transformers/modeling_roberta.py:639:0
  %8 : Long() = prim::NumToTensor(%7)
  %9 : int = aten::Int(%8)
  %10 : int = prim::Constant[value=1]() # transformers/modeling_roberta.py:639:0
  %11 : int = aten::size(%input_ids, %10) # transformers/modeling_roberta.py:639:0
  %12 : Long() = prim::NumToTensor(%11)
  %13 : int = aten::Int(%12)
  %14 : int[] = prim::ListConstruct(%9, %13)
  %15 : int = prim::Constant[value=4]() # transformers/modeling_roberta.py:650:0
  %16 : int = prim::Constant[value=0]() # transformers/modeling_roberta.py:650:0
  %17 : Device = prim::Constant[value="cpu"]() # transformers/modeling_roberta.py:650:0
  %18 : bool = prim::Constant[value=0]() # transformers/modeling_roberta.py:650:0
  %input.2 : Long(17:13, 13:1) = aten::zeros(%14, %15, %16, %17, %18) # transformers/modeling_roberta.py:650:0
  %20 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %21 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %22 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
  %23 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %24 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %20, %21, %22, %23) # transformers/modeling_utils.py:244:0
  %25 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %26 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%24, %25) # transformers/modeling_utils.py:244:0
  %27 : int = prim::Constant[value=2]() # transformers/modeling_utils.py:244:0
  %28 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%26, %27) # transformers/modeling_utils.py:244:0
  %29 : int = prim::Constant[value=3]() # transformers/modeling_utils.py:244:0
  %30 : int = prim::Constant[value=0]() # transformers/modeling_utils.py:244:0
  %31 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_utils.py:244:0
  %32 : int = prim::Constant[value=1]() # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%28, %29, %30, %31, %32) # transformers/modeling_utils.py:244:0
  %34 : int = prim::Constant[value=6]() # transformers/modeling_utils.py:257:0
  %35 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
  %36 : bool = prim::Constant[value=0]() # transformers/modeling_utils.py:257:0
  %37 : None = prim::Constant()
  %38 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %34, %35, %36, %37) # transformers/modeling_utils.py:257:0
  %39 : float = prim::Constant[value=1.]() # torch/tensor.py:396:0
  %40 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
  %41 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%38, %39, %40) # torch/tensor.py:396:0
  %42 : Double() = prim::Constant[value={-10000}]() # transformers/modeling_utils.py:258:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%41, %42) # transformers/modeling_utils.py:258:0
  %48 : float = prim::Constant[value=0.10000000000000001](), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %49 : int = prim::Constant[value=768](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %50 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %51 : bool = prim::Constant[value=1](), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %52 : int = prim::Constant[value=-1](), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %53 : Device = prim::Constant[value="cpu"](), scope: __module.embeddings # transformers/modeling_roberta.py:98:0
  %54 : int = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_roberta.py:98:0
  %55 : Long() = prim::Constant[value={1}](), scope: __module.embeddings # transformers/modeling_roberta.py:1334:0
  %56 : int = prim::Constant[value=4](), scope: __module.embeddings # transformers/modeling_roberta.py:1334:0
  %57 : None = prim::Constant(), scope: __module.embeddings
  %58 : bool = prim::Constant[value=0](), scope: __module.embeddings # transformers/modeling_roberta.py:1332:0
  %59 : int = prim::Constant[value=3](), scope: __module.embeddings # transformers/modeling_roberta.py:1332:0
  %60 : int = prim::Constant[value=1](), scope: __module.embeddings # transformers/modeling_roberta.py:1332:0
  %61 : __torch__.torch.nn.modules.normalization.___torch_mangle_33535.LayerNorm = prim::GetAttr[name="LayerNorm"](%5)
  %62 : __torch__.torch.nn.modules.sparse.___torch_mangle_33534.Embedding = prim::GetAttr[name="token_type_embeddings"](%5)
  %63 : __torch__.torch.nn.modules.sparse.___torch_mangle_33533.Embedding = prim::GetAttr[name="position_embeddings"](%5)
  %64 : __torch__.torch.nn.modules.sparse.___torch_mangle_33532.Embedding = prim::GetAttr[name="word_embeddings"](%5)
  %65 : Bool(17:13, 13:1) = aten::ne(%input_ids, %60), scope: __module.embeddings # transformers/modeling_roberta.py:1332:0
  %mask : Int(17:13, 13:1) = aten::to(%65, %59, %58, %58, %57), scope: __module.embeddings # transformers/modeling_roberta.py:1332:0
  %67 : Long(17:13, 13:1) = aten::cumsum(%mask, %60, %57), scope: __module.embeddings # transformers/modeling_roberta.py:1333:0
  %68 : Int(17:13, 13:1) = aten::type_as(%67, %mask), scope: __module.embeddings # transformers/modeling_roberta.py:1333:0
  %incremental_indices : Int(17:13, 13:1) = aten::mul(%68, %mask), scope: __module.embeddings # transformers/modeling_roberta.py:1333:0
  %70 : Long(17:13, 13:1) = aten::to(%incremental_indices, %56, %58, %58, %57), scope: __module.embeddings # transformers/modeling_roberta.py:1334:0
  %71 : Long(17:13, 13:1) = aten::add(%70, %55, %60), scope: __module.embeddings # transformers/modeling_roberta.py:1334:0
  %input.1 : Long(17:13, 13:1) = aten::to(%71, %56, %54, %53, %58, %58, %58, %57), scope: __module.embeddings # transformers/modeling_roberta.py:98:0
  %73 : Tensor = prim::GetAttr[name="weight"](%64)
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::embedding(%73, %input_ids, %60, %58, %58), scope: __module.embeddings/__module.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %75 : Tensor = prim::GetAttr[name="weight"](%63)
  %position_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%75, %input.1, %60, %58, %58), scope: __module.embeddings/__module.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %77 : Tensor = prim::GetAttr[name="weight"](%62)
  %token_type_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%77, %input.2, %52, %58, %58), scope: __module.embeddings/__module.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %79 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %60), scope: __module.embeddings # transformers/modeling_roberta.py:121:0
  %input.3 : Float(17:9984, 13:768, 768:1) = aten::add(%79, %token_type_embeddings, %60), scope: __module.embeddings # transformers/modeling_roberta.py:121:0
  %81 : Tensor = prim::GetAttr[name="bias"](%61)
  %82 : Tensor = prim::GetAttr[name="weight"](%61)
  %83 : int[] = prim::ListConstruct(%49), scope: __module.embeddings/__module.embeddings.LayerNorm
  %input.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %83, %82, %81, %50, %51), scope: __module.embeddings/__module.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.4, %48, %58), scope: __module.embeddings/__module.embeddings.dropout # torch/nn/functional.py:973:0
  %86 : bool = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %87 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %88 : int = prim::Constant[value=1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %89 : int = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %90 : int = prim::Constant[value=12](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %91 : int = prim::Constant[value=64](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %92 : int = prim::Constant[value=2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %93 : int = prim::Constant[value=3](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %94 : int = prim::Constant[value=-1](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %95 : int = prim::Constant[value=-2](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %96 : Double() = prim::Constant[value={8}](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %97 : None = prim::Constant(), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %98 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %99 : float = prim::Constant[value=0.10000000000000001](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %100 : int = prim::Constant[value=768](), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:215:0
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %102 : __torch__.transformers.modeling_roberta.___torch_mangle_33741.RobertaLayer = prim::GetAttr[name="11"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %104 : __torch__.transformers.modeling_roberta.___torch_mangle_33724.RobertaLayer = prim::GetAttr[name="10"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %106 : __torch__.transformers.modeling_roberta.___torch_mangle_33707.RobertaLayer = prim::GetAttr[name="9"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %108 : __torch__.transformers.modeling_roberta.___torch_mangle_33690.RobertaLayer = prim::GetAttr[name="8"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %110 : __torch__.transformers.modeling_roberta.___torch_mangle_33673.RobertaLayer = prim::GetAttr[name="7"](%109)
  %111 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %112 : __torch__.transformers.modeling_roberta.___torch_mangle_33656.RobertaLayer = prim::GetAttr[name="6"](%111)
  %113 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %114 : __torch__.transformers.modeling_roberta.___torch_mangle_33639.RobertaLayer = prim::GetAttr[name="5"](%113)
  %115 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %116 : __torch__.transformers.modeling_roberta.___torch_mangle_33622.RobertaLayer = prim::GetAttr[name="4"](%115)
  %117 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %118 : __torch__.transformers.modeling_roberta.___torch_mangle_33605.RobertaLayer = prim::GetAttr[name="3"](%117)
  %119 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %120 : __torch__.transformers.modeling_roberta.___torch_mangle_33588.RobertaLayer = prim::GetAttr[name="2"](%119)
  %121 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %122 : __torch__.transformers.modeling_roberta.___torch_mangle_33571.RobertaLayer = prim::GetAttr[name="1"](%121)
  %123 : __torch__.torch.nn.modules.container.___torch_mangle_33742.ModuleList = prim::GetAttr[name="layer"](%4)
  %124 : __torch__.transformers.modeling_roberta.___torch_mangle_33554.RobertaLayer = prim::GetAttr[name="0"](%123)
  %125 : __torch__.transformers.modeling_roberta.___torch_mangle_33553.RobertaOutput = prim::GetAttr[name="output"](%124)
  %126 : __torch__.transformers.modeling_roberta.___torch_mangle_33549.RobertaIntermediate = prim::GetAttr[name="intermediate"](%124)
  %127 : __torch__.transformers.modeling_roberta.___torch_mangle_33547.RobertaAttention = prim::GetAttr[name="attention"](%124)
  %128 : __torch__.transformers.modeling_roberta.___torch_mangle_33546.RobertaSelfOutput = prim::GetAttr[name="output"](%127)
  %129 : __torch__.transformers.modeling_roberta.___torch_mangle_33542.RobertaSelfAttention = prim::GetAttr[name="self"](%127)
  %130 : __torch__.torch.nn.modules.linear.___torch_mangle_33540.Linear = prim::GetAttr[name="value"](%129)
  %131 : __torch__.torch.nn.modules.linear.___torch_mangle_33539.Linear = prim::GetAttr[name="key"](%129)
  %132 : __torch__.torch.nn.modules.linear.___torch_mangle_33538.Linear = prim::GetAttr[name="query"](%129)
  %133 : Tensor = prim::GetAttr[name="bias"](%132)
  %134 : Tensor = prim::GetAttr[name="weight"](%132)
  %135 : Float(768:1, 768:768) = aten::t(%134), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %135), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %133, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %138 : Tensor = prim::GetAttr[name="bias"](%131)
  %139 : Tensor = prim::GetAttr[name="weight"](%131)
  %140 : Float(768:1, 768:768) = aten::t(%139), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %140), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %138, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %143 : Tensor = prim::GetAttr[name="bias"](%130)
  %144 : Tensor = prim::GetAttr[name="weight"](%130)
  %145 : Float(768:1, 768:768) = aten::t(%144), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %145), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %143, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %148 : int = aten::size(%x.1, %89), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %149 : int = aten::size(%x.1, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %150 : int[] = prim::ListConstruct(%148, %149, %90, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %150), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %152 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %152), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %154 : int = aten::size(%x.3, %89), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %155 : int = aten::size(%x.3, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %156 : int[] = prim::ListConstruct(%154, %155, %90, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %156), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %158 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %158), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %160 : int = aten::size(%x.5, %89), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %161 : int = aten::size(%x.5, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %162 : int[] = prim::ListConstruct(%160, %161, %90, %91), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %x.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %162), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %164 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %164), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %166 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %94, %95), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %166), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %96), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:198:0
  %input.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %94, %97), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %99, %98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self/__module.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:211:0
  %173 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %174 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %173), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%174, %89), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %176 : int = aten::size(%context_layer.2, %89), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %177 : int = aten::size(%context_layer.2, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %178 : int[] = prim::ListConstruct(%176, %177, %100), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.2, %178), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.self # transformers/modeling_roberta.py:215:0
  %180 : __torch__.torch.nn.modules.normalization.___torch_mangle_33544.LayerNorm = prim::GetAttr[name="LayerNorm"](%128)
  %181 : __torch__.torch.nn.modules.linear.___torch_mangle_33543.Linear = prim::GetAttr[name="dense"](%128)
  %182 : Tensor = prim::GetAttr[name="bias"](%181)
  %183 : Tensor = prim::GetAttr[name="weight"](%181)
  %184 : Float(768:1, 768:768) = aten::t(%183), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.8, %184), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.4, %182, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.9, %99, %98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output # transformers/modeling_roberta.py:232:0
  %189 : Tensor = prim::GetAttr[name="bias"](%180)
  %190 : Tensor = prim::GetAttr[name="weight"](%180)
  %191 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %191, %190, %189, %87, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.attention/__module.encoder.layer.0.attention.output/__module.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %193 : __torch__.torch.nn.modules.linear.___torch_mangle_33548.Linear = prim::GetAttr[name="dense"](%126)
  %194 : Tensor = prim::GetAttr[name="bias"](%193)
  %195 : Tensor = prim::GetAttr[name="weight"](%193)
  %196 : Float(768:1, 3072:768) = aten::t(%195), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %196), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.5, %194, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate/__module.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %200 : __torch__.torch.nn.modules.normalization.___torch_mangle_33551.LayerNorm = prim::GetAttr[name="LayerNorm"](%125)
  %201 : __torch__.torch.nn.modules.linear.___torch_mangle_33550.Linear = prim::GetAttr[name="dense"](%125)
  %202 : Tensor = prim::GetAttr[name="bias"](%201)
  %203 : Tensor = prim::GetAttr[name="weight"](%201)
  %204 : Float(3072:1, 768:3072) = aten::t(%203), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %204), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %202, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %99, %98), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %88), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output # transformers/modeling_roberta.py:311:0
  %209 : Tensor = prim::GetAttr[name="bias"](%200)
  %210 : Tensor = prim::GetAttr[name="weight"](%200)
  %211 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm
  %input.15 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %211, %210, %209, %87, %86), scope: __module.encoder/__module.encoder.layer.0/__module.encoder.layer.0.output/__module.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %213 : __torch__.transformers.modeling_roberta.___torch_mangle_33570.RobertaOutput = prim::GetAttr[name="output"](%122)
  %214 : __torch__.transformers.modeling_roberta.___torch_mangle_33566.RobertaIntermediate = prim::GetAttr[name="intermediate"](%122)
  %215 : __torch__.transformers.modeling_roberta.___torch_mangle_33564.RobertaAttention = prim::GetAttr[name="attention"](%122)
  %216 : __torch__.transformers.modeling_roberta.___torch_mangle_33563.RobertaSelfOutput = prim::GetAttr[name="output"](%215)
  %217 : __torch__.transformers.modeling_roberta.___torch_mangle_33559.RobertaSelfAttention = prim::GetAttr[name="self"](%215)
  %218 : __torch__.torch.nn.modules.linear.___torch_mangle_33557.Linear = prim::GetAttr[name="value"](%217)
  %219 : __torch__.torch.nn.modules.linear.___torch_mangle_33556.Linear = prim::GetAttr[name="key"](%217)
  %220 : __torch__.torch.nn.modules.linear.___torch_mangle_33555.Linear = prim::GetAttr[name="query"](%217)
  %221 : Tensor = prim::GetAttr[name="bias"](%220)
  %222 : Tensor = prim::GetAttr[name="weight"](%220)
  %223 : Float(768:1, 768:768) = aten::t(%222), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %223), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %221, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %226 : Tensor = prim::GetAttr[name="bias"](%219)
  %227 : Tensor = prim::GetAttr[name="weight"](%219)
  %228 : Float(768:1, 768:768) = aten::t(%227), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %228), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %226, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %231 : Tensor = prim::GetAttr[name="bias"](%218)
  %232 : Tensor = prim::GetAttr[name="weight"](%218)
  %233 : Float(768:1, 768:768) = aten::t(%232), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %233), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.9, %231, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %236 : int = aten::size(%x.7, %89), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %237 : int = aten::size(%x.7, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %238 : int[] = prim::ListConstruct(%236, %237, %90, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %238), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %240 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %240), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %242 : int = aten::size(%x.9, %89), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %243 : int = aten::size(%x.9, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %244 : int[] = prim::ListConstruct(%242, %243, %90, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %244), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %246 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %246), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %248 : int = aten::size(%x.11, %89), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %249 : int = aten::size(%x.11, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %250 : int[] = prim::ListConstruct(%248, %249, %90, %91), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %x.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %250), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %252 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %252), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %254 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %94, %95), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %254), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %96), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:195:0
  %input.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:198:0
  %input.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %94, %97), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %99, %98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self/__module.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:211:0
  %261 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %262 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %261), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%262, %89), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %264 : int = aten::size(%context_layer.4, %89), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %265 : int = aten::size(%context_layer.4, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %266 : int[] = prim::ListConstruct(%264, %265, %100), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.4, %266), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.self # transformers/modeling_roberta.py:215:0
  %268 : __torch__.torch.nn.modules.normalization.___torch_mangle_33561.LayerNorm = prim::GetAttr[name="LayerNorm"](%216)
  %269 : __torch__.torch.nn.modules.linear.___torch_mangle_33560.Linear = prim::GetAttr[name="dense"](%216)
  %270 : Tensor = prim::GetAttr[name="bias"](%269)
  %271 : Tensor = prim::GetAttr[name="weight"](%269)
  %272 : Float(768:1, 768:768) = aten::t(%271), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.18, %272), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %270, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.19, %99, %98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output # transformers/modeling_roberta.py:232:0
  %277 : Tensor = prim::GetAttr[name="bias"](%268)
  %278 : Tensor = prim::GetAttr[name="weight"](%268)
  %279 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %279, %278, %277, %87, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.attention/__module.encoder.layer.1.attention.output/__module.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %281 : __torch__.torch.nn.modules.linear.___torch_mangle_33565.Linear = prim::GetAttr[name="dense"](%214)
  %282 : Tensor = prim::GetAttr[name="bias"](%281)
  %283 : Tensor = prim::GetAttr[name="weight"](%281)
  %284 : Float(768:1, 3072:768) = aten::t(%283), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %284), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.11, %282, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate/__module.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %288 : __torch__.torch.nn.modules.normalization.___torch_mangle_33568.LayerNorm = prim::GetAttr[name="LayerNorm"](%213)
  %289 : __torch__.torch.nn.modules.linear.___torch_mangle_33567.Linear = prim::GetAttr[name="dense"](%213)
  %290 : Tensor = prim::GetAttr[name="bias"](%289)
  %291 : Tensor = prim::GetAttr[name="weight"](%289)
  %292 : Float(3072:1, 768:3072) = aten::t(%291), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.22, %292), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %290, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.23, %99, %98), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %88), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output # transformers/modeling_roberta.py:311:0
  %297 : Tensor = prim::GetAttr[name="bias"](%288)
  %298 : Tensor = prim::GetAttr[name="weight"](%288)
  %299 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %299, %298, %297, %87, %86), scope: __module.encoder/__module.encoder.layer.1/__module.encoder.layer.1.output/__module.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %301 : __torch__.transformers.modeling_roberta.___torch_mangle_33587.RobertaOutput = prim::GetAttr[name="output"](%120)
  %302 : __torch__.transformers.modeling_roberta.___torch_mangle_33583.RobertaIntermediate = prim::GetAttr[name="intermediate"](%120)
  %303 : __torch__.transformers.modeling_roberta.___torch_mangle_33581.RobertaAttention = prim::GetAttr[name="attention"](%120)
  %304 : __torch__.transformers.modeling_roberta.___torch_mangle_33580.RobertaSelfOutput = prim::GetAttr[name="output"](%303)
  %305 : __torch__.transformers.modeling_roberta.___torch_mangle_33576.RobertaSelfAttention = prim::GetAttr[name="self"](%303)
  %306 : __torch__.torch.nn.modules.linear.___torch_mangle_33574.Linear = prim::GetAttr[name="value"](%305)
  %307 : __torch__.torch.nn.modules.linear.___torch_mangle_33573.Linear = prim::GetAttr[name="key"](%305)
  %308 : __torch__.torch.nn.modules.linear.___torch_mangle_33572.Linear = prim::GetAttr[name="query"](%305)
  %309 : Tensor = prim::GetAttr[name="bias"](%308)
  %310 : Tensor = prim::GetAttr[name="weight"](%308)
  %311 : Float(768:1, 768:768) = aten::t(%310), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %311), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %309, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %314 : Tensor = prim::GetAttr[name="bias"](%307)
  %315 : Tensor = prim::GetAttr[name="weight"](%307)
  %316 : Float(768:1, 768:768) = aten::t(%315), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %316), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.14, %314, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %319 : Tensor = prim::GetAttr[name="bias"](%306)
  %320 : Tensor = prim::GetAttr[name="weight"](%306)
  %321 : Float(768:1, 768:768) = aten::t(%320), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %321), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %319, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %324 : int = aten::size(%x.13, %89), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %325 : int = aten::size(%x.13, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %326 : int[] = prim::ListConstruct(%324, %325, %90, %91), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %326), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %328 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %328), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %330 : int = aten::size(%x.15, %89), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %331 : int = aten::size(%x.15, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %332 : int[] = prim::ListConstruct(%330, %331, %90, %91), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %332), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %334 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %334), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %336 : int = aten::size(%x.17, %89), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %337 : int = aten::size(%x.17, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %338 : int[] = prim::ListConstruct(%336, %337, %90, %91), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %x.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %338), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %340 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %340), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %342 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %94, %95), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %342), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %96), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:195:0
  %input.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:198:0
  %input.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %94, %97), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %99, %98), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self/__module.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:211:0
  %349 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %350 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %349), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%350, %89), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %352 : int = aten::size(%context_layer.6, %89), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %353 : int = aten::size(%context_layer.6, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %354 : int[] = prim::ListConstruct(%352, %353, %100), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.6, %354), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.self # transformers/modeling_roberta.py:215:0
  %356 : __torch__.torch.nn.modules.normalization.___torch_mangle_33578.LayerNorm = prim::GetAttr[name="LayerNorm"](%304)
  %357 : __torch__.torch.nn.modules.linear.___torch_mangle_33577.Linear = prim::GetAttr[name="dense"](%304)
  %358 : Tensor = prim::GetAttr[name="bias"](%357)
  %359 : Tensor = prim::GetAttr[name="weight"](%357)
  %360 : Float(768:1, 768:768) = aten::t(%359), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.28, %360), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %358, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.29, %99, %98), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output # transformers/modeling_roberta.py:232:0
  %365 : Tensor = prim::GetAttr[name="bias"](%356)
  %366 : Tensor = prim::GetAttr[name="weight"](%356)
  %367 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %367, %366, %365, %87, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.attention/__module.encoder.layer.2.attention.output/__module.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %369 : __torch__.torch.nn.modules.linear.___torch_mangle_33582.Linear = prim::GetAttr[name="dense"](%302)
  %370 : Tensor = prim::GetAttr[name="bias"](%369)
  %371 : Tensor = prim::GetAttr[name="weight"](%369)
  %372 : Float(768:1, 3072:768) = aten::t(%371), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %372), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.17, %370, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate/__module.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %376 : __torch__.torch.nn.modules.normalization.___torch_mangle_33585.LayerNorm = prim::GetAttr[name="LayerNorm"](%301)
  %377 : __torch__.torch.nn.modules.linear.___torch_mangle_33584.Linear = prim::GetAttr[name="dense"](%301)
  %378 : Tensor = prim::GetAttr[name="bias"](%377)
  %379 : Tensor = prim::GetAttr[name="weight"](%377)
  %380 : Float(3072:1, 768:3072) = aten::t(%379), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.32, %380), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %378, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.33, %99, %98), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %88), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output # transformers/modeling_roberta.py:311:0
  %385 : Tensor = prim::GetAttr[name="bias"](%376)
  %386 : Tensor = prim::GetAttr[name="weight"](%376)
  %387 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %387, %386, %385, %87, %86), scope: __module.encoder/__module.encoder.layer.2/__module.encoder.layer.2.output/__module.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %389 : __torch__.transformers.modeling_roberta.___torch_mangle_33604.RobertaOutput = prim::GetAttr[name="output"](%118)
  %390 : __torch__.transformers.modeling_roberta.___torch_mangle_33600.RobertaIntermediate = prim::GetAttr[name="intermediate"](%118)
  %391 : __torch__.transformers.modeling_roberta.___torch_mangle_33598.RobertaAttention = prim::GetAttr[name="attention"](%118)
  %392 : __torch__.transformers.modeling_roberta.___torch_mangle_33597.RobertaSelfOutput = prim::GetAttr[name="output"](%391)
  %393 : __torch__.transformers.modeling_roberta.___torch_mangle_33593.RobertaSelfAttention = prim::GetAttr[name="self"](%391)
  %394 : __torch__.torch.nn.modules.linear.___torch_mangle_33591.Linear = prim::GetAttr[name="value"](%393)
  %395 : __torch__.torch.nn.modules.linear.___torch_mangle_33590.Linear = prim::GetAttr[name="key"](%393)
  %396 : __torch__.torch.nn.modules.linear.___torch_mangle_33589.Linear = prim::GetAttr[name="query"](%393)
  %397 : Tensor = prim::GetAttr[name="bias"](%396)
  %398 : Tensor = prim::GetAttr[name="weight"](%396)
  %399 : Float(768:1, 768:768) = aten::t(%398), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %399), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.19, %397, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %402 : Tensor = prim::GetAttr[name="bias"](%395)
  %403 : Tensor = prim::GetAttr[name="weight"](%395)
  %404 : Float(768:1, 768:768) = aten::t(%403), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %404), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %402, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %407 : Tensor = prim::GetAttr[name="bias"](%394)
  %408 : Tensor = prim::GetAttr[name="weight"](%394)
  %409 : Float(768:1, 768:768) = aten::t(%408), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %409), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %407, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %412 : int = aten::size(%x.19, %89), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %413 : int = aten::size(%x.19, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %414 : int[] = prim::ListConstruct(%412, %413, %90, %91), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %414), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %416 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %416), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %418 : int = aten::size(%x.21, %89), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %419 : int = aten::size(%x.21, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %420 : int[] = prim::ListConstruct(%418, %419, %90, %91), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %420), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %422 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %422), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %424 : int = aten::size(%x.23, %89), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %425 : int = aten::size(%x.23, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %426 : int[] = prim::ListConstruct(%424, %425, %90, %91), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %x.24 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %426), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %428 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %428), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %430 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %94, %95), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %430), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %96), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:195:0
  %input.36 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:198:0
  %input.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %94, %97), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %99, %98), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self/__module.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:211:0
  %437 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %438 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %437), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%438, %89), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %440 : int = aten::size(%context_layer.8, %89), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %441 : int = aten::size(%context_layer.8, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %442 : int[] = prim::ListConstruct(%440, %441, %100), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self
  %input.38 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.8, %442), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.self # transformers/modeling_roberta.py:215:0
  %444 : __torch__.torch.nn.modules.normalization.___torch_mangle_33595.LayerNorm = prim::GetAttr[name="LayerNorm"](%392)
  %445 : __torch__.torch.nn.modules.linear.___torch_mangle_33594.Linear = prim::GetAttr[name="dense"](%392)
  %446 : Tensor = prim::GetAttr[name="bias"](%445)
  %447 : Tensor = prim::GetAttr[name="weight"](%445)
  %448 : Float(768:1, 768:768) = aten::t(%447), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.38, %448), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %446, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.39, %99, %98), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output # transformers/modeling_roberta.py:232:0
  %453 : Tensor = prim::GetAttr[name="bias"](%444)
  %454 : Tensor = prim::GetAttr[name="weight"](%444)
  %455 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %455, %454, %453, %87, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.attention/__module.encoder.layer.3.attention.output/__module.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %457 : __torch__.torch.nn.modules.linear.___torch_mangle_33599.Linear = prim::GetAttr[name="dense"](%390)
  %458 : Tensor = prim::GetAttr[name="bias"](%457)
  %459 : Tensor = prim::GetAttr[name="weight"](%457)
  %460 : Float(768:1, 3072:768) = aten::t(%459), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %460), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.23, %458, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate/__module.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %464 : __torch__.torch.nn.modules.normalization.___torch_mangle_33602.LayerNorm = prim::GetAttr[name="LayerNorm"](%389)
  %465 : __torch__.torch.nn.modules.linear.___torch_mangle_33601.Linear = prim::GetAttr[name="dense"](%389)
  %466 : Tensor = prim::GetAttr[name="bias"](%465)
  %467 : Tensor = prim::GetAttr[name="weight"](%465)
  %468 : Float(3072:1, 768:3072) = aten::t(%467), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.42, %468), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.24, %466, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.43, %99, %98), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %88), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output # transformers/modeling_roberta.py:311:0
  %473 : Tensor = prim::GetAttr[name="bias"](%464)
  %474 : Tensor = prim::GetAttr[name="weight"](%464)
  %475 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm
  %input.45 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %475, %474, %473, %87, %86), scope: __module.encoder/__module.encoder.layer.3/__module.encoder.layer.3.output/__module.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %477 : __torch__.transformers.modeling_roberta.___torch_mangle_33621.RobertaOutput = prim::GetAttr[name="output"](%116)
  %478 : __torch__.transformers.modeling_roberta.___torch_mangle_33617.RobertaIntermediate = prim::GetAttr[name="intermediate"](%116)
  %479 : __torch__.transformers.modeling_roberta.___torch_mangle_33615.RobertaAttention = prim::GetAttr[name="attention"](%116)
  %480 : __torch__.transformers.modeling_roberta.___torch_mangle_33614.RobertaSelfOutput = prim::GetAttr[name="output"](%479)
  %481 : __torch__.transformers.modeling_roberta.___torch_mangle_33610.RobertaSelfAttention = prim::GetAttr[name="self"](%479)
  %482 : __torch__.torch.nn.modules.linear.___torch_mangle_33608.Linear = prim::GetAttr[name="value"](%481)
  %483 : __torch__.torch.nn.modules.linear.___torch_mangle_33607.Linear = prim::GetAttr[name="key"](%481)
  %484 : __torch__.torch.nn.modules.linear.___torch_mangle_33606.Linear = prim::GetAttr[name="query"](%481)
  %485 : Tensor = prim::GetAttr[name="bias"](%484)
  %486 : Tensor = prim::GetAttr[name="weight"](%484)
  %487 : Float(768:1, 768:768) = aten::t(%486), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %487), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.25, %485, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %490 : Tensor = prim::GetAttr[name="bias"](%483)
  %491 : Tensor = prim::GetAttr[name="weight"](%483)
  %492 : Float(768:1, 768:768) = aten::t(%491), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %492), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.26, %490, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %495 : Tensor = prim::GetAttr[name="bias"](%482)
  %496 : Tensor = prim::GetAttr[name="weight"](%482)
  %497 : Float(768:1, 768:768) = aten::t(%496), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %497), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.27, %495, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %500 : int = aten::size(%x.25, %89), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %501 : int = aten::size(%x.25, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %502 : int[] = prim::ListConstruct(%500, %501, %90, %91), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %502), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %504 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %504), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %506 : int = aten::size(%x.27, %89), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %507 : int = aten::size(%x.27, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %508 : int[] = prim::ListConstruct(%506, %507, %90, %91), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.28 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %508), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %510 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %510), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %512 : int = aten::size(%x.29, %89), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %513 : int = aten::size(%x.29, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %514 : int[] = prim::ListConstruct(%512, %513, %90, %91), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %x.30 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %514), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %516 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %516), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %518 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %94, %95), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %518), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %96), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:195:0
  %input.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:198:0
  %input.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %94, %97), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %99, %98), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self/__module.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:211:0
  %525 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %526 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %525), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%526, %89), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %528 : int = aten::size(%context_layer.10, %89), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %529 : int = aten::size(%context_layer.10, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %530 : int[] = prim::ListConstruct(%528, %529, %100), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self
  %input.48 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.10, %530), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.self # transformers/modeling_roberta.py:215:0
  %532 : __torch__.torch.nn.modules.normalization.___torch_mangle_33612.LayerNorm = prim::GetAttr[name="LayerNorm"](%480)
  %533 : __torch__.torch.nn.modules.linear.___torch_mangle_33611.Linear = prim::GetAttr[name="dense"](%480)
  %534 : Tensor = prim::GetAttr[name="bias"](%533)
  %535 : Tensor = prim::GetAttr[name="weight"](%533)
  %536 : Float(768:1, 768:768) = aten::t(%535), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.48, %536), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.28, %534, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.49, %99, %98), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output # transformers/modeling_roberta.py:232:0
  %541 : Tensor = prim::GetAttr[name="bias"](%532)
  %542 : Tensor = prim::GetAttr[name="weight"](%532)
  %543 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %543, %542, %541, %87, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.attention/__module.encoder.layer.4.attention.output/__module.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %545 : __torch__.torch.nn.modules.linear.___torch_mangle_33616.Linear = prim::GetAttr[name="dense"](%478)
  %546 : Tensor = prim::GetAttr[name="bias"](%545)
  %547 : Tensor = prim::GetAttr[name="weight"](%545)
  %548 : Float(768:1, 3072:768) = aten::t(%547), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %548), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.29, %546, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate/__module.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %552 : __torch__.torch.nn.modules.normalization.___torch_mangle_33619.LayerNorm = prim::GetAttr[name="LayerNorm"](%477)
  %553 : __torch__.torch.nn.modules.linear.___torch_mangle_33618.Linear = prim::GetAttr[name="dense"](%477)
  %554 : Tensor = prim::GetAttr[name="bias"](%553)
  %555 : Tensor = prim::GetAttr[name="weight"](%553)
  %556 : Float(3072:1, 768:3072) = aten::t(%555), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.52, %556), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.30, %554, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.53, %99, %98), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %88), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output # transformers/modeling_roberta.py:311:0
  %561 : Tensor = prim::GetAttr[name="bias"](%552)
  %562 : Tensor = prim::GetAttr[name="weight"](%552)
  %563 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm
  %input.55 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %563, %562, %561, %87, %86), scope: __module.encoder/__module.encoder.layer.4/__module.encoder.layer.4.output/__module.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %565 : __torch__.transformers.modeling_roberta.___torch_mangle_33638.RobertaOutput = prim::GetAttr[name="output"](%114)
  %566 : __torch__.transformers.modeling_roberta.___torch_mangle_33634.RobertaIntermediate = prim::GetAttr[name="intermediate"](%114)
  %567 : __torch__.transformers.modeling_roberta.___torch_mangle_33632.RobertaAttention = prim::GetAttr[name="attention"](%114)
  %568 : __torch__.transformers.modeling_roberta.___torch_mangle_33631.RobertaSelfOutput = prim::GetAttr[name="output"](%567)
  %569 : __torch__.transformers.modeling_roberta.___torch_mangle_33627.RobertaSelfAttention = prim::GetAttr[name="self"](%567)
  %570 : __torch__.torch.nn.modules.linear.___torch_mangle_33625.Linear = prim::GetAttr[name="value"](%569)
  %571 : __torch__.torch.nn.modules.linear.___torch_mangle_33624.Linear = prim::GetAttr[name="key"](%569)
  %572 : __torch__.torch.nn.modules.linear.___torch_mangle_33623.Linear = prim::GetAttr[name="query"](%569)
  %573 : Tensor = prim::GetAttr[name="bias"](%572)
  %574 : Tensor = prim::GetAttr[name="weight"](%572)
  %575 : Float(768:1, 768:768) = aten::t(%574), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %575), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.31, %573, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %578 : Tensor = prim::GetAttr[name="bias"](%571)
  %579 : Tensor = prim::GetAttr[name="weight"](%571)
  %580 : Float(768:1, 768:768) = aten::t(%579), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %580), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.32, %578, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %583 : Tensor = prim::GetAttr[name="bias"](%570)
  %584 : Tensor = prim::GetAttr[name="weight"](%570)
  %585 : Float(768:1, 768:768) = aten::t(%584), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %585), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.33, %583, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %588 : int = aten::size(%x.31, %89), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %589 : int = aten::size(%x.31, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %590 : int[] = prim::ListConstruct(%588, %589, %90, %91), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.32 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %590), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %592 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %592), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %594 : int = aten::size(%x.33, %89), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %595 : int = aten::size(%x.33, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %596 : int[] = prim::ListConstruct(%594, %595, %90, %91), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.34 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %596), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %598 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %598), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %600 : int = aten::size(%x.35, %89), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %601 : int = aten::size(%x.35, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %602 : int[] = prim::ListConstruct(%600, %601, %90, %91), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %x.36 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %602), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %604 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %604), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %606 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %94, %95), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %606), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %96), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:195:0
  %input.56 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:198:0
  %input.57 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %94, %97), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %99, %98), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self/__module.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:211:0
  %613 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %614 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %613), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%614, %89), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %616 : int = aten::size(%context_layer.12, %89), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %617 : int = aten::size(%context_layer.12, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %618 : int[] = prim::ListConstruct(%616, %617, %100), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self
  %input.58 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.12, %618), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.self # transformers/modeling_roberta.py:215:0
  %620 : __torch__.torch.nn.modules.normalization.___torch_mangle_33629.LayerNorm = prim::GetAttr[name="LayerNorm"](%568)
  %621 : __torch__.torch.nn.modules.linear.___torch_mangle_33628.Linear = prim::GetAttr[name="dense"](%568)
  %622 : Tensor = prim::GetAttr[name="bias"](%621)
  %623 : Tensor = prim::GetAttr[name="weight"](%621)
  %624 : Float(768:1, 768:768) = aten::t(%623), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.58, %624), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.34, %622, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.59, %99, %98), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output # transformers/modeling_roberta.py:232:0
  %629 : Tensor = prim::GetAttr[name="bias"](%620)
  %630 : Tensor = prim::GetAttr[name="weight"](%620)
  %631 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %631, %630, %629, %87, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.attention/__module.encoder.layer.5.attention.output/__module.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %633 : __torch__.torch.nn.modules.linear.___torch_mangle_33633.Linear = prim::GetAttr[name="dense"](%566)
  %634 : Tensor = prim::GetAttr[name="bias"](%633)
  %635 : Tensor = prim::GetAttr[name="weight"](%633)
  %636 : Float(768:1, 3072:768) = aten::t(%635), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %636), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.35, %634, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate/__module.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %640 : __torch__.torch.nn.modules.normalization.___torch_mangle_33636.LayerNorm = prim::GetAttr[name="LayerNorm"](%565)
  %641 : __torch__.torch.nn.modules.linear.___torch_mangle_33635.Linear = prim::GetAttr[name="dense"](%565)
  %642 : Tensor = prim::GetAttr[name="bias"](%641)
  %643 : Tensor = prim::GetAttr[name="weight"](%641)
  %644 : Float(3072:1, 768:3072) = aten::t(%643), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.62, %644), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.36, %642, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.63, %99, %98), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %88), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output # transformers/modeling_roberta.py:311:0
  %649 : Tensor = prim::GetAttr[name="bias"](%640)
  %650 : Tensor = prim::GetAttr[name="weight"](%640)
  %651 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm
  %input.65 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %651, %650, %649, %87, %86), scope: __module.encoder/__module.encoder.layer.5/__module.encoder.layer.5.output/__module.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %653 : __torch__.transformers.modeling_roberta.___torch_mangle_33655.RobertaOutput = prim::GetAttr[name="output"](%112)
  %654 : __torch__.transformers.modeling_roberta.___torch_mangle_33651.RobertaIntermediate = prim::GetAttr[name="intermediate"](%112)
  %655 : __torch__.transformers.modeling_roberta.___torch_mangle_33649.RobertaAttention = prim::GetAttr[name="attention"](%112)
  %656 : __torch__.transformers.modeling_roberta.___torch_mangle_33648.RobertaSelfOutput = prim::GetAttr[name="output"](%655)
  %657 : __torch__.transformers.modeling_roberta.___torch_mangle_33644.RobertaSelfAttention = prim::GetAttr[name="self"](%655)
  %658 : __torch__.torch.nn.modules.linear.___torch_mangle_33642.Linear = prim::GetAttr[name="value"](%657)
  %659 : __torch__.torch.nn.modules.linear.___torch_mangle_33641.Linear = prim::GetAttr[name="key"](%657)
  %660 : __torch__.torch.nn.modules.linear.___torch_mangle_33640.Linear = prim::GetAttr[name="query"](%657)
  %661 : Tensor = prim::GetAttr[name="bias"](%660)
  %662 : Tensor = prim::GetAttr[name="weight"](%660)
  %663 : Float(768:1, 768:768) = aten::t(%662), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %663), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.37, %661, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %666 : Tensor = prim::GetAttr[name="bias"](%659)
  %667 : Tensor = prim::GetAttr[name="weight"](%659)
  %668 : Float(768:1, 768:768) = aten::t(%667), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %668), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.38, %666, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %671 : Tensor = prim::GetAttr[name="bias"](%658)
  %672 : Tensor = prim::GetAttr[name="weight"](%658)
  %673 : Float(768:1, 768:768) = aten::t(%672), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %673), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.39, %671, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %676 : int = aten::size(%x.37, %89), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %677 : int = aten::size(%x.37, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %678 : int[] = prim::ListConstruct(%676, %677, %90, %91), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.38 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %678), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %680 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %680), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %682 : int = aten::size(%x.39, %89), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %683 : int = aten::size(%x.39, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %684 : int[] = prim::ListConstruct(%682, %683, %90, %91), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.40 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %684), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %686 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %686), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %688 : int = aten::size(%x.41, %89), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %689 : int = aten::size(%x.41, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %690 : int[] = prim::ListConstruct(%688, %689, %90, %91), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %x.42 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %690), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %692 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %692), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %694 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %94, %95), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %694), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.14 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %96), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:195:0
  %input.66 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:198:0
  %input.67 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %94, %97), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %99, %98), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self/__module.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:211:0
  %701 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %702 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %701), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%702, %89), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %704 : int = aten::size(%context_layer.14, %89), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %705 : int = aten::size(%context_layer.14, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %706 : int[] = prim::ListConstruct(%704, %705, %100), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self
  %input.68 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.14, %706), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.self # transformers/modeling_roberta.py:215:0
  %708 : __torch__.torch.nn.modules.normalization.___torch_mangle_33646.LayerNorm = prim::GetAttr[name="LayerNorm"](%656)
  %709 : __torch__.torch.nn.modules.linear.___torch_mangle_33645.Linear = prim::GetAttr[name="dense"](%656)
  %710 : Tensor = prim::GetAttr[name="bias"](%709)
  %711 : Tensor = prim::GetAttr[name="weight"](%709)
  %712 : Float(768:1, 768:768) = aten::t(%711), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.68, %712), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.40, %710, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.69, %99, %98), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output # transformers/modeling_roberta.py:232:0
  %717 : Tensor = prim::GetAttr[name="bias"](%708)
  %718 : Tensor = prim::GetAttr[name="weight"](%708)
  %719 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %719, %718, %717, %87, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.attention/__module.encoder.layer.6.attention.output/__module.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %721 : __torch__.torch.nn.modules.linear.___torch_mangle_33650.Linear = prim::GetAttr[name="dense"](%654)
  %722 : Tensor = prim::GetAttr[name="bias"](%721)
  %723 : Tensor = prim::GetAttr[name="weight"](%721)
  %724 : Float(768:1, 3072:768) = aten::t(%723), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %724), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.41, %722, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate/__module.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %728 : __torch__.torch.nn.modules.normalization.___torch_mangle_33653.LayerNorm = prim::GetAttr[name="LayerNorm"](%653)
  %729 : __torch__.torch.nn.modules.linear.___torch_mangle_33652.Linear = prim::GetAttr[name="dense"](%653)
  %730 : Tensor = prim::GetAttr[name="bias"](%729)
  %731 : Tensor = prim::GetAttr[name="weight"](%729)
  %732 : Float(3072:1, 768:3072) = aten::t(%731), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.72, %732), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.42, %730, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.73, %99, %98), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %88), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output # transformers/modeling_roberta.py:311:0
  %737 : Tensor = prim::GetAttr[name="bias"](%728)
  %738 : Tensor = prim::GetAttr[name="weight"](%728)
  %739 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm
  %input.75 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %739, %738, %737, %87, %86), scope: __module.encoder/__module.encoder.layer.6/__module.encoder.layer.6.output/__module.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %741 : __torch__.transformers.modeling_roberta.___torch_mangle_33672.RobertaOutput = prim::GetAttr[name="output"](%110)
  %742 : __torch__.transformers.modeling_roberta.___torch_mangle_33668.RobertaIntermediate = prim::GetAttr[name="intermediate"](%110)
  %743 : __torch__.transformers.modeling_roberta.___torch_mangle_33666.RobertaAttention = prim::GetAttr[name="attention"](%110)
  %744 : __torch__.transformers.modeling_roberta.___torch_mangle_33665.RobertaSelfOutput = prim::GetAttr[name="output"](%743)
  %745 : __torch__.transformers.modeling_roberta.___torch_mangle_33661.RobertaSelfAttention = prim::GetAttr[name="self"](%743)
  %746 : __torch__.torch.nn.modules.linear.___torch_mangle_33659.Linear = prim::GetAttr[name="value"](%745)
  %747 : __torch__.torch.nn.modules.linear.___torch_mangle_33658.Linear = prim::GetAttr[name="key"](%745)
  %748 : __torch__.torch.nn.modules.linear.___torch_mangle_33657.Linear = prim::GetAttr[name="query"](%745)
  %749 : Tensor = prim::GetAttr[name="bias"](%748)
  %750 : Tensor = prim::GetAttr[name="weight"](%748)
  %751 : Float(768:1, 768:768) = aten::t(%750), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %751), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.43, %749, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %754 : Tensor = prim::GetAttr[name="bias"](%747)
  %755 : Tensor = prim::GetAttr[name="weight"](%747)
  %756 : Float(768:1, 768:768) = aten::t(%755), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %756), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.44, %754, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %759 : Tensor = prim::GetAttr[name="bias"](%746)
  %760 : Tensor = prim::GetAttr[name="weight"](%746)
  %761 : Float(768:1, 768:768) = aten::t(%760), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %761), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.45, %759, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %764 : int = aten::size(%x.43, %89), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %765 : int = aten::size(%x.43, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %766 : int[] = prim::ListConstruct(%764, %765, %90, %91), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.44 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %766), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %768 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %768), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %770 : int = aten::size(%x.45, %89), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %771 : int = aten::size(%x.45, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %772 : int[] = prim::ListConstruct(%770, %771, %90, %91), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.46 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %772), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %774 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %774), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %776 : int = aten::size(%x.47, %89), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %777 : int = aten::size(%x.47, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %778 : int[] = prim::ListConstruct(%776, %777, %90, %91), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %x.48 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %778), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %780 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %780), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %782 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %94, %95), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %782), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %96), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:195:0
  %input.76 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:198:0
  %input.77 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %94, %97), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %99, %98), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self/__module.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:211:0
  %789 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %790 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %789), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%790, %89), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %792 : int = aten::size(%context_layer.16, %89), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %793 : int = aten::size(%context_layer.16, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %794 : int[] = prim::ListConstruct(%792, %793, %100), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self
  %input.78 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.16, %794), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.self # transformers/modeling_roberta.py:215:0
  %796 : __torch__.torch.nn.modules.normalization.___torch_mangle_33663.LayerNorm = prim::GetAttr[name="LayerNorm"](%744)
  %797 : __torch__.torch.nn.modules.linear.___torch_mangle_33662.Linear = prim::GetAttr[name="dense"](%744)
  %798 : Tensor = prim::GetAttr[name="bias"](%797)
  %799 : Tensor = prim::GetAttr[name="weight"](%797)
  %800 : Float(768:1, 768:768) = aten::t(%799), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.78, %800), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.46, %798, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.79, %99, %98), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output # transformers/modeling_roberta.py:232:0
  %805 : Tensor = prim::GetAttr[name="bias"](%796)
  %806 : Tensor = prim::GetAttr[name="weight"](%796)
  %807 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %807, %806, %805, %87, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.attention/__module.encoder.layer.7.attention.output/__module.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %809 : __torch__.torch.nn.modules.linear.___torch_mangle_33667.Linear = prim::GetAttr[name="dense"](%742)
  %810 : Tensor = prim::GetAttr[name="bias"](%809)
  %811 : Tensor = prim::GetAttr[name="weight"](%809)
  %812 : Float(768:1, 3072:768) = aten::t(%811), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %812), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.47, %810, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate/__module.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %816 : __torch__.torch.nn.modules.normalization.___torch_mangle_33670.LayerNorm = prim::GetAttr[name="LayerNorm"](%741)
  %817 : __torch__.torch.nn.modules.linear.___torch_mangle_33669.Linear = prim::GetAttr[name="dense"](%741)
  %818 : Tensor = prim::GetAttr[name="bias"](%817)
  %819 : Tensor = prim::GetAttr[name="weight"](%817)
  %820 : Float(3072:1, 768:3072) = aten::t(%819), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.82, %820), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.48, %818, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.83, %99, %98), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %88), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output # transformers/modeling_roberta.py:311:0
  %825 : Tensor = prim::GetAttr[name="bias"](%816)
  %826 : Tensor = prim::GetAttr[name="weight"](%816)
  %827 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm
  %input.85 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %827, %826, %825, %87, %86), scope: __module.encoder/__module.encoder.layer.7/__module.encoder.layer.7.output/__module.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %829 : __torch__.transformers.modeling_roberta.___torch_mangle_33689.RobertaOutput = prim::GetAttr[name="output"](%108)
  %830 : __torch__.transformers.modeling_roberta.___torch_mangle_33685.RobertaIntermediate = prim::GetAttr[name="intermediate"](%108)
  %831 : __torch__.transformers.modeling_roberta.___torch_mangle_33683.RobertaAttention = prim::GetAttr[name="attention"](%108)
  %832 : __torch__.transformers.modeling_roberta.___torch_mangle_33682.RobertaSelfOutput = prim::GetAttr[name="output"](%831)
  %833 : __torch__.transformers.modeling_roberta.___torch_mangle_33678.RobertaSelfAttention = prim::GetAttr[name="self"](%831)
  %834 : __torch__.torch.nn.modules.linear.___torch_mangle_33676.Linear = prim::GetAttr[name="value"](%833)
  %835 : __torch__.torch.nn.modules.linear.___torch_mangle_33675.Linear = prim::GetAttr[name="key"](%833)
  %836 : __torch__.torch.nn.modules.linear.___torch_mangle_33674.Linear = prim::GetAttr[name="query"](%833)
  %837 : Tensor = prim::GetAttr[name="bias"](%836)
  %838 : Tensor = prim::GetAttr[name="weight"](%836)
  %839 : Float(768:1, 768:768) = aten::t(%838), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %839), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.49, %837, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %842 : Tensor = prim::GetAttr[name="bias"](%835)
  %843 : Tensor = prim::GetAttr[name="weight"](%835)
  %844 : Float(768:1, 768:768) = aten::t(%843), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %844), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.50, %842, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %847 : Tensor = prim::GetAttr[name="bias"](%834)
  %848 : Tensor = prim::GetAttr[name="weight"](%834)
  %849 : Float(768:1, 768:768) = aten::t(%848), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %849), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.51, %847, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %852 : int = aten::size(%x.49, %89), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %853 : int = aten::size(%x.49, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %854 : int[] = prim::ListConstruct(%852, %853, %90, %91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.50 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %854), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %856 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %856), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %858 : int = aten::size(%x.51, %89), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %859 : int = aten::size(%x.51, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %860 : int[] = prim::ListConstruct(%858, %859, %90, %91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.52 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %860), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %862 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %862), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %864 : int = aten::size(%x.53, %89), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %865 : int = aten::size(%x.53, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %866 : int[] = prim::ListConstruct(%864, %865, %90, %91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %x.54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %866), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %868 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %868), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %870 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %94, %95), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %870), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.18 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %96), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:195:0
  %input.86 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:198:0
  %input.87 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %94, %97), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %99, %98), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self/__module.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:211:0
  %877 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %878 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %877), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%878, %89), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %880 : int = aten::size(%context_layer.18, %89), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %881 : int = aten::size(%context_layer.18, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %882 : int[] = prim::ListConstruct(%880, %881, %100), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self
  %input.88 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.18, %882), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.self # transformers/modeling_roberta.py:215:0
  %884 : __torch__.torch.nn.modules.normalization.___torch_mangle_33680.LayerNorm = prim::GetAttr[name="LayerNorm"](%832)
  %885 : __torch__.torch.nn.modules.linear.___torch_mangle_33679.Linear = prim::GetAttr[name="dense"](%832)
  %886 : Tensor = prim::GetAttr[name="bias"](%885)
  %887 : Tensor = prim::GetAttr[name="weight"](%885)
  %888 : Float(768:1, 768:768) = aten::t(%887), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.88, %888), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.52, %886, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.89, %99, %98), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output # transformers/modeling_roberta.py:232:0
  %893 : Tensor = prim::GetAttr[name="bias"](%884)
  %894 : Tensor = prim::GetAttr[name="weight"](%884)
  %895 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %895, %894, %893, %87, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.attention/__module.encoder.layer.8.attention.output/__module.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %897 : __torch__.torch.nn.modules.linear.___torch_mangle_33684.Linear = prim::GetAttr[name="dense"](%830)
  %898 : Tensor = prim::GetAttr[name="bias"](%897)
  %899 : Tensor = prim::GetAttr[name="weight"](%897)
  %900 : Float(768:1, 3072:768) = aten::t(%899), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %900), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.53, %898, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate/__module.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %904 : __torch__.torch.nn.modules.normalization.___torch_mangle_33687.LayerNorm = prim::GetAttr[name="LayerNorm"](%829)
  %905 : __torch__.torch.nn.modules.linear.___torch_mangle_33686.Linear = prim::GetAttr[name="dense"](%829)
  %906 : Tensor = prim::GetAttr[name="bias"](%905)
  %907 : Tensor = prim::GetAttr[name="weight"](%905)
  %908 : Float(3072:1, 768:3072) = aten::t(%907), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.92, %908), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.54, %906, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.93, %99, %98), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %88), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output # transformers/modeling_roberta.py:311:0
  %913 : Tensor = prim::GetAttr[name="bias"](%904)
  %914 : Tensor = prim::GetAttr[name="weight"](%904)
  %915 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm
  %input.95 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %915, %914, %913, %87, %86), scope: __module.encoder/__module.encoder.layer.8/__module.encoder.layer.8.output/__module.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %917 : __torch__.transformers.modeling_roberta.___torch_mangle_33706.RobertaOutput = prim::GetAttr[name="output"](%106)
  %918 : __torch__.transformers.modeling_roberta.___torch_mangle_33702.RobertaIntermediate = prim::GetAttr[name="intermediate"](%106)
  %919 : __torch__.transformers.modeling_roberta.___torch_mangle_33700.RobertaAttention = prim::GetAttr[name="attention"](%106)
  %920 : __torch__.transformers.modeling_roberta.___torch_mangle_33699.RobertaSelfOutput = prim::GetAttr[name="output"](%919)
  %921 : __torch__.transformers.modeling_roberta.___torch_mangle_33695.RobertaSelfAttention = prim::GetAttr[name="self"](%919)
  %922 : __torch__.torch.nn.modules.linear.___torch_mangle_33693.Linear = prim::GetAttr[name="value"](%921)
  %923 : __torch__.torch.nn.modules.linear.___torch_mangle_33692.Linear = prim::GetAttr[name="key"](%921)
  %924 : __torch__.torch.nn.modules.linear.___torch_mangle_33691.Linear = prim::GetAttr[name="query"](%921)
  %925 : Tensor = prim::GetAttr[name="bias"](%924)
  %926 : Tensor = prim::GetAttr[name="weight"](%924)
  %927 : Float(768:1, 768:768) = aten::t(%926), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %927), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.55, %925, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %930 : Tensor = prim::GetAttr[name="bias"](%923)
  %931 : Tensor = prim::GetAttr[name="weight"](%923)
  %932 : Float(768:1, 768:768) = aten::t(%931), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %932), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.56, %930, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %935 : Tensor = prim::GetAttr[name="bias"](%922)
  %936 : Tensor = prim::GetAttr[name="weight"](%922)
  %937 : Float(768:1, 768:768) = aten::t(%936), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %937), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.57, %935, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %940 : int = aten::size(%x.55, %89), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %941 : int = aten::size(%x.55, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %942 : int[] = prim::ListConstruct(%940, %941, %90, %91), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.56 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %942), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %944 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %944), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %946 : int = aten::size(%x.57, %89), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %947 : int = aten::size(%x.57, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %948 : int[] = prim::ListConstruct(%946, %947, %90, %91), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.58 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %948), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %950 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %950), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %952 : int = aten::size(%x.59, %89), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %953 : int = aten::size(%x.59, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %954 : int[] = prim::ListConstruct(%952, %953, %90, %91), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %x.60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %954), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %956 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %956), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %958 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %94, %95), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.19 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %958), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.20 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %96), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:195:0
  %input.96 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:198:0
  %input.97 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %94, %97), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %99, %98), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self/__module.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:211:0
  %965 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %966 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %965), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%966, %89), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %968 : int = aten::size(%context_layer.20, %89), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %969 : int = aten::size(%context_layer.20, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %970 : int[] = prim::ListConstruct(%968, %969, %100), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self
  %input.98 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.20, %970), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.self # transformers/modeling_roberta.py:215:0
  %972 : __torch__.torch.nn.modules.normalization.___torch_mangle_33697.LayerNorm = prim::GetAttr[name="LayerNorm"](%920)
  %973 : __torch__.torch.nn.modules.linear.___torch_mangle_33696.Linear = prim::GetAttr[name="dense"](%920)
  %974 : Tensor = prim::GetAttr[name="bias"](%973)
  %975 : Tensor = prim::GetAttr[name="weight"](%973)
  %976 : Float(768:1, 768:768) = aten::t(%975), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.98, %976), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.58, %974, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.99, %99, %98), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output # transformers/modeling_roberta.py:232:0
  %981 : Tensor = prim::GetAttr[name="bias"](%972)
  %982 : Tensor = prim::GetAttr[name="weight"](%972)
  %983 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %983, %982, %981, %87, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.attention/__module.encoder.layer.9.attention.output/__module.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %985 : __torch__.torch.nn.modules.linear.___torch_mangle_33701.Linear = prim::GetAttr[name="dense"](%918)
  %986 : Tensor = prim::GetAttr[name="bias"](%985)
  %987 : Tensor = prim::GetAttr[name="weight"](%985)
  %988 : Float(768:1, 3072:768) = aten::t(%987), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %988), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.59, %986, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate/__module.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %992 : __torch__.torch.nn.modules.normalization.___torch_mangle_33704.LayerNorm = prim::GetAttr[name="LayerNorm"](%917)
  %993 : __torch__.torch.nn.modules.linear.___torch_mangle_33703.Linear = prim::GetAttr[name="dense"](%917)
  %994 : Tensor = prim::GetAttr[name="bias"](%993)
  %995 : Tensor = prim::GetAttr[name="weight"](%993)
  %996 : Float(3072:1, 768:3072) = aten::t(%995), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.102, %996), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.60, %994, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.103, %99, %98), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %88), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output # transformers/modeling_roberta.py:311:0
  %1001 : Tensor = prim::GetAttr[name="bias"](%992)
  %1002 : Tensor = prim::GetAttr[name="weight"](%992)
  %1003 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm
  %input.105 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %1003, %1002, %1001, %87, %86), scope: __module.encoder/__module.encoder.layer.9/__module.encoder.layer.9.output/__module.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %1005 : __torch__.transformers.modeling_roberta.___torch_mangle_33723.RobertaOutput = prim::GetAttr[name="output"](%104)
  %1006 : __torch__.transformers.modeling_roberta.___torch_mangle_33719.RobertaIntermediate = prim::GetAttr[name="intermediate"](%104)
  %1007 : __torch__.transformers.modeling_roberta.___torch_mangle_33717.RobertaAttention = prim::GetAttr[name="attention"](%104)
  %1008 : __torch__.transformers.modeling_roberta.___torch_mangle_33716.RobertaSelfOutput = prim::GetAttr[name="output"](%1007)
  %1009 : __torch__.transformers.modeling_roberta.___torch_mangle_33712.RobertaSelfAttention = prim::GetAttr[name="self"](%1007)
  %1010 : __torch__.torch.nn.modules.linear.___torch_mangle_33710.Linear = prim::GetAttr[name="value"](%1009)
  %1011 : __torch__.torch.nn.modules.linear.___torch_mangle_33709.Linear = prim::GetAttr[name="key"](%1009)
  %1012 : __torch__.torch.nn.modules.linear.___torch_mangle_33708.Linear = prim::GetAttr[name="query"](%1009)
  %1013 : Tensor = prim::GetAttr[name="bias"](%1012)
  %1014 : Tensor = prim::GetAttr[name="weight"](%1012)
  %1015 : Float(768:1, 768:768) = aten::t(%1014), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %1015), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.61, %1013, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %1018 : Tensor = prim::GetAttr[name="bias"](%1011)
  %1019 : Tensor = prim::GetAttr[name="weight"](%1011)
  %1020 : Float(768:1, 768:768) = aten::t(%1019), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %1020), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.62, %1018, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %1023 : Tensor = prim::GetAttr[name="bias"](%1010)
  %1024 : Tensor = prim::GetAttr[name="weight"](%1010)
  %1025 : Float(768:1, 768:768) = aten::t(%1024), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %1025), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %1023, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %1028 : int = aten::size(%x.61, %89), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1029 : int = aten::size(%x.61, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1030 : int[] = prim::ListConstruct(%1028, %1029, %90, %91), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.62 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %1030), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1032 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %1032), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1034 : int = aten::size(%x.63, %89), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1035 : int = aten::size(%x.63, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1036 : int[] = prim::ListConstruct(%1034, %1035, %90, %91), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.64 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %1036), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1038 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1038), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1040 : int = aten::size(%x.65, %89), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1041 : int = aten::size(%x.65, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1042 : int[] = prim::ListConstruct(%1040, %1041, %90, %91), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %x.66 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1042), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1044 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1044), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1046 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %94, %95), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.21 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1046), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %96), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:195:0
  %input.106 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:198:0
  %input.107 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %94, %97), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %99, %98), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self/__module.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:211:0
  %1053 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %1054 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1053), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1054, %89), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %1056 : int = aten::size(%context_layer.22, %89), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1057 : int = aten::size(%context_layer.22, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1058 : int[] = prim::ListConstruct(%1056, %1057, %100), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self
  %input.108 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1058), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.self # transformers/modeling_roberta.py:215:0
  %1060 : __torch__.torch.nn.modules.normalization.___torch_mangle_33714.LayerNorm = prim::GetAttr[name="LayerNorm"](%1008)
  %1061 : __torch__.torch.nn.modules.linear.___torch_mangle_33713.Linear = prim::GetAttr[name="dense"](%1008)
  %1062 : Tensor = prim::GetAttr[name="bias"](%1061)
  %1063 : Tensor = prim::GetAttr[name="weight"](%1061)
  %1064 : Float(768:1, 768:768) = aten::t(%1063), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.108, %1064), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %1062, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.109, %99, %98), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output # transformers/modeling_roberta.py:232:0
  %1069 : Tensor = prim::GetAttr[name="bias"](%1060)
  %1070 : Tensor = prim::GetAttr[name="weight"](%1060)
  %1071 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1071, %1070, %1069, %87, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.attention/__module.encoder.layer.10.attention.output/__module.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1073 : __torch__.torch.nn.modules.linear.___torch_mangle_33718.Linear = prim::GetAttr[name="dense"](%1006)
  %1074 : Tensor = prim::GetAttr[name="bias"](%1073)
  %1075 : Tensor = prim::GetAttr[name="weight"](%1073)
  %1076 : Float(768:1, 3072:768) = aten::t(%1075), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1076), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1074, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate/__module.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1080 : __torch__.torch.nn.modules.normalization.___torch_mangle_33721.LayerNorm = prim::GetAttr[name="LayerNorm"](%1005)
  %1081 : __torch__.torch.nn.modules.linear.___torch_mangle_33720.Linear = prim::GetAttr[name="dense"](%1005)
  %1082 : Tensor = prim::GetAttr[name="bias"](%1081)
  %1083 : Tensor = prim::GetAttr[name="weight"](%1081)
  %1084 : Float(3072:1, 768:3072) = aten::t(%1083), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.112, %1084), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.66, %1082, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.113, %99, %98), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %88), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output # transformers/modeling_roberta.py:311:0
  %1089 : Tensor = prim::GetAttr[name="bias"](%1080)
  %1090 : Tensor = prim::GetAttr[name="weight"](%1080)
  %1091 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm
  %input.115 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1091, %1090, %1089, %87, %86), scope: __module.encoder/__module.encoder.layer.10/__module.encoder.layer.10.output/__module.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1093 : __torch__.transformers.modeling_roberta.___torch_mangle_33740.RobertaOutput = prim::GetAttr[name="output"](%102)
  %1094 : __torch__.transformers.modeling_roberta.___torch_mangle_33736.RobertaIntermediate = prim::GetAttr[name="intermediate"](%102)
  %1095 : __torch__.transformers.modeling_roberta.___torch_mangle_33734.RobertaAttention = prim::GetAttr[name="attention"](%102)
  %1096 : __torch__.transformers.modeling_roberta.___torch_mangle_33733.RobertaSelfOutput = prim::GetAttr[name="output"](%1095)
  %1097 : __torch__.transformers.modeling_roberta.___torch_mangle_33729.RobertaSelfAttention = prim::GetAttr[name="self"](%1095)
  %1098 : __torch__.torch.nn.modules.linear.___torch_mangle_33727.Linear = prim::GetAttr[name="value"](%1097)
  %1099 : __torch__.torch.nn.modules.linear.___torch_mangle_33726.Linear = prim::GetAttr[name="key"](%1097)
  %1100 : __torch__.torch.nn.modules.linear.___torch_mangle_33725.Linear = prim::GetAttr[name="query"](%1097)
  %1101 : Tensor = prim::GetAttr[name="bias"](%1100)
  %1102 : Tensor = prim::GetAttr[name="weight"](%1100)
  %1103 : Float(768:1, 768:768) = aten::t(%1102), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1103), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %1101, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1106 : Tensor = prim::GetAttr[name="bias"](%1099)
  %1107 : Tensor = prim::GetAttr[name="weight"](%1099)
  %1108 : Float(768:1, 768:768) = aten::t(%1107), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1108), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %1106, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1111 : Tensor = prim::GetAttr[name="bias"](%1098)
  %1112 : Tensor = prim::GetAttr[name="weight"](%1098)
  %1113 : Float(768:1, 768:768) = aten::t(%1112), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1113), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %1111, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1116 : int = aten::size(%x.67, %89), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1117 : int = aten::size(%x.67, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1118 : int[] = prim::ListConstruct(%1116, %1117, %90, %91), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x.68 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1118), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1120 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %query_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1120), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1122 : int = aten::size(%x.69, %89), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1123 : int = aten::size(%x.69, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1124 : int[] = prim::ListConstruct(%1122, %1123, %90, %91), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x.70 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1124), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1126 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %key_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1126), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1128 : int = aten::size(%x.71, %89), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1129 : int = aten::size(%x.71, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1130 : int[] = prim::ListConstruct(%1128, %1129, %90, %91), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %x : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1130), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1132 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %value_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1132), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1134 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %94, %95), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1134), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %96), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:195:0
  %input.116 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:198:0
  %input.117 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %94, %97), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %99, %98), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self/__module.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:211:0
  %1141 : int[] = prim::ListConstruct(%89, %92, %88, %93), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %1142 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1141), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1142, %89), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %1144 : int = aten::size(%context_layer, %89), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1145 : int = aten::size(%context_layer, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1146 : int[] = prim::ListConstruct(%1144, %1145, %100), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self
  %input.118 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer, %1146), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.self # transformers/modeling_roberta.py:215:0
  %1148 : __torch__.torch.nn.modules.normalization.___torch_mangle_33731.LayerNorm = prim::GetAttr[name="LayerNorm"](%1096)
  %1149 : __torch__.torch.nn.modules.linear.___torch_mangle_33730.Linear = prim::GetAttr[name="dense"](%1096)
  %1150 : Tensor = prim::GetAttr[name="bias"](%1149)
  %1151 : Tensor = prim::GetAttr[name="weight"](%1149)
  %1152 : Float(768:1, 768:768) = aten::t(%1151), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.118, %1152), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %1150, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.119, %99, %98), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output # transformers/modeling_roberta.py:232:0
  %1157 : Tensor = prim::GetAttr[name="bias"](%1148)
  %1158 : Tensor = prim::GetAttr[name="weight"](%1148)
  %1159 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1159, %1158, %1157, %87, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.attention/__module.encoder.layer.11.attention.output/__module.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1161 : __torch__.torch.nn.modules.linear.___torch_mangle_33735.Linear = prim::GetAttr[name="dense"](%1094)
  %1162 : Tensor = prim::GetAttr[name="bias"](%1161)
  %1163 : Tensor = prim::GetAttr[name="weight"](%1161)
  %1164 : Float(768:1, 3072:768) = aten::t(%1163), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1164), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1162, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate/__module.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1168 : __torch__.torch.nn.modules.normalization.___torch_mangle_33738.LayerNorm = prim::GetAttr[name="LayerNorm"](%1093)
  %1169 : __torch__.torch.nn.modules.linear.___torch_mangle_33737.Linear = prim::GetAttr[name="dense"](%1093)
  %1170 : Tensor = prim::GetAttr[name="bias"](%1169)
  %1171 : Tensor = prim::GetAttr[name="weight"](%1169)
  %1172 : Float(3072:1, 768:3072) = aten::t(%1171), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.122, %1172), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add_(%output, %1170, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.24 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.123, %99, %98), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.24, %input_tensor, %88), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output # transformers/modeling_roberta.py:311:0
  %1177 : Tensor = prim::GetAttr[name="bias"](%1168)
  %1178 : Tensor = prim::GetAttr[name="weight"](%1168)
  %1179 : int[] = prim::ListConstruct(%100), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm
  %hidden_states : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1179, %1178, %1177, %87, %86), scope: __module.encoder/__module.encoder.layer.11/__module.encoder.layer.11.output/__module.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1181 : int = prim::Constant[value=1](), scope: __module.pooler # transformers/modeling_roberta.py:450:0
  %1182 : int = prim::Constant[value=9223372036854775807](), scope: __module.pooler # transformers/modeling_roberta.py:450:0
  %1183 : int = prim::Constant[value=0](), scope: __module.pooler # transformers/modeling_roberta.py:450:0
  %1184 : __torch__.torch.nn.modules.linear.___torch_mangle_33744.Linear = prim::GetAttr[name="dense"](%3)
  %1185 : Float(17:9984, 13:768, 768:1) = aten::slice(%hidden_states, %1183, %1183, %1182, %1181), scope: __module.pooler # transformers/modeling_roberta.py:450:0
  %input.125 : Float(17:9984, 768:1) = aten::select(%1185, %1181, %1183), scope: __module.pooler # transformers/modeling_roberta.py:450:0
  %1187 : Tensor = prim::GetAttr[name="bias"](%1184)
  %1188 : Tensor = prim::GetAttr[name="weight"](%1184)
  %1189 : Float(768:1, 768:768) = aten::t(%1188), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
  %input : Float(17:768, 768:1) = aten::addmm(%1187, %input.125, %1189, %1181, %1181), scope: __module.pooler/__module.pooler.dense # torch/nn/functional.py:1674:0
  %1191 : Float(17:768, 768:1) = aten::tanh(%input), scope: __module.pooler/__module.pooler.activation # torch/nn/modules/activation.py:350:0
  %47 : (Float(17:9984, 13:768, 768:1), Float(17:768, 768:1)) = prim::TupleConstruct(%hidden_states, %1191)
  return (%47)
